
<!DOCTYPE HTML>
<html lang="zh-hans" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>torch.nn · Pytorch 中文文档</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        <meta name="author" content="ApacheCN">
        
        
    
    <link rel="stylesheet" href="gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-insert-logo/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-back-to-top-button/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-code/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-pageview-count/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-emphasize/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-alerts/style.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-auto-scroll-table/website.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-page-toc-button/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-tbfed-pagefooter/footer.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-advanced-emoji/emoji-website.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-expandable-chapters/expandable-chapters.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-splitter/splitter.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-search-pro/search.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="nn.functional.html" />
    
    
    <link rel="prev" href="storage.html" />
    

    <style>
    @media only screen and (max-width: 640px) {
        .book-header .hidden-mobile {
            display: none;
        }
    }
    </style>
    <script>
        window["gitbook-plugin-github-buttons"] = {"buttons":[{"user":"apachecn","repo":"pytorch-doc-zh","type":"star","count":true,"size":"small"}]};
    </script>

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="输入并搜索" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="./">
            
                <a href="./">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" >
            
                <span>
            
                    
                    中文教程
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.1" >
            
                <span>
            
                    
                    入门
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.1.1" data-path="beginner/deep_learning_60min_blitz.html">
            
                <a href="beginner/deep_learning_60min_blitz.html">
            
                    
                    PyTorch 深度学习: 60 分钟极速入门
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.2" data-path="beginner/data_loading_tutorial.html">
            
                <a href="beginner/data_loading_tutorial.html">
            
                    
                    数据加载和处理教程
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.3" data-path="beginner/pytorch_with_examples.html">
            
                <a href="beginner/pytorch_with_examples.html">
            
                    
                    用例子学习 PyTorch
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.4" data-path="beginner/transfer_learning_tutorial.html">
            
                <a href="beginner/transfer_learning_tutorial.html">
            
                    
                    迁移学习教程
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.5" data-path="beginner/deploy_seq2seq_hybrid_frontend_tutorial.html">
            
                <a href="beginner/deploy_seq2seq_hybrid_frontend_tutorial.html">
            
                    
                    部署与TorchScript一个Seq2Seq模型
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.6" data-path="intermediate/tensorboard_tutorial.html">
            
                <a href="intermediate/tensorboard_tutorial.html">
            
                    
                    可视化模型，数据，和与训练TensorBoard
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.7" data-path="beginner/saving_loading_models.html">
            
                <a href="beginner/saving_loading_models.html">
            
                    
                    保存和加载模型
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.8" data-path="beginner/nn_tutorial.html">
            
                <a href="beginner/nn_tutorial.html">
            
                    
                    torch.nn 到底是什么？
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.2" >
            
                <span>
            
                    
                    图片
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.2.1" data-path="intermediate/torchvision_tutorial.html">
            
                <a href="intermediate/torchvision_tutorial.html">
            
                    
                    TorchVision对象检测教程细化和微调
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2.2" data-path="beginner/finetuning_torchvision_models_tutorial.html">
            
                <a href="beginner/finetuning_torchvision_models_tutorial.html">
            
                    
                    微调Torchvision模型
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2.3" data-path="intermediate/spatial_transformer_tutorial.html">
            
                <a href="intermediate/spatial_transformer_tutorial.html">
            
                    
                    空间变压器网络教程
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2.4" data-path="advanced/neural_style_tutorial.html">
            
                <a href="advanced/neural_style_tutorial.html">
            
                    
                    使用PyTorch进行神经网络传递
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2.5" data-path="beginner/fgsm_tutorial.html">
            
                <a href="beginner/fgsm_tutorial.html">
            
                    
                    对抗性示例生成
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2.6" data-path="beginner/dcgan_faces_tutorial.html">
            
                <a href="beginner/dcgan_faces_tutorial.html">
            
                    
                    DCGAN教程
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.3" >
            
                <span>
            
                    
                    音频
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.3.1" data-path="beginner/audio_preprocessing_tutorial.html">
            
                <a href="beginner/audio_preprocessing_tutorial.html">
            
                    
                    torchaudio教程
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.4" >
            
                <span>
            
                    
                    文本
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.4.1" data-path="intermediate/char_rnn_classification_tutorial.html">
            
                <a href="intermediate/char_rnn_classification_tutorial.html">
            
                    
                    NLP从头：判断名称与字符级RNN
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.4.2" data-path="intermediate/char_rnn_generation_tutorial.html">
            
                <a href="intermediate/char_rnn_generation_tutorial.html">
            
                    
                    NLP从头：生成名称与字符级RNN
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.4.3" data-path="intermediate/seq2seq_translation_tutorial.html">
            
                <a href="intermediate/seq2seq_translation_tutorial.html">
            
                    
                    NLP从无到有：用序列到序列网络和翻译注意
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.4.4" data-path="beginner/text_sentiment_ngrams_tutorial.html">
            
                <a href="beginner/text_sentiment_ngrams_tutorial.html">
            
                    
                    文本分类与TorchText 
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.4.5" data-path="beginner/torchtext_translation_tutorial.html">
            
                <a href="beginner/torchtext_translation_tutorial.html">
            
                    
                    语言翻译与TorchText 
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.4.6" data-path="beginner/transformer_tutorial.html">
            
                <a href="beginner/transformer_tutorial.html">
            
                    
                    序列到序列与nn.Transformer和TorchText建模
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.5" >
            
                <span>
            
                    
                    在生产部署PyTorch模型
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.5.1" data-path="intermediate/flask_rest_api_tutorial.html">
            
                <a href="intermediate/flask_rest_api_tutorial.html">
            
                    
                    1.部署PyTorch在Python经由REST API从Flask
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5.2" data-path="beginner/Intro_to_TorchScript_tutorial.html">
            
                <a href="beginner/Intro_to_TorchScript_tutorial.html">
            
                    
                    2.介绍TorchScript
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5.3" data-path="advanced/cpp_export.html">
            
                <a href="advanced/cpp_export.html">
            
                    
                    3.装载++一个TorchScript模型在C 
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5.4" data-path="advanced/super_resolution_with_onnxruntime.html">
            
                <a href="advanced/super_resolution_with_onnxruntime.html">
            
                    
                    4.（可选）从导出到PyTorch一个ONNX模型并使用ONNX运行时运行它
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.6" >
            
                <span>
            
                    
                    并行和分布式训练
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.6.1" data-path="intermediate/model_parallel_tutorial.html">
            
                <a href="intermediate/model_parallel_tutorial.html">
            
                    
                    1.型号并行最佳实践
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6.2" data-path="intermediate/ddp_tutorial.html">
            
                <a href="intermediate/ddp_tutorial.html">
            
                    
                    2.入门分布式数据并行
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6.3" data-path="intermediate/dist_tuto.html">
            
                <a href="intermediate/dist_tuto.html">
            
                    
                    3. PyTorch编写分布式应用
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6.4" data-path="beginner/aws_distributed_training_tutorial.html">
            
                <a href="beginner/aws_distributed_training_tutorial.html">
            
                    
                    4.（高级）PyTorch 1.0分布式训练与Amazon AWS
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.7" >
            
                <span>
            
                    
                    扩展PyTorch
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.7.1" data-path="advanced/torch_script_custom_ops.html">
            
                <a href="advanced/torch_script_custom_ops.html">
            
                    
                    使用自定义 C++ 扩展算TorchScript 
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.2" data-path="advanced/numpy_extensions_tutorial.html">
            
                <a href="advanced/numpy_extensions_tutorial.html">
            
                    
                    创建扩展使用numpy的和SciPy的
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.3" data-path="advanced/cpp_extension.html">
            
                <a href="advanced/cpp_extension.html">
            
                    
                    自定义 C++ 和CUDA扩展
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.8" >
            
                <span>
            
                    
                    PyTorch在其他语言
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.8.1" data-path="advanced/cpp_frontend.html">
            
                <a href="advanced/cpp_frontend.html">
            
                    
                    使用PyTorch C++ 前端
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3" >
            
                <span>
            
                    
                    中文文档
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.1" >
            
                <span>
            
                    
                    注解
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.1.1" data-path="notes/autograd.html">
            
                <a href="notes/autograd.html">
            
                    
                    自动求导机制
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.2" data-path="notes/broadcasting.html">
            
                <a href="notes/broadcasting.html">
            
                    
                    广播语义
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.3" data-path="notes/cpu_threading_torchscript_inference.html">
            
                <a href="notes/cpu_threading_torchscript_inference.html">
            
                    
                    CPU线程和TorchScript推理
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.4" data-path="notes/cuda.html">
            
                <a href="notes/cuda.html">
            
                    
                    CUDA语义
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.5" data-path="notes/extending.html">
            
                <a href="notes/extending.html">
            
                    
                    扩展PyTorch
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.6" data-path="notes/faq.html">
            
                <a href="notes/faq.html">
            
                    
                    常见问题
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.7" data-path="notes/large_scale_deployments.html">
            
                <a href="notes/large_scale_deployments.html">
            
                    
                    对于大规模部署的特点
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.8" data-path="notes/multiprocessing.html">
            
                <a href="notes/multiprocessing.html">
            
                    
                    多处理最佳实践
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.9" data-path="notes/randomness.html">
            
                <a href="notes/randomness.html">
            
                    
                    重复性
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.10" data-path="notes/serialization.html">
            
                <a href="notes/serialization.html">
            
                    
                    序列化语义
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.11" data-path="notes/windows.html">
            
                <a href="notes/windows.html">
            
                    
                    Windows 常见问题
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3.2" >
            
                <span>
            
                    
                    社区
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.2.1" data-path="community/contribution_guide.html">
            
                <a href="community/contribution_guide.html">
            
                    
                    PyTorch贡献说明书
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.2" data-path="community/governance.html">
            
                <a href="community/governance.html">
            
                    
                    PyTorch治理
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.3" data-path="community/persons_of_interest.html">
            
                <a href="community/persons_of_interest.html">
            
                    
                    PyTorch治|兴趣的人
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3.3" >
            
                <span>
            
                    
                    封装参考文献
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.3.1" data-path="torch.html">
            
                <a href="torch.html">
            
                    
                    torch
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.2" data-path="tensors.html">
            
                <a href="tensors.html">
            
                    
                    torch.Tensor
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.3" data-path="tensor_attributes.html">
            
                <a href="tensor_attributes.html">
            
                    
                    Tensor Attributes
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.4" data-path="type_info.html">
            
                <a href="type_info.html">
            
                    
                    Type Info
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.5" data-path="sparse.html">
            
                <a href="sparse.html">
            
                    
                    torch.sparse
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.6" data-path="cuda.html">
            
                <a href="cuda.html">
            
                    
                    torch.cuda
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.7" data-path="storage.html">
            
                <a href="storage.html">
            
                    
                    torch.Storage
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="1.3.3.8" data-path="nn.html">
            
                <a href="nn.html">
            
                    
                    torch.nn
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.9" data-path="nn.functional.html">
            
                <a href="nn.functional.html">
            
                    
                    torch.nn.functional
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.10" data-path="nn.init.html">
            
                <a href="nn.init.html">
            
                    
                    torch.nn.init
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.11" data-path="optim.html">
            
                <a href="optim.html">
            
                    
                    torch.optim
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.12" data-path="autograd.html">
            
                <a href="autograd.html">
            
                    
                    torch.autograd
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.13" data-path="distributed.html">
            
                <a href="distributed.html">
            
                    
                    torch.distributed
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.14" data-path="distributions.html">
            
                <a href="distributions.html">
            
                    
                    torch.distributions
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.15" data-path="hub.html">
            
                <a href="hub.html">
            
                    
                    torch.hub
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.16" data-path="jit.html">
            
                <a href="jit.html">
            
                    
                    torch.jit
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.17" data-path="multiprocessing.html">
            
                <a href="multiprocessing.html">
            
                    
                    torch.multiprocessing
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.18" data-path="random.html">
            
                <a href="random.html">
            
                    
                    torch.random
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.19" data-path="bottleneck.html">
            
                <a href="bottleneck.html">
            
                    
                    torch.utils.bottleneck
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.20" data-path="checkpoint.html">
            
                <a href="checkpoint.html">
            
                    
                    torch.utils.checkpoint
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.21" data-path="cpp_extension.html">
            
                <a href="cpp_extension.html">
            
                    
                    torch.utils.cpp_extension
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.22" data-path="data.html">
            
                <a href="data.html">
            
                    
                    torch.utils.data
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.23" data-path="dlpack.html">
            
                <a href="dlpack.html">
            
                    
                    torch.utils.dlpack
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.24" data-path="model_zoo.html">
            
                <a href="model_zoo.html">
            
                    
                    torch.utils.model_zoo
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.25" data-path="tensorboard.html">
            
                <a href="tensorboard.html">
            
                    
                    torch.utils.tensorboard
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.26" data-path="onnx.html">
            
                <a href="onnx.html">
            
                    
                    torch.onnx
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.27" data-path="__config__.md">
            
                <span>
            
                    
                    torch. config
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3.4" >
            
                <span>
            
                    
                    torchvision 参考文献
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.4.1" data-path="torchvision/">
            
                <a href="torchvision/">
            
                    
                    torchvision
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3.5" >
            
                <span>
            
                    
                    torchaudio Reference
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.5.1" >
            
                <a target="_blank" href="https://pytorch.org/audio">
            
                    
                    torchaudio
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3.6" >
            
                <span>
            
                    
                    torchtext Reference
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.6.1" >
            
                <a target="_blank" href="https://pytorch.org/text">
            
                    
                    torchtext
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

            </ul>
            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            本书使用 GitBook 发布
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href="." >torch.nn</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h1 id="torchnn">torch.nn</h1>
<h2 id="&#x53C2;&#x6570;">&#x53C2;&#x6570;</h2>
<p><em>class</em><code>torch.nn.``Parameter</code><a href="_modules/torch/nn/parameter.html#Parameter">[source]</a></p>
<p>&#x6709;&#x79CD;&#x5F20;&#x91CF;&#x5C06;&#x88AB;&#x8BA4;&#x4E3A;&#x662F;&#x6A21;&#x5757;&#x53C2;&#x6570;&#x3002;</p>
<p>&#x53C2;&#x6570;&#x662F;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <code>&#x5F20;&#x91CF;</code></a>&#x4E9A;&#x7C7B;&#xFF0C;&#x5177;&#x6709; <code>&#x6A21;&#x5757;</code>
[HTG11&#x4F7F;&#x7528;&#x65F6;&#xFF0C;&#x662F;&#x5177;&#x6709;&#x4E00;&#x4E2A;&#x975E;&#x5E38;&#x7279;&#x6B8A;&#x7684;&#x5C5E;&#x6027;] S - &#x5F53;&#x4ED6;&#x4EEC;&#x6307;&#x5B9A;&#x4E3A;&#x6A21;&#x5757;&#x5C5E;&#x6027;&#xFF0C;&#x5B83;&#x4EEC;&#x4F1A;&#x81EA;&#x52A8;&#x6DFB;&#x52A0;&#x5230;&#x5176;&#x53C2;&#x6570;&#x5217;&#x8868;&#xFF0C;&#x5E76;&#x4F1A;&#x51FA;&#x73B0;&#x5982;&#x5728; <code>&#x53C2;&#x6570;&#xFF08;&#xFF09;</code>
&#x8FED;&#x4EE3;&#x5668;&#x3002;&#x6307;&#x5B9A;&#x4E00;&#x4E2A;&#x5F20;&#x91CF;&#x5E76;&#x6CA1;&#x6709;&#x8FD9;&#x6837;&#x7684;&#x6548;&#x679C;&#x3002;&#x8FD9;&#x662F;&#x56E0;&#x4E3A;&#x4EBA;&#x4EEC;&#x53EF;&#x80FD;&#x4F1A;&#x60F3;&#x7F13;&#x5B58;&#x4E00;&#x4E9B;&#x4E34;&#x65F6;&#x7684;&#x72B6;&#x6001;&#xFF0C;&#x5C31;&#x50CF;RNN&#x7684;&#x6700;&#x540E;&#x4E00;&#x4E2A;&#x9690;&#x85CF;&#x72B6;&#x6001;&#xFF0C;&#x5728;&#x6A21;&#x578B;&#x4E2D;&#x3002;&#x5982;&#x679C;&#x6CA1;&#x6709;&#x8FD9;&#x6837;&#x7684;&#x7C7B;&#x4E3A; <code>&#x53C2;&#x6570;</code>
&#xFF0C;&#x8FD9;&#x4E9B;&#x4E34;&#x65F6;&#x5DE5;&#x5C06;&#x83B7;&#x5F97;&#x6CE8;&#x518C;&#x8FC7;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x6570;&#x636E;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x53C2;&#x6570;&#x5F20;&#x91CF;&#x3002;</p>
</li>
<li><p><strong>requires_grad</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5982;&#x679C;&#x53C2;&#x6570;&#x9700;&#x8981;&#x68AF;&#x5EA6;&#x3002;&#x770B;&#x5230;&#x4ECE;&#x5411;&#x540E; <a href="notes/autograd.html#excluding-subgraphs"> &#x4E0D;&#x5305;&#x62EC;&#x5B50;&#x56FE;&#x7684;&#x66F4;&#x591A;&#x7EC6;&#x8282;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;&#x771F;</a></p>
</li>
</ul>
<h2 id="&#x5BB9;&#x5668;">&#x5BB9;&#x5668;</h2>
<h3 id="&#x6A21;&#x5757;">&#x6A21;&#x5757;</h3>
<p><em>class</em><code>torch.nn.``Module</code><a href="_modules/torch/nn/modules/module.html#Module">[source]</a></p>
<p>&#x57FA;&#x7C7B;&#x7684;&#x6240;&#x6709;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x6A21;&#x5757;&#x3002;</p>
<p>&#x4F60;&#x7684;&#x8F66;&#x578B;&#x4E5F;&#x5E94;&#x8BE5;&#x7EE7;&#x627F;&#x8FD9;&#x4E2A;&#x7C7B;&#x3002;</p>
<p>&#x6A21;&#x5757;&#x4E5F;&#x53EF;&#x4EE5;&#x5305;&#x542B;&#x5176;&#x4ED6;&#x7684;&#x6A21;&#x5757;&#xFF0C;&#x5141;&#x8BB8;&#x5176;&#x5D4C;&#x5957;&#x5728;&#x4E00;&#x4E2A;&#x6811;&#x7ED3;&#x6784;&#x3002;&#x60A8;&#x53EF;&#x4EE5;&#x5206;&#x914D;&#x7684;&#x5B50;&#x6A21;&#x5757;&#x7684;&#x5E38;&#x89C4;&#x5C5E;&#x6027;&#xFF1A;</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre><p>&#x4EE5;&#x8FD9;&#x79CD;&#x65B9;&#x5F0F;&#x5206;&#x914D;&#x7684;&#x5B50;&#x6A21;&#x5757;&#x5C06;&#x88AB;&#x6CE8;&#x518C;&#xFF0C;&#x5E76;&#x6709;&#x81EA;&#x5DF1;&#x7684;&#x53C2;&#x6570;&#x8F6C;&#x6362;&#x8FC7;&#xFF0C;&#x5F53;&#x4F60;&#x8C03;&#x7528; <code>&#x4EE5;&#xFF08;&#xFF09;</code>&#x7B49;&#x3002;</p>
<p><code>add_module</code>( <em>name</em> , <em>module</em>
)<a href="_modules/torch/nn/modules/module.html#Module.add_module">[source]</a></p>
<p>&#x6DFB;&#x52A0;&#x4E00;&#x4E2A;&#x5B50;&#x6A21;&#x5757;&#xFF0C;&#x5F53;&#x524D;&#x6A21;&#x5757;&#x3002;</p>
<p>&#x8BE5;&#x6A21;&#x5757;&#x53EF;&#x4EE5;&#x4F5C;&#x4E3A;&#x4F7F;&#x7528;&#x7ED9;&#x5B9A;&#x540D;&#x79F0;&#x7684;&#x5C5E;&#x6027;&#x8FDB;&#x884C;&#x8BBF;&#x95EE;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x540D;</strong> &#xFF08; <em>&#x4E32;</em> &#xFF09; - &#x5B50;&#x6A21;&#x5757;&#x7684;&#x540D;&#x79F0;&#x3002;&#x5B50;&#x6A21;&#x5757;&#x53EF;&#x4EE5;&#x4ECE;&#x8BE5;&#x6A21;&#x5757;&#x4F7F;&#x7528;&#x7ED9;&#x5B9A;&#x7684;&#x540D;&#x79F0;&#x6765;&#x8BBF;&#x95EE;</p>
</li>
<li><p><strong>&#x6A21;&#x5757;</strong> &#xFF08; <em>&#x6A21;&#x5757;</em> &#xFF09; - &#x5B50;&#x6A21;&#x5757;&#x88AB;&#x6DFB;&#x52A0;&#x5230;&#x8BE5;&#x6A21;&#x5757;&#x3002;</p>
</li>
</ul>
<p><code>apply</code>( <em>fn</em> )<a href="_modules/torch/nn/modules/module.html#Module.apply">[source]</a></p>
<p>&#x9002;&#x7528;<code>FN</code>&#x9012;&#x5F52;&#x5730;&#x5BF9;&#x6BCF;&#x4E2A;&#x5B50;&#x6A21;&#x5757;&#x4EE5;&#x53CA;&#x81EA;&#xFF08;&#x5982;&#x7531;<code>&#x3002;&#x513F;&#x7AE5;&#xFF08;&#xFF09;</code>&#x8FD4;&#x56DE;&#xFF09;&#x3002;&#x5178;&#x578B;&#x7528;&#x9014;&#x5305;&#x62EC;&#x521D;&#x59CB;&#x5316;&#x4E00;&#x4E2A;&#x6A21;&#x578B;&#x7684;&#x53C2;&#x6570;&#xFF08;&#x4E5F;&#x89C1;torch-NN-INIT &#xFF09;&#x3002;</p>
<p>Parameters</p>
<p><strong>FN</strong> &#xFF08; <code>&#x6A21;&#x5757;</code>- &amp; GT ;&#x65E0;&#xFF09; - &#x51FD;&#x6570;&#x88AB;&#x5E94;&#x7528;&#x5230;&#x6BCF;&#x4E2A;&#x5B50;&#x6A21;&#x5757;</p>
<p>Returns</p>
<p>&#x81EA;</p>
<p>Return type</p>
<p>&#x6A21;&#x5757;</p>
<p>&#x4F8B;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; def init_weights(m):
&gt;&gt;&gt;     print(m)
&gt;&gt;&gt;     if type(m) == nn.Linear:
&gt;&gt;&gt;         m.weight.data.fill_(1.0)
&gt;&gt;&gt;         print(m.weight)
&gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))
&gt;&gt;&gt; net.apply(init_weights)
Linear(in_features=2, out_features=2, bias=True)
Parameter containing:
tensor([[ 1.,  1.],
        [ 1.,  1.]])
Linear(in_features=2, out_features=2, bias=True)
Parameter containing:
tensor([[ 1.,  1.],
        [ 1.,  1.]])
Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
)
Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
)
</code></pre><p><code>buffers</code>( <em>recurse=True</em>
)<a href="_modules/torch/nn/modules/module.html#Module.buffers">[source]</a></p>
<p>&#x8FD4;&#x56DE;&#x5728;&#x6A21;&#x5757;&#x7F13;&#x51B2;&#x533A;&#x7684;&#x8FED;&#x4EE3;&#x5668;&#x3002;</p>
<p>Parameters</p>
<p><strong>&#x9012;&#x5F52;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in
Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a>&#xFF09; - &#x5982;&#x679C;&#x4E3A;True&#xFF0C;&#x5219;&#x4EA7;&#x751F;&#x8BE5;&#x6A21;&#x5757;&#x548C;&#x6240;&#x6709;&#x5B50;&#x6A21;&#x5757;&#x7684;&#x7F13;&#x51B2;&#x5668;&#x3002;&#x5426;&#x5219;&#xFF0C;&#x4EC5;&#x4EA7;&#x751F;&#x662F;&#x8BE5;&#x6A21;&#x5757;&#x7684;&#x76F4;&#x63A5;&#x6210;&#x5458;&#x7684;&#x7F13;&#x51B2;&#x533A;&#x3002;</p>
<p>Yields</p>
<p><em>torch.Tensor</em> - &#x6A21;&#x5757;&#x7F13;&#x51B2;&#x5668;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; for buf in model.buffers():
&gt;&gt;&gt;     print(type(buf.data), buf.size())
&lt;class &apos;torch.FloatTensor&apos;&gt; (20L,)
&lt;class &apos;torch.FloatTensor&apos;&gt; (20L, 1L, 5L, 5L)
</code></pre><p><code>children</code>()<a href="_modules/torch/nn/modules/module.html#Module.children">[source]</a></p>
<p>&#x8FD4;&#x56DE;&#x5728;&#x5373;&#x65F6;&#x513F;&#x7AE5;&#x6A21;&#x5757;&#x7684;&#x8FED;&#x4EE3;&#x5668;&#x3002;</p>
<p>Yields</p>
<p><em>&#x6A21;&#x5757;</em> - &#x4E00;&#x4E2A;&#x5B50;&#x6A21;&#x5757;</p>
<p><code>cpu</code>()<a href="_modules/torch/nn/modules/module.html#Module.cpu">[source]</a></p>
<p>&#x79FB;&#x52A8;&#x6240;&#x6709;&#x6A21;&#x578B;&#x53C2;&#x6570;&#x548C;&#x7F13;&#x51B2;&#x533A;&#x7684;CPU&#x3002;</p>
<p>Returns</p>
<p>self</p>
<p>Return type</p>
<p>Module</p>
<p><code>cuda</code>( <em>device=None</em>
)<a href="_modules/torch/nn/modules/module.html#Module.cuda">[source]</a></p>
<p>&#x79FB;&#x52A8;&#x6240;&#x6709;&#x6A21;&#x578B;&#x53C2;&#x6570;&#x548C;&#x7F13;&#x51B2;&#x533A;&#x7684;GPU&#x3002;</p>
<p>&#x8FD9;&#x4E5F;&#x4F7F;&#x5F97;&#x76F8;&#x5173;&#x7684;&#x53C2;&#x6570;&#x548C;&#x7F13;&#x51B2;&#x533A;&#x4E0D;&#x540C;&#x7684;&#x5BF9;&#x8C61;&#x3002;&#x6240;&#x4EE5;&#x5E94;&#x8BE5;&#x6784;&#x5EFA;&#x4F18;&#x5316;&#x6A21;&#x5757;&#x662F;&#x5426;&#x5C06;&#x751F;&#x6D3B;&#x5728;GPU&#x540C;&#x65F6;&#x8FDB;&#x884C;&#x4F18;&#x5316;&#x4E4B;&#x524D;&#x88AB;&#x8C03;&#x7528;&#x3002;</p>
<p>Parameters</p>
<p><strong>&#x88C5;&#x7F6E;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in
Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5982;&#x679C;&#x6307;&#x5B9A;&#xFF0C;&#x6240;&#x6709;&#x7684;&#x53C2;&#x6570;&#x5C06;&#x88AB;&#x590D;&#x5236;&#x5230;&#x8BE5;&#x8BBE;&#x5907;</p>
<p>Returns</p>
<p>self</p>
<p>Return type</p>
<p>Module</p>
<p><code>double</code>()<a href="_modules/torch/nn/modules/module.html#Module.double">[source]</a></p>
<p>&#x65BD;&#x653E;&#x6240;&#x6709;&#x6D6E;&#x70B9;&#x53C2;&#x6570;&#x548C;&#x7F13;&#x51B2;&#x6DB2;&#x4EE5;<code>&#x53CC;</code>&#x6570;&#x636E;&#x7C7B;&#x578B;&#x3002;</p>
<p>Returns</p>
<p>self</p>
<p>Return type</p>
<p>Module</p>
<p><code>dump_patches</code><em>= False</em></p>
<p>&#x8FD9;&#x5141;&#x8BB8;&#x66F4;&#x597D;BC&#x652F;&#x6301;<code>load_state_dict&#xFF08;&#xFF09;</code>&#x3002;&#x5728; <code>state_dict&#xFF08;&#xFF09;</code>&#xFF0C;&#x7248;&#x672C;&#x53F7;&#x5C06;&#x88AB;&#x4FDD;&#x5B58;&#x4E3A;&#x5728;&#x8FD4;&#x56DE;&#x7684;&#x72B6;&#x6001;&#x5B57;&#x5178;&#x7684;&#x5C5E;&#x6027;
_metadata &#xFF0C;&#x56E0;&#x6B64;&#x9178;&#x6D17;&#x3002;  _metadata &#x662F;&#x4E0E;&#x540E;&#x9762;&#x72B6;&#x6001;&#x5B57;&#x5178;&#x7684;&#x547D;&#x540D;&#x7EA6;&#x5B9A;&#x952E;&#x7684;&#x5B57;&#x5178;&#x3002;&#x53C2;&#x89C1;<code>_load_from_state_dict</code>&#x5982;&#x4F55;&#x5728;&#x52A0;&#x8F7D;&#x4F7F;&#x7528;&#x8BE5;&#x4FE1;&#x606F;&#x3002;</p>
<p>&#x5982;&#x679C;&#x6DFB;&#x52A0;&#x4E86;&#x65B0;&#x7684;&#x53C2;&#x6570;/&#x7F13;&#x51B2;&#x5668;/&#x4ECE;&#x6A21;&#x5757;&#x4E2D;&#x53D6;&#x51FA;&#xFF0C;&#x8FD9;&#x4E2A;&#x6570;&#x5B57;&#x5C06;&#x88AB;&#x78B0;&#x649E;&#xFF0C;&#x4EE5;&#x53CA;&#x6A21;&#x5757;&#x7684; _load_from_state_dict
&#x65B9;&#x6CD5;&#x53EF;&#x4EE5;&#x6BD4;&#x8F83;&#x7684;&#x7248;&#x672C;&#x53F7;&#xFF0C;&#x5E76;&#x505A;&#x9002;&#x5F53;&#x7684;&#x4FEE;&#x6539;&#xFF0C;&#x5982;&#x679C;&#x72B6;&#x6001;&#x5B57;&#x5178;&#x662F;&#x4ECE;&#x6539;&#x53D8;&#x4E4B;&#x524D;&#x3002;</p>
<p><code>eval</code>()<a href="_modules/torch/nn/modules/module.html#Module.eval">[source]</a></p>
<p>&#x8BBE;&#x7F6E;&#x5728;&#x8BC4;&#x4F30;&#x6A21;&#x5F0F;&#x4E0B;&#x7684;&#x6A21;&#x5757;&#x3002;</p>
<p>&#x8FD9;&#x53EA;&#x6709;&#x5728;&#x67D0;&#x4E9B;&#x6A21;&#x5757;&#x6CA1;&#x6709;&#x4EFB;&#x4F55;&#x5F71;&#x54CD;&#x3002;&#x89C1;&#x7279;&#x5B9A;&#x6A21;&#x5757;&#x7684;&#x5355;&#x8BC1;&#x5728;&#x57F9;&#x8BAD;/&#x8BC4;&#x4F30;&#x6A21;&#x5F0F;&#xFF0C;&#x5982;&#x679C;&#x4ED6;&#x4EEC;&#x53D7;&#x5230;&#x5F71;&#x54CD;&#xFF0C;&#x4F8B;&#x5982;&#x4ED6;&#x4EEC;&#x7684;&#x884C;&#x4E3A;&#x7684;&#x7EC6;&#x8282; <code>&#x964D;</code>&#xFF0C;<code>BatchNorm</code>&#x7B49;</p>
<p>&#x8FD9;&#x76F8;&#x5F53;&#x4E8E;&#x4E0E; <code>self.train&#xFF08;&#x5047;&#xFF09;</code>&#x3002;</p>
<p>Returns</p>
<p>self</p>
<p>Return type</p>
<p>Module</p>
<p><code>extra_repr</code>()<a href="_modules/torch/nn/modules/module.html#Module.extra_repr">[source]</a></p>
<p>&#x8BBE;&#x7F6E;&#x6A21;&#x5757;&#x7684;&#x989D;&#x5916;&#x4EE3;&#x8868;&#x6027;</p>
<p>&#x8981;&#x6253;&#x5370;&#x5B9A;&#x5236;&#x989D;&#x5916;&#x7684;&#x4FE1;&#x606F;&#xFF0C;&#x4F60;&#x5E94;&#x8BE5;&#x5728;&#x4F60;&#x81EA;&#x5DF1;&#x7684;&#x6A21;&#x5757;&#x91CD;&#x65B0;&#x5B9E;&#x73B0;&#x6B64;&#x65B9;&#x6CD5;&#x3002;&#x65E2;&#x5355;&#x884C;&#x548C;&#x591A;&#x884C;&#x5B57;&#x7B26;&#x4E32;&#x662F;&#x53EF;&#x63A5;&#x53D7;&#x7684;&#x3002;</p>
<p><code>float</code>()<a href="_modules/torch/nn/modules/module.html#Module.float">[source]</a></p>
<p>&#x65BD;&#x653E;&#x6240;&#x6709;&#x6D6E;&#x70B9;&#x53C2;&#x6570;&#x548C;&#x7F13;&#x51B2;&#x533A;&#x6D6E;&#x52A8;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x3002;</p>
<p>Returns</p>
<p>self</p>
<p>Return type</p>
<p>Module</p>
<p><code>forward</code>( <em>*input</em>
)<a href="_modules/torch/nn/modules/module.html#Module.forward">[source]</a></p>
<p>&#x5B9A;&#x4E49;&#x5728;&#x6BCF;&#x4E2A;&#x547C;&#x53EB;&#x8FDB;&#x884C;&#x8BA1;&#x7B97;&#x3002;</p>
<p>&#x5E94;&#x8BE5;&#x7531;&#x6240;&#x6709;&#x5B50;&#x7C7B;&#x8986;&#x76D6;&#x3002;</p>
<p>&#x6CE8;&#x610F;</p>
<p>&#x867D;&#x7136;&#x5FC5;&#x987B;&#x5728;&#x8FD9;&#x4E2A;&#x51FD;&#x6570;&#x4E2D;&#x5B9A;&#x4E49;&#x7684;&#x76F4;&#x4F20;&#x98DF;&#x8C31;&#xFF0C;&#x5E94;&#x8BE5;&#x53EB; <code>&#x6A21;&#x5757;</code>&#x5B9E;&#x4F8B;&#x4E4B;&#x540E;&#xFF0C;&#x800C;&#x4E0D;&#x662F;&#x8FD9;&#x4E2A;&#xFF0C;&#x56E0;&#x4E3A;&#x524D;&#x8005;&#x9700;&#x8981;&#x8FD0;&#x884C;&#x7684;&#x62A4;&#x7406;&#x6CE8;&#x518C;&#x6302;&#x94A9;&#xFF0C;&#x800C;&#x540E;&#x8005;&#x9ED8;&#x9ED8;&#x5730;&#x5FFD;&#x7565;&#x5B83;&#x4EEC;&#x3002;</p>
<p><code>half</code>()<a href="_modules/torch/nn/modules/module.html#Module.half">[source]</a></p>
<p>&#x65BD;&#x653E;&#x6240;&#x6709;&#x6D6E;&#x70B9;&#x53C2;&#x6570;&#x548C;&#x7F13;&#x51B2;&#x6DB2;&#x4EE5;<code>&#x4E00;&#x534A;</code>&#x6570;&#x636E;&#x7C7B;&#x578B;&#x3002;</p>
<p>Returns</p>
<p>self</p>
<p>Return type</p>
<p>Module</p>
<p><code>load_state_dict</code>( <em>state_dict</em> , <em>strict=True</em>
)<a href="_modules/torch/nn/modules/module.html#Module.load_state_dict">[source]</a></p>
<p>&#x4EFD;&#x53C2;&#x6570;&#x548C;&#x7F13;&#x51B2;&#x6DB2;&#x4ECE; <code>state_dict</code>&#x5230;&#x8FD9;&#x4E2A;&#x6A21;&#x5757;&#x53CA;&#x5176;&#x540E;&#x4EE3;&#x3002;&#x5982;&#x679C;<code>&#x4E25;&#x683C;</code>&#x662F;<code>&#x771F;</code>&#xFF0C;&#x5219; <code>&#x952E;state_dict</code>
&#x5FC5;&#x987B;&#x5B8C;&#x5168;&#x7B26;&#x5408;&#x672C;&#x6A21;&#x5757;&#x7684; <code>state_dict&#xFF08;&#xFF09;</code>&#x51FD;&#x6570;&#x7684;&#x8FD4;&#x56DE;&#x952E;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>state_dict</strong> &#xFF08;<a href="https://docs.python.org/3/library/stdtypes.html#dict" title="\(in Python v3.7\)" target="_blank"> <em>DICT</em> </a>&#xFF09; - &#x5305;&#x542B;&#x53C2;&#x6570;&#x548C;&#x6301;&#x4E45;&#x6027;&#x7F13;&#x51B2;&#x533A;&#x7684;&#x5B57;&#x5178;&#x3002;</p>
</li>
<li><p><strong>&#x4E25;&#x683C;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x662F;&#x5426;&#x4E25;&#x683C;&#x6267;&#x884C;&#xFF0C;&#x5728;&#x952E;<code>state_dict</code>&#x5339;&#x914D;&#x7531;&#x8BE5;&#x6A21;&#x5757;&#x7684; <code>state_dict&#xFF08;&#xFF09;</code>&#x51FD;&#x6570;&#x8FD4;&#x56DE;&#x7684;&#x952E;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x771F;</code></p>
</li>
</ul>
<p>Returns</p>
<ul>
<li><p><strong>missing_keys</strong> &#x662F;STR&#x7684;&#x5305;&#x542B;&#x4E22;&#x5931;&#x5BC6;&#x94A5;&#x7684;&#x5217;&#x8868;</p>
</li>
<li><p><strong>unexpected_keys</strong> &#x662F;STR&#x7684;&#x542B;&#x6709;&#x610F;&#x60F3;&#x4E0D;&#x5230;&#x7684;&#x952E;&#x7684;&#x5217;&#x8868;</p>
</li>
</ul>
<p>Return type</p>
<p><code>NamedTuple</code>&#x4E0E;<code>missing_keys</code>&#x548C;<code>unexpected_keys</code>&#x5B57;&#x6BB5;</p>
<p><code>modules</code>()<a href="_modules/torch/nn/modules/module.html#Module.modules">[source]</a></p>
<p>&#x8FD4;&#x56DE;&#x5728;&#x7F51;&#x7EDC;&#x4E2D;&#x7684;&#x6240;&#x6709;&#x6A21;&#x5757;&#x7684;&#x8FED;&#x4EE3;&#x5668;&#x3002;</p>
<p>Yields</p>
<p>&#x7F51;&#x7EDC;&#x4E2D;&#x7684;&#x6A21;&#x5757; - <em>&#x6A21;&#x5757;</em></p>
<p>Note</p>
<p>&#x91CD;&#x590D;&#x6A21;&#x5757;&#x53EA;&#x8FD4;&#x56DE;&#x4E00;&#x6B21;&#x3002;&#x5728;&#x4EE5;&#x4E0B;&#x793A;&#x4F8B;&#x4E2D;&#xFF0C;<code>L</code>&#x5C06;&#x53EA;&#x8FD4;&#x56DE;&#x4E00;&#x6B21;&#x3002;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; l = nn.Linear(2, 2)
&gt;&gt;&gt; net = nn.Sequential(l, l)
&gt;&gt;&gt; for idx, m in enumerate(net.modules()):
        print(idx, &apos;-&gt;&apos;, m)

0 -&gt; Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
)
1 -&gt; Linear(in_features=2, out_features=2, bias=True)
</code></pre><p><code>named_buffers</code>( <em>prefix=&apos;&apos;</em> , <em>recurse=True</em>
)<a href="_modules/torch/nn/modules/module.html#Module.named_buffers">[source]</a></p>
<p>&#x8FD4;&#x56DE;&#x5728;&#x6A21;&#x5757;&#x7F13;&#x51B2;&#x533A;&#x7684;&#x8FED;&#x4EE3;&#x5668;&#xFF0C;&#x4EA7;&#x751F;&#x7F13;&#x51B2;&#x4E2D;&#x7684;&#x4E24;&#x4E2A;&#x540D;&#x5B57;&#x4EE5;&#x53CA;&#x7F13;&#x51B2;&#x533A;&#x672C;&#x8EAB;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x524D;&#x7F00;</strong> &#xFF08;<a href="https://docs.python.org/3/library/stdtypes.html#str" title="\(in Python v3.7\)" target="_blank"> <em>STR</em> </a>&#xFF09; - &#x524D;&#x7F00;&#x65F6;&#xFF0C;&#x9884;&#x5148;&#x51C6;&#x5907;&#x6240;&#x6709;&#x7F13;&#x51B2;&#x5668;&#x7684;&#x540D;&#x5B57;&#x3002;</p>
</li>
<li><p><strong>recurse</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a>) &#x2013; if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.</p>
</li>
</ul>
<p>Yields</p>
<p><em>&#xFF08;&#x5B57;&#x7B26;&#x4E32;&#xFF0C;torch.Tensor&#xFF09;</em> - &#x5143;&#x7EC4;&#x5305;&#x542B;&#x540D;&#x79F0;&#x548C;&#x7F13;&#x51B2;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; for name, buf in self.named_buffers():
&gt;&gt;&gt;    if name in [&apos;running_var&apos;]:
&gt;&gt;&gt;        print(buf.size())
</code></pre><p><code>named_children</code>()<a href="_modules/torch/nn/modules/module.html#Module.named_children">[source]</a></p>
<p>&#x8FD4;&#x56DE;&#x5728;&#x5373;&#x65F6;&#x513F;&#x7AE5;&#x6A21;&#x5757;&#x7684;&#x8FED;&#x4EE3;&#x5668;&#xFF0C;&#x4EA7;&#x751F;&#x6A21;&#x5757;&#x7684;&#x4E24;&#x4E2A;&#x540D;&#x5B57;&#xFF0C;&#x4EE5;&#x53CA;&#x6A21;&#x5757;&#x672C;&#x8EAB;&#x3002;</p>
<p>Yields</p>
<p><em>&#xFF08;&#x5B57;&#x7B26;&#x4E32;&#xFF0C;&#x6A21;&#x5757;&#xFF09;</em> - &#x542B;&#x6709;&#x540D;&#x79F0;&#x548C;&#x5B50;&#x6A21;&#x5757;&#x7684;Tuple</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; for name, module in model.named_children():
&gt;&gt;&gt;     if name in [&apos;conv4&apos;, &apos;conv5&apos;]:
&gt;&gt;&gt;         print(module)
</code></pre><p><code>named_modules</code>( <em>memo=None</em> , <em>prefix=&apos;&apos;</em>
)<a href="_modules/torch/nn/modules/module.html#Module.named_modules">[source]</a></p>
<p>&#x8FD4;&#x56DE;&#x7F51;&#x7EDC;&#xFF0C;&#x5728;&#x6240;&#x6709;&#x6A21;&#x5757;&#x7684;&#x8FED;&#x4EE3;&#x5668;&#xFF0C;&#x4EA7;&#x751F;&#x6A21;&#x5757;&#x7684;&#x4E24;&#x4E2A;&#x540D;&#x5B57;&#xFF0C;&#x4EE5;&#x53CA;&#x6A21;&#x5757;&#x672C;&#x8EAB;&#x3002;</p>
<p>Yields</p>
<p><em>&#xFF08;&#x5B57;&#x7B26;&#x4E32;&#xFF0C;&#x6A21;&#x5757;&#xFF09;</em> - &#x540D;&#x79F0;&#x548C;&#x6A21;&#x5757;&#x7684;&#x5143;&#x7EC4;</p>
<p>Note</p>
<p>Duplicate modules are returned only once. In the following example, <code>l</code>will
be returned only once.</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; l = nn.Linear(2, 2)
&gt;&gt;&gt; net = nn.Sequential(l, l)
&gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):
        print(idx, &apos;-&gt;&apos;, m)

0 -&gt; (&apos;&apos;, Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
))
1 -&gt; (&apos;0&apos;, Linear(in_features=2, out_features=2, bias=True))
</code></pre><p><code>named_parameters</code>( <em>prefix=&apos;&apos;</em> , <em>recurse=True</em>
)<a href="_modules/torch/nn/modules/module.html#Module.named_parameters">[source]</a></p>
<p>&#x8FD4;&#x56DE;&#x5728;&#x6A21;&#x5757;&#x53C2;&#x6570;&#x7684;&#x8FED;&#x4EE3;&#x5668;&#xFF0C;&#x4EA7;&#x751F;&#x53C2;&#x6570;&#x7684;&#x4E24;&#x4E2A;&#x540D;&#x79F0;&#x4EE5;&#x53CA;&#x53C2;&#x6570;&#x672C;&#x8EAB;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x524D;&#x7F00;</strong> &#xFF08;<a href="https://docs.python.org/3/library/stdtypes.html#str" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D77;&#x5CE1;</em> </a>&#xFF09; - &#x524D;&#x7F00;&#x9884;&#x5148;&#x8003;&#x8651;&#x5230;&#x6240;&#x6709;&#x7684;&#x53C2;&#x6570;&#x540D;&#x79F0;&#x3002;</p>
</li>
<li><p><strong>&#x9012;&#x5F52;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a>&#xFF09; - &#x5982;&#x679C;&#x4E3A;True&#xFF0C;&#x5219;&#x4EA7;&#x751F;&#x8BE5;&#x6A21;&#x5757;&#x548C;&#x6240;&#x6709;&#x5B50;&#x6A21;&#x5757;&#x7684;&#x53C2;&#x6570;&#x3002;&#x5426;&#x5219;&#xFF0C;&#x4EC5;&#x4EA7;&#x751F;&#x662F;&#x8BE5;&#x6A21;&#x5757;&#x7684;&#x76F4;&#x63A5;&#x6210;&#x5458;&#x53C2;&#x6570;&#x3002;</p>
</li>
</ul>
<p>Yields</p>
<p><em>&#xFF08;&#x5B57;&#x7B26;&#x4E32;&#xFF0C;&#x53C2;&#x6570;&#xFF09;</em> - &#x5305;&#x542B;&#x5143;&#x7EC4;&#x7684;&#x540D;&#x79F0;&#x548C;&#x53C2;&#x6570;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; for name, param in self.named_parameters():
&gt;&gt;&gt;    if name in [&apos;bias&apos;]:
&gt;&gt;&gt;        print(param.size())
</code></pre><p><code>parameters</code>( <em>recurse=True</em>
)<a href="_modules/torch/nn/modules/module.html#Module.parameters">[source]</a></p>
<p>&#x8FD4;&#x56DE;&#x5728;&#x6A21;&#x5757;&#x53C2;&#x6570;&#x7684;&#x8FED;&#x4EE3;&#x5668;&#x3002;</p>
<p>&#x8FD9;&#x901A;&#x5E38;&#x662F;&#x901A;&#x8FC7;&#x7ED9;&#x4F18;&#x5316;&#x3002;</p>
<p>Parameters</p>
<p><strong>recurse</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a>) &#x2013; if True, then yields parameters of this module and
all submodules. Otherwise, yields only parameters that are direct members of
this module.</p>
<p>Yields</p>
<p><em>&#x53C2;&#x6570;</em> - &#x6A21;&#x5757;&#x53C2;&#x6570;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; for param in model.parameters():
&gt;&gt;&gt;     print(type(param.data), param.size())
&lt;class &apos;torch.FloatTensor&apos;&gt; (20L,)
&lt;class &apos;torch.FloatTensor&apos;&gt; (20L, 1L, 5L, 5L)
</code></pre><p><code>register_backward_hook</code>( <em>hook</em>
)<a href="_modules/torch/nn/modules/module.html#Module.register_backward_hook">[source]</a></p>
<p>&#x5BC4;&#x5B58;&#x5668;&#x6A21;&#x5757;&#x4E0A;&#x7684;&#x5411;&#x540E;&#x94A9;&#x3002;</p>
<p>&#x94A9;&#x5C06;&#x88AB;&#x79F0;&#x4E3A;&#x6BCF;&#x6B21;&#x76F8;&#x5BF9;&#x4E8E;&#x68AF;&#x5EA6;&#x4EE5;&#x6A21;&#x5757;&#x8F93;&#x5165;&#x88AB;&#x8BA1;&#x7B97;&#x3002;&#x94A9;&#x5B50;&#x5E94;&#x8BE5;&#x5177;&#x6709;&#x4EE5;&#x4E0B;&#x7279;&#x5F81;&#xFF1A;</p>
<pre><code>hook(module, grad_input, grad_output) -&gt; Tensor or None
</code></pre><p>&#x7684;<code>grad_input</code>&#x548C;<code>grad_output</code>&#x53EF;&#x4EE5;&#x662F;&#x5143;&#x7EC4;&#x5982;&#x679C;&#x6A21;&#x5757;&#x5177;&#x6709;&#x591A;&#x4E2A;&#x8F93;&#x5165;&#x6216;&#x8F93;&#x51FA;&#x3002;&#x94A9;&#x4E0D;&#x5E94;&#x4FEE;&#x6539;&#x5176;&#x53C2;&#x6570;&#xFF0C;&#x4F46;&#x5B83;&#x53EF;&#x4EE5;&#x4EFB;&#x9009;&#x5730;&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x65B0;&#x7684;&#x68AF;&#x5EA6;&#x76F8;&#x5BF9;&#x4E8E;&#x8F93;&#x5165;&#xFF0C;&#x5C06;&#x4EE3;&#x66FF;<code>grad_input</code>&#x5728;&#x968F;&#x540E;&#x7684;&#x8BA1;&#x7B97;&#x4E2D;&#x4F7F;&#x7528;&#x3002;</p>
<p>Returns</p>
<p>&#x53EF;&#x4EE5;&#x4F7F;&#x7528;&#x7684;&#x4E00;&#x4E2A;&#x624B;&#x67C4;&#x901A;&#x8FC7;&#x8C03;&#x7528;<code>handle.remove&#xFF08;&#xFF09;&#x4EE5;&#x53BB;&#x9664;&#x6240;&#x6DFB;&#x52A0;&#x7684;&#x94A9;</code></p>
<p>Return type</p>
<p><code>torch.utils.hooks.RemovableHandle</code></p>
<p>&#x8B66;&#x544A;</p>
<p>&#x5F53;&#x524D;&#x7684;&#x5B9E;&#x73B0;&#x4E0D;&#x4F1A;&#x5BF9;&#x590D;&#x6742;&#x7684; <code>&#x6A21;&#x5757;</code>&#x6267;&#x884C;&#x8BB8;&#x591A;&#x64CD;&#x4F5C;&#x6240;&#x5448;&#x73B0;&#x7684;&#x884C;&#x4E3A;&#x3002;&#x5728;&#x4E00;&#x4E9B;&#x5931;&#x8D25;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;<code>grad_input</code>&#x548C;<code>grad_output</code>&#x5C06;&#x53EA;&#x5305;&#x542B;&#x5BF9;&#x7684;&#x8F93;&#x5165;&#x548C;&#x8F93;&#x51FA;&#x7684;&#x4E00;&#x4E2A;&#x5B50;&#x96C6;&#x7684;&#x68AF;&#x5EA6;&#x200B;&#x200B;&#x3002;&#x5BF9;&#x4E8E;&#x8FD9;&#x6837;&#x7684; <code>&#x6A21;&#x5757;</code>&#xFF0C;&#x5219;&#x5E94;&#x8BE5;&#x4F7F;&#x7528;<a href="tensors.html#torch.Tensor.register_hook" title="torch.Tensor.register_hook"> <code>torch.Tensor.register_hook&#xFF08;&#xFF09;</code>
</a>&#x76F4;&#x63A5;&#x5728;&#x4E00;&#x4E2A;&#x7279;&#x5B9A;&#x7684;&#x8F93;&#x5165;&#x6216;&#x8F93;&#x51FA;&#xFF0C;&#x4EE5;&#x83B7;&#x5F97;&#x6240;&#x9700;&#x7684;&#x68AF;&#x5EA6;&#x3002;</p>
<p><code>register_buffer</code>( <em>name</em> , <em>tensor</em>
)<a href="_modules/torch/nn/modules/module.html#Module.register_buffer">[source]</a></p>
<p>&#x6DFB;&#x52A0;&#x4E00;&#x4E2A;&#x6301;&#x4E45;&#x7F13;&#x51B2;&#x533A;&#x5230;&#x6A21;&#x5757;&#x3002;</p>
<p>&#x8FD9;&#x901A;&#x5E38;&#x662F;&#x7528;&#x4E8E;&#x6CE8;&#x518C;&#x4E0D;&#x5E94;&#x88AB;&#x8BA4;&#x4E3A;&#x662F;&#x4E00;&#x4E2A;&#x6A21;&#x578B;&#x53C2;&#x6570;&#x7684;&#x7F13;&#x51B2;&#x5668;&#x3002;&#x4F8B;&#x5982;&#xFF0C;BatchNorm&#x7684;<code>running_mean</code>&#x4E0D;&#x662F;&#x53C2;&#x6570;&#xFF0C;&#x4F46;&#x662F;&#x6301;&#x4E45;&#x72B6;&#x6001;&#x7684;&#x4E00;&#x90E8;&#x5206;&#x3002;</p>
<p>&#x7F13;&#x51B2;&#x533A;&#x53EF;&#x4EE5;&#x4E3A;&#x4F7F;&#x7528;&#x7ED9;&#x5B9A;&#x7684;&#x540D;&#x79F0;&#x5C5E;&#x6027;&#x6765;&#x8BBF;&#x95EE;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x540D;&#x79F0;</strong> &#xFF08; <em>&#x4E32;</em> &#xFF09; - &#x7F13;&#x51B2;&#x7684;&#x540D;&#x79F0;&#x3002;&#x7F13;&#x51B2;&#x5668;&#x53EF;&#x4EE5;&#x4ECE;&#x8BE5;&#x6A21;&#x5757;&#x4F7F;&#x7528;&#x7ED9;&#x5B9A;&#x7684;&#x540D;&#x79F0;&#x6765;&#x8BBF;&#x95EE;</p>
</li>
<li><p><strong>&#x5F20;&#x91CF;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x7F13;&#x51B2;&#x6DB2;&#x8FDB;&#x884C;&#x6CE8;&#x518C;&#x3002;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; self.register_buffer(&apos;running_mean&apos;, torch.zeros(num_features))
</code></pre><p><code>register_forward_hook</code>( <em>hook</em>
)<a href="_modules/torch/nn/modules/module.html#Module.register_forward_hook">[source]</a></p>
<p>&#x5BC4;&#x5B58;&#x5668;&#x6A21;&#x5757;&#x4E0A;&#x7684;&#x524D;&#x94A9;&#x3002;</p>
<p>&#x94A9;&#x5C06;&#x88AB;&#x79F0;&#x4E3A;&#x6BCF;&#x6B21;&#x4E4B;&#x540E; <code>&#x5411;&#x524D;&#xFF08;&#xFF09;</code>&#x5DF2;&#x7ECF;&#x8BA1;&#x7B97;&#x7684;&#x8F93;&#x51FA;&#x3002;&#x5B83;&#x5E94;&#x8BE5;&#x5177;&#x6709;&#x4EE5;&#x4E0B;&#x7279;&#x5F81;&#xFF1A;</p>
<pre><code>hook(module, input, output) -&gt; None or modified output
</code></pre><p>&#x94A9;&#x53EF;&#x4EE5;&#x4FEE;&#x6539;&#x7684;&#x8F93;&#x51FA;&#x3002;&#x5B83;&#x53EF;&#x4EE5;&#x4FEE;&#x6539;&#x8F93;&#x5165;&#x5C31;&#x5730;&#xFF0C;&#x4F46;&#x5B83;&#x4E0D;&#x4F1A;&#x5BF9;&#x8F6C;&#x53D1;&#x7684;&#x5F71;&#x54CD;&#xFF0C;&#x56E0;&#x4E3A;&#x8FD9;&#x662F;&#x540E; <code>&#x5411;&#x524D;&#x79F0;&#x4E3A;&#xFF08;&#xFF09;</code>&#x88AB;&#x8C03;&#x7528;&#x3002;</p>
<p>Returns</p>
<p>a handle that can be used to remove the added hook by calling
<code>handle.remove()</code></p>
<p>Return type</p>
<p><code>torch.utils.hooks.RemovableHandle</code></p>
<p><code>register_forward_pre_hook</code>( <em>hook</em>
)<a href="_modules/torch/nn/modules/module.html#Module.register_forward_pre_hook">[source]</a></p>
<p>&#x5BC4;&#x5B58;&#x5668;&#x6A21;&#x5757;&#x4E0A;&#x7684;&#x524D;&#x9884;&#x6302;&#x94A9;&#x3002;</p>
<p>&#x94A9;&#x5C06;&#x6BCF;&#x6B21;&#x8C03;&#x7528;&#x4E4B;&#x524D; <code>&#x5411;&#x524D;&#xFF08;&#xFF09;</code>&#x88AB;&#x8C03;&#x7528;&#x3002;&#x5B83;&#x5E94;&#x8BE5;&#x5177;&#x6709;&#x4EE5;&#x4E0B;&#x7279;&#x5F81;&#xFF1A;</p>
<pre><code>hook(module, input) -&gt; None or modified input
</code></pre><p>&#x94A9;&#x53EF;&#x4EE5;&#x4FEE;&#x6539;&#x8F93;&#x5165;&#x3002;&#x7528;&#x6237;&#x53EF;&#x4EE5;&#x8FD4;&#x56DE;&#x7684;&#x5143;&#x7EC4;&#x6216;&#x5728;&#x94A9;&#x7684;&#x5355;&#x4E2A;&#x4FEE;&#x9970;&#x7684;&#x503C;&#x3002;&#x6700;&#x540E;&#xFF0C;&#x6211;&#x4EEC;&#x5C06;&#x503C;&#x63D2;&#x5165;&#x5230;&#x4E00;&#x4E2A;&#x5143;&#x7EC4;&#x5982;&#x679C;&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x503C;&#xFF08;&#x9664;&#x975E;&#x8BE5;&#x503C;&#x5DF2;&#x7ECF;&#x662F;&#x4E00;&#x4E2A;&#x5143;&#x7EC4;&#xFF09;&#x3002;</p>
<p>Returns</p>
<p>a handle that can be used to remove the added hook by calling
<code>handle.remove()</code></p>
<p>Return type</p>
<p><code>torch.utils.hooks.RemovableHandle</code></p>
<p><code>register_parameter</code>( <em>name</em> , <em>param</em>
)<a href="_modules/torch/nn/modules/module.html#Module.register_parameter">[source]</a></p>
<p>&#x6DFB;&#x52A0;&#x4E00;&#x4E2A;&#x53C2;&#x6570;&#x5230;&#x6A21;&#x5757;&#x3002;</p>
<p>&#x8BE5;&#x53C2;&#x6570;&#x53EF;&#x4EE5;&#x4F5C;&#x4E3A;&#x4F7F;&#x7528;&#x5B9A;&#x540D;&#x79F0;&#x7684;&#x5C5E;&#x6027;&#x6765;&#x8BBF;&#x95EE;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x540D;&#x79F0;</strong> &#xFF08; <em>&#x4E32;</em> &#xFF09; - &#x53C2;&#x6570;&#x7684;&#x540D;&#x79F0;&#x3002;&#x8BE5;&#x53C2;&#x6570;&#x53EF;&#x4EE5;&#x4ECE;&#x8BE5;&#x6A21;&#x5757;&#x4F7F;&#x7528;&#x7ED9;&#x5B9A;&#x7684;&#x540D;&#x79F0;&#x6765;&#x8BBF;&#x95EE;</p>
</li>
<li><p><strong>PARAM</strong> &#xFF08; <em>&#x53C2;&#x6570;</em> &#xFF09; - &#x53C2;&#x6570;&#x88AB;&#x6DFB;&#x52A0;&#x5230;&#x8BE5;&#x6A21;&#x5757;&#x3002;</p>
</li>
</ul>
<p><code>requires_grad_</code>( <em>requires_grad=True</em>
)<a href="_modules/torch/nn/modules/module.html#Module.requires_grad_">[source]</a></p>
<p>&#x6539;&#x53D8;&#xFF0C;&#x5982;&#x679C;autograd&#x5E94;&#x5728;&#x6B64;&#x6A21;&#x5757;&#x4E2D;&#x7684;&#x53C2;&#x6570;&#x8BB0;&#x5F55;&#x7B49;&#x64CD;&#x4F5C;&#x3002;</p>
<p>&#x6B64;&#x65B9;&#x6CD5;&#x8BBE;&#x7F6E;&#x7684;&#x53C2;&#x6570;<code>requires_grad</code>&#x5C31;&#x5730;&#x5C5E;&#x6027;&#x3002;</p>
<p>&#x6B64;&#x65B9;&#x6CD5;&#x662F;&#x7528;&#x4E8E;&#x5FAE;&#x8C03;&#x6216;&#x5355;&#x72EC;&#x8BAD;&#x7EC3;&#x7684;&#x6A21;&#x578B;&#x7684;&#x90E8;&#x5206;&#xFF08;&#x4F8B;&#x5982;&#xFF0C;GAN&#x57F9;&#x8BAD;&#xFF09;&#x51B7;&#x51BB;&#x6240;&#x8FF0;&#x6A21;&#x5757;&#x7684;&#x4E00;&#x90E8;&#x5206;&#x6709;&#x5E2E;&#x52A9;&#x7684;&#x3002;</p>
<p>Parameters</p>
<p><strong>requires_grad</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em>
</a>&#xFF09;</p>
<ul>
<li>autograd&#x662F;&#x5426;&#x5E94;&#x8BE5;&#x5728;&#x8BE5;&#x6A21;&#x5757;&#x4E2D;&#x7684;&#x53C2;&#x6570;&#x8BB0;&#x5F55;&#x64CD;&#x4F5C;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x771F; [HTG9&#x3002;</code></li>
</ul>
<p>Returns</p>
<p>self</p>
<p>Return type</p>
<p>Module</p>
<p><code>state_dict</code>( <em>destination=None</em> , <em>prefix=&apos;&apos;</em> , <em>keep_vars=False</em>
)<a href="_modules/torch/nn/modules/module.html#Module.state_dict">[source]</a></p>
<p>&#x8FD4;&#x56DE;&#x5305;&#x542B;&#x6A21;&#x5757;&#x7684;&#x6574;&#x4F53;&#x72B6;&#x6001;&#x7684;&#x5B57;&#x5178;&#x3002;</p>
<p>&#x8FD9;&#x4E24;&#x4E2A;&#x53C2;&#x6570;&#x548C;&#x6301;&#x4E45;&#x6027;&#x7F13;&#x51B2;&#x6DB2;&#xFF08;&#x4F8B;&#x5982;&#x8FD0;&#x884C;&#x5E73;&#x5747;&#x503C;&#xFF09;&#x4E5F;&#x5305;&#x62EC;&#x5728;&#x5185;&#x3002;&#x952E;&#x5BF9;&#x5E94;&#x7684;&#x53C2;&#x6570;&#x548C;&#x7F13;&#x51B2;&#x533A;&#x540D;&#x5B57;&#x3002;</p>
<p>Returns</p>
<p>&#x5305;&#x542B;&#x6A21;&#x5757;&#x7684;&#x6574;&#x4E2A;&#x72B6;&#x6001;&#x7684;&#x5B57;&#x5178;</p>
<p>Return type</p>
<p><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="\(in Python
v3.7\)" target="_blank">&#x5B57;&#x5178;</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; module.state_dict().keys()
[&apos;bias&apos;, &apos;weight&apos;]
</code></pre><p><code>to</code>( <em>*args</em> , <em>**kwargs</em>
)<a href="_modules/torch/nn/modules/module.html#Module.to">[source]</a></p>
<p>&#x79FB;&#x52A8;&#x548C;/&#x6216;&#x6CE8;&#x5851;&#x53C2;&#x6570;&#x548C;&#x7F13;&#x51B2;&#x533A;&#x3002;</p>
<p>&#x8FD9;&#x53EF;&#x4EE5;&#x88AB;&#x79F0;&#x4E3A;</p>
<p><code>to</code>( <em>device=None</em> , <em>dtype=None</em> , <em>non_blocking=False</em>
)<a href="_modules/torch/nn/modules/module.html#Module.to">[source]</a></p>
<p><code>to</code>( <em>dtype</em> , <em>non_blocking=False</em>
)<a href="_modules/torch/nn/modules/module.html#Module.to">[source]</a></p>
<p><code>to</code>( <em>tensor</em> , <em>non_blocking=False</em>
)<a href="_modules/torch/nn/modules/module.html#Module.to">[source]</a></p>
<p>&#x5B83;&#x7684;&#x7B7E;&#x540D;&#x662F;&#x7C7B;&#x4F3C;&#x4E8E;<a href="tensors.html#torch.Tensor.to" title="torch.Tensor.to"> <code>torch.Tensor.to&#xFF08;&#xFF09;</code></a>&#xFF0C;&#x4F46;&#x4EC5;&#x63A5;&#x53D7;&#x6D6E;&#x70B9;&#x6240;&#x9700;&#x7684;<code>DTYPE</code>&#x79D2;&#x3002;&#x6B64;&#x5916;&#xFF0C;&#x8FD9;&#x79CD;&#x65B9;&#x6CD5;&#x5C06;&#x53EA;&#x6295;&#x6D6E;&#x70B9;&#x53C2;&#x6570;&#x548C;&#x7F13;&#x51B2;&#x6DB2;&#x4EE5;<code>DTYPE</code>&#xFF08;&#x5982;&#x6709;&#xFF09;&#x3002;&#x79EF;&#x5206;&#x53C2;&#x6570;&#x548C;&#x7F13;&#x51B2;&#x6DB2;&#x5C06;&#x88AB;&#x79FB;&#x81F3;<code>&#x88C5;&#x7F6E;</code>&#xFF0C;&#x5982;&#x679C;&#x7ED9;&#x5B9A;&#xFF0C;&#x4F46;&#x4E0E;dtypes&#x4E0D;&#x53D8;&#x3002;&#x5F53;<code>non_blocking</code>&#x88AB;&#x8BBE;&#x5B9A;&#xFF0C;&#x5B83;&#x4F1A;&#x5C1D;&#x8BD5;&#x8F6C;&#x6362;/&#x5982;&#x679C;&#x53EF;&#x80FD;&#x5F02;&#x6B65;&#x76F8;&#x5BF9;&#x4E8E;&#x79FB;&#x52A8;&#x5230;&#x4E3B;&#x673A;&#xFF0C;&#x4F8B;&#x5982;&#xFF0C;&#x79FB;&#x52A8;CPU&#x5F20;&#x91CF;&#x4E0E;&#x56FA;&#x5B9A;&#x5185;&#x5B58;&#x5230;CUDA&#x8BBE;&#x5907;&#x3002;</p>
<p>&#x8BF7;&#x53C2;&#x89C1;&#x4E0B;&#x9762;&#x7684;&#x4F8B;&#x5B50;&#x3002;</p>
<p>Note</p>
<p>&#x6B64;&#x65B9;&#x6CD5;&#x4F1A;&#x4FEE;&#x6539;&#x5C31;&#x5730;&#x6A21;&#x5757;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x88C5;&#x7F6E;</strong> &#xFF08;<code>torch.device</code>&#xFF09; - &#x7684;&#x53C2;&#x6570;&#x548C;&#x7F13;&#x51B2;&#x5668;&#x5728;&#x8BE5;&#x6A21;&#x5757;&#x4E2D;&#x7684;&#x6240;&#x671F;&#x671B;&#x7684;&#x8BBE;&#x5907;</p>
</li>
<li><p>&#x6B64;&#x6A21;&#x5757;&#x4E2D;&#x7684;&#x6D6E;&#x70B9;&#x53C2;&#x6570;&#x548C;&#x7F13;&#x51B2;&#x6DB2;&#x7684;&#x6240;&#x5E0C;&#x671B;&#x7684;&#x6D6E;&#x70B9;&#x7C7B;&#x578B; - <strong>DTYPE</strong> &#xFF08;<code>torch.dtype</code>&#xFF09;</p>
</li>
<li><p><strong>&#x5F20;&#x91CF;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>torch.Tensor</em> </a>&#xFF09; - &#x5F20;&#x91CF;&#xFF0C;&#x5176;D&#x578B;&#x7EC6;&#x80DE;&#x548C;&#x88C5;&#x7F6E;&#x6240;&#x9700;&#x7684;D&#x578B;&#x7EC6;&#x80DE;&#x548C;&#x88C5;&#x7F6E;&#x6B64;&#x6A21;&#x5757;&#x4E2D;&#x7684;&#x6240;&#x6709;&#x53C2;&#x6570;&#x548C;&#x7F13;&#x51B2;&#x5668;</p>
</li>
</ul>
<p>Returns</p>
<p>self</p>
<p>Return type</p>
<p>Module</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; linear = nn.Linear(2, 2)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1913, -0.3420],
        [-0.5113, -0.2325]])
&gt;&gt;&gt; linear.to(torch.double)
Linear(in_features=2, out_features=2, bias=True)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1913, -0.3420],
        [-0.5113, -0.2325]], dtype=torch.float64)
&gt;&gt;&gt; gpu1 = torch.device(&quot;cuda:1&quot;)
&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)
Linear(in_features=2, out_features=2, bias=True)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1914, -0.3420],
        [-0.5112, -0.2324]], dtype=torch.float16, device=&apos;cuda:1&apos;)
&gt;&gt;&gt; cpu = torch.device(&quot;cpu&quot;)
&gt;&gt;&gt; linear.to(cpu)
Linear(in_features=2, out_features=2, bias=True)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1914, -0.3420],
        [-0.5112, -0.2324]], dtype=torch.float16)
</code></pre><p><code>train</code>( <em>mode=True</em>
)<a href="_modules/torch/nn/modules/module.html#Module.train">[source]</a></p>
<p>&#x8BBE;&#x7F6E;&#x5728;&#x8BAD;&#x7EC3;&#x6A21;&#x5F0F;&#x4E0B;&#x7684;&#x6A21;&#x5757;&#x3002;</p>
<p>This has any effect only on certain modules. See documentations of particular
modules for details of their behaviors in training/evaluation mode, if they
are affected, e.g. <code>Dropout</code>, <code>BatchNorm</code>, etc.</p>
<p>Parameters</p>
<p><strong>&#x6A21;&#x5F0F;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in
Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a>&#xFF09; - &#x662F;&#x5426;&#x8BBE;&#x5B9A;&#x8BAD;&#x7EC3;&#x6A21;&#x5F0F;&#xFF08;<code>&#x771F;</code>&#xFF09;&#x6216;&#x8BC4;&#x4F30;&#x6A21;&#x5F0F;&#xFF08;<code>&#x5047;</code>&#xFF09;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x771F; [HTG17&#x3002;</code></p>
<p>Returns</p>
<p>self</p>
<p>Return type</p>
<p>Module</p>
<p><code>type</code>( <em>dst_type</em>
)<a href="_modules/torch/nn/modules/module.html#Module.type">[source]</a></p>
<p>&#x65BD;&#x653E;&#x7684;&#x6240;&#x6709;&#x53C2;&#x6570;&#x548C;&#x7F13;&#x51B2;&#x6DB2;&#x4EE5;<code>dst_type</code>&#x3002;</p>
<p>Parameters</p>
<p><strong>dst_type</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#type" title="\(in Python v3.7\)" target="_blank"> <em>&#x8F93;&#x5165;</em> </a> <em>&#x6216;</em> <em>&#x4E32;</em> &#xFF09; - &#x6240;&#x9700;&#x7684;&#x7C7B;&#x578B;</p>
<p>Returns</p>
<p>self</p>
<p>Return type</p>
<p>Module</p>
<p><code>zero_grad</code>()<a href="_modules/torch/nn/modules/module.html#Module.zero_grad">[source]</a></p>
<p>&#x5C06;&#x6240;&#x6709;&#x6A21;&#x578B;&#x53C2;&#x6570;&#x4E3A;&#x96F6;&#x7684;&#x68AF;&#x5EA6;&#x3002;</p>
<h3 id="&#x5E8F;&#x8D2F;">&#x5E8F;&#x8D2F;</h3>
<p><em>class</em><code>torch.nn.``Sequential</code>( <em>*args</em>
)<a href="_modules/torch/nn/modules/container.html#Sequential">[source]</a></p>
<p>&#x987A;&#x5E8F;&#x5BB9;&#x5668;&#x3002;&#x6A21;&#x5757;&#x5C06;&#x88AB;&#x6DFB;&#x52A0;&#x5230;&#x5B83;&#x5728;&#x5B83;&#x4EEC;&#x5728;&#x6784;&#x9020;&#x51FD;&#x6570;&#x4E2D;&#x4F20;&#x9012;&#x7684;&#x987A;&#x5E8F;&#x3002;&#x53EF;&#x66FF;&#x4EE3;&#x5730;&#xFF0C;&#x6A21;&#x5757;&#x7684;&#x6709;&#x5E8F;&#x5B57;&#x5178;&#x4E5F;&#x53EF;&#x4EE5;&#x901A;&#x8FC7;&#x3002;</p>
<p>&#x4E3A;&#x4E86;&#x4FBF;&#x4E8E;&#x7406;&#x89E3;&#xFF0C;&#x8FD9;&#x91CC;&#x662F;&#x4E00;&#x4E2A;&#x5C0F;&#x4F8B;&#x5B50;&#xFF1A;</p>
<pre><code># Example of using Sequential
model = nn.Sequential(
          nn.Conv2d(1,20,5),
          nn.ReLU(),
          nn.Conv2d(20,64,5),
          nn.ReLU()
        )

# Example of using Sequential with OrderedDict
model = nn.Sequential(OrderedDict([
          (&apos;conv1&apos;, nn.Conv2d(1,20,5)),
          (&apos;relu1&apos;, nn.ReLU()),
          (&apos;conv2&apos;, nn.Conv2d(20,64,5)),
          (&apos;relu2&apos;, nn.ReLU())
        ]))
</code></pre><h3 id="modulelist">ModuleList</h3>
<p><em>class</em><code>torch.nn.``ModuleList</code>( <em>modules=None</em>
)<a href="_modules/torch/nn/modules/container.html#ModuleList">[source]</a></p>
<p>&#x6301;&#x6709;&#x5217;&#x8868;&#x5B50;&#x6A21;&#x5757;&#x3002;</p>
<p><code>ModuleList</code>&#x53EF;&#x4EE5;&#x88AB;&#x7D22;&#x5F15;&#x50CF;&#x4E00;&#x4E2A;&#x666E;&#x901A;&#x7684;Python&#x5217;&#x8868;&#xFF0C;&#x4F46;&#x5B83;&#x5305;&#x542B;&#x7684;&#x6A21;&#x5757;&#x6B63;&#x786E;&#x6CE8;&#x518C;&#xFF0C;&#x5E76;&#x4E14;&#x5C06;&#x6240;&#x6709; <code>[HTG8&#x53EF;&#x89C1;]&#x6A21;&#x5757;</code>&#x7684;&#x65B9;&#x6CD5;&#x3002;</p>
<p>Parameters</p>
<p><strong>&#x6A21;&#x5757;</strong> &#xFF08; <em>&#x53EF;&#x8FED;&#x4EE3;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x6A21;&#x5757;&#x7684;&#x4E00;&#x4E2A;&#x53EF;&#x8FED;&#x4EE3;&#x6DFB;&#x52A0;</p>
<p>Example:</p>
<pre><code>class MyModule(nn.Module):
    def __init__(self):
        super(MyModule, self).__init__()
        self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])

    def forward(self, x):
        # ModuleList can act as an iterable, or be indexed using ints
        for i, l in enumerate(self.linears):
            x = self.linears[i // 2](x) + l(x)
        return x
</code></pre><p><code>append</code>( <em>module</em>
)<a href="_modules/torch/nn/modules/container.html#ModuleList.append">[source]</a></p>
<p>&#x8FFD;&#x52A0;&#x7ED9;&#x5B9A;&#x7684;&#x6A21;&#x5757;&#x5230;&#x5217;&#x8868;&#x7684;&#x672B;&#x5C3E;&#x3002;</p>
<p>Parameters</p>
<p><strong>&#x6A21;&#x5757;</strong> &#xFF08; <em>nn.Module</em> &#xFF09; - &#x6A21;&#x5757;&#x8FFD;&#x52A0;</p>
<p><code>extend</code>( <em>modules</em>
)<a href="_modules/torch/nn/modules/container.html#ModuleList.extend">[source]</a></p>
<p>&#x4ECE;&#x8FFD;&#x52A0;&#x4E00;&#x4E2A;Python&#x6A21;&#x5757;&#x53EF;&#x8FED;&#x4EE3;&#x5230;&#x5217;&#x8868;&#x7684;&#x672B;&#x5C3E;&#x3002;</p>
<p>Parameters</p>
<p><strong>&#x6A21;&#x5757;</strong> &#xFF08; <em>&#x53EF;&#x8FED;&#x4EE3;</em> &#xFF09; - &#x6A21;&#x5757;&#x7684;&#x53EF;&#x8FED;&#x4EE3;&#x8FFD;&#x52A0;</p>
<p><code>insert</code>( <em>index</em> , <em>module</em>
)<a href="_modules/torch/nn/modules/container.html#ModuleList.insert">[source]</a></p>
<p>&#x5217;&#x8868;&#x4E2D;&#x7684;&#x7ED9;&#x5B9A;&#x7D22;&#x5F15;&#x524D;&#x63D2;&#x5165;&#x4E00;&#x4E2A;&#x7279;&#x5B9A;&#x7684;&#x6A21;&#x5757;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x7D22;&#x5F15;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x8981;&#x63D2;&#x5165;&#x7684;&#x7D22;&#x5F15;&#x3002;</p>
</li>
<li><p><strong>&#x6A21;&#x5757;</strong> &#xFF08; <em>nn.Module</em> &#xFF09; - &#x6A21;&#x5757;&#x63D2;&#x5165;</p>
</li>
</ul>
<h3 id="moduledict">ModuleDict</h3>
<p><em>class</em><code>torch.nn.``ModuleDict</code>( <em>modules=None</em>
)<a href="_modules/torch/nn/modules/container.html#ModuleDict">[source]</a></p>
<p>&#x6301;&#x6709;&#x5B57;&#x5178;&#x5B50;&#x6A21;&#x5757;&#x3002;</p>
<p><code>ModuleDict</code>&#x53EF;&#x4EE5;&#x88AB;&#x7D22;&#x5F15;&#x50CF;&#x4E00;&#x4E2A;&#x666E;&#x901A;&#x7684;Python&#x5B57;&#x5178;&#xFF0C;&#x4F46;&#x5B83;&#x5305;&#x542B;&#x7684;&#x6A21;&#x5757;&#x6B63;&#x786E;&#x6CE8;&#x518C;&#xFF0C;&#x5E76;&#x4E14;&#x5C06;&#x6240;&#x6709; <code>[HTG8&#x53EF;&#x89C1;]&#x6A21;&#x5757;</code>&#x7684;&#x65B9;&#x6CD5;&#x3002;</p>
<p><code>ModuleDict</code>&#x662F;&#x5C0A;&#x91CD;&#x4E86;&#x4E00;&#x4E2A; <strong>&#x6709;&#x5E8F;</strong> &#x5B57;&#x5178;</p>
<ul>
<li><p>&#x63D2;&#x5165;&#x7684;&#x987A;&#x5E8F;&#xFF0C;&#x5E76;</p>
</li>
<li><p>&#x5728; <code>&#x66F4;&#x65B0;&#xFF08;&#xFF09;</code>&#x7684;&#x987A;&#x5E8F;&#x8FDB;&#x884C;&#x5408;&#x5E76;<code>OrderedDict</code>&#x6216;&#x53E6;&#x4E00;&#x4E2A; <code>ModuleDict</code>&#xFF08;&#x5C06;&#x53C2;&#x6570; <code>&#x66F4;&#x65B0;&#xFF08;&#xFF09;</code>&#xFF09;&#x3002;</p>
</li>
</ul>
<p>&#x9700;&#x8981;&#x6CE8;&#x610F;&#x7684;&#x662F; <code>&#x66F4;&#x65B0;&#xFF08;&#xFF09;</code>&#x4E0E;&#x5176;&#x4ED6;&#x65E0;&#x5E8F;&#x6620;&#x5C04;&#x7C7B;&#x578B;&#xFF08;&#x4F8B;&#x5982;&#xFF0C;Python&#x7684;&#x5E73;&#x539F;<code>&#x5FEB;&#x8BD1;&#x901A;</code>&#xFF09;&#x4E0D;&#x4FDD;&#x7559;&#x5408;&#x5E76;&#x540E;&#x7684;&#x6620;&#x5C04;&#x7684;&#x987A;&#x5E8F;&#x3002;</p>
<p>Parameters</p>
<p><strong>&#x6A21;&#x5757;</strong> &#xFF08; <em>&#x53EF;&#x8FED;&#x4EE3;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#xFF08;&#x5B57;&#x7B26;&#x4E32;&#xFF1A;&#x6A21;&#x5757;&#xFF09;&#x7684;&#x6620;&#x5C04;&#xFF08;&#x5B57;&#x5178;&#xFF09;&#x6216;&#x952E; - &#x503C;&#x5BF9;&#x7684;&#x4E00;&#x4E2A;&#x53EF;&#x8FED;&#x4EE3;&#x578B;&#x7684;&#xFF08;&#x5B57;&#x7B26;&#x4E32;&#xFF0C;&#x6A21;&#x5757;&#xFF09;</p>
<p>Example:</p>
<pre><code>class MyModule(nn.Module):
    def __init__(self):
        super(MyModule, self).__init__()
        self.choices = nn.ModuleDict({
                &apos;conv&apos;: nn.Conv2d(10, 10, 3),
                &apos;pool&apos;: nn.MaxPool2d(3)
        })
        self.activations = nn.ModuleDict([
                [&apos;lrelu&apos;, nn.LeakyReLU()],
                [&apos;prelu&apos;, nn.PReLU()]
        ])

    def forward(self, x, choice, act):
        x = self.choices[choice](x)
        x = self.activations[act](x)
        return x
</code></pre><p><code>clear</code>()<a href="_modules/torch/nn/modules/container.html#ModuleDict.clear">[source]</a></p>
<p>&#x53D6;&#x4E0B;ModuleDict&#x7684;&#x6240;&#x6709;&#x9879;&#x76EE;&#x3002;</p>
<p><code>items</code>()<a href="_modules/torch/nn/modules/container.html#ModuleDict.items">[source]</a></p>
<p>&#x8FD4;&#x56DE;ModuleDict&#x952E;/&#x503C;&#x5BF9;&#x7684;&#x8FED;&#x4EE3;&#x3002;</p>
<p><code>keys</code>()<a href="_modules/torch/nn/modules/container.html#ModuleDict.keys">[source]</a></p>
<p>&#x8FD4;&#x56DE;ModuleDict&#x952E;&#x7684;&#x8FED;&#x4EE3;&#x3002;</p>
<p><code>pop</code>( <em>key</em>
)<a href="_modules/torch/nn/modules/container.html#ModuleDict.pop">[source]</a></p>
<p>&#x4ECE;ModuleDict&#x5220;&#x9664;&#x952E;&#x548C;&#x8FD4;&#x56DE;&#x5B83;&#x7684;&#x6A21;&#x5757;&#x3002;</p>
<p>Parameters</p>
<p><strong>&#x952E;</strong> &#xFF08; <em>&#x4E32;</em> &#xFF09; - &#x952E;&#x4ECE;ModuleDict&#x5F39;&#x51FA;</p>
<p><code>update</code>( <em>modules</em>
)<a href="_modules/torch/nn/modules/container.html#ModuleDict.update">[source]</a></p>
<p>&#x66F4;&#x65B0; <code>ModuleDict</code>&#x4E0E;&#x6765;&#x81EA;&#x6620;&#x5C04;&#x952E;&#x503C;&#x5BF9;&#x6216;&#x53EF;&#x8FED;&#x4EE3;&#xFF0C;&#x8986;&#x76D6;&#x73B0;&#x6709;&#x7684;&#x5BC6;&#x94A5;&#x3002;</p>
<p>Note</p>
<p>&#x5982;&#x679C;<code>&#x6A21;&#x5757;</code>&#x662F;<code>OrderedDict</code>&#xFF0C;A<code>ModuleDict</code>&#x6216;&#x952E; - &#x503C;&#x5BF9;&#x7684;&#x4E00;&#x4E2A;&#x53EF;&#x8FED;&#x4EE3;&#xFF0C;&#x5B83;&#x5728;&#x65B0;&#x7684;&#x5143;&#x7D20;&#x7684;&#x987A;&#x5E8F;&#x88AB;&#x4FDD;&#x7559;&#x3002;</p>
<p>Parameters</p>
<p><strong>&#x6A21;&#x5757;</strong> &#xFF08; <em>&#x53EF;&#x8FED;&#x4EE3;</em> &#xFF09; - &#x6620;&#x5C04;&#xFF08;&#x5B57;&#x5178;&#xFF09;&#x4ECE;&#x5B57;&#x7B26;&#x4E32; <code>&#x6A21;&#x5757;</code>&#xFF0C;&#x6216;&#x5BC6;&#x94A5;&#x7684;&#x8FED;&#x4EE3;&#x503C;&#x5BF9;&#x7C7B;&#x578B;&#xFF08;&#x5B57;&#x7B26;&#x4E32;&#xFF0C; <code>&#x6A21;&#x5757;</code>&#xFF09;</p>
<p><code>values</code>()<a href="_modules/torch/nn/modules/container.html#ModuleDict.values">[source]</a></p>
<p>&#x8FD4;&#x56DE;ModuleDict&#x503C;&#x7684;&#x8FED;&#x4EE3;&#x3002;</p>
<h3 id="&#x53C2;&#x6570;&#x5217;&#x8868;">&#x53C2;&#x6570;&#x5217;&#x8868;</h3>
<p><em>class</em><code>torch.nn.``ParameterList</code>( <em>parameters=None</em>
)<a href="_modules/torch/nn/modules/container.html#ParameterList">[source]</a></p>
<p>&#x6301;&#x6709;&#x5217;&#x8868;&#x53C2;&#x6570;&#x3002;</p>
<p><code>&#x53C2;&#x6570;&#x5217;&#x8868;</code>&#x53EF;&#x4EE5;&#x88AB;&#x7D22;&#x5F15;&#x50CF;&#x4E00;&#x4E2A;&#x666E;&#x901A;&#x7684;Python&#x5217;&#x8868;&#xFF0C;&#x4F46;&#x53C2;&#x6570;&#x5B83;&#x6240;&#x5305;&#x542B;&#x6B63;&#x786E;&#x6CE8;&#x518C;&#xFF0C;&#x5E76;&#x4E14;&#x5C06;&#x6240;&#x6709; <code>[HTG8&#x53EF;&#x89C1;]&#x6A21;&#x5757;</code>&#x7684;&#x65B9;&#x6CD5;&#x3002;</p>
<p>Parameters</p>
<p><strong>&#x53C2;&#x6570;</strong> &#xFF08; <em>&#x53EF;&#x8FED;&#x4EE3;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x7684; <code>&#x53C2;&#x6570;&#x53EF;&#x8FED;&#x4EE3;</code>&#x6DFB;&#x52A0;</p>
<p>Example:</p>
<pre><code>class MyModule(nn.Module):
    def __init__(self):
        super(MyModule, self).__init__()
        self.params = nn.ParameterList([nn.Parameter(torch.randn(10, 10)) for i in range(10)])

    def forward(self, x):
        # ParameterList can act as an iterable, or be indexed using ints
        for i, p in enumerate(self.params):
            x = self.params[i // 2].mm(x) + p.mm(x)
        return x
</code></pre><p><code>append</code>( <em>parameter</em>
)<a href="_modules/torch/nn/modules/container.html#ParameterList.append">[source]</a></p>
<p>&#x9644;&#x52A0;&#x5728;&#x5217;&#x8868;&#x7684;&#x6700;&#x540E;&#x4E00;&#x4E2A;&#x7ED9;&#x5B9A;&#x7684;&#x53C2;&#x6570;&#x3002;</p>
<p>Parameters</p>
<p><strong>&#x53C2;&#x6570;</strong> &#xFF08; <em>nn.Parameter</em> &#xFF09; - &#x53C2;&#x6570;&#x5230;&#x9644;&#x52A0;</p>
<p><code>extend</code>( <em>parameters</em>
)<a href="_modules/torch/nn/modules/container.html#ParameterList.extend">[source]</a></p>
<p>&#x4ECE;&#x8FFD;&#x52A0;&#x4E00;&#x4E2A;Python&#x53C2;&#x6570;&#x8FED;&#x4EE3;&#x5230;&#x5217;&#x8868;&#x7684;&#x672B;&#x5C3E;&#x3002;</p>
<p>Parameters</p>
<p><strong>&#x53C2;&#x6570;</strong> &#xFF08; <em>&#x53EF;&#x8FED;&#x4EE3;</em> &#xFF09; - &#x7684;&#x53C2;&#x6570;&#x53EF;&#x8FED;&#x4EE3;&#x5230;&#x8FFD;&#x52A0;</p>
<h3 id="parameterdict">ParameterDict</h3>
<p><em>class</em><code>torch.nn.``ParameterDict</code>( <em>parameters=None</em>
)<a href="_modules/torch/nn/modules/container.html#ParameterDict">[source]</a></p>
<p>&#x62E5;&#x6709;&#x4E00;&#x672C;&#x5B57;&#x5178;&#x7684;&#x53C2;&#x6570;&#x3002;</p>
<p>ParameterDict&#x80FD;&#x591F;&#x88AB;&#x7D22;&#x5F15;&#x50CF;&#x4E00;&#x4E2A;&#x666E;&#x901A;&#x7684;Python&#x5B57;&#x5178;&#xFF0C;&#x4F46;&#x53C2;&#x6570;&#x5B83;&#x6240;&#x5305;&#x542B;&#x6B63;&#x786E;&#x6CE8;&#x518C;&#xFF0C;&#x5E76;&#x4E14;&#x5C06;&#x6240;&#x6709;&#x6A21;&#x5757;&#x7684;&#x65B9;&#x6CD5;&#x53EF;&#x89C1;&#x3002;</p>
<p><code>ParameterDict</code>&#x662F;&#x5C0A;&#x91CD;&#x4E86;&#x4E00;&#x4E2A; <strong>&#x6709;&#x5E8F;</strong> &#x5B57;&#x5178;</p>
<ul>
<li><p>the order of insertion, and</p>
</li>
<li><p>&#x5728; <code>&#x66F4;&#x65B0;&#xFF08;&#xFF09;</code>&#x7684;&#x987A;&#x5E8F;&#x8FDB;&#x884C;&#x5408;&#x5E76;<code>OrderedDict</code>&#x6216;&#x53E6;&#x4E00;&#x4E2A; <code>ParameterDict</code>&#xFF08;&#x5C06;&#x53C2;&#x6570; <code>&#x66F4;&#x65B0;&#xFF08;&#xFF09;</code>&#xFF09;&#x3002;</p>
</li>
</ul>
<p>&#x9700;&#x8981;&#x6CE8;&#x610F;&#x7684;&#x662F; <code>&#x66F4;&#x65B0;&#xFF08;&#xFF09;</code>&#x4E0E;&#x5176;&#x4ED6;&#x65E0;&#x5E8F;&#x6620;&#x5C04;&#x7C7B;&#x578B;&#xFF08;&#x4F8B;&#x5982;&#xFF0C;Python&#x7684;&#x5E73;&#x539F;<code>&#x5FEB;&#x8BD1;&#x901A;</code>&#xFF09;&#x4E0D;&#x4FDD;&#x7559;&#x5408;&#x5E76;&#x540E;&#x7684;&#x6620;&#x5C04;&#x7684;&#x987A;&#x5E8F;&#x3002;</p>
<p>Parameters</p>
<p><strong>&#x53C2;&#x6570;</strong> &#xFF08; <em>&#x53EF;&#x8FED;&#x4EE3;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#xFF08;&#x4E32;&#x7684;&#x6620;&#x5C04;&#xFF08;&#x5B57;&#x5178;&#xFF09;&#xFF1A; <code>&#x53C2;&#x6570;</code>&#xFF09;&#x6216;&#x7C7B;&#x578B;&#x7684;&#x952E; - &#x503C;&#x5BF9;&#xFF08;&#x4E32;&#x7684;&#x8FED;&#x4EE3;&#xFF0C; <code>&#x53C2;&#x6570;</code>&#xFF09;</p>
<p>Example:</p>
<pre><code>class MyModule(nn.Module):
    def __init__(self):
        super(MyModule, self).__init__()
        self.params = nn.ParameterDict({
                &apos;left&apos;: nn.Parameter(torch.randn(5, 10)),
                &apos;right&apos;: nn.Parameter(torch.randn(5, 10))
        })

    def forward(self, x, choice):
        x = self.params[choice].mm(x)
        return x
</code></pre><p><code>clear</code>()<a href="_modules/torch/nn/modules/container.html#ParameterDict.clear">[source]</a></p>
<p>&#x53D6;&#x4E0B;ParameterDict&#x7684;&#x6240;&#x6709;&#x9879;&#x76EE;&#x3002;</p>
<p><code>items</code>()<a href="_modules/torch/nn/modules/container.html#ParameterDict.items">[source]</a></p>
<p>&#x8FD4;&#x56DE;ParameterDict&#x952E;/&#x503C;&#x5BF9;&#x7684;&#x8FED;&#x4EE3;&#x3002;</p>
<p><code>keys</code>()<a href="_modules/torch/nn/modules/container.html#ParameterDict.keys">[source]</a></p>
<p>&#x8FD4;&#x56DE;ParameterDict&#x952E;&#x7684;&#x8FED;&#x4EE3;&#x3002;</p>
<p><code>pop</code>( <em>key</em>
)<a href="_modules/torch/nn/modules/container.html#ParameterDict.pop">[source]</a></p>
<p>&#x4ECE;ParameterDict&#x5220;&#x9664;&#x952E;&#x548C;&#x8FD4;&#x56DE;&#x5B83;&#x7684;&#x53C2;&#x6570;&#x3002;</p>
<p>Parameters</p>
<p><strong>&#x952E;</strong> &#xFF08; <em>&#x4E32;</em> &#xFF09; - &#x952E;&#x4ECE;ParameterDict&#x5F39;&#x51FA;</p>
<p><code>update</code>( <em>parameters</em>
)<a href="_modules/torch/nn/modules/container.html#ParameterDict.update">[source]</a></p>
<p>&#x66F4;&#x65B0; <code>ParameterDict</code>&#x4E0E;&#x6765;&#x81EA;&#x6620;&#x5C04;&#x952E;&#x503C;&#x5BF9;&#x6216;&#x53EF;&#x8FED;&#x4EE3;&#xFF0C;&#x8986;&#x76D6;&#x73B0;&#x6709;&#x7684;&#x5BC6;&#x94A5;&#x3002;</p>
<p>Note</p>
<p>&#x5982;&#x679C;<code>&#x53C2;&#x6570;</code>&#x662F;<code>OrderedDict</code>&#xFF0C;A<code>ParameterDict</code>&#x6216;&#x952E; - &#x503C;&#x5BF9;&#x7684;&#x4E00;&#x4E2A;&#x53EF;&#x8FED;&#x4EE3;&#xFF0C;&#x5B83;&#x5728;&#x65B0;&#x7684;&#x5143;&#x7D20;&#x7684;&#x987A;&#x5E8F;&#x88AB;&#x4FDD;&#x7559;&#x3002;</p>
<p>Parameters</p>
<p><strong>&#x53C2;&#x6570;</strong> &#xFF08; <em>&#x53EF;&#x8FED;&#x4EE3;</em> &#xFF09; - &#x4E00;&#x4E2A;&#x4ECE;&#x4E32;&#x6620;&#x5C04;&#xFF08;&#x5B57;&#x5178;&#xFF09;&#x4E3A; <code>&#x53C2;&#x6570;</code>&#xFF0C;&#x6216;&#x5BC6;&#x94A5;&#x7684;&#x8FED;&#x4EE3;&#x503C;&#x5BF9;&#x7C7B;&#x578B;&#xFF08;&#x5B57;&#x7B26;&#x4E32;&#xFF0C; <code>&#x53C2;&#x6570;</code>&#xFF09;</p>
<p><code>values</code>()<a href="_modules/torch/nn/modules/container.html#ParameterDict.values">[source]</a></p>
<p>&#x8FD4;&#x56DE;ParameterDict&#x503C;&#x7684;&#x8FED;&#x4EE3;&#x3002;</p>
<h2 id="&#x5377;&#x79EF;&#x5C42;">&#x5377;&#x79EF;&#x5C42;</h2>
<h3 id="conv1d">Conv1d</h3>
<p><em>class</em><code>torch.nn.``Conv1d</code>( <em>in_channels</em> , <em>out_channels</em> , <em>kernel_size</em> ,
<em>stride=1</em> , <em>padding=0</em> , <em>dilation=1</em> , <em>groups=1</em> , <em>bias=True</em> ,
<em>padding_mode=&apos;zeros&apos;</em>
)<a href="_modules/torch/nn/modules/conv.html#Conv1d">[source]</a></p>
<p>&#x65BD;&#x52A0;1D&#x5377;&#x79EF;&#x5728;&#x51E0;&#x4E2A;&#x8F93;&#x5165;&#x5E73;&#x9762;&#x7EC4;&#x6210;&#x7684;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x3002;</p>
<p>&#x5728;&#x6700;&#x7B80;&#x5355;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x6240;&#x8FF0;&#x5C42;&#x7684;&#x4E0E;&#x8F93;&#x5165;&#x5927;&#x5C0F; &#x7684;&#x8F93;&#x51FA;&#x503C;&#xFF08; N  C  &#x5728; &#xFF0C; L  &#xFF09; &#xFF08;N&#xFF0C;C <em> {\&#x6587;&#x672C;{&#x5728;}}&#xFF0C;L&#xFF09; &#xFF08; N  &#xFF0C; C  &#x5728; &#xFF0C; L  &#xFF09;
&#x548C;&#x8F93;&#x51FA; &#xFF08; N  &#xFF0C; C  OUT  &#xFF0C; L  OUT  &#xFF09;  &#xFF08;N&#xFF0C;C </em> {\&#x6587;&#x672C;{&#x51FA;}}&#xFF0C;L _ {\&#x6587;&#x672C;{&#x51FA;}}&#xFF09; &#xFF08; N  &#xFF0C; C  OUT  &#xFF0C;
&#x5927;&#x53F7; OUT  &#xFF09; &#x53EF;&#x4EE5;&#x7CBE;&#x786E;&#x5730;&#x63CF;&#x8FF0;&#x4E3A;&#xFF1A;</p>
<p>out(Ni,Coutj)=bias(Coutj)+&#x2211;k=0Cin&#x2212;1weight(Coutj,k)&#x22C6;input(Ni,k)\text{out}(N<em>i,
C</em>{\text{out}<em>j}) = \text{bias}(C</em>{\text{out}<em>j}) + \sum</em>{k = 0}^{C<em>{in} - 1}
\text{weight}(C</em>{\text{out}_j}, k) \star \text{input}(N_i, k)
out(Ni&#x200B;,Coutj&#x200B;&#x200B;)=bias(Coutj&#x200B;&#x200B;)+k=0&#x2211;Cin&#x200B;&#x2212;1&#x200B;weight(Coutj&#x200B;&#x200B;,k)&#x22C6;input(Ni&#x200B;,k)</p>
<p>&#x5176;&#x4E2D; &#x22C6; \&#x661F; &#x22C6; &#x662F;&#x6709;&#x6548;&#x7684;<a href="https://en.wikipedia.org/wiki/Cross-correlation" target="_blank">&#x4E92;&#x76F8;&#x5173;</a>&#x8FD0;&#x7B97;&#x7B26;&#xFF0C; N  N
N  &#x662F;&#x4E00;&#x4E2A;&#x6279;&#x91CF;&#x5927;&#x5C0F;&#xFF0C; C  C  C  &#x8868;&#x793A;&#x7684;&#x6570;&#x7684;&#x4FE1;&#x9053;&#xFF0C; L  L  L  &#x662F;&#x4EBA;&#x4FE1;&#x53F7;&#x5E8F;&#x5217;&#x7684;ength&#x3002;</p>
<ul>
<li><p><code>&#x6B65;&#x5E45;</code>&#x63A7;&#x5236;&#x7528;&#x4E8E;&#x4EA4;&#x53C9;&#x76F8;&#x5173;&#xFF0C;&#x5355;&#x4E2A;&#x6570;&#x5B57;&#x6216;&#x4E00;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x5143;&#x7EC4;&#x7684;&#x6B65;&#x5E45;&#x3002;</p>
</li>
<li><p><code>&#x586B;&#x5145;</code>&#x63A7;&#x5236;&#x9690;&#x542B;&#x96F6;&#x586B;&#x8865;&#x5904;&#x7406;&#x7684;&#x53CC;&#x65B9;&#x7684;&#x91CF;&#x4E3A;<code>&#x586B;&#x5145;</code>&#x70B9;&#x6570;&#x3002;</p>
</li>
<li><p><code>&#x6269;&#x5F20;</code>&#x63A7;&#x5236;&#x5185;&#x6838;&#x70B9;&#x4E4B;&#x95F4;&#x7684;&#x95F4;&#x9694;;&#x4E5F;&#x88AB;&#x79F0;&#x4E3A;&#x5288;&#x7A97;&#x7B97;&#x6CD5;&#x3002;&#x8FD9;&#x662F;&#x5F88;&#x96BE;&#x5F62;&#x5BB9;&#xFF0C;&#x4F46;&#x8FD9;&#x79CD;<a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md" target="_blank">&#x94FE;&#x63A5;</a>&#x6709;&#x4EC0;&#x4E48;<code>&#x6269;&#x5F20;</code>&#x505A;&#x4E00;&#x4E2A;&#x5F88;&#x597D;&#x7684;&#x53EF;&#x89C6;&#x5316;&#x3002;</p>
</li>
<li><p><code>&#x57FA;&#x56E2;</code>&#x63A7;&#x5236;&#x8F93;&#x5165;&#x548C;&#x8F93;&#x51FA;&#x4E4B;&#x95F4;&#x7684;&#x8FDE;&#x63A5;&#x3002; <code>in_channels</code>&#x548C;<code>out_channels</code>&#x5FC5;&#x987B;&#x90FD;&#x662F;&#x7531;<code>&#x57FA;&#x56E2;</code>&#x6574;&#x9664;&#x3002;&#x4F8B;&#x5982;&#xFF0C;</p>
</li>
</ul>
<blockquote>
<pre><code>* &#x5728;&#x57FA;&#x56E2;= 1&#xFF0C;&#x6240;&#x6709;&#x7684;&#x8F93;&#x5165;&#x88AB;&#x5377;&#x79EF;&#x4EE5;&#x6240;&#x6709;&#x8F93;&#x51FA;&#x3002;
</code></pre></blockquote>
<p>&gt;</p>
<blockquote>
<pre><code>* &#x5728;&#x7EC4;= 2&#xFF0C;&#x64CD;&#x4F5C;&#x53D8;&#x5F97;&#x7B49;&#x6548;&#x4E8E;&#x5177;&#x6709;&#x7531;&#x4E00;&#x4FA7;&#x4E0A;&#x7684;&#x4E24;&#x4E2A;CONV&#x5C42;&#x4FA7;&#xFF0C;&#x6BCF;&#x4E2A;&#x770B;&#x5230;&#x4E00;&#x534A;&#x7684;&#x8F93;&#x5165;&#x901A;&#x9053;&#xFF0C;&#x548C;&#x4EA7;&#x751F;&#x4E00;&#x534A;&#x7684;&#x8F93;&#x51FA;&#x901A;&#x9053;&#xFF0C;&#x5E76;&#x4E14;&#x4E24;&#x4E2A;&#x968F;&#x540E;&#x8FDE;&#x63A5;&#x5728;&#x4E00;&#x8D77;&#x3002;
</code></pre></blockquote>
<p>&gt;</p>
<blockquote>
<pre><code>* &#x5728;&#x57FA;&#x56E2;= `in_channels`&#x4E2D;&#xFF0C;&#x6BCF;&#x4E2A;&#x8F93;&#x5165;&#x4FE1;&#x9053;&#x8FDB;&#x884C;&#x5377;&#x79EF;&#x4EE5;&#x5B83;&#x81EA;&#x5DF1;&#x7684;&#x4E00;&#x5957;&#x8FC7;&#x6EE4;&#x5668;&#xFF0C;&#x5927;&#x5C0F;&#x7684; &#x230A; O  U  T  _  C  H  &#x4E00;
</code></pre><p>n&#x7684; n&#x7684; E  L  S  i&#x7684; n&#x7684; <em>  C  H  &#x4E00; n&#x7684; n&#x7684; E  L  S  &#x230B; \&#x5DE6;\ lfloor \&#x538B;&#x88C2;{&#x51FA;\ _channels}
{&#x5728;\ _channels} \&#x53F3;\ rfloor  &#x230A; i&#x7684; n&#x7684; </em>  C  H  &#x4E00; n&#x7684; n&#x7684; E  L  S  O  U  T  _  C  H
&#x4E00; n&#x7684; n&#x7684; E  L  S  &#x230B; &#x3002;</p>
</blockquote>
<p>Note</p>
<p>&#x6839;&#x636E;&#x4F60;&#x7684;&#x5185;&#x6838;&#x7684;&#x5927;&#x5C0F;&#xFF0C;&#x51E0;&#x4E2A;&#xFF08;&#x6700;&#x540E;&#xFF09;&#x8F93;&#x5165;&#x7684;&#x5217;&#x53EF;&#x80FD;&#x4F1A;&#x4E22;&#x5931;&#xFF0C;&#x56E0;&#x4E3A;&#x5B83;&#x662F;&#x4E00;&#x4E2A;&#x6709;&#x6548;&#x7684;<a href="https://en.wikipedia.org/wiki/Cross-
correlation" target="_blank">&#x4E92;&#x76F8;&#x5173;</a>&#xFF0C;&#x800C;&#x4E0D;&#x662F;&#x4E00;&#x4E2A;&#x5B8C;&#x6574;&#x7684;<a href="https://en.wikipedia.org/wiki/Cross-correlation" target="_blank">&#x4E92;&#x76F8;&#x5173;</a>
&#x3002;&#x5B83;&#x662F;&#x7531;&#x7528;&#x6237;&#x6DFB;&#x52A0;&#x9002;&#x5F53;&#x7684;&#x586B;&#x5145;&#x3002;</p>
<p>Note</p>
<p>&#x5F53;&#x57FA;&#x56E2;== in_channels &#x548C; out_channels == K * in_channels &#xFF0C;&#x5176;&#x4E2D; K
&#x662F;&#x4E00;&#x4E2A;&#x6B63;&#x6574;&#x6570;&#xFF0C;&#x8BE5;&#x64CD;&#x4F5C;&#x4E5F;&#x88AB;&#x79F0;&#x4E3A;&#x5728;&#x6587;&#x732E;&#x4E2D;&#x4F5C;&#x4E3A;&#x6DF1;&#x5EA6;&#x65B9;&#x5411;&#x5377;&#x79EF;&#x3002;</p>
<p>&#x6362;&#x53E5;&#x8BDD;&#x8BF4;&#xFF0C;&#x4E3A;&#x7684;&#x5927;&#x5C0F; &#xFF08; N  &#xFF0C;&#x8F93;&#x5165; C  i&#x7684; n&#x7684; &#xFF0C; L  i&#x7684; n&#x7684; &#xFF09; &#xFF08;N&#xFF0C;C<em> {IN}&#xFF0C;L</em> {&#x5728;}&#xFF09; &#xFF08; N  &#xFF0C; C  i&#x7684; n&#x7684; &#xFF0C;
L  &#x4E00;&#x4E16;  n&#x7684; &#xFF09; &#xFF0C;&#x5177;&#x6709;&#x6DF1;&#x5EA6;&#x65B9;&#x5411;&#x4E58;&#x6CD5;&#x5668;&#x6DF1;&#x5EA6;&#x65B9;&#x5411;&#x5377;&#x79EF; K &#xFF0C;&#x53EF;&#x901A;&#x8FC7;&#x53C2;&#x6570;[&#x6784;&#x9020;HTG130]  &#xFF08; C  &#x5728; =  C  i&#x7684; n&#x7684; &#xFF0C; C  OUT
=  C  i&#x7684; n&#x7684; &#xD7; K  &#xFF0C; &#x3002;  &#x3002;  &#x3002;  &#xFF0C; &#x57FA;&#x56E2; =  C  i&#x7684; n&#x7684; &#xFF09; &#xFF08;C<em> \&#x6587;&#x672C;{IN} = C</em> {}&#x4E2D;&#xFF0C;C<em> \&#x6587;&#x672C;{&#x51FA;}
= C</em> {&#x5728;} \&#x500D;K&#xFF0C;.. &#x3002;&#xFF0C;\&#x6587;&#x672C;{&#x57FA;} = C_ {&#x5728;}&#xFF09; &#xFF08; C  &#x5728; =  C  &#x200B;&#x200B; i&#x7684; n&#x7684; &#xFF0C; C  OUT  =  C  i&#x7684; n&#x7684;
&#xD7; K  &#xFF0C; &#x3002;  &#x3002;  &#x3002;  &#xFF0C; &#x57FA;&#x56E2; =  C  I  n&#x7684; &#xFF09; &#x3002;</p>
<p>Note</p>
<p>&#x5728;&#x4F7F;&#x7528;CUDA&#x540E;&#x7AEF;&#x4E0E;CuDNN&#x5F53;&#x67D0;&#x4E9B;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x8FD9;&#x79CD;&#x64CD;&#x4F5C;&#x8005;&#x53EF;&#x4EE5;&#x9009;&#x62E9;&#x4E0D;&#x786E;&#x5B9A;&#x6027;&#x7684;&#x7B97;&#x6CD5;&#x6765;&#x63D0;&#x9AD8;&#x6027;&#x80FD;&#x3002;&#x5982;&#x679C;&#x8FD9;&#x662F;&#x4E0D;&#x53EF;&#x53D6;&#x7684;&#xFF0C;&#x4F60;&#x53EF;&#x4EE5;&#x5C1D;&#x8BD5;&#x901A;&#x8FC7;&#x8BBE;&#x7F6E;<code>torch.backends.cudnn.deterministic  =  &#x771F;[&#x4F7F;&#x64CD;&#x4F5C;&#x786E;&#x5B9A;&#x6027;&#xFF08;&#x53EF;&#x80FD;&#x4EE5;&#x6027;&#x80FD;&#x4E3A;&#x4EE3;&#x4EF7;&#xFF09; HTG6]</code>&#x3002;&#x8BF7;&#x53C2;&#x9605;<a href="notes/randomness.html"> &#x91CD;&#x590D;&#x6027;
</a>&#x4E3A;&#x80CC;&#x666F;&#x7684;&#x97F3;&#x7B26;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p>&#x5728;&#x8F93;&#x5165;&#x56FE;&#x50CF;&#x4E2D;&#x901A;&#x9053;&#x6570; - <strong>in_channels</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09;</p>
</li>
<li><p><strong>out_channels</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x7531;&#x5377;&#x79EF;&#x4EA7;&#x751F;&#x901A;&#x9053;&#x6570;</p>
</li>
<li><p><strong>kernel_size</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#x6216;</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>&#x5143;&#x7EC4;</em> </a>&#xFF09; - &#x7684;&#x5377;&#x79EF;&#x5185;&#x6838;&#x7684;&#x5927;&#x5C0F;</p>
</li>
<li><p><strong>&#x6B65;&#x5E45;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#x6216;</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>&#x5143;&#x7EC4;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;&#x7684;</em> &#xFF09; - &#x5377;&#x79EF;&#x7684;&#x6B65;&#x5E45;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;1</p>
</li>
<li><p><strong>&#x586B;&#x5145;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#x6216;</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>&#x5143;&#x7EC4;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;&#x7684;</em> &#xFF09; - &#x8865;&#x96F6;&#x52A0;&#x5230;&#x8F93;&#x5165;&#x7684;&#x4E24;&#x4FA7;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;0</p>
</li>
<li><p><strong>padding_mode</strong> &#xFF08; <em>&#x4E32;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x96F6;</p>
</li>
<li><p><strong>&#x6269;&#x5F20;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#x6216;</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>&#x5143;&#x7EC4;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;&#x7684;</em> &#xFF09; - &#x5185;&#x6838;&#x5143;&#x4EF6;&#x4E4B;&#x95F4;&#x7684;&#x95F4;&#x9694;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;1</p>
</li>
<li><p><strong>&#x57FA;&#x56E2;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x4ECE;&#x8F93;&#x5165;&#x4FE1;&#x9053;&#x5230;&#x8F93;&#x51FA;&#x901A;&#x9053;&#x963B;&#x585E;&#x7684;&#x8FDE;&#x63A5;&#x7684;&#x6570;&#x76EE;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;1</p>
</li>
<li><p><strong>&#x504F;&#x538B;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5982;&#x679C;<code>&#x771F;</code>&#xFF0C;&#x589E;&#x52A0;&#x4E86;&#x4E00;&#x4E2A;&#x53EF;&#x5B66;&#x4E60;&#x504F;&#x538B;&#x5230;&#x8F93;&#x51FA;&#x7AEF;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x771F;</code></p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>&#x8F93;&#x5165;&#xFF1A; &#xFF08; N  &#xFF0C; C  i&#x7684; n&#x7684; &#xFF0C; L  i&#x7684; n&#x7684; &#xFF09; &#xFF08;N&#xFF0C;C<em> {IN}&#xFF0C;L</em> {&#x5728;}&#xFF09; &#xFF08; N  &#xFF0C; C  i&#x7684; n&#x7684; &#xFF0C; L  i&#x7684; n&#x7684; &#xFF09;</p>
</li>
<li><p>&#x8F93;&#x51FA;&#xFF1A; &#xFF08; N  &#xFF0C; C  O  U  T  &#xFF0C; L  O  U  T  &#xFF09; &#xFF08;N&#xFF0C;C<em> {&#x51FA;}&#xFF0C;L</em> {&#x51FA;}&#xFF09; &#xFF08; N  &#xFF0C; C  O  U  T  &#xFF0C; L  [HTG1 05]  O  U  T  &#xFF09; &#x5176;&#x4E2D;</p>
</li>
</ul>
<p>Lout=&#x230A;Lin+2&#xD7;padding&#x2212;dilation&#xD7;(kernel<em>size&#x2212;1)&#x2212;1stride+1&#x230B;L</em>{out} =
\left\lfloor\frac{L_{in} + 2 \times \text{padding} - \text{dilation} \times
(\text{kernel\_size} - 1) - 1}{\text{stride}} + 1\right\rfloor
Lout&#x200B;=&#x230A;strideLin&#x200B;+2&#xD7;padding&#x2212;dilation&#xD7;(kernel_size&#x2212;1)&#x2212;1&#x200B;+1&#x230B;</p>
<p>Variables</p>
<ul>
<li><p><strong>&#x301C;Conv1d.weight</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x7684;&#x5F62;&#x72B6; [&#x8BE5;&#x6A21;&#x5757;&#x7684;&#x53EF;&#x5B66;&#x4E60;&#x6743;&#x91CD;HTG11] &#xFF08; out<em>channels  &#xFF0C; in_channels  &#x57FA;&#x56E2; &#xFF0C; kernel_size  &#xFF09; &#xFF08;\&#x6587;&#x672C;{&#x51FA;\ _channels}&#xFF0C;\&#x538B;&#x88C2;{\&#x6587;&#x672C;{&#x5728;\ _channels}} {\&#x6587;&#x672C;{&#x57FA;}}&#xFF0C;\&#x6587;&#x672C;{&#x5185;&#x6838;\ _size}&#xFF09; &#xFF08; out_channels  &#xFF0C; &#x57FA;&#x56E2; in_channels  [H TG86]  &#xFF0C; kernel_size  &#xFF09; &#x3002;&#x8FD9;&#x4E9B;&#x6743;&#x91CD;&#x7684;&#x503C;&#x662F;&#x4ECE; &#x53D6;&#x6837;U  &#xFF08; -  K  &#xFF0C; K  &#xFF09; \ mathcal&#x3010;U}&#xFF08; - \ SQRT {K}&#xFF0C;\ SQRT {K}&#xFF09; U  &#xFF08; -  K  &#xFF0C; K  [H TG206]  &#xFF09; &#x5176;&#x4E2D; K  =  1  C  &#x5728; *  kernel_size  K = \&#x538B;&#x88C2;{1} {C</em> \&#x6587;&#x672C;{IN} <em> \&#x6587;&#x672C;{&#x5185;&#x6838;\ _size}}  &#x200B;&#x200B;  K  =  &#xE7;  &#x5728; </em>  kernel_size  1 </p>
</li>
<li><p><strong>&#x301C;Conv1d.bias</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x5F62;&#x72B6;&#xFF08;out<em>channels&#xFF09;&#x7684;&#x6A21;&#x5757;&#x7684;&#x53EF;&#x5B66;&#x4E60;&#x504F;&#x5DEE;&#x3002;&#x5982;&#x679C;<code>&#x504F;&#x538B;</code>&#x662F;<code>&#x771F;</code>&#xFF0C;&#x7136;&#x540E;&#x8FD9;&#x4E9B;&#x6743;&#x91CD;&#x7684;&#x503C;&#x662F;&#x4ECE; &#x53D6;&#x6837; U  &#xFF08; -  K  &#xFF0C; K  &#xFF09; \ mathcal&#x3010;U}&#xFF08; - \ SQRT {K}&#xFF0C;\ SQRT {K}&#xFF09; U  &#xFF08; -  K  K  &#xFF09; &#x5176;&#x4E2D; K  =  1  C  &#x5728; *  kernel_size  K = \&#x538B;&#x88C2;{1} {C</em> \&#x6587;&#x672C;{IN} <em> \&#x6587;&#x672C;{&#x5185;&#x6838;\ _size}}  K  =  C  &#x5728; </em>  kernel_size  1 </p>
</li>
</ul>
<p>&#x4F8B;&#x5B50;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; m = nn.Conv1d(16, 33, 3, stride=2)
&gt;&gt;&gt; input = torch.randn(20, 16, 50)
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="conv2d">Conv2d</h3>
<p><em>class</em><code>torch.nn.``Conv2d</code>( <em>in_channels</em> , <em>out_channels</em> , <em>kernel_size</em> ,
<em>stride=1</em> , <em>padding=0</em> , <em>dilation=1</em> , <em>groups=1</em> , <em>bias=True</em> ,
<em>padding_mode=&apos;zeros&apos;</em>
)<a href="_modules/torch/nn/modules/conv.html#Conv2d">[source]</a></p>
<p>&#x65BD;&#x52A0;&#x4E8C;&#x7EF4;&#x5377;&#x79EF;&#x5728;&#x51E0;&#x4E2A;&#x8F93;&#x5165;&#x5E73;&#x9762;&#x7EC4;&#x6210;&#x7684;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x3002;</p>
<p>&#x5728;&#x6700;&#x7B80;&#x5355;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x6240;&#x8FF0;&#x5C42;&#x7684;&#x4E0E;&#x8F93;&#x5165;&#x5927;&#x5C0F; &#x7684;&#x8F93;&#x51FA;&#x503C;&#xFF08; N  C  &#x5728; &#xFF0C; H  &#xFF0C; W  &#xFF09; &#xFF08;N&#xFF0C;C <em> {\&#x6587;&#x672C;{&#x5728;}}&#xFF0C;H&#xFF0C;W&#xFF09; &#xFF08; N  &#xFF0C; C  &#x5728;
&#xFF0C; H  &#xFF0C; W  &#xFF09; &#x548C;&#x8F93;&#x51FA; &#xFF08; N  &#xFF0C; C  OUT  &#xFF0C; H  OUT  &#xFF0C; W  OUT  &#xFF09; &#xFF08;N&#xFF0C;C </em> {\&#x6587;&#x672C;{&#x51FA;}}&#xFF0C;H <em>
{\&#x6587;&#x672C;{&#x51FA;}}&#xFF0C;W </em> {\&#x6587;&#x672C;{&#x51FA;}} &#xFF09; &#xFF08; N  &#xFF0C; C  OUT  &#xFF0C; H  OUT  &#xFF0C; W  OUT  &#xFF09; &#x53EF;&#x4EE5;&#x7CBE;&#x786E;&#x5730;&#x63CF;&#x8FF0;&#x4E3A;&#xFF1A;</p>
<p>out(Ni,Coutj)=bias(Coutj)+&#x2211;k=0Cin&#x2212;1weight(Coutj,k)&#x22C6;input(Ni,k)\text{out}(N<em>i,
C</em>{\text{out}<em>j}) = \text{bias}(C</em>{\text{out}<em>j}) + \sum</em>{k =
0}^{C<em>{\text{in}} - 1} \text{weight}(C</em>{\text{out}_j}, k) \star
\text{input}(N_i, k)
out(Ni&#x200B;,Coutj&#x200B;&#x200B;)=bias(Coutj&#x200B;&#x200B;)+k=0&#x2211;Cin&#x200B;&#x2212;1&#x200B;weight(Coutj&#x200B;&#x200B;,k)&#x22C6;input(Ni&#x200B;,k)</p>
<p>&#x5176;&#x4E2D; &#x22C6; \&#x661F; &#x22C6; &#x662F;&#x6709;&#x6548;&#x7684;2D <a href="https://en.wikipedia.org/wiki/Cross-correlation" target="_blank">&#x4E92;&#x76F8;&#x5173;</a>&#x8FD0;&#x7B97;&#x7B26;&#xFF0C; N
N  N  &#x662F;&#x4E00;&#x4E2A;&#x6279;&#x91CF;&#x5927;&#x5C0F;&#xFF0C; C  C  C  &#x8868;&#x793A;&#x7684;&#x6570;&#x4FE1;&#x9053;&#xFF0C; H  H  H  &#x662F;&#x4EE5;&#x50CF;&#x7D20;&#x4E3A;&#x5355;&#x4F4D;&#x8F93;&#x5165;&#x5E73;&#x9762;&#x7684;&#x9AD8;&#x5EA6;&#xFF0C;&#x5E76; W  W  W  &#x662F;&#x4EE5;&#x50CF;&#x7D20;&#x4E3A;&#x5355;&#x4F4D;&#x7684;&#x5BBD;&#x5EA6;&#x3002;</p>
<ul>
<li><p><code>&#x6B65;&#x5E45;</code>&#x63A7;&#x5236;&#x7528;&#x4E8E;&#x4EA4;&#x53C9;&#x76F8;&#x5173;&#xFF0C;&#x5355;&#x4E2A;&#x6570;&#x5B57;&#x6216;&#x4E00;&#x4E2A;&#x5143;&#x7EC4;&#x7684;&#x6B65;&#x5E45;&#x3002;</p>
</li>
<li><p><code>&#x586B;&#x5145;</code>&#x63A7;&#x5236;&#x9690;&#x542B;&#x96F6;&#x586B;&#x8865;&#x5904;&#x7406;&#x7684;&#x53CC;&#x65B9;&#x7684;&#x91CF;&#x4E3A;<code>&#x586B;&#x5145;</code>&#x70B9;&#x6570;&#x4E3A;&#x6BCF;&#x4E2A;&#x7EF4;&#x5EA6;&#x3002;</p>
</li>
<li><p><code>dilation</code>controls the spacing between the kernel points; also known as the &#xE0; trous algorithm. It is harder to describe, but this <a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md" target="_blank">link</a> has a nice visualization of what <code>dilation</code>does.</p>
</li>
<li><p><code>groups</code>controls the connections between inputs and outputs. <code>in_channels</code>and <code>out_channels</code>must both be divisible by <code>groups</code>. For example,</p>
</li>
</ul>
<blockquote>
<pre><code>* At groups=1, all inputs are convolved to all outputs.
</code></pre></blockquote>
<p>&gt;</p>
<blockquote>
<pre><code>* At groups=2, the operation becomes equivalent to having two conv
</code></pre><p>layers side by side, each seeing half the input channels, and producing half
the output channels, and both subsequently concatenated.</p>
</blockquote>
<p>&gt;</p>
<blockquote>
<pre><code>* &#x5728;&#x57FA;&#x56E2;= `in_channels`&#x4E2D;&#xFF0C;&#x6BCF;&#x4E2A;&#x8F93;&#x5165;&#x4FE1;&#x9053;&#x8FDB;&#x884C;&#x5377;&#x79EF;&#x4EE5;&#x5B83;&#x81EA;&#x5DF1;&#x7684;&#x4E00;&#x5957;&#x8FC7;&#x6EE4;&#x5668;&#xFF0C;&#x5927;&#x5C0F;&#xFF1A; &#x230A; O  U  T  _  C  H  &#x4E00;
</code></pre><p>n&#x7684; n&#x7684; E  L  S  i&#x7684; n&#x7684; <em>  C  H  &#x4E00; n&#x7684; n&#x7684; E  L  S  &#x230B; \&#x5DE6;\ lfloor \&#x538B;&#x88C2;{&#x51FA;\ _channels}
{&#x5728;\ _channels} \&#x53F3;\ rfloor  &#x230A; [HTG9 1]  i&#x7684; n&#x7684; </em>  C  H  &#x4E00; N  n&#x7684; E  L  S  O  U  T
_  C  H  &#x4E00; n&#x7684; n&#x7684; E  L  S  &#x230B; &#x3002;</p>
</blockquote>
<p>&#x53C2;&#x6570;<code>kernel_size</code>&#xFF0C;<code>&#x6B65;&#x5E45;</code>&#xFF0C;<code>&#x586B;&#x5145;</code>&#xFF0C;<code>&#x6269;&#x5F20;</code>&#x53EF;&#x4EE5;&#x662F;&#xFF1A;</p>
<blockquote>
<ul>
<li>&#x5355;&#x4E00;<code>INT</code>- &#x5728;&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;&#x76F8;&#x540C;&#x7684;&#x503C;&#x88AB;&#x7528;&#x4E8E;&#x9AD8;&#x5EA6;&#x548C;&#x5BBD;&#x5EA6;&#x5C3A;&#x5BF8;</li>
</ul>
</blockquote>
<p>&gt;</p>
<blockquote>
<ul>
<li>&#x4E00;&#x4E2A;<code>&#x5143;&#x7EC4;</code>&#x7684;&#x4E24;&#x4E2A;&#x6574;&#x6570; - &#x5728;&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x7B2C;&#x4E00;&#x4E2A; INT&#x7528;&#x6765;&#x4E3A;&#x9AD8;&#x5EA6;&#x5C3A;&#x5BF8;&#xFF0C;&#x5E76;&#x4E14;&#x6240;&#x8FF0;&#x7B2C;&#x4E8C; INT &#x4E3A;&#x5BBD;&#x5EA6;&#x5C3A;&#x5BF8;</li>
</ul>
</blockquote>
<p>&gt;</p>
<p>&gt;</p>
<p>Note</p>
<p>&#x6839;&#x636E;&#x4F60;&#x7684;&#x5185;&#x6838;&#x7684;&#x5927;&#x5C0F;&#xFF0C;&#x51E0;&#x4E2A;&#xFF08;&#x6700;&#x540E;&#xFF09;&#x8F93;&#x5165;&#x7684;&#x5217;&#x53EF;&#x80FD;&#x4F1A;&#x4E22;&#x5931;&#xFF0C;&#x56E0;&#x4E3A;&#x5B83;&#x662F;&#x4E00;&#x4E2A;&#x6709;&#x6548;&#x7684;<a href="https://en.wikipedia.org/wiki/Cross-
correlation" target="_blank">&#x4E92;&#x76F8;&#x5173;</a>&#xFF0C;&#x800C;&#x4E0D;&#x662F;&#x4E00;&#x4E2A;&#x5B8C;&#x6574;&#x7684;<a href="https://en.wikipedia.org/wiki/Cross-correlation" target="_blank">&#x4E92;&#x76F8;&#x5173;</a>
&#x3002;&#x5B83;&#x662F;&#x7531;&#x7528;&#x6237;&#x6DFB;&#x52A0;&#x9002;&#x5F53;&#x7684;&#x586B;&#x5145;&#x3002;</p>
<p>Note</p>
<p>When groups == in_channels and out_channels == K * in_channels, where K is a
positive integer, this operation is also termed in literature as depthwise
convolution.</p>
<p>&#x6362;&#x53E5;&#x8BDD;&#x8BF4;&#xFF0C;&#x4E3A;&#x7684;&#x5927;&#x5C0F; &#xFF08; N  &#xFF0C;&#x8F93;&#x5165; C  i&#x7684; n&#x7684; &#xFF0C; H  i&#x7684; n&#x7684; &#xFF0C; W  i&#x7684; n&#x7684; &#xFF09; &#xFF08;N&#xFF0C;C<em> {IN}&#xFF0C;H</em> {IN}&#xFF0C;W<em> {&#x5728;}&#xFF09;
&#xFF08; N  &#xFF0C; &#xE7;  i&#x7684; n&#x7684; &#xFF0C; H  i&#x7684; n&#x7684; &#xFF0C; W  i&#x7684; n&#x7684; &#xFF09; &#xFF0C;&#x5177;&#x6709;&#x6DF1;&#x5EA6;&#x65B9;&#x5411;&#x4E58;&#x6CD5;&#x5668;&#x6DF1;&#x5EA6;&#x65B9;&#x5411;&#x5377;&#x79EF; K &#xFF0C;&#x53EF;&#x7531;&#x53C2;&#x6570; &#xFF08; i&#x7684; n&#x7684; </em>
[H&#x6784;&#x5EFA;TG191]  C  H  &#x4E00; n&#x7684; n&#x7684; E  L  S  =  C  i&#x7684; n&#x7684; &#xFF0C; O  U  T  <em>  C  H  &#x4E00;&#x4E2A; n&#x7684; n&#x7684; E
L  S  =  C  i&#x7684; n&#x7684; &#xD7; K  &#xFF0C; &#x3002;  &#x3002;  &#x3002;  &#x200B;&#x200B;&#xFF0C; &#x514B; R  O  U  P  S  =  C  i&#x7684; n&#x7684; &#xFF09; &#xFF08;&#x5728;\
_channels = C</em> {}&#x4E2D;&#xFF0C;&#x51FA;\ <em>channels = C</em> {&#x5728;} \&#x500D;K&#xFF0C;...&#xFF0C;&#x7EC4;= C<em> {&#x5728;}&#xFF09; &#xFF08; i&#x7684; n&#x7684; </em>  C  H  &#x4E00;
n&#x7684; n&#x7684; E  L  S  =  C  i&#x7684; n&#x7684; O  U  T  _  C  H  &#x4E00; n&#x7684; n&#x7684; E  L  S  =  C  i&#x7684; n&#x7684; &#xD7; K
&#xFF0C; &#x3002;  &#x3002;  &#x3002;  &#xFF0C; &#x514B; R  O  U  P  S  =  C  i&#x7684; n&#x7684; &#xFF09; &#x3002;</p>
<p>Note</p>
<p>In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at a
performance cost) by setting <code>torch.backends.cudnn.deterministic = True</code>.
Please see the notes on <a href="notes/randomness.html">Reproducibility</a> for
background.</p>
<p>Parameters</p>
<ul>
<li><p><strong>in_channels</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a>) &#x2013; Number of channels in the input image</p>
</li>
<li><p><strong>out_channels</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a>) &#x2013; Number of channels produced by the convolution</p>
</li>
<li><p><strong>kernel_size</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>or</em><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>tuple</em></a>) &#x2013; Size of the convolving kernel</p>
</li>
<li><p><strong>stride</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>or</em><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>tuple</em></a> <em>,</em> <em>optional</em> ) &#x2013; Stride of the convolution. Default: 1</p>
</li>
<li><p><strong>&#x586B;&#x5145;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#x6216;</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>&#x5143;&#x7EC4;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;&#x7684;</em> &#xFF09; - &#x8865;&#x96F6;&#x52A0;&#x5230;&#x8F93;&#x5165;&#x7684;&#x4E24;&#x4FA7;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;0</p>
</li>
<li><p><strong>padding_mode</strong> ( <em>string</em> <em>,</em> <em>optional</em> ) &#x2013; zeros</p>
</li>
<li><p><strong>&#x6269;&#x5F20;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#x6216;</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>&#x5143;&#x7EC4;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;&#x7684;</em> &#xFF09; - &#x5185;&#x6838;&#x5143;&#x4EF6;&#x4E4B;&#x95F4;&#x7684;&#x95F4;&#x9694;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;1</p>
</li>
<li><p><strong>&#x57FA;&#x56E2;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x4ECE;&#x8F93;&#x5165;&#x4FE1;&#x9053;&#x5230;&#x8F93;&#x51FA;&#x901A;&#x9053;&#x963B;&#x585E;&#x7684;&#x8FDE;&#x63A5;&#x7684;&#x6570;&#x76EE;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;1</p>
</li>
<li><p><strong>bias</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; If <code>True</code>, adds a learnable bias to the output. Default: <code>True</code></p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>&#x8F93;&#x5165;&#xFF1A; &#xFF08; N  &#xFF0C; C  i&#x7684; n&#x7684; &#xFF0C; H  i&#x7684; n&#x7684; &#xFF0C; W  i&#x7684; n&#x7684; &#xFF09; &#xFF08;N&#xFF0C;C<em> {IN}&#xFF0C;H</em> {IN}&#xFF0C;W_ {&#x5728;}&#xFF09; &#xFF08; N  &#xFF0C; C  i&#x7684; n&#x7684; &#xFF0C; H  i&#x7684; n&#x7684; &#xFF0C; W  i&#x7684; N  &#xFF09;</p>
</li>
<li><p>&#x8F93;&#x51FA;&#xFF1A; &#xFF08; N  &#xFF0C; C  O  U  T  &#xFF0C; H  O  U  T  &#xFF0C; W  O  U  T  &#xFF09; &#xFF08;N&#xFF0C;C<em> {&#x51FA;}&#xFF0C;H</em> {&#x51FA;} &#xFF0C;W_ {&#x51FA;}&#xFF09; &#xFF08; N  &#xFF0C; C  O  U  T  [HT G104]  &#xFF0C; H  O  U  T  &#xFF0C; W  O  U  T  &#xFF09; &#x5176;&#x4E2D;</p>
</li>
</ul>
<p>Hout=&#x230A;Hin+2&#xD7;padding[0]&#x2212;dilation[0]&#xD7;(kernel<em>size[0]&#x2212;1)&#x2212;1stride[0]+1&#x230B;H</em>{out} =
\left\lfloor\frac{H_{in} + 2 \times \text{padding}[0] - \text{dilation}[0]
\times (\text{kernel\_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor
Hout&#x200B;=&#x230A;stride[0]Hin&#x200B;+2&#xD7;padding[0]&#x2212;dilation[0]&#xD7;(kernel_size[0]&#x2212;1)&#x2212;1&#x200B;+1&#x230B;</p>
<p>Wout=&#x230A;Win+2&#xD7;padding[1]&#x2212;dilation[1]&#xD7;(kernel<em>size[1]&#x2212;1)&#x2212;1stride[1]+1&#x230B;W</em>{out} =
\left\lfloor\frac{W_{in} + 2 \times \text{padding}[1] - \text{dilation}[1]
\times (\text{kernel\_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor
Wout&#x200B;=&#x230A;stride[1]Win&#x200B;+2&#xD7;padding[1]&#x2212;dilation[1]&#xD7;(kernel_size[1]&#x2212;1)&#x2212;1&#x200B;+1&#x230B;</p>
<p>Variables</p>
<ul>
<li><p><strong>&#x301C;Conv2d.weight</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x7684;&#x5F62;&#x72B6; [&#x8BE5;&#x6A21;&#x5757;&#x7684;&#x53EF;&#x5B66;&#x4E60;&#x6743;&#x91CD;HTG11] &#xFF08; out<em>channels  &#xFF0C; in_channels  &#x57FA;&#x56E2; &#xFF0C; &#xFF08;\&#x6587;&#x672C;{&#x51FA;\ _channels}&#xFF0C;\&#x538B;&#x88C2;{\&#x6587;&#x672C;{&#x5728;\ _channels}} {\&#x6587;&#x672C;{&#x57FA;}}&#xFF0C; &#xFF08; out_channels  &#xFF0C; &#x57FA;&#x56E2; in_channels  [HTG8 9]  &#xFF0C; kernel_size [0]  &#xFF0C; kernel_size [1]  &#xFF09; \&#x6587;&#x672C;{&#x5185;&#x6838;\ _size [0]}&#xFF0C;\&#x6587;&#x672C;{&#x5185;&#x6838;\ _size [1]}&#xFF09; kernel_size [0]  &#xFF0C; kernel_size [1]  &#xFF09; &#x3002;&#x8FD9;&#x4E9B;&#x6743;&#x91CD;&#x7684;&#x503C;&#x662F;&#x4ECE; &#x53D6;&#x6837;U  &#xFF08; -  K  &#xFF0C; K  &#xFF09; \ mathcal&#x3010;U}&#xFF08; - \ SQRT {K}&#xFF0C;\ SQRT {K}&#xFF09; U  &#xFF08; -  K  &#xFF0C; K  [H TG236]  &#xFF09; &#x5176;&#x4E2D; &#x200B;&#x200B;  K  =  1  C  &#x5728; *  &#x3A0; i&#x7684; =  0  1  kernel_size  [ i&#x7684; K = \&#x538B;&#x88C2;{1} {C</em> \&#x6587;&#x672C;{IN} <em> \ prod_ {I = 0} ^ {1} \&#x6587;&#x672C;{&#x5185;&#x6838;\ _size} [I]}  K  =  C  &#x5728; </em>  &#x3A0; i&#x7684; =  0  1  kernel_size  [ i&#x7684; 1 </p>
</li>
<li><p><strong>&#x301C;Conv2d.bias</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x5F62;&#x72B6;&#xFF08;out<em>channels&#xFF09;&#x7684;&#x6A21;&#x5757;&#x7684;&#x53EF;&#x5B66;&#x4E60;&#x504F;&#x5DEE;&#x3002;&#x5982;&#x679C;<code>&#x504F;&#x538B;</code>&#x662F;<code>&#x771F;</code>&#xFF0C;&#x7136;&#x540E;&#x8FD9;&#x4E9B;&#x6743;&#x91CD;&#x7684;&#x503C;&#x662F;&#x4ECE; &#x53D6;&#x6837; U  &#xFF08; -  K  &#xFF0C; K  &#xFF09; \ mathcal&#x3010;U}&#xFF08; - \ SQRT {K}&#xFF0C;\ SQRT {K}&#xFF09; U  &#xFF08; -  K  K  &#xFF09; &#x5176;&#x4E2D; K  =  1  C  &#x5728; *  &#x3A0; i&#x7684; =  0  1  kernel_size  [ i&#x7684; K = \&#x538B;&#x88C2;{1} {C</em> \&#x6587;&#x672C;{IN} <em> \ prod_ {I = 0} ^ {1} \&#x6587;&#x672C;{&#x5185;&#x6838;\ _size} [I]}  K  =  C  &#x5728; </em>  &#x3A0; i&#x7684; =  0  &#x200B;&#x200B;  1  kernel_size  [ i&#x7684; 1 </p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; # With square kernels and equal stride
&gt;&gt;&gt; m = nn.Conv2d(16, 33, 3, stride=2)
&gt;&gt;&gt; # non-square kernels and unequal stride and with padding
&gt;&gt;&gt; m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))
&gt;&gt;&gt; # non-square kernels and unequal stride and with padding and dilation
&gt;&gt;&gt; m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))
&gt;&gt;&gt; input = torch.randn(20, 16, 50, 100)
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="conv3d">Conv3d</h3>
<p><em>class</em><code>torch.nn.``Conv3d</code>( <em>in_channels</em> , <em>out_channels</em> , <em>kernel_size</em> ,
<em>stride=1</em> , <em>padding=0</em> , <em>dilation=1</em> , <em>groups=1</em> , <em>bias=True</em> ,
<em>padding_mode=&apos;zeros&apos;</em>
)<a href="_modules/torch/nn/modules/conv.html#Conv3d">[source]</a></p>
<p>&#x65BD;&#x52A0;&#x4E09;&#x7EF4;&#x5377;&#x79EF;&#x5728;&#x51E0;&#x4E2A;&#x8F93;&#x5165;&#x5E73;&#x9762;&#x7EC4;&#x6210;&#x7684;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x3002;</p>
<p>&#x5728;&#x6700;&#x7B80;&#x5355;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x6240;&#x8FF0;&#x5C42;&#x7684;&#x4E0E;&#x8F93;&#x5165;&#x5927;&#x5C0F; &#x7684;&#x8F93;&#x51FA;&#x503C;&#xFF08; N  C  i&#x7684; n&#x7684; &#xFF0C; d  &#xFF0C; H  &#xFF0C; W  &#xFF09; &#xFF08;N&#xFF0C;C<em> {IN}&#xFF0C;d &#xFF0C;H&#xFF0C;W&#xFF09; &#xFF08; N
&#xFF0C; C  i&#x7684; n&#x7684; &#xFF0C; d  &#xFF0C; H  [H T G99]  W  &#xFF09; &#x548C;&#x8F93;&#x51FA; &#xFF08; N  &#xFF0C; C  O  U  T  &#xFF0C; d  O  U  T  &#xFF0C; H
O  U  &#x5428; &#xFF0C; W  O  U  T  &#xFF09; &#xFF08;N&#xFF0C;C</em> {&#x51FA;}&#xFF0C;D<em> {&#x51FA;}&#xFF0C;H</em> {&#x51FA;}&#xFF0C;W_ {&#x51FA;}&#xFF09; &#xFF08; N [HTG1 91] &#xFF0C; C  O
U  T  &#xFF0C; d  O  U  T  &#xFF0C; &#x200B;&#x200B;  H  O  U  T  &#xFF0C; W  O  U  T  &#xFF09; &#x53EF;&#x4EE5;&#x7CBE;&#x786E;&#x5730;&#x63CF;&#x8FF0;&#x4E3A;&#xFF1A;</p>
<p>out(Ni,Coutj)=bias(Coutj)+&#x2211;k=0Cin&#x2212;1weight(Coutj,k)&#x22C6;input(Ni,k)out(N<em>i,
C</em>{out<em>j}) = bias(C</em>{out<em>j}) + \sum</em>{k = 0}^{C<em>{in} - 1} weight(C</em>{out_j}, k)
\star input(N_i, k)
out(Ni&#x200B;,Coutj&#x200B;&#x200B;)=bias(Coutj&#x200B;&#x200B;)+k=0&#x2211;Cin&#x200B;&#x2212;1&#x200B;weight(Coutj&#x200B;&#x200B;,k)&#x22C6;input(Ni&#x200B;,k)</p>
<p>&#x5176;&#x4E2D; &#x22C6; \&#x661F; &#x22C6; &#x662F;&#x6709;&#x6548;&#x7684;3D <a href="https://en.wikipedia.org/wiki/Cross-correlation" target="_blank">&#x4E92;&#x76F8;&#x5173;</a>&#x64CD;&#x4F5C;&#x8005;</p>
<ul>
<li><p><code>&#x6B65;&#x5E45;</code>&#x63A7;&#x5236;&#x7528;&#x4E8E;&#x4E92;&#x76F8;&#x5173;&#x7684;&#x6B65;&#x5E45;&#x3002;</p>
</li>
<li><p><code>padding</code>controls the amount of implicit zero-paddings on both sides for <code>padding</code>number of points for each dimension.</p>
</li>
<li><p><code>&#x6269;&#x5F20;</code>&#x63A7;&#x5236;&#x5185;&#x6838;&#x70B9;&#x4E4B;&#x95F4;&#x7684;&#x95F4;&#x9694;;&#x4E5F;&#x88AB;&#x79F0;&#x4E3A;&#x5288;&#x7A97;&#x7B97;&#x6CD5;&#x3002;&#x8FD9;&#x662F;&#x5F88;&#x96BE;&#x5F62;&#x5BB9;&#xFF0C;&#x4F46;&#x8FD9;&#x79CD;<a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md" target="_blank">&#x94FE;&#x63A5;</a>&#x6709;&#x4EC0;&#x4E48;<code>&#x6269;&#x5F20;</code>&#x505A;&#x4E00;&#x4E2A;&#x5F88;&#x597D;&#x7684;&#x53EF;&#x89C6;&#x5316;&#x3002;</p>
</li>
<li><p><code>groups</code>controls the connections between inputs and outputs. <code>in_channels</code>and <code>out_channels</code>must both be divisible by <code>groups</code>. For example,</p>
</li>
</ul>
<blockquote>
<pre><code>* At groups=1, all inputs are convolved to all outputs.
</code></pre></blockquote>
<p>&gt;</p>
<blockquote>
<pre><code>* At groups=2, the operation becomes equivalent to having two conv
</code></pre><p>layers side by side, each seeing half the input channels, and producing half
the output channels, and both subsequently concatenated.</p>
</blockquote>
<p>&gt;</p>
<blockquote>
<pre><code>* &#x5728;&#x57FA;&#x56E2;= `in_channels`&#x4E2D;&#xFF0C;&#x6BCF;&#x4E2A;&#x8F93;&#x5165;&#x4FE1;&#x9053;&#x8FDB;&#x884C;&#x5377;&#x79EF;&#x4EE5;&#x5B83;&#x81EA;&#x5DF1;&#x7684;&#x4E00;&#x5957;&#x8FC7;&#x6EE4;&#x5668;&#xFF0C;&#x5927;&#x5C0F;&#x7684; &#x230A; O  U  T  _  C  H  &#x4E00;
</code></pre><p>n&#x7684; n&#x7684; E  L  S  i&#x7684; n&#x7684; <em>  C  H  &#x4E00; n&#x7684; n&#x7684; E  L  S  &#x230B; \&#x5DE6;\ lfloor \&#x538B;&#x88C2;{&#x51FA;\ _channels}
{&#x5728;\ _channels} \&#x53F3;\ rfloor  &#x230A; i&#x7684; n&#x7684; </em>  C  H  &#x4E00; n&#x7684; n&#x7684; E  L  S  O  U  T  _  C  H
&#x4E00; n&#x7684; n&#x7684; E  L  S  &#x230B; &#x3002;</p>
</blockquote>
<p>The parameters <code>kernel_size</code>, <code>stride</code>, <code>padding</code>, <code>dilation</code>can either be:</p>
<blockquote>
<ul>
<li>&#x5355;&#x4E00;<code>INT</code>- &#x5728;&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;&#x76F8;&#x540C;&#x7684;&#x503C;&#x88AB;&#x7528;&#x4E8E;&#x6DF1;&#x5EA6;&#xFF0C;&#x9AD8;&#x5EA6;&#x548C;&#x5BBD;&#x5EA6;&#x5C3A;&#x5BF8;</li>
</ul>
</blockquote>
<p>&gt;</p>
<blockquote>
<ul>
<li>&#x4E00;&#x4E2A;<code>&#x5143;&#x7EC4;</code>&#x4E09;&#x4E2A;&#x6574;&#x6570; - &#x5728;&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x7B2C;&#x4E00;&#x4E2A; INT&#x7528;&#x4E8E;&#x6DF1;&#x5EA6;&#x5C3A;&#x5BF8;&#xFF0C;&#x6240;&#x8FF0;&#x7B2C;&#x4E8C; INT &#x4E3A;&#x9AD8;&#x5EA6;&#x7EF4;&#x5EA6;&#x548C;&#x7B2C;&#x4E09; INT &#x4E3A;&#x5BBD;&#x5EA6;&#x5C3A;&#x5BF8;</li>
</ul>
</blockquote>
<p>&gt;</p>
<p>&gt;</p>
<p>Note</p>
<p>Depending of the size of your kernel, several (of the last) columns of the
input might be lost, because it is a valid <a href="https://en.wikipedia.org/wiki/Cross-correlation" target="_blank">cross-
correlation</a>, and not a full
<a href="https://en.wikipedia.org/wiki/Cross-correlation" target="_blank">cross-correlation</a>. It is up
to the user to add proper padding.</p>
<p>Note</p>
<p>When groups == in_channels and out_channels == K * in_channels, where K is a
positive integer, this operation is also termed in literature as depthwise
convolution.</p>
<p>&#x6362;&#x53E5;&#x8BDD;&#x8BF4;&#xFF0C;&#x4E3A;&#x7684;&#x5927;&#x5C0F; &#xFF08; N  &#xFF0C;&#x8F93;&#x5165; C  i&#x7684; n&#x7684; &#xFF0C; d  i&#x7684; n&#x7684; &#xFF0C; H  i&#x7684; n&#x7684; &#xFF0C; W  i&#x7684; n&#x7684; &#xFF09; &#xFF08;N&#xFF0C;C<em> {IN}&#xFF0C;D</em>
{IN}&#xFF0C;H<em> {IN}&#xFF0C;W</em> {&#x5728;}&#xFF09; &#xFF08; N  &#xFF0C; C  i&#x7684; n&#x7684; &#xFF0C; d  i&#x7684; n&#x7684; &#xFF0C; H  i&#x7684; N  &#xFF0C; W  i&#x7684;[H TG199]
n&#x7684; &#xFF09; &#xFF0C;&#x5177;&#x6709;&#x6DF1;&#x5EA6;&#x65B9;&#x5411;&#x4E58;&#x6CD5;&#x5668;&#x6DF1;&#x5EA6;&#x65B9;&#x5411;&#x5377;&#x79EF; K &#xFF0C;&#x53EF;&#x4EE5;&#x901A;&#x8FC7;&#x53C2;&#x6784;&#x9020; &#xFF08; i&#x7684; n&#x7684; <em>  C  H  &#x4E00; n&#x7684; n&#x7684; E  L  S  =  C  i&#x7684; n&#x7684;
&#xFF0C; &#x95EE;&#x9898;o  U  T  </em>  C  H  &#x4E00; n&#x7684; n&#x7684; E  L  S  =  C  i&#x7684; n&#x7684; &#xD7; K  &#xFF0C; &#x3002;  &#x3002;  &#x3002;  &#xFF0C; &#x514B; R  O  U
P  S  =  C  i&#x7684; n&#x7684; &#xFF09; &#xFF08;&#x5728;\ <em>channels = C</em> {}&#x4E2D;&#xFF0C;&#x51FA;\ <em>channels = C</em> {&#x5728;} \&#x500D;K&#xFF0C;...&#xFF0C;&#x7EC4;= C<em>
{&#x5728;}&#xFF09; &#xFF08; i&#x7684; n&#x7684; </em>  C  H  &#x4E00; n&#x7684; n&#x7684; E  L  S  =  C  i&#x7684; n&#x7684; O  U  T  _  C  H  &#x4E00; n&#x7684; n&#x7684; E
L  S  =  C  i&#x7684; n&#x7684; &#xD7; K  &#xFF0C; &#x3002;  &#x3002;  &#x3002;  &#xFF0C; &#x514B; R  O  U  P  S  =  C  i&#x7684; n&#x7684; &#xFF09; &#x3002;</p>
<p>Note</p>
<p>In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at a
performance cost) by setting <code>torch.backends.cudnn.deterministic = True</code>.
Please see the notes on <a href="notes/randomness.html">Reproducibility</a> for
background.</p>
<p>Parameters</p>
<ul>
<li><p><strong>in_channels</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a>) &#x2013; Number of channels in the input image</p>
</li>
<li><p><strong>out_channels</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a>) &#x2013; Number of channels produced by the convolution</p>
</li>
<li><p><strong>kernel_size</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>or</em><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>tuple</em></a>) &#x2013; Size of the convolving kernel</p>
</li>
<li><p><strong>stride</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>or</em><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>tuple</em></a> <em>,</em> <em>optional</em> ) &#x2013; Stride of the convolution. Default: 1</p>
</li>
<li><p><strong>&#x586B;&#x5145;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#x6216;</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>&#x5143;&#x7EC4;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;&#x7684;</em> &#xFF09; - &#x8865;&#x96F6;&#x52A0;&#x5230;&#x8F93;&#x5165;&#x7684;&#x6240;&#x6709;&#x4E09;&#x4E2A;&#x4FA7;&#x9762;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;0</p>
</li>
<li><p><strong>padding_mode</strong> ( <em>string</em> <em>,</em> <em>optional</em> ) &#x2013; zeros</p>
</li>
<li><p><strong>dilation</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>or</em><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>tuple</em></a> <em>,</em> <em>optional</em> ) &#x2013; Spacing between kernel elements. Default: 1</p>
</li>
<li><p><strong>groups</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>,</em> <em>optional</em> ) &#x2013; Number of blocked connections from input channels to output channels. Default: 1</p>
</li>
<li><p><strong>bias</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; If <code>True</code>, adds a learnable bias to the output. Default: <code>True</code></p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>&#x8F93;&#x5165;&#xFF1A; &#xFF08; N  &#xFF0C; C  i&#x7684; n&#x7684; &#xFF0C; d  i&#x7684; n&#x7684; &#xFF0C; H  i&#x7684; n&#x7684; &#xFF0C; W  i&#x7684; n&#x7684; &#xFF09; &#xFF08;N&#xFF0C;C<em> {IN}&#xFF0C;D</em> {IN}&#xFF0C;H<em> {IN}&#xFF0C;W</em> {&#x5728;}&#xFF09; &#xFF08; N  &#xFF0C; C  i&#x7684; n&#x7684; &#xFF0C; d  i&#x7684; n&#x7684; &#xFF0C; H  i&#x7684; n&#x7684; &#xFF0C; W  i&#x7684; n&#x7684; &#xFF09;</p>
</li>
<li><p>&#x8F93;&#x51FA;&#xFF1A; &#xFF08; N  &#xFF0C; C  O  U  T  &#xFF0C; d  O  U  T  &#xFF0C; H  O  U  T  &#xFF0C; W  O  U  T  &#xFF09; &#xFF08;N&#xFF0C;C<em> {&#x51FA;}&#xFF0C;D</em> {&#x51FA;}&#xFF0C;H<em> {&#x51FA;}&#xFF0C;W</em> {&#x51FA;}&#xFF09; &#xFF08; N  C  O  U  &#x5428;[H TG103]  &#xFF0C; d  O  U  T  &#xFF0C; H  O  U  T  &#xFF0C; [H TG201] W  O  U  T  &#xFF09; &#x5176;&#x4E2D;</p>
</li>
</ul>
<p>Dout=&#x230A;Din+2&#xD7;padding[0]&#x2212;dilation[0]&#xD7;(kernel<em>size[0]&#x2212;1)&#x2212;1stride[0]+1&#x230B;D</em>{out} =
\left\lfloor\frac{D_{in} + 2 \times \text{padding}[0] - \text{dilation}[0]
\times (\text{kernel\_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor
Dout&#x200B;=&#x230A;stride[0]Din&#x200B;+2&#xD7;padding[0]&#x2212;dilation[0]&#xD7;(kernel_size[0]&#x2212;1)&#x2212;1&#x200B;+1&#x230B;</p>
<p>Hout=&#x230A;Hin+2&#xD7;padding[1]&#x2212;dilation[1]&#xD7;(kernel<em>size[1]&#x2212;1)&#x2212;1stride[1]+1&#x230B;H</em>{out} =
\left\lfloor\frac{H_{in} + 2 \times \text{padding}[1] - \text{dilation}[1]
\times (\text{kernel\_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor
Hout&#x200B;=&#x230A;stride[1]Hin&#x200B;+2&#xD7;padding[1]&#x2212;dilation[1]&#xD7;(kernel_size[1]&#x2212;1)&#x2212;1&#x200B;+1&#x230B;</p>
<p>Wout=&#x230A;Win+2&#xD7;padding[2]&#x2212;dilation[2]&#xD7;(kernel<em>size[2]&#x2212;1)&#x2212;1stride[2]+1&#x230B;W</em>{out} =
\left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] - \text{dilation}[2]
\times (\text{kernel\_size}[2] - 1) - 1}{\text{stride}[2]} + 1\right\rfloor
Wout&#x200B;=&#x230A;stride[2]Win&#x200B;+2&#xD7;padding[2]&#x2212;dilation[2]&#xD7;(kernel_size[2]&#x2212;1)&#x2212;1&#x200B;+1&#x230B;</p>
<p>Variables</p>
<ul>
<li><p><strong>&#x301C;Conv3d.weight</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x7684;&#x5F62;&#x72B6; [&#x8BE5;&#x6A21;&#x5757;&#x7684;&#x53EF;&#x5B66;&#x4E60;&#x6743;&#x91CD;HTG11] &#xFF08; out<em>channels  &#xFF0C; in_channels  &#x57FA;&#x56E2; &#xFF0C; &#xFF08;\&#x6587;&#x672C;{&#x51FA;\ _channels}&#xFF0C;\&#x538B;&#x88C2;{\&#x6587;&#x672C;{&#x5728;\ _channels}} {\&#x6587;&#x672C;{&#x57FA;}}&#xFF0C; &#xFF08; out_channels  &#xFF0C; &#x57FA;&#x56E2; in_channels  [HTG8 9]  &#xFF0C; kernel_size [0]  &#xFF0C; kernel_size [1]  &#xFF0C; kernel_size [2]  &#xFF09; \&#x6587;&#x672C;{&#x5185;&#x6838;\ _size [0]}&#xFF0C;\&#x6587;&#x672C;{&#x5185;&#x6838;\ _size [1]}&#xFF0C;\&#x6587;&#x672C;{&#x5185;&#x6838;\ _size [2 ]}&#xFF09; kernel_size [0]  &#xFF0C; kernel_size [1]  &#xFF0C; kernel_size [2]  &#xFF09; &#x3002;&#x8FD9;&#x4E9B;&#x6743;&#x91CD;&#x7684;&#x503C;&#x662F;&#x4ECE; &#x53D6;&#x6837;U  &#xFF08; -  K  &#xFF0C; K  &#xFF09; \ mathcal&#x3010;U}&#xFF08; - \ SQRT {K}&#xFF0C;\ SQRT {K}&#xFF09; U  &#xFF08; -  K  &#xFF0C; K  [H TG248]  &#x200B;&#x200B;  &#xFF09; &#x5176;&#x4E2D; K  =  1  C  &#x5728; *  &#x3A0; i&#x7684; =  0  2  kernel_size  [ i&#x7684; K = \&#x538B;&#x88C2;{1} {C</em> \&#x6587;&#x672C;{IN} <em> \ prod_ {I = 0} ^ {2} \&#x6587;&#x672C;{&#x5185;&#x6838;\ _size} [I]}  K  =  C  &#x5728; </em>  &#x3A0; i&#x7684; =  0  2  kernel_size  [ i&#x7684; 1 </p>
</li>
<li><p><strong>&#x301C;Conv3d.bias</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x5F62;&#x72B6;&#xFF08;out<em>channels&#xFF09;&#x7684;&#x6A21;&#x5757;&#x7684;&#x53EF;&#x5B66;&#x4E60;&#x504F;&#x5DEE;&#x3002;&#x5982;&#x679C;<code>&#x504F;&#x538B;</code>&#x662F;<code>&#x771F;</code>&#xFF0C;&#x7136;&#x540E;&#x8FD9;&#x4E9B;&#x6743;&#x91CD;&#x7684;&#x503C;&#x662F;&#x4ECE; &#x53D6;&#x6837; U  &#xFF08; -  K  &#xFF0C; K  &#xFF09; \ mathcal&#x3010;U}&#xFF08; - \ SQRT {K}&#xFF0C;\ SQRT {K}&#xFF09; U  &#xFF08; -  K  K  &#xFF09; &#x5176;&#x4E2D; K  =  1  C  &#x5728; *  &#x3A0; i&#x7684; =  0  2  kernel_size  [ i&#x7684; K = \&#x538B;&#x88C2;{1} {C</em> \&#x6587;&#x672C;{IN} <em> \ prod_ {I = 0} ^ {2} \&#x6587;&#x672C;{&#x5185;&#x6838;\ _size} [I]}  K  =  C  &#x5728; </em>  &#x3A0; i&#x7684; =  0  &#x200B;&#x200B;  2  kernel_size  [ i&#x7684; 1 </p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; # With square kernels and equal stride
&gt;&gt;&gt; m = nn.Conv3d(16, 33, 3, stride=2)
&gt;&gt;&gt; # non-square kernels and unequal stride and with padding
&gt;&gt;&gt; m = nn.Conv3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(4, 2, 0))
&gt;&gt;&gt; input = torch.randn(20, 16, 10, 50, 100)
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="convtranspose1d">ConvTranspose1d</h3>
<p><em>class</em><code>torch.nn.``ConvTranspose1d</code>( <em>in_channels</em> , <em>out_channels</em> ,
<em>kernel_size</em> , <em>stride=1</em> , <em>padding=0</em> , <em>output_padding=0</em> , <em>groups=1</em> ,
<em>bias=True</em> , <em>dilation=1</em> , <em>padding_mode=&apos;zeros&apos;</em>
)<a href="_modules/torch/nn/modules/conv.html#ConvTranspose1d">[source]</a></p>
<p>&#x5E94;&#x7528;&#x5728;&#x591A;&#x4E2A;&#x8F93;&#x5165;&#x5E73;&#x9762;&#x6784;&#x6210;&#x7684;&#x8F93;&#x5165;&#x56FE;&#x50CF;&#x7684;1D&#x6362;&#x4F4D;&#x5377;&#x79EF;&#x8FD0;&#x7B97;&#x3002;</p>
<p>&#x6B64;&#x6A21;&#x5757;&#x53EF;&#x4EE5;&#x88AB;&#x89C6;&#x4E3A;Conv1d&#x7684;&#x68AF;&#x5EA6;&#x76F8;&#x5BF9;&#x4E8E;&#x5B83;&#x7684;&#x8F93;&#x5165;&#x3002;&#x5B83;&#x4E5F;&#x88AB;&#x79F0;&#x4E3A;&#x4E00;&#x5206;&#x7EA7;-&#x8DE8;&#x8DDD;&#x5377;&#x79EF;&#x6216;&#x53BB;&#x5377;&#x79EF;&#xFF08;&#x5C3D;&#x7BA1;&#x5B83;&#x4E0D;&#x662F;&#x4E00;&#x4E2A;&#x5B9E;&#x9645;&#x7684;&#x53BB;&#x5377;&#x79EF;&#x8FD0;&#x7B97;&#xFF09;&#x3002;</p>
<ul>
<li><p><code>stride</code>controls the stride for the cross-correlation.</p>
</li>
<li><p><code>&#x586B;&#x5145;</code>&#x63A7;&#x5236;&#x9690;&#x542B;&#x96F6;&#x8865;&#x767D;&#x7684;&#x4E24;&#x4FA7;&#x4E3A;<code>&#x7684;&#x91CF;&#x6269;&#x5F20; *  &#xFF08;kernel_size  -  1&#xFF09; -  &#x586B;&#x5145;</code>&#x6570;&#x91CF;&#x7684;&#x70B9;&#x3002;&#x8BF7;&#x53C2;&#x9605;&#x4E0B;&#x9762;&#x6CE8;&#x91CA;&#x8BE6;&#x60C5;&#x3002;</p>
</li>
<li><p><code>output_padding</code>&#x63A7;&#x5236;&#x6DFB;&#x52A0;&#x5230;&#x8F93;&#x51FA;&#x5F62;&#x72B6;&#x7684;&#x4E00;&#x4FA7;&#x4E0A;&#x7684;&#x989D;&#x5916;&#x5C3A;&#x5BF8;&#x3002;&#x8BF7;&#x53C2;&#x9605;&#x4E0B;&#x9762;&#x6CE8;&#x91CA;&#x8BE6;&#x60C5;&#x3002;</p>
</li>
<li><p><code>dilation</code>controls the spacing between the kernel points; also known as the &#xE0; trous algorithm. It is harder to describe, but this <a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md" target="_blank">link</a> has a nice visualization of what <code>dilation</code>does.</p>
</li>
<li><p><code>groups</code>controls the connections between inputs and outputs. <code>in_channels</code>and <code>out_channels</code>must both be divisible by <code>groups</code>. For example,</p>
</li>
</ul>
<blockquote>
<pre><code>* At groups=1, all inputs are convolved to all outputs.
</code></pre></blockquote>
<p>&gt;</p>
<blockquote>
<pre><code>* At groups=2, the operation becomes equivalent to having two conv
</code></pre><p>layers side by side, each seeing half the input channels, and producing half
the output channels, and both subsequently concatenated.</p>
</blockquote>
<p>&gt;</p>
<blockquote>
<pre><code>* &#x5728;&#x57FA;&#x56E2;= `in_channels`&#x4E2D;&#xFF0C;&#x6BCF;&#x4E2A;&#x8F93;&#x5165;&#x4FE1;&#x9053;&#x8FDB;&#x884C;&#x5377;&#x79EF;&#x4EE5;&#x5B83;&#x81EA;&#x5DF1;&#x7684;&#x4E00;&#x5957;&#x6EE4;&#x6CE2;&#x5668;&#xFF08;&#x5927;&#x5C0F;[&#x7684; HTG10]&#x230A;  O  U  T  _
</code></pre><p>C  H  &#x4E00; n&#x7684; n&#x7684; E  L  S  i&#x7684; n&#x7684; <em>  C  H  &#x4E00; n&#x7684; n&#x7684; E  L  S  &#x230B; \&#x5DE6;\ lfloor \&#x538B;&#x88C2;{&#x51FA;\
_channels} {&#x5728;\ _channels} \&#x53F3;\ rfloor  &#x230A; i&#x7684; n&#x7684; </em>  C  H  &#x4E00; n&#x7684; n&#x7684; E  L  S  O  U
T  _  C  H  &#x4E00; n&#x7684; n&#x7684; E  L  S  &#x230B; &#xFF09;&#x3002;</p>
</blockquote>
<p>Note</p>
<p>Depending of the size of your kernel, several (of the last) columns of the
input might be lost, because it is a valid <a href="https://en.wikipedia.org/wiki/Cross-correlation" target="_blank">cross-
correlation</a>, and not a full
<a href="https://en.wikipedia.org/wiki/Cross-correlation" target="_blank">cross-correlation</a>. It is up
to the user to add proper padding.</p>
<p>Note</p>
<p>&#x7684;<code>&#x586B;&#x5145;</code>&#x53C2;&#x6570;&#x6709;&#x6548;&#x5730;&#x589E;&#x52A0;&#x4E86;<code>&#x6269;&#x5F20; *  &#xFF08;kernel_size  -  1&#xFF09; -  &#x586B;&#x5145;</code>&#x96F6;&#x586B;&#x5145;&#x7684;&#x91CF;&#x4E0E;&#x8F93;&#x5165;&#x7684;&#x4E24;&#x4E2A;&#x5C3A;&#x5BF8;&#x3002;&#x6B64;&#x88AB;&#x8BBE;&#x7F6E;&#x6210;&#x4F7F;&#x5F97;&#x5F53;&#x4E00;&#x4E2A; <code>Conv1d</code>&#x548C;a<code>ConvTranspose1d</code>&#x4E0E;&#x521D;&#x59CB;&#x5316;&#x76F8;&#x540C;&#x7684;&#x53C2;&#x6570;&#xFF0C;&#x5B83;&#x4EEC;&#x662F;&#x5728;&#x8003;&#x8651;&#x5230;&#x8F93;&#x5165;&#x548C;&#x8F93;&#x51FA;&#x5F62;&#x72B6;&#x5F7C;&#x6B64;&#x7684;&#x9006;&#x3002;&#x7136;&#x800C;&#xFF0C;&#x5F53;<code>&#x6B65;&#x5E45; &amp; GT
;  1</code>&#xFF0C; <code>Conv1d</code>&#x6620;&#x5C04;&#x591A;&#x4E2A;&#x8F93;&#x5165;&#x7684;&#x5F62;&#x72B6;&#xFF0C;&#x4EE5;&#x76F8;&#x540C;&#x7684;&#x8F93;&#x51FA;&#x7684;&#x5F62;&#x72B6;&#x3002; <code>output_padding</code>&#x63D0;&#x4F9B;&#x4E00;&#x79CD;&#x901A;&#x8FC7;&#x6709;&#x6548;&#x5730;&#x589E;&#x52A0;&#x5728;&#x4E00;&#x4FA7;&#x4E0A;&#x6240;&#x8BA1;&#x7B97;&#x51FA;&#x7684;&#x8F93;&#x51FA;&#x7684;&#x5F62;&#x72B6;&#x6765;&#x89E3;&#x51B3;&#x6B64;&#x6A21;&#x7CCA;&#x6027;&#x3002;&#x9700;&#x8981;&#x6CE8;&#x610F;&#x7684;&#x662F;<code>output_padding</code>&#x4EC5;&#x7528;&#x4E8E;&#x67E5;&#x627E;&#x8F93;&#x51FA;&#x7684;&#x5F62;&#x72B6;&#xFF0C;&#x4F46;&#x5B9E;&#x9645;&#x4E0A;&#x5E76;&#x6CA1;&#x6709;&#x589E;&#x52A0;&#x96F6;&#x586B;&#x5145;&#x8F93;&#x51FA;&#x3002;</p>
<p>Note</p>
<p>In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at a
performance cost) by setting <code>torch.backends.cudnn.deterministic = True</code>.
Please see the notes on <a href="notes/randomness.html">Reproducibility</a> for
background.</p>
<p>Parameters</p>
<ul>
<li><p><strong>in_channels</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a>) &#x2013; Number of channels in the input image</p>
</li>
<li><p><strong>out_channels</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a>) &#x2013; Number of channels produced by the convolution</p>
</li>
<li><p><strong>kernel_size</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>or</em><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>tuple</em></a>) &#x2013; Size of the convolving kernel</p>
</li>
<li><p><strong>stride</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>or</em><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>tuple</em></a> <em>,</em> <em>optional</em> ) &#x2013; Stride of the convolution. Default: 1</p>
</li>
<li><p><strong>&#x586B;&#x5145;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#x6216;</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>&#x5143;&#x7EC4;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;&#x7684;</em> &#xFF09; - <code>&#x6269;&#x5F20; *  &#xFF08;kernel_size  -  1&#xFF09; -  &#x586B;&#x5145;</code>&#x96F6;&#x586B;&#x5145;&#x5C06;&#x88AB;&#x6DFB;&#x52A0;&#x5230;&#x8F93;&#x5165;&#x7684;&#x4E24;&#x4FA7;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;0</p>
</li>
<li><p><strong>output_padding</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#x6216;</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>&#x5143;&#x7EC4;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;&#x7684;</em> &#xFF09; - &#x6DFB;&#x52A0;&#x5230;&#x8F93;&#x51FA;&#x5F62;&#x72B6;&#x7684;&#x4E00;&#x4FA7;&#x7684;&#x5176;&#x4ED6;&#x5C3A;&#x5BF8;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;0</p>
</li>
<li><p><strong>groups</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>,</em> <em>optional</em> ) &#x2013; Number of blocked connections from input channels to output channels. Default: 1</p>
</li>
<li><p><strong>bias</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; If <code>True</code>, adds a learnable bias to the output. Default: <code>True</code></p>
</li>
<li><p><strong>dilation</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>or</em><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>tuple</em></a> <em>,</em> <em>optional</em> ) &#x2013; Spacing between kernel elements. Default: 1</p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>Input: (N,Cin,Lin)(N, C<em>{in}, L</em>{in})(N,Cin&#x200B;,Lin&#x200B;)</p>
</li>
<li><p>Output: (N,Cout,Lout)(N, C<em>{out}, L</em>{out})(N,Cout&#x200B;,Lout&#x200B;) where</p>
</li>
</ul>
<p>Lout=(Lin&#x2212;1)&#xD7;stride&#x2212;2&#xD7;padding+dilation&#xD7;(kernel<em>size&#x2212;1)+output_padding+1L</em>{out}
= (L_{in} - 1) \times \text{stride} - 2 \times \text{padding} +
\text{dilation} \times (\text{kernel\_size} - 1) + \text{output\_padding} +
1 Lout&#x200B;=(Lin&#x200B;&#x2212;1)&#xD7;stride&#x2212;2&#xD7;padding+dilation&#xD7;(kernel_size&#x2212;1)+output_padding+1</p>
<p>Variables</p>
<ul>
<li><p><strong>&#x301C;ConvTranspose1d.weight</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x7684;&#x5F62;&#x72B6; [&#x8BE5;&#x6A21;&#x5757;&#x7684;&#x53EF;&#x5B66;&#x4E60;&#x6743;&#x91CD;HTG11] &#xFF08; in<em>channels  &#xFF0C; out_channels  &#x57FA;&#x56E2; &#xFF0C; &#xFF08;\&#x6587;&#x672C;{&#x5728;\ _channels}&#xFF0C;\&#x538B;&#x88C2;{\&#x6587;&#x672C;{&#x51FA;\ _channels}} {\&#x6587;&#x672C;{&#x57FA;}}&#xFF0C; &#xFF08; in_channels  &#xFF0C; &#x57FA;&#x56E2; out_channels  [HTG8 8]  &#xFF0C; kernel_size  &#xFF09; \&#x6587;&#x672C;{&#x5185;&#x6838;\ _size}&#xFF09; kernel_size  &#xFF09; &#x3002;&#x8FD9;&#x4E9B;&#x6743;&#x91CD;&#x7684;&#x503C;&#x662F;&#x4ECE; &#x53D6;&#x6837;U  &#xFF08; -  K  &#xFF0C; K  &#xFF09; \ mathcal&#x3010;U}&#xFF08; - \ SQRT {K}&#xFF0C;\ SQRT {K}&#xFF09; U  &#xFF08; -  K  &#xFF0C; K  [H TG224]  &#xFF09; &#x5176;&#x4E2D; K  =  1  C &#x200B;&#x200B; &#x5728; *  kernel_size  K = \&#x538B;&#x88C2;{1} {C</em> \&#x6587;&#x672C;{IN} <em> \&#x6587;&#x672C;{&#x5185;&#x6838;\ _size}}  K  =  &#xE7;  &#x5728; </em>  kernel_size  1 </p>
</li>
<li><p><strong>&#x301C;ConvTranspose1d.bias</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x5F62;&#x72B6;&#xFF08;out<em>channels&#xFF09;&#x7684;&#x6A21;&#x5757;&#x7684;&#x53EF;&#x5B66;&#x4E60;&#x504F;&#x5DEE;&#x3002;&#x5982;&#x679C;<code>&#x504F;&#x538B;</code>&#x662F;<code>&#x771F;</code>&#xFF0C;&#x7136;&#x540E;&#x8FD9;&#x4E9B;&#x6743;&#x91CD;&#x7684;&#x503C;&#x662F;&#x4ECE; &#x53D6;&#x6837; U  &#xFF08; -  K  &#xFF0C; K  &#xFF09; \ mathcal&#x3010;U}&#xFF08; - \ SQRT {K}&#xFF0C;\ SQRT {K}&#xFF09; U  &#xFF08; -  K  K  &#xFF09; &#x5176;&#x4E2D; K  =  1  C  &#x5728; *  kernel_size  K = \&#x538B;&#x88C2;{1} {C</em> \&#x6587;&#x672C;{IN} <em> \&#x6587;&#x672C;{&#x5185;&#x6838;\ _size}}  K  =  C  &#x5728; </em>  kernel_size  1 </p>
</li>
</ul>
<h3 id="convtranspose2d">ConvTranspose2d</h3>
<p><em>class</em><code>torch.nn.``ConvTranspose2d</code>( <em>in_channels</em> , <em>out_channels</em> ,
<em>kernel_size</em> , <em>stride=1</em> , <em>padding=0</em> , <em>output_padding=0</em> , <em>groups=1</em> ,
<em>bias=True</em> , <em>dilation=1</em> , <em>padding_mode=&apos;zeros&apos;</em>
)<a href="_modules/torch/nn/modules/conv.html#ConvTranspose2d">[source]</a></p>
<p>&#x5E94;&#x7528;&#x5728;&#x591A;&#x4E2A;&#x8F93;&#x5165;&#x5E73;&#x9762;&#x6784;&#x6210;&#x7684;&#x8F93;&#x5165;&#x56FE;&#x50CF;&#x7684;2D&#x8F6C;&#x5377;&#x79EF;&#x8FD0;&#x7B97;&#x3002;</p>
<p>&#x6B64;&#x6A21;&#x5757;&#x53EF;&#x4EE5;&#x88AB;&#x89C6;&#x4E3A;Conv2d&#x7684;&#x68AF;&#x5EA6;&#x76F8;&#x5BF9;&#x4E8E;&#x5B83;&#x7684;&#x8F93;&#x5165;&#x3002;&#x5B83;&#x4E5F;&#x88AB;&#x79F0;&#x4E3A;&#x4E00;&#x5206;&#x7EA7;-&#x8DE8;&#x8DDD;&#x5377;&#x79EF;&#x6216;&#x53BB;&#x5377;&#x79EF;&#xFF08;&#x5C3D;&#x7BA1;&#x5B83;&#x4E0D;&#x662F;&#x4E00;&#x4E2A;&#x5B9E;&#x9645;&#x7684;&#x53BB;&#x5377;&#x79EF;&#x8FD0;&#x7B97;&#xFF09;&#x3002;</p>
<ul>
<li><p><code>stride</code>controls the stride for the cross-correlation.</p>
</li>
<li><p><code>padding</code>controls the amount of implicit zero-paddings on both sides for <code>dilation * (kernel_size - 1) - padding</code>number of points. See note below for details.</p>
</li>
<li><p><code>output_padding</code>controls the additional size added to one side of the output shape. See note below for details.</p>
</li>
<li><p><code>dilation</code>controls the spacing between the kernel points; also known as the &#xE0; trous algorithm. It is harder to describe, but this <a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md" target="_blank">link</a> has a nice visualization of what <code>dilation</code>does.</p>
</li>
<li><p><code>groups</code>controls the connections between inputs and outputs. <code>in_channels</code>and <code>out_channels</code>must both be divisible by <code>groups</code>. For example,</p>
</li>
</ul>
<blockquote>
<pre><code>* At groups=1, all inputs are convolved to all outputs.
</code></pre></blockquote>
<p>&gt;</p>
<blockquote>
<pre><code>* At groups=2, the operation becomes equivalent to having two conv
</code></pre><p>layers side by side, each seeing half the input channels, and producing half
the output channels, and both subsequently concatenated.</p>
</blockquote>
<p>&gt;</p>
<blockquote>
<pre><code>* At groups= `in_channels`, each input channel is convolved with its own
</code></pre><p>set of filters (of size
&#x230A;out_channelsin_channels&#x230B;\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor&#x230A;in_channelsout_channels&#x200B;&#x230B;
).</p>
</blockquote>
<p>&#x53C2;&#x6570;<code>kernel_size</code>&#xFF0C;<code>&#x6B65;&#x5E45;</code>&#xFF0C;<code>&#x586B;&#x5145;</code>&#xFF0C;<code>output_padding</code>&#x53EF;&#x4EE5;&#x662F;&#xFF1A;</p>
<blockquote>
<ul>
<li>&#x5355;&#x4E00;<code>INT</code>- &#x5728;&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;&#x76F8;&#x540C;&#x7684;&#x503C;&#x88AB;&#x7528;&#x4E8E;&#x9AD8;&#x5EA6;&#x548C;&#x5BBD;&#x5EA6;&#x5C3A;&#x5BF8;</li>
</ul>
</blockquote>
<p>&gt;</p>
<blockquote>
<ul>
<li>a <code>tuple</code>of two ints &#x2013; in which case, the first int is used for the
height dimension, and the second int for the width dimension</li>
</ul>
</blockquote>
<p>&gt;</p>
<p>&gt;</p>
<p>Note</p>
<p>Depending of the size of your kernel, several (of the last) columns of the
input might be lost, because it is a valid <a href="https://en.wikipedia.org/wiki/Cross-correlation" target="_blank">cross-
correlation</a>, and not a full
<a href="https://en.wikipedia.org/wiki/Cross-correlation" target="_blank">cross-correlation</a>. It is up
to the user to add proper padding.</p>
<p>Note</p>
<p>&#x7684;<code>&#x586B;&#x5145;</code>&#x53C2;&#x6570;&#x6709;&#x6548;&#x5730;&#x589E;&#x52A0;&#x4E86;<code>&#x6269;&#x5F20; *  &#xFF08;kernel_size  -  1&#xFF09; -  &#x586B;&#x5145;</code>&#x96F6;&#x586B;&#x5145;&#x7684;&#x91CF;&#x4E0E;&#x8F93;&#x5165;&#x7684;&#x4E24;&#x4E2A;&#x5C3A;&#x5BF8;&#x3002;&#x6B64;&#x88AB;&#x8BBE;&#x7F6E;&#x6210;&#x4F7F;&#x5F97;&#x5F53;&#x4E00;&#x4E2A; <code>Conv2d</code>&#x548C;a<code>ConvTranspose2d</code>&#x4E0E;&#x521D;&#x59CB;&#x5316;&#x76F8;&#x540C;&#x7684;&#x53C2;&#x6570;&#xFF0C;&#x5B83;&#x4EEC;&#x662F;&#x5728;&#x8003;&#x8651;&#x5230;&#x8F93;&#x5165;&#x548C;&#x8F93;&#x51FA;&#x5F62;&#x72B6;&#x5F7C;&#x6B64;&#x7684;&#x9006;&#x3002;&#x7136;&#x800C;&#xFF0C;&#x5F53;<code>&#x6B65;&#x5E45; &amp; GT
;  1</code>&#xFF0C; <code>Conv2d</code>&#x6620;&#x5C04;&#x591A;&#x4E2A;&#x8F93;&#x5165;&#x7684;&#x5F62;&#x72B6;&#xFF0C;&#x4EE5;&#x76F8;&#x540C;&#x7684;&#x8F93;&#x51FA;&#x7684;&#x5F62;&#x72B6;&#x3002; <code>output_padding</code>&#x63D0;&#x4F9B;&#x4E00;&#x79CD;&#x901A;&#x8FC7;&#x6709;&#x6548;&#x5730;&#x589E;&#x52A0;&#x5728;&#x4E00;&#x4FA7;&#x4E0A;&#x6240;&#x8BA1;&#x7B97;&#x51FA;&#x7684;&#x8F93;&#x51FA;&#x7684;&#x5F62;&#x72B6;&#x6765;&#x89E3;&#x51B3;&#x6B64;&#x6A21;&#x7CCA;&#x6027;&#x3002;&#x9700;&#x8981;&#x6CE8;&#x610F;&#x7684;&#x662F;<code>output_padding</code>&#x4EC5;&#x7528;&#x4E8E;&#x67E5;&#x627E;&#x8F93;&#x51FA;&#x7684;&#x5F62;&#x72B6;&#xFF0C;&#x4F46;&#x5B9E;&#x9645;&#x4E0A;&#x5E76;&#x6CA1;&#x6709;&#x589E;&#x52A0;&#x96F6;&#x586B;&#x5145;&#x8F93;&#x51FA;&#x3002;</p>
<p>Note</p>
<p>In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at a
performance cost) by setting <code>torch.backends.cudnn.deterministic = True</code>.
Please see the notes on <a href="notes/randomness.html">Reproducibility</a> for
background.</p>
<p>Parameters</p>
<ul>
<li><p><strong>in_channels</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a>) &#x2013; Number of channels in the input image</p>
</li>
<li><p><strong>out_channels</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a>) &#x2013; Number of channels produced by the convolution</p>
</li>
<li><p><strong>kernel_size</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>or</em><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>tuple</em></a>) &#x2013; Size of the convolving kernel</p>
</li>
<li><p><strong>stride</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>or</em><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>tuple</em></a> <em>,</em> <em>optional</em> ) &#x2013; Stride of the convolution. Default: 1</p>
</li>
<li><p><strong>&#x586B;&#x5145;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#x6216;</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>&#x5143;&#x7EC4;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;&#x7684;</em> &#xFF09; - <code>&#x6269;&#x5F20; *  &#xFF08;kernel_size  -  1&#xFF09; -  &#x586B;&#x5145;</code>&#x96F6;&#x586B;&#x5145;&#x5C06;&#x88AB;&#x6DFB;&#x52A0;&#x5230;&#x8F93;&#x5165;&#x4E2D;&#x7684;&#x6BCF;&#x4E2A;&#x7EF4;&#x5EA6;&#x7684;&#x4E24;&#x4FA7;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;0</p>
</li>
<li><p><strong>output_padding</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#x6216;</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>&#x5143;&#x7EC4;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;&#x7684;</em> &#xFF09; - &#x52A0;&#x5165;&#x5230;&#x6BCF;&#x4E2A;&#x7EF4;&#x5EA6;&#x4E2D;&#x7684;&#x4E00;&#x4E2A;&#x4FA7;&#x7684;&#x8F93;&#x51FA;&#x5F62;&#x72B6;&#x7684;&#x5176;&#x4ED6;&#x5C3A;&#x5BF8;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;0</p>
</li>
<li><p><strong>groups</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>,</em> <em>optional</em> ) &#x2013; Number of blocked connections from input channels to output channels. Default: 1</p>
</li>
<li><p><strong>bias</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; If <code>True</code>, adds a learnable bias to the output. Default: <code>True</code></p>
</li>
<li><p><strong>dilation</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>or</em><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>tuple</em></a> <em>,</em> <em>optional</em> ) &#x2013; Spacing between kernel elements. Default: 1</p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>Input: (N,Cin,Hin,Win)(N, C<em>{in}, H</em>{in}, W_{in})(N,Cin&#x200B;,Hin&#x200B;,Win&#x200B;)</p>
</li>
<li><p>Output: (N,Cout,Hout,Wout)(N, C<em>{out}, H</em>{out}, W_{out})(N,Cout&#x200B;,Hout&#x200B;,Wout&#x200B;) where</p>
</li>
</ul>
<p>Hout=(Hin&#x2212;1)&#xD7;stride[0]&#x2212;2&#xD7;padding[0]+dilation[0]&#xD7;(kernel<em>size[0]&#x2212;1)+output_padding[0]+1H</em>{out}
= (H_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] +
\text{dilation}[0] \times (\text{kernel\_size}[0] - 1) +
\text{output\_padding}[0] + 1
Hout&#x200B;=(Hin&#x200B;&#x2212;1)&#xD7;stride[0]&#x2212;2&#xD7;padding[0]+dilation[0]&#xD7;(kernel_size[0]&#x2212;1)+output_padding[0]+1</p>
<p>Wout=(Win&#x2212;1)&#xD7;stride[1]&#x2212;2&#xD7;padding[1]+dilation[1]&#xD7;(kernel<em>size[1]&#x2212;1)+output_padding[1]+1W</em>{out}
= (W_{in} - 1) \times \text{stride}[1] - 2 \times \text{padding}[1] +
\text{dilation}[1] \times (\text{kernel\_size}[1] - 1) +
\text{output\_padding}[1] + 1
Wout&#x200B;=(Win&#x200B;&#x2212;1)&#xD7;stride[1]&#x2212;2&#xD7;padding[1]+dilation[1]&#xD7;(kernel_size[1]&#x2212;1)+output_padding[1]+1</p>
<p>Variables</p>
<ul>
<li><p><strong>&#x301C;ConvTranspose2d.weight</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x7684;&#x5F62;&#x72B6; [&#x8BE5;&#x6A21;&#x5757;&#x7684;&#x53EF;&#x5B66;&#x4E60;&#x6743;&#x91CD;HTG11] &#xFF08; in<em>channels  &#xFF0C; out_channels  &#x57FA;&#x56E2; &#xFF0C; &#xFF08;\&#x6587;&#x672C;{&#x5728;\ _channels}&#xFF0C;\&#x538B;&#x88C2;{\&#x6587;&#x672C;{&#x51FA;\ _channels}} {\&#x6587;&#x672C;{&#x57FA;}}&#xFF0C; &#xFF08; in_channels  &#xFF0C; &#x57FA;&#x56E2; out_channels  [HTG8 8]  &#xFF0C; kernel_size [0]  &#xFF0C; kernel_size [1]  &#xFF09; \&#x6587;&#x672C;{&#x5185;&#x6838;\ _size [0]}&#xFF0C;\&#x6587;&#x672C;{&#x5185;&#x6838;\ _size [1]}&#xFF09; kernel_size [0]  &#xFF0C; kernel_size [1]  &#xFF09; &#x3002;&#x8FD9;&#x4E9B;&#x6743;&#x91CD;&#x7684;&#x503C;&#x662F;&#x4ECE; &#x53D6;&#x6837;U  &#xFF08; -  K  &#xFF0C; K  &#xFF09; \ mathcal&#x3010;U}&#xFF08; - \ SQRT {K}&#xFF0C;\ SQRT {K}&#xFF09; U  &#xFF08; -  K  &#xFF0C; K  [H TG236]  &#xFF09; &#x5176;&#x4E2D; &#x200B;&#x200B;  K  =  1  C  &#x5728; *  &#x3A0; i&#x7684; =  0  1  kernel_size  [ i&#x7684; K = \&#x538B;&#x88C2;{1} {C</em> \&#x6587;&#x672C;{IN} <em> \ prod_ {I = 0} ^ {1} \&#x6587;&#x672C;{&#x5185;&#x6838;\ _size} [I]}  K  =  C  &#x5728; </em>  &#x3A0; i&#x7684; =  0  1  kernel_size  [ i&#x7684; 1 </p>
</li>
<li><p><strong>&#x301C;ConvTranspose2d.bias</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x5F62;&#x72B6;&#xFF08;out<em>channels&#xFF09;&#x7684;&#x6A21;&#x5757;&#x7684;&#x53EF;&#x5B66;&#x4E60;&#x504F;&#x538B;&#x5982;&#x679C;<code>&#x504F;&#x538B;</code>&#x662F;<code>&#x771F;</code>&#xFF0C;&#x7136;&#x540E;&#x8FD9;&#x4E9B;&#x6743;&#x91CD;&#x7684;&#x503C;&#x4ECE;&#x91C7;&#x6837; U  &#xFF08; -  K  &#xFF0C; K  &#xFF09; \ mathcal&#x3010;U}&#xFF08; - \ SQRT {K}&#xFF0C;\ SQRT {K}&#xFF09; U  &#xFF08; -  K  &#xFF0C; K  &#xFF09; &#x5176;&#x4E2D; K  =  1  C  &#x5728; *  &#x3A0; i&#x7684; =  0  1  kernel_size  [ i&#x7684; &#x137; = \&#x538B;&#x88C2;{1} {C</em> \&#x6587;&#x672C;{IN} <em> \ prod_ {I = 0} ^ {1} \&#x6587;&#x672C;{&#x5185;&#x6838;\ _size} [I]}  K  =  C  &#x5728; </em>  &#x3A0; i&#x7684; =  0  &#x200B;&#x200B;  1  kernel_size  [ i&#x7684; 1 </p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; # With square kernels and equal stride
&gt;&gt;&gt; m = nn.ConvTranspose2d(16, 33, 3, stride=2)
&gt;&gt;&gt; # non-square kernels and unequal stride and with padding
&gt;&gt;&gt; m = nn.ConvTranspose2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))
&gt;&gt;&gt; input = torch.randn(20, 16, 50, 100)
&gt;&gt;&gt; output = m(input)
&gt;&gt;&gt; # exact output size can be also specified as an argument
&gt;&gt;&gt; input = torch.randn(1, 16, 12, 12)
&gt;&gt;&gt; downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1)
&gt;&gt;&gt; upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1)
&gt;&gt;&gt; h = downsample(input)
&gt;&gt;&gt; h.size()
torch.Size([1, 16, 6, 6])
&gt;&gt;&gt; output = upsample(h, output_size=input.size())
&gt;&gt;&gt; output.size()
torch.Size([1, 16, 12, 12])
</code></pre><h3 id="convtranspose3d">ConvTranspose3d</h3>
<p><em>class</em><code>torch.nn.``ConvTranspose3d</code>( <em>in_channels</em> , <em>out_channels</em> ,
<em>kernel_size</em> , <em>stride=1</em> , <em>padding=0</em> , <em>output_padding=0</em> , <em>groups=1</em> ,
<em>bias=True</em> , <em>dilation=1</em> , <em>padding_mode=&apos;zeros&apos;</em>
)<a href="_modules/torch/nn/modules/conv.html#ConvTranspose3d">[source]</a></p>
<p>&#x5E94;&#x7528;&#x5728;&#x591A;&#x4E2A;&#x8F93;&#x5165;&#x5E73;&#x9762;&#x6784;&#x6210;&#x7684;&#x8F93;&#x5165;&#x56FE;&#x50CF;&#x7684;3D&#x6362;&#x4F4D;&#x5377;&#x79EF;&#x8FD0;&#x7B97;&#x3002;&#x8F6C;&#x7F6E;&#x5377;&#x79EF;&#x8FD0;&#x7B97;&#x7B26;&#x4E58;&#x4EE5;&#x5728;&#x4ECE;&#x6240;&#x6709;&#x8F93;&#x5165;&#x7279;&#x5F81;&#x5E73;&#x9762;&#x8F93;&#x51FA;&#x6BCF;&#x4E2A;&#x8F93;&#x5165;&#x503C;&#x9010;&#x5143;&#x7D20;&#x7531;&#x4E00;&#x4E2A;&#x53EF;&#x5B66;&#x4E60;&#x7684;&#x5185;&#x6838;&#xFF0C;&#x548C;&#x6C42;&#x548C;&#x3002;</p>
<p>&#x6B64;&#x6A21;&#x5757;&#x53EF;&#x4EE5;&#x88AB;&#x89C6;&#x4E3A;Conv3d&#x7684;&#x68AF;&#x5EA6;&#x76F8;&#x5BF9;&#x4E8E;&#x5B83;&#x7684;&#x8F93;&#x5165;&#x3002;&#x5B83;&#x4E5F;&#x88AB;&#x79F0;&#x4E3A;&#x4E00;&#x5206;&#x7EA7;-&#x8DE8;&#x8DDD;&#x5377;&#x79EF;&#x6216;&#x53BB;&#x5377;&#x79EF;&#xFF08;&#x5C3D;&#x7BA1;&#x5B83;&#x4E0D;&#x662F;&#x4E00;&#x4E2A;&#x5B9E;&#x9645;&#x7684;&#x53BB;&#x5377;&#x79EF;&#x8FD0;&#x7B97;&#xFF09;&#x3002;</p>
<ul>
<li><p><code>stride</code>controls the stride for the cross-correlation.</p>
</li>
<li><p><code>padding</code>controls the amount of implicit zero-paddings on both sides for <code>dilation * (kernel_size - 1) - padding</code>number of points. See note below for details.</p>
</li>
<li><p><code>output_padding</code>controls the additional size added to one side of the output shape. See note below for details.</p>
</li>
<li><p><code>dilation</code>controls the spacing between the kernel points; also known as the &#xE0; trous algorithm. It is harder to describe, but this <a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md" target="_blank">link</a> has a nice visualization of what <code>dilation</code>does.</p>
</li>
<li><p><code>groups</code>controls the connections between inputs and outputs. <code>in_channels</code>and <code>out_channels</code>must both be divisible by <code>groups</code>. For example,</p>
</li>
</ul>
<blockquote>
<pre><code>* At groups=1, all inputs are convolved to all outputs.
</code></pre></blockquote>
<p>&gt;</p>
<blockquote>
<pre><code>* At groups=2, the operation becomes equivalent to having two conv
</code></pre><p>layers side by side, each seeing half the input channels, and producing half
the output channels, and both subsequently concatenated.</p>
</blockquote>
<p>&gt;</p>
<blockquote>
<pre><code>* At groups= `in_channels`, each input channel is convolved with its own
</code></pre><p>set of filters (of size
&#x230A;out_channelsin_channels&#x230B;\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor&#x230A;in_channelsout_channels&#x200B;&#x230B;
).</p>
</blockquote>
<p>The parameters <code>kernel_size</code>, <code>stride</code>, <code>padding</code>, <code>output_padding</code>can either
be:</p>
<blockquote>
<ul>
<li>&#x5355;&#x4E00;<code>INT</code>- &#x5728;&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;&#x76F8;&#x540C;&#x7684;&#x503C;&#x88AB;&#x7528;&#x4E8E;&#x6DF1;&#x5EA6;&#xFF0C;&#x9AD8;&#x5EA6;&#x548C;&#x5BBD;&#x5EA6;&#x5C3A;&#x5BF8;</li>
</ul>
</blockquote>
<p>&gt;</p>
<blockquote>
<ul>
<li>a <code>tuple</code>of three ints &#x2013; in which case, the first int is used for the
depth dimension, the second int for the height dimension and the third int for
the width dimension</li>
</ul>
</blockquote>
<p>&gt;</p>
<p>&gt;</p>
<p>Note</p>
<p>Depending of the size of your kernel, several (of the last) columns of the
input might be lost, because it is a valid <a href="https://en.wikipedia.org/wiki/Cross-correlation" target="_blank">cross-
correlation</a>, and not a full
<a href="https://en.wikipedia.org/wiki/Cross-correlation" target="_blank">cross-correlation</a>. It is up
to the user to add proper padding.</p>
<p>Note</p>
<p>&#x7684;<code>&#x586B;&#x5145;</code>&#x53C2;&#x6570;&#x6709;&#x6548;&#x5730;&#x589E;&#x52A0;&#x4E86;<code>&#x6269;&#x5F20; *  &#xFF08;kernel_size  -  1&#xFF09; -  &#x586B;&#x5145;</code>&#x96F6;&#x586B;&#x5145;&#x7684;&#x91CF;&#x4E0E;&#x8F93;&#x5165;&#x7684;&#x4E24;&#x4E2A;&#x5C3A;&#x5BF8;&#x3002;&#x6B64;&#x88AB;&#x8BBE;&#x7F6E;&#x6210;&#x4F7F;&#x5F97;&#x5F53;&#x4E00;&#x4E2A; <code>Conv3d</code>&#x548C;a<code>ConvTranspose3d</code>&#x4E0E;&#x521D;&#x59CB;&#x5316;&#x76F8;&#x540C;&#x7684;&#x53C2;&#x6570;&#xFF0C;&#x5B83;&#x4EEC;&#x662F;&#x5728;&#x8003;&#x8651;&#x5230;&#x8F93;&#x5165;&#x548C;&#x8F93;&#x51FA;&#x5F62;&#x72B6;&#x5F7C;&#x6B64;&#x7684;&#x9006;&#x3002;&#x7136;&#x800C;&#xFF0C;&#x5F53;<code>&#x6B65;&#x5E45; &amp; GT
;  1</code>&#xFF0C; <code>Conv3d</code>&#x6620;&#x5C04;&#x591A;&#x4E2A;&#x8F93;&#x5165;&#x7684;&#x5F62;&#x72B6;&#xFF0C;&#x4EE5;&#x76F8;&#x540C;&#x7684;&#x8F93;&#x51FA;&#x7684;&#x5F62;&#x72B6;&#x3002; <code>output_padding</code>&#x63D0;&#x4F9B;&#x4E00;&#x79CD;&#x901A;&#x8FC7;&#x6709;&#x6548;&#x5730;&#x589E;&#x52A0;&#x5728;&#x4E00;&#x4FA7;&#x4E0A;&#x6240;&#x8BA1;&#x7B97;&#x51FA;&#x7684;&#x8F93;&#x51FA;&#x7684;&#x5F62;&#x72B6;&#x6765;&#x89E3;&#x51B3;&#x6B64;&#x6A21;&#x7CCA;&#x6027;&#x3002;&#x9700;&#x8981;&#x6CE8;&#x610F;&#x7684;&#x662F;<code>output_padding</code>&#x4EC5;&#x7528;&#x4E8E;&#x67E5;&#x627E;&#x8F93;&#x51FA;&#x7684;&#x5F62;&#x72B6;&#xFF0C;&#x4F46;&#x5B9E;&#x9645;&#x4E0A;&#x5E76;&#x6CA1;&#x6709;&#x589E;&#x52A0;&#x96F6;&#x586B;&#x5145;&#x8F93;&#x51FA;&#x3002;</p>
<p>Note</p>
<p>In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at a
performance cost) by setting <code>torch.backends.cudnn.deterministic = True</code>.
Please see the notes on <a href="notes/randomness.html">Reproducibility</a> for
background.</p>
<p>Parameters</p>
<ul>
<li><p><strong>in_channels</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a>) &#x2013; Number of channels in the input image</p>
</li>
<li><p><strong>out_channels</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a>) &#x2013; Number of channels produced by the convolution</p>
</li>
<li><p><strong>kernel_size</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>or</em><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>tuple</em></a>) &#x2013; Size of the convolving kernel</p>
</li>
<li><p><strong>stride</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>or</em><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>tuple</em></a> <em>,</em> <em>optional</em> ) &#x2013; Stride of the convolution. Default: 1</p>
</li>
<li><p><strong>padding</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>or</em><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>tuple</em></a> <em>,</em> <em>optional</em> ) &#x2013; <code>dilation * (kernel_size - 1) - padding</code>zero-padding will be added to both sides of each dimension in the input. Default: 0</p>
</li>
<li><p><strong>output_padding</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>or</em><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>tuple</em></a> <em>,</em> <em>optional</em> ) &#x2013; Additional size added to one side of each dimension in the output shape. Default: 0</p>
</li>
<li><p><strong>groups</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>,</em> <em>optional</em> ) &#x2013; Number of blocked connections from input channels to output channels. Default: 1</p>
</li>
<li><p><strong>bias</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; If <code>True</code>, adds a learnable bias to the output. Default: <code>True</code></p>
</li>
<li><p><strong>dilation</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>or</em><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>tuple</em></a> <em>,</em> <em>optional</em> ) &#x2013; Spacing between kernel elements. Default: 1</p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>Input: (N,Cin,Din,Hin,Win)(N, C<em>{in}, D</em>{in}, H<em>{in}, W</em>{in})(N,Cin&#x200B;,Din&#x200B;,Hin&#x200B;,Win&#x200B;)</p>
</li>
<li><p>Output: (N,Cout,Dout,Hout,Wout)(N, C<em>{out}, D</em>{out}, H<em>{out}, W</em>{out})(N,Cout&#x200B;,Dout&#x200B;,Hout&#x200B;,Wout&#x200B;) where</p>
</li>
</ul>
<p>Dout=(Din&#x2212;1)&#xD7;stride[0]&#x2212;2&#xD7;padding[0]+dilation[0]&#xD7;(kernel<em>size[0]&#x2212;1)+output_padding[0]+1D</em>{out}
= (D_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] +
\text{dilation}[0] \times (\text{kernel\_size}[0] - 1) +
\text{output\_padding}[0] + 1
Dout&#x200B;=(Din&#x200B;&#x2212;1)&#xD7;stride[0]&#x2212;2&#xD7;padding[0]+dilation[0]&#xD7;(kernel_size[0]&#x2212;1)+output_padding[0]+1</p>
<p>Hout=(Hin&#x2212;1)&#xD7;stride[1]&#x2212;2&#xD7;padding[1]+dilation[1]&#xD7;(kernel<em>size[1]&#x2212;1)+output_padding[1]+1H</em>{out}
= (H_{in} - 1) \times \text{stride}[1] - 2 \times \text{padding}[1] +
\text{dilation}[1] \times (\text{kernel\_size}[1] - 1) +
\text{output\_padding}[1] + 1
Hout&#x200B;=(Hin&#x200B;&#x2212;1)&#xD7;stride[1]&#x2212;2&#xD7;padding[1]+dilation[1]&#xD7;(kernel_size[1]&#x2212;1)+output_padding[1]+1</p>
<p>Wout=(Win&#x2212;1)&#xD7;stride[2]&#x2212;2&#xD7;padding[2]+dilation[2]&#xD7;(kernel<em>size[2]&#x2212;1)+output_padding[2]+1W</em>{out}
= (W_{in} - 1) \times \text{stride}[2] - 2 \times \text{padding}[2] +
\text{dilation}[2] \times (\text{kernel\_size}[2] - 1) +
\text{output\_padding}[2] + 1
Wout&#x200B;=(Win&#x200B;&#x2212;1)&#xD7;stride[2]&#x2212;2&#xD7;padding[2]+dilation[2]&#xD7;(kernel_size[2]&#x2212;1)+output_padding[2]+1</p>
<p>Variables</p>
<ul>
<li><p><strong>&#x301C;ConvTranspose3d.weight</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x7684;&#x5F62;&#x72B6; [&#x8BE5;&#x6A21;&#x5757;&#x7684;&#x53EF;&#x5B66;&#x4E60;&#x6743;&#x91CD;HTG11] &#xFF08; in<em>channels  &#xFF0C; out_channels  &#x57FA;&#x56E2; &#xFF0C; &#xFF08;\&#x6587;&#x672C;{&#x5728;\ _channels}&#xFF0C;\&#x538B;&#x88C2;{\&#x6587;&#x672C;{&#x51FA;\ _channels}} {\&#x6587;&#x672C;{&#x57FA;}}&#xFF0C; &#xFF08; in_channels  &#xFF0C; &#x57FA;&#x56E2; out_channels  [HTG8 8]  &#xFF0C; kernel_size [0]  &#xFF0C; kernel_size [1]  &#xFF0C; kernel_size [2]  &#xFF09; \&#x6587;&#x672C;{&#x5185;&#x6838;\ _size [0]}&#xFF0C;\&#x6587;&#x672C;{&#x5185;&#x6838;\ _size [1]}&#xFF0C;\&#x6587;&#x672C;{&#x5185;&#x6838;\ _size [2]}&#xFF09; kernel_size [0]  kernel_size [1]  &#xFF0C; kernel_size [2]  &#xFF09; &#x3002;&#x8FD9;&#x4E9B;&#x6743;&#x91CD;&#x7684;&#x503C;&#x662F;&#x4ECE; &#x53D6;&#x6837;U  &#xFF08; -  K  &#xFF0C; K  &#xFF09; \ mathcal&#x3010;U}&#xFF08; - \ SQRT {K}&#xFF0C;\ SQRT {K}&#xFF09; U  &#xFF08; -  K  &#xFF0C; K  [H TG248]  &#x200B;&#x200B;  &#xFF09; &#x5176;&#x4E2D; K  =  1  C  &#x5728; *  &#x3A0; i&#x7684; =  0  2  kernel_size  [ i&#x7684; K = \&#x538B;&#x88C2;{1} {C</em> \&#x6587;&#x672C;{IN} <em> \ prod_ {I = 0} ^ {2} \&#x6587;&#x672C;{&#x5185;&#x6838;\ _size} [I]}  K  =  C  &#x5728; </em>  &#x3A0; i&#x7684; =  0  2  kernel_size  [ i&#x7684; 1 </p>
</li>
<li><p><strong>&#x301C;ConvTranspose3d.bias</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x5F62;&#x72B6;&#xFF08;out<em>channels&#xFF09;&#x7684;&#x6A21;&#x5757;&#x7684;&#x53EF;&#x5B66;&#x4E60;&#x504F;&#x538B;&#x5982;&#x679C;<code>&#x504F;&#x538B;</code>&#x662F;<code>&#x771F;</code>&#xFF0C;&#x7136;&#x540E;&#x8FD9;&#x4E9B;&#x6743;&#x91CD;&#x7684;&#x503C;&#x4ECE;&#x91C7;&#x6837; U  &#xFF08; -  K  &#xFF0C; K  &#xFF09; \ mathcal&#x3010;U}&#xFF08; - \ SQRT {K}&#xFF0C;\ SQRT {K}&#xFF09; U  &#xFF08; -  K  &#xFF0C; K  &#xFF09; &#x5176;&#x4E2D; K  =  1  C  &#x5728; *  &#x3A0; i&#x7684; =  0  2  kernel_size  [ i&#x7684; &#x137; = \&#x538B;&#x88C2;{1} {C</em> \&#x6587;&#x672C;{IN} <em> \ prod_ {I = 0} ^ {2} \&#x6587;&#x672C;{&#x5185;&#x6838;\ _size} [I]}  K  =  C  &#x5728; </em>  &#x3A0; i&#x7684; =  0  &#x200B;&#x200B;  2  kernel_size  [ i&#x7684; 1 </p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; # With square kernels and equal stride
&gt;&gt;&gt; m = nn.ConvTranspose3d(16, 33, 3, stride=2)
&gt;&gt;&gt; # non-square kernels and unequal stride and with padding
&gt;&gt;&gt; m = nn.ConvTranspose3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(0, 4, 2))
&gt;&gt;&gt; input = torch.randn(20, 16, 10, 50, 100)
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="&#x5C55;&#x5F00;">&#x5C55;&#x5F00;</h3>
<p><em>class</em><code>torch.nn.``Unfold</code>( <em>kernel_size</em> , <em>dilation=1</em> , <em>padding=0</em> ,
<em>stride=1</em> )<a href="_modules/torch/nn/modules/fold.html#Unfold">[source]</a></p>
<p>&#x63D0;&#x53D6;&#x7269;&#x6210;&#x6279;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x6ED1;&#x52A8;&#x5C40;&#x90E8;&#x5757;&#x3002;</p>
<p>&#x8003;&#x8651;&#x4E00;&#x4E2A;&#x6210;&#x6279;<code>&#x8F93;&#x5165;</code>&#x7684;&#x5F20;&#x91CF;&#x5F62;&#x72B6; &#xFF08; N  &#xFF0C; C  &#xFF0C; <em>  &#xFF09; &#xFF08;N&#xFF0C;C&#xFF0C;</em>&#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF0C; <em>  &#xFF09; &#xFF0C;&#x5176;&#x4E2D; N  N  N
&#x662F;&#x6279;&#x5904;&#x7406;&#x5C3A;&#x5BF8;&#xFF0C; &#xE7;  C  C  &#x662F;&#x4FE1;&#x9053;&#x5C3A;&#x5BF8;&#xFF0C;&#x5E76; </em>  <em>  </em>  &#x8868;&#x793A;&#x4EFB;&#x610F;&#x7684;&#x7A7A;&#x95F4;&#x5C3A;&#x5BF8;&#x3002;&#x8BE5;&#x64CD;&#x4F5C;&#x53D8;&#x5E73;&#x7684;<code>&#x8F93;&#x5165;</code>&#x7684;&#x7A7A;&#x95F4;&#x5C3A;&#x5BF8;&#x4E2D;&#x7684;&#x6BCF;&#x4E2A;&#x6ED1;&#x52A8;<code>kernel_size</code>&#x5C3A;&#x5EA6;&#x7684;&#x5757;&#x5212;&#x5206;&#x6210;&#x7684;&#x5217;&#xFF08;&#x5373;&#xFF0C;&#x6700;&#x540E;&#x7684;&#x5C3A;&#x5BF8;&#xFF09; 3-d <code>&#x8F93;&#x51FA;</code>&#x5F62;&#x72B6;&#x7684;&#x5F20;&#x91CF; &#xFF08; N  &#xFF0C; C  &#xD7; &#x3A0; &#xFF08; kernel_size
&#xFF09; &#xFF0C; L  &#xFF09; &#xFF08;N&#xFF0C;C \&#x500D;\ PROD&#xFF08;\&#x6587;&#x672C;{&#x5185;&#x6838;\ _size}&#xFF09;&#xFF0C;L&#xFF09; &#xFF08; N  &#xFF0C; C  &#xD7; &#x3A0; &#xFF08;[H TG203]
kernel_size  &#xFF09; &#xFF0C; L  &#xFF09; &#xFF0C;&#x5176;&#x4E2D; C  &#xD7; &#x3A0; &#xFF08; kernel_size  &#xFF09; &#xE7;\&#x500D;\ PROD&#xFF08;\&#x6587;&#x672C;{&#x5185;&#x6838;\ _size}&#xFF09; C
&#xD7; &#x3A0; &#xFF08; kernel_size  &#x200B;&#x200B;&#xFF09; &#x662F;&#x503C;&#x7684;&#x6BCF;&#x4E2A;&#x5757;&#x5185;&#x7684;&#x603B;&#x6570;&#x76EE;&#xFF08;&#x4E00;&#x4E2A;&#x5757;&#x5177;&#x6709; &#x3A0; &#xFF08; kernel_size  &#xFF09; \ PROD&#xFF08;\&#x6587;&#x672C;{&#x5185;&#x6838;\
_size}&#xFF09; &#x3A0; &#xFF08; kernel_size  &#xFF09; &#x7684;&#x7A7A;&#x95F4;&#x4F4D;&#x7F6E;&#x6BCF;&#x4E00;&#x4E2A;&#x5305;&#x542B; C  C  C  -channeled&#x77E2;&#x91CF;&#xFF09;&#xFF0C;&#x548C; L  L  L
&#x662F;&#x8FD9;&#x6837;&#x7684;&#x5757;&#x7684;&#x603B;&#x6570;&#xFF1A;</p>
<p>L=&#x220F;d&#x230A;spatial_size[d]+2&#xD7;padding[d]&#x2212;dilation[d]&#xD7;(kernel_size[d]&#x2212;1)&#x2212;1stride[d]+1&#x230B;,L
= \prod_d \left\lfloor\frac{\text{spatial\_size}[d] + 2 \times
\text{padding}[d] % - \text{dilation}[d] \times (\text{kernel\_size}[d] - 1)</p>
<ul>
<li>1}{\text{stride}[d]} + 1\right\rfloor,
L=d&#x220F;&#x200B;&#x230A;stride[d]spatial_size[d]+2&#xD7;padding[d]&#x2212;dilation[d]&#xD7;(kernel_size[d]&#x2212;1)&#x2212;1&#x200B;+1&#x230B;,</li>
</ul>
<p>&#x5176;&#x4E2D; spatial_size  \&#x6587;&#x672C;{&#x7A7A;&#x95F4;\ _size}  spatial_size  &#x662F;&#x7531;<code>[HTG27&#x7A7A;&#x95F4;&#x5C3A;&#x5BF8;&#x5F62;&#x6210;]&#x8F93;&#x5165;</code>&#xFF08; <em>  </em>  *
&#x6BB5;&#xFF09;&#xFF0C;&#x548C; d  d  d  &#x662F;&#x6240;&#x6709;&#x7A7A;&#x95F4;&#x7EF4;&#x5EA6;&#x3002;</p>
<p>&#x56E0;&#x6B64;&#xFF0C;&#x7D22;&#x5F15;<code>&#x8F93;&#x51FA;</code>&#x5728;&#x6700;&#x540E;&#x4E00;&#x7EF4;&#xFF08;&#x5217;&#x7EF4;&#x5EA6;&#xFF09;&#x7ED9;&#x51FA;&#x7279;&#x5B9A;&#x5757;&#x5185;&#x7684;&#x6240;&#x6709;&#x503C;&#x3002;</p>
<p>&#x7684;<code>&#x586B;&#x5145;</code>&#xFF0C;<code>&#x6B65;&#x5E45;</code>&#x548C;<code>&#x6269;&#x5F20;</code>&#x53C2;&#x6570;&#x6307;&#x5B9A;&#x7684;&#x6ED1;&#x52A8;&#x5757;&#x5982;&#x4F55;&#x68C0;&#x7D22;&#x3002;</p>
<ul>
<li><p><code>&#x6B65;&#x5E45;</code>&#x63A7;&#x5236;&#x7528;&#x4E8E;&#x6240;&#x8FF0;&#x6ED1;&#x52A8;&#x5757;&#x7684;&#x6B65;&#x5E45;&#x3002;</p>
</li>
<li><p><code>&#x586B;&#x5145;</code>&#x63A7;&#x5236;&#x9690;&#x542B;&#x96F6;&#x586B;&#x8865;&#x5904;&#x7406;&#x7684;&#x53CC;&#x65B9;&#x7684;&#x91CF;&#x4E3A;<code>&#x586B;&#x5145;</code>&#x70B9;&#x6570;&#x91CD;&#x5851;&#x4E4B;&#x524D;&#x6BCF;&#x4E2A;&#x7EF4;&#x5EA6;&#x3002;</p>
</li>
<li><p><code>dilation</code>controls the spacing between the kernel points; also known as the &#xE0; trous algorithm. It is harder to describe, but this <a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md" target="_blank">link</a> has a nice visualization of what <code>dilation</code>does.</p>
</li>
</ul>
<p>Parameters</p>
<ul>
<li><p><strong>kernel_size</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#x6216;</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>&#x5143;&#x7EC4;</em> </a>&#xFF09; - &#x6ED1;&#x52A8;&#x5757;&#x7684;&#x5927;&#x5C0F;</p>
</li>
<li><p><strong>&#x6B65;&#x5E45;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#x6216;</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>&#x5143;&#x7EC4;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;&#x7684;</em> &#xFF09; - &#x5728;&#x8F93;&#x5165;&#x7A7A;&#x95F4;&#x7EF4;&#x5EA6;&#x7684;&#x6ED1;&#x52A8;&#x5757;&#x7684;&#x6B65;&#x5E45;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;1</p>
</li>
<li><p><strong>&#x586B;&#x5145;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#x6216;</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>&#x5143;&#x7EC4;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;&#x7684;</em> &#xFF09; - &#x9690;&#x5F0F;&#x96F6;&#x586B;&#x5145;&#x5230;&#x4E0A;&#x8F93;&#x5165;&#x7684;&#x4E24;&#x4FA7;&#x6DFB;&#x52A0;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;0</p>
</li>
<li><p><strong>&#x6269;&#x5F20;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#x6216;</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>&#x5143;&#x7EC4;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;&#x7684;</em> &#xFF09; - &#x4E00;&#x4E2A;&#x63A7;&#x5236;&#x90BB;&#x57DF;&#x5185;&#x7684;&#x5143;&#x7D20;&#x7684;&#x6B65;&#x5E45;&#x7684;&#x53C2;&#x6570;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;1</p>
</li>
<li><p>&#x5982;&#x679C;<code>kernel_size</code>&#xFF0C;<code>&#x6269;&#x5F20;</code>&#xFF0C;<code>&#x586B;&#x5145;</code>&#x6216;<code>&#x6B65;&#x5E45;</code>&#x662F;&#x4E00;&#x4E2A;int&#x6216;&#x957F;&#x5EA6;&#x4E3A;1&#x7684;&#x5143;&#x7EC4;&#xFF0C;&#x5B83;&#x4EEC;&#x7684;&#x503C;&#x5C06;&#x5728;&#x6240;&#x6709;&#x7A7A;&#x95F4;&#x7EF4;&#x5EA6;&#x4E0A;&#x590D;&#x5236;&#x3002;</p>
</li>
<li><p>&#x5BF9;&#x4E8E;&#x4E24;&#x4E2A;&#x8F93;&#x5165;&#x7A7A;&#x95F4;&#x7EF4;&#x5EA6;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x8BE5;&#x64CD;&#x4F5C;&#x6709;&#x65F6;&#x88AB;&#x79F0;&#x4E3A;<code>im2col</code>&#x3002;</p>
</li>
</ul>
<p>Note</p>
<p><code>&#x6298;</code>&#x901A;&#x8FC7;&#x4ECE;&#x542B;&#x6709;&#x6240;&#x6709;&#x5757;&#x4E2D;&#x7684;&#x6240;&#x6709;&#x503C;&#x6C42;&#x548C;&#x6765;&#x8BA1;&#x7B97;&#x5728;&#x6240;&#x5F97;&#x5230;&#x7684;&#x5927;&#x5F20;&#x91CF;&#x7684;&#x6BCF;&#x4E2A;&#x7EC4;&#x5408;&#x7684;&#x503C;&#x3002;<code>&#x5C55;&#x5F00;</code>
&#x901A;&#x8FC7;&#x4ECE;&#x5927;&#x5F20;&#x91CF;&#x63D0;&#x200B;&#x200B;&#x53D6;&#x590D;&#x5236;&#x5728;&#x5C40;&#x90E8;&#x5757;&#x4E2D;&#x7684;&#x503C;&#x3002;&#x6240;&#x4EE5;&#xFF0C;&#x5982;&#x679C;&#x5757;&#x91CD;&#x53E0;&#xFF0C;&#x5B83;&#x4EEC;&#x4E0D;&#x662F;&#x5F7C;&#x6B64;&#x7684;&#x9006;&#x3002;</p>
<p>Warning</p>
<p>&#x76EE;&#x524D;&#xFF0C;&#x53EA;&#x6709;4-d&#x7684;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#xFF08;&#x6210;&#x6279;&#x56FE;&#x50CF;&#x6837;&#x5F20;&#x91CF;&#xFF09;&#x7684;&#x652F;&#x6301;&#x3002;</p>
<p>Shape:</p>
<ul>
<li><p>&#x8F93;&#x5165;&#xFF1A; &#xFF08; N  &#xFF0C; C  &#xFF0C; <em>  &#xFF09; &#xFF08;N&#xFF0C;C&#xFF0C;</em>&#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF0C; *  &#xFF09;</p>
</li>
<li><p>&#x8F93;&#x51FA;&#xFF1A; &#xFF08; N  &#xFF0C; C  &#xD7; &#x3A0; &#xFF08; kernel_size  &#xFF09; &#xFF0C; L  &#xFF09; &#xFF08;N&#xFF0C;C \&#x500D;\ PROD&#xFF08;\&#x6587;&#x672C;{&#x5185;&#x6838;\ _size}&#xFF09;&#xFF0C;L&#xFF09; &#xFF08; N  &#xFF0C; C  &#xD7; &#x3A0; &#xFF08; kernel_size  &#xFF09;  &#xFF0C; L  &#xFF09; &#x5982;&#x4E0A;&#x6240;&#x8FF0;</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; unfold = nn.Unfold(kernel_size=(2, 3))
&gt;&gt;&gt; input = torch.randn(2, 5, 3, 4)
&gt;&gt;&gt; output = unfold(input)
&gt;&gt;&gt; # each patch contains 30 values (2x3=6 vectors, each of 5 channels)
&gt;&gt;&gt; # 4 blocks (2x3 kernels) in total in the 3x4 input
&gt;&gt;&gt; output.size()
torch.Size([2, 30, 4])

&gt;&gt;&gt; # Convolution is equivalent with Unfold + Matrix Multiplication + Fold (or view to output shape)
&gt;&gt;&gt; inp = torch.randn(1, 3, 10, 12)
&gt;&gt;&gt; w = torch.randn(2, 3, 4, 5)
&gt;&gt;&gt; inp_unf = torch.nn.functional.unfold(inp, (4, 5))
&gt;&gt;&gt; out_unf = inp_unf.transpose(1, 2).matmul(w.view(w.size(0), -1).t()).transpose(1, 2)
&gt;&gt;&gt; out = torch.nn.functional.fold(out_unf, (7, 8), (1, 1))
&gt;&gt;&gt; # or equivalently (and avoiding a copy),
&gt;&gt;&gt; # out = out_unf.view(1, 2, 7, 8)
&gt;&gt;&gt; (torch.nn.functional.conv2d(inp, w) - out).abs().max()
tensor(1.9073e-06)
</code></pre><h3 id="&#x6298;&#x53E0;">&#x6298;&#x53E0;</h3>
<p><em>class</em><code>torch.nn.``Fold</code>( <em>output_size</em> , <em>kernel_size</em> , <em>dilation=1</em> ,
<em>padding=0</em> , <em>stride=1</em>
)<a href="_modules/torch/nn/modules/fold.html#Fold">[source]</a></p>
<p>&#x7ED3;&#x5408;&#x6ED1;&#x52A8;&#x5C40;&#x90E8;&#x5757;&#x5230;&#x5927;&#x91CF;&#x542B;&#x6709;&#x5F20;&#x91CF;&#x7684;&#x9635;&#x5217;&#x3002;</p>
<p>&#x8003;&#x8651;&#x5305;&#x542B;&#x6ED1;&#x52A8;&#x5C40;&#x90E8;&#x5757;&#xFF0C;&#x4F8B;&#x5982;&#x5206;&#x6279;<code>&#x8F93;&#x5165;</code>&#x5F20;&#x91CF;&#xFF0C;&#x56FE;&#x50CF;&#x7684;&#x8865;&#x4E01;&#xFF0C;&#x7684;&#x5F62;&#x72B6; &#xFF08; N  &#xFF0C; C  &#xD7; &#x3A0; &#xFF08; kernel_size  &#xFF09; &#xFF0C; L  &#xFF09; &#xFF08;N&#xFF0C;C
\&#x500D;\ PROD&#xFF08;\&#x6587;&#x672C;{&#x5185;&#x6838;\ _size}&#xFF09;&#xFF0C;L&#xFF09; &#xFF08; N  C  &#xD7; &#x3A0; &#xFF08; kernel_size  &#xFF09; &#xFF0C; L  &#xFF09; &#xFF0C;&#x5176;&#x4E2D; N  N  N
&#x662F;&#x6279;&#x6B21;&#x5C3A;&#x5BF8;&#xFF0C; C  &#xD7; &#x3A0; &#xFF08; kernel_size  &#xFF09; &#xE7;\&#x500D;\ PROD&#xFF08;\&#x6587;&#x672C;{&#x5185;&#x6838;\ _size}&#xFF09; C  &#xD7; &#x3A0; &#xFF08; kernel_size
&#xFF09; &#x662F;&#x503C;&#x7684;&#x5757;&#x5185;&#xFF08;&#x6570;&#x7684;&#x90A3;&#x79CD;&#x5757;&#x5177;&#x6709; &#x3A0; &#xFF08; &#x67EF;rnel_size  &#xFF09; \ PROD&#xFF08;\&#x6587;&#x672C;{&#x5185;&#x6838;\ _size}&#xFF09; &#x3A0; &#xFF08; kernel_size  &#xFF09;
&#x7684;&#x7A7A;&#x95F4;&#x4F4D;&#x7F6E;&#x6BCF;&#x4E00;&#x4E2A;&#x5305;&#x542B; C  C  C  -channeled&#x77E2;&#x91CF;&#xFF09;&#xFF0C;&#x548C; L  L  L  &#x662F;&#x5757;&#x7684;&#x603B;&#x6570;&#x3002; &#xFF08;&#x8FD9;&#x662F;&#x5B8C;&#x5168;&#x4E00;&#x6837;&#x7684;&#x8BF4;&#x660E;&#x4E66;&#x4E2D;&#x7684; <code>&#x5C55;&#x5F00;</code>
&#x7684;&#x8F93;&#x51FA;&#x7684;&#x5F62;&#x72B6;&#x3002;&#xFF09;&#x6B64;&#x64CD;&#x4F5C;&#x8FD9;&#x4E9B;&#x5C40;&#x90E8;&#x5757;&#x7ED3;&#x5408;&#x5230;&#x5927;<code>&#x8F93;&#x51FA;</code>&#x7684;&#x5F20;&#x91CF;&#x5F62;&#x72B6; &#xFF08; N  &#x200B;&#x200B;&#xFF0C; C  &#xFF0C; output_size  [ 0  &#xFF0C; output_size
[ 1  &#xFF0C; ...  &#xFF09; &#xFF08;N&#xFF0C;C&#xFF0C;\&#x6587;&#x672C;{&#x8F93;&#x51FA;\ _size} [0]&#xFF0C;\&#x6587;&#x672C;{&#x8F93;&#x51FA;\ _size} [1]&#xFF0C;\&#x70B9;&#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF0C;
output_size  [ 0  &#xFF0C; output_size  [ 1  &#xFF0C; ...  &#xFF09; &#x7531;&#x91CD;&#x53E0;&#x503C;&#x6C42;&#x548C;&#x3002;&#x7C7B;&#x4F3C;&#x4E8E; <code>&#x5C55;&#x5F00;</code>&#xFF0C;&#x53C2;&#x6570;&#x5FC5;&#x987B;&#x6EE1;&#x8DB3;</p>
<p>L=&#x220F;d&#x230A;output_size[d]+2&#xD7;padding[d]&#x2212;dilation[d]&#xD7;(kernel_size[d]&#x2212;1)&#x2212;1stride[d]+1&#x230B;,L
= \prod_d \left\lfloor\frac{\text{output\_size}[d] + 2 \times
\text{padding}[d] % - \text{dilation}[d] \times (\text{kernel\_size}[d] - 1)</p>
<ul>
<li>1}{\text{stride}[d]} + 1\right\rfloor,
L=d&#x220F;&#x200B;&#x230A;stride[d]output_size[d]+2&#xD7;padding[d]&#x2212;dilation[d]&#xD7;(kernel_size[d]&#x2212;1)&#x2212;1&#x200B;+1&#x230B;,</li>
</ul>
<p>&#x5176;&#x4E2D; d  d  d  &#x662F;&#x6240;&#x6709;&#x7A7A;&#x95F4;&#x7EF4;&#x5EA6;&#x3002;</p>
<ul>
<li><code>output_size</code>&#x63CF;&#x8FF0;&#x7684;&#x6ED1;&#x52A8;&#x5C40;&#x90E8;&#x5757;&#x7684;&#x5927;&#x542B;&#x6709;&#x5F20;&#x91CF;&#x7684;&#x7A7A;&#x95F4;&#x5F62;&#x72B6;&#x3002;&#x5B83;&#x5F53;&#x591A;&#x4E2A;&#x8F93;&#x5165;&#x5F62;&#x72B6;&#x5730;&#x56FE;&#x5230;&#x76F8;&#x540C;&#x6570;&#x91CF;&#x7684;&#x6ED1;&#x52A8;&#x5757;&#xFF0C;&#x4F8B;&#x5982;&#xFF0C;&#x5177;&#x6709;<code>&#x6765;&#x89E3;&#x51B3;&#x591A;&#x4E49;&#x6027;&#x662F;&#x6709;&#x7528;&#x7684;&#x6B65;&#x5E45; &amp; GT ;  0</code>&#x3002;</li>
</ul>
<p>The <code>padding</code>, <code>stride</code>and <code>dilation</code>arguments specify how the sliding
blocks are retrieved.</p>
<ul>
<li><p><code>stride</code>controls the stride for the sliding blocks.</p>
</li>
<li><p><code>padding</code>controls the amount of implicit zero-paddings on both sides for <code>padding</code>number of points for each dimension before reshaping.</p>
</li>
<li><p><code>dilation</code>controls the spacing between the kernel points; also known as the &#xE0; trous algorithm. It is harder to describe, but this <a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md" target="_blank">link</a> has a nice visualization of what <code>dilation</code>does.</p>
</li>
</ul>
<p>Parameters</p>
<ul>
<li><p><strong>output_size</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#x6216;</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>&#x5143;&#x7EC4;</em> </a>&#xFF09; - &#x7684;&#x7A7A;&#x95F4;&#x5C3A;&#x5BF8;&#x7684;&#x5F62;&#x72B6;&#x8BE5;&#x8F93;&#x51FA;&#xFF08;&#x5373;&#xFF0C;<code>output.sizes&#xFF08;&#xFF09;[2&#xFF1A;]</code>&#xFF09;</p>
</li>
<li><p><strong>kernel_size</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>or</em><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>tuple</em></a>) &#x2013; the size of the sliding blocks</p>
</li>
<li><p><strong>&#x6B65;&#x5E45;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#x6216;</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>&#x5143;&#x7EC4;</em> </a>&#xFF09; - &#x6ED1;&#x52A8;&#x5757;&#x7684;&#x6B65;&#x5E45;&#x8F93;&#x5165;&#x7684;&#x7A7A;&#x95F4;&#x5C3A;&#x5BF8;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;1</p>
</li>
<li><p><strong>padding</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>or</em><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>tuple</em></a> <em>,</em> <em>optional</em> ) &#x2013; implicit zero padding to be added on both sides of input. Default: 0</p>
</li>
<li><p><strong>dilation</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>or</em><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>tuple</em></a> <em>,</em> <em>optional</em> ) &#x2013; a parameter that controls the stride of elements within the neighborhood. Default: 1</p>
</li>
<li><p>&#x5982;&#x679C;<code>output_size</code>&#xFF0C;<code>kernel_size</code>&#xFF0C;<code>&#x6269;&#x5F20;</code>&#xFF0C;<code>&#x586B;&#x5145;</code>&#x6216;<code>&#x6B65;&#x5E45;</code>&#x662F;int&#x6216;&#x7136;&#x540E;&#x5B83;&#x4EEC;&#x7684;&#x503C;&#x5C06;&#x5728;&#x6240;&#x6709;&#x7A7A;&#x95F4;&#x7EF4;&#x5EA6;&#x4E0A;&#x590D;&#x5236;&#x957F;&#x5EA6;&#x4E3A;1&#x7684;&#x5143;&#x7EC4;&#x3002;</p>
</li>
<li><p>&#x5BF9;&#x4E8E;&#x4E24;&#x4E2A;&#x8F93;&#x51FA;&#x7A7A;&#x95F4;&#x7EF4;&#x5EA6;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x8BE5;&#x64CD;&#x4F5C;&#x6709;&#x65F6;&#x88AB;&#x79F0;&#x4E3A;<code>col2im</code>&#x3002;</p>
</li>
</ul>
<p>Note</p>
<p><code>Fold</code>calculates each combined value in the resulting large tensor by summing
all values from all containing blocks. <code>Unfold</code>extracts the values in the
local blocks by copying from the large tensor. So, if the blocks overlap, they
are not inverses of each other.</p>
<p>Warning</p>
<p>&#x76EE;&#x524D;&#xFF0C;&#x53EA;&#x6709;4-d&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#xFF08;&#x6210;&#x6279;&#x56FE;&#x50CF;&#x6837;&#x5F20;&#x91CF;&#xFF09;&#x7684;&#x652F;&#x6301;&#x3002;</p>
<p>Shape:</p>
<ul>
<li><p>&#x8F93;&#x5165;&#xFF1A; &#xFF08; N  &#xFF0C; C  &#xD7; &#x3A0; &#xFF08; kernel_size  &#xFF09; &#xFF0C; L  &#xFF09; &#xFF08;N&#xFF0C;C \&#x500D;\ PROD&#xFF08;\&#x6587;&#x672C;{&#x5185;&#x6838;\ _size}&#xFF09;&#xFF0C;L&#xFF09; &#xFF08; N  &#xFF0C; C  &#xD7; &#x3A0; &#xFF08; kernel_size  &#xFF09;  &#xFF0C; L  &#xFF09;</p>
</li>
<li><p>&#x8F93;&#x51FA;&#xFF1A; &#xFF08; N  &#xFF0C; C  &#xFF0C; output_size  [ 0  &#xFF0C; output_size  [ 1  &#xFF0C; ...  &#xFF09; &#xFF08;N&#xFF0C;C&#xFF0C;\&#x6587;&#x672C;{&#x8F93;&#x51FA;\ _size} [0]&#xFF0C;\&#x6587;&#x672C;{&#x8F93;&#x51FA;\ _size} [1]&#xFF0C;\&#x70B9;&#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF0C; output_size  [ 0  &#xFF0C; output_size  [ 1  &#xFF0C; ... [HTG9 5]  &#xFF09; &#x5982;&#x4E0A;&#x6240;&#x8FF0;</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; fold = nn.Fold(output_size=(4, 5), kernel_size=(2, 2))
&gt;&gt;&gt; input = torch.randn(1, 3 * 2 * 2, 12)
&gt;&gt;&gt; output = fold(input)
&gt;&gt;&gt; output.size()
torch.Size([1, 3, 4, 5])
</code></pre><h2 id="&#x6C60;&#x5C42;">&#x6C60;&#x5C42;</h2>
<h3 id="maxpool1d">MaxPool1d</h3>
<p><em>class</em><code>torch.nn.``MaxPool1d</code>( <em>kernel_size</em> , <em>stride=None</em> , <em>padding=0</em> ,
<em>dilation=1</em> , <em>return_indices=False</em> , <em>ceil_mode=False</em>
)<a href="_modules/torch/nn/modules/pooling.html#MaxPool1d">[source]</a></p>
<p>&#x5E94;&#x7528;&#x4E8E;&#x4E00;&#x7EF4;&#x7684;&#x6700;&#x5927;&#x6C47;&#x96C6;&#x4E86;&#x591A;&#x4E2A;&#x8F93;&#x5165;&#x98DE;&#x673A;&#x7EC4;&#x6210;&#x7684;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x3002;</p>
<p>&#x5728;&#x6700;&#x7B80;&#x5355;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x6240;&#x8FF0;&#x5C42;&#x7684;&#x4E0E;&#x8F93;&#x5165;&#x5927;&#x5C0F; &#x7684;&#x8F93;&#x51FA;&#x503C;&#xFF08; N  C  &#xFF0C; L  &#xFF09; &#xFF08;N&#xFF0C;C&#xFF0C;L&#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF0C; L  &#xFF09; &#x548C;&#x8F93;&#x51FA; &#xFF08; N  &#xFF0C; C  &#xFF0C;
L  O  U  T  &#xFF09; &#xFF08; N&#xFF0C;C&#xFF0C;L_ {&#x51FA;}&#xFF09; &#xFF08; N  C  &#xFF0C; L  O  U  T  &#xFF09; &#x53EF;&#x4EE5;&#x7CBE;&#x786E;&#x5730;&#x63CF;&#x8FF0;&#x4E3A;&#xFF1A;</p>
<p>out(Ni,Cj,k)=max&#x2061;m=0,&#x2026;,kernel<em>size&#x2212;1input(Ni,Cj,stride&#xD7;k+m)out(N_i, C_j, k) =
\max</em>{m=0, \ldots, \text{kernel\_size} - 1} input(N_i, C_j, stride \times k +
m) out(Ni&#x200B;,Cj&#x200B;,k)=m=0,&#x2026;,kernel_size&#x2212;1max&#x200B;input(Ni&#x200B;,Cj&#x200B;,stride&#xD7;k+m)</p>
<p>&#x5982;&#x679C;<code>&#x586B;&#x5145;</code>&#x662F;&#x975E;&#x96F6;&#xFF0C;&#x5219;&#x8F93;&#x5165;&#x662F;&#x9690;&#x5F0F;&#x5730;&#x5728;&#x4E24;&#x4FA7;&#x4E0A;&#x7528;&#x96F6;&#x586B;&#x5145;&#x4E3A;<code>&#x586B;&#x5145;</code>&#x70B9;&#x6570;&#x3002; <code>&#x6269;&#x5F20;</code>&#x63A7;&#x5236;&#x5185;&#x6838;&#x70B9;&#x4E4B;&#x95F4;&#x7684;&#x95F4;&#x9694;&#x3002;&#x8FD9;&#x662F;&#x5F88;&#x96BE;&#x5F62;&#x5BB9;&#xFF0C;&#x4F46;&#x8FD9;&#x79CD;<a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md" target="_blank">&#x94FE;&#x63A5;</a>&#x6709;&#x4EC0;&#x4E48;<code>&#x6269;&#x5F20;</code>&#x505A;&#x4E00;&#x4E2A;&#x5F88;&#x597D;&#x7684;&#x53EF;&#x89C6;&#x5316;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>kernel_size</strong> - &#x7A97;&#x53E3;&#x7684;&#x5927;&#x5C0F;&#xFF0C;&#x4EE5;&#x91C7;&#x53D6;&#x6700;&#x5927;&#x8FC7;</p>
</li>
<li><p><strong>&#x6B65;&#x5E45;</strong> - &#x7A97;&#x53E3;&#x7684;&#x6B65;&#x5E45;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#x4E3A;<code>kernel_size</code></p>
</li>
<li><p><strong>&#x586B;&#x5145;</strong> - &#x9690;&#x542B;&#x96F6;&#x586B;&#x5145;&#x5230;&#x5728;&#x4E24;&#x4FA7;&#x88AB;&#x6DFB;&#x52A0;</p>
</li>
<li><p><strong>&#x6269;&#x5F20;</strong> - &#x4E00;&#x4E2A;&#x63A7;&#x5236;&#x5143;&#x4EF6;&#x7684;&#x6B65;&#x5E45;&#x5728;&#x7A97;&#x53E3;&#x7684;&#x53C2;&#x6570;</p>
</li>
<li><p><strong>return_indices</strong> - &#x5982;&#x679C;<code>&#x771F;</code>&#xFF0C;&#x5C06;&#x8FD4;&#x56DE;&#x6700;&#x5927;&#x6307;&#x6570;&#x4E2D;&#xFF0C;&#x4EA7;&#x51FA;&#x4E00;&#x8D77;&#x3002;&#x6709;&#x7528;&#x7684; <code>torch.nn.MaxUnpool1d</code>&#x4EE5;&#x540E;</p>
</li>
<li><p><strong>ceil_mode</strong> - &#x771F;&#x65F6;&#xFF0C;&#x5C06;&#x4F7F;&#x7528;&#x5C0F;&#x533A;&#x800C;&#x975E;&#x5730;&#x677F;&#x6765;&#x8BA1;&#x7B97;&#x8F93;&#x51FA;&#x5F62;&#x72B6;</p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>&#x8F93;&#x5165;&#xFF1A; &#xFF08; N  &#xFF0C; C  &#xFF0C; L  i&#x7684; n&#x7684; &#xFF09; &#xFF08;N&#xFF0C;C&#xFF0C;L_ {&#x5728;}&#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF0C; L  i&#x7684; n&#x7684; &#xFF09;</p>
</li>
<li><p>&#x8F93;&#x51FA;&#xFF1A; &#xFF08; N  &#xFF0C; C  &#xFF0C; L  O  U  T  &#xFF09; &#xFF08;N&#xFF0C;C&#xFF0C;L_ {&#x51FA;}&#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF0C; L  O  U  T  &#xFF09;  &#xFF0C;&#x5176;&#x4E2D;</p>
</li>
</ul>
<p>Lout=&#x230A;Lin+2&#xD7;padding&#x2212;dilation&#xD7;(kernel<em>size&#x2212;1)&#x2212;1stride+1&#x230B;L</em>{out} = \left\lfloor
\frac{L_{in} + 2 \times \text{padding} - \text{dilation} \times
(\text{kernel\_size} - 1) - 1}{\text{stride}} + 1\right\rfloor
Lout&#x200B;=&#x230A;strideLin&#x200B;+2&#xD7;padding&#x2212;dilation&#xD7;(kernel_size&#x2212;1)&#x2212;1&#x200B;+1&#x230B;</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; # pool of size=3, stride=2
&gt;&gt;&gt; m = nn.MaxPool1d(3, stride=2)
&gt;&gt;&gt; input = torch.randn(20, 16, 50)
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="maxpool2d">MaxPool2d</h3>
<p><em>class</em><code>torch.nn.``MaxPool2d</code>( <em>kernel_size</em> , <em>stride=None</em> , <em>padding=0</em> ,
<em>dilation=1</em> , <em>return_indices=False</em> , <em>ceil_mode=False</em>
)<a href="_modules/torch/nn/modules/pooling.html#MaxPool2d">[source]</a></p>
<p>&#x65BD;&#x52A0;&#x6700;&#x5927;&#x7684;2D&#x6C47;&#x96C6;&#x5728;&#x51E0;&#x4E2A;&#x8F93;&#x5165;&#x5E73;&#x9762;&#x7EC4;&#x6210;&#x7684;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x3002;</p>
<p>&#x5728;&#x6700;&#x7B80;&#x5355;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x6240;&#x8FF0;&#x5C42;&#x7684;&#x4E0E;&#x8F93;&#x5165;&#x5927;&#x5C0F; &#x7684;&#x8F93;&#x51FA;&#x503C;&#xFF08; N  C  &#xFF0C; H  &#xFF0C; W  &#xFF09; &#xFF08;N&#xFF0C;C&#xFF0C;H&#xFF0C;W&#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF0C; H  &#xFF0C; W  &#xFF09; &#xFF0C;&#x8F93;&#x51FA;
&#xFF08; N  &#xFF0C; C  &#xFF0C; H  O  U  T  &#xFF0C; W  O  U  T  &#xFF09; &#xFF08;N&#xFF0C;C&#xFF0C;H<em> {&#x51FA;}&#xFF0C;W</em> {&#x51FA;}&#xFF09; &#xFF08; N  &#xFF0C; &#xE7;  &#xFF0C; H  O
U  T  &#xFF0C; W  O  U  T  [H TG194]  &#xFF09; &#x548C;<code>kernel_size</code>&#xFF08; K  H  &#xFF0C; K  W  &#xFF09; &#xFF08;KH&#xFF0C;&#x5343;&#x74E6;&#xFF09; &#xFF08;
K  H  &#xFF0C; K  W  &#xFF09; &#x53EF;&#x4EE5;&#x7CBE;&#x786E;&#x5730;&#x63CF;&#x8FF0;&#x4E3A;&#xFF1A;</p>
<p>out(Ni,Cj,h,w)=max&#x2061;m=0,&#x2026;,kH&#x2212;1max&#x2061;n=0,&#x2026;,kW&#x2212;1input(Ni,Cj,stride[0]&#xD7;h+m,stride[1]&#xD7;w+n)\begin{aligned}
out(N<em>i, C_j, h, w) ={} &amp; \max</em>{m=0, \ldots, kH-1} \max_{n=0, \ldots, kW-1}
\\ &amp; \text{input}(N_i, C_j, \text{stride[0]} \times h + m, \text{stride[1]}
\times w + n) \end{aligned}
out(Ni&#x200B;,Cj&#x200B;,h,w)=&#x200B;m=0,&#x2026;,kH&#x2212;1max&#x200B;n=0,&#x2026;,kW&#x2212;1max&#x200B;input(Ni&#x200B;,Cj&#x200B;,stride[0]&#xD7;h+m,stride[1]&#xD7;w+n)&#x200B;</p>
<p>If <code>padding</code>is non-zero, then the input is implicitly zero-padded on both
sides for <code>padding</code>number of points. <code>dilation</code>controls the spacing between
the kernel points. It is harder to describe, but this
<a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md" target="_blank">link</a> has
a nice visualization of what <code>dilation</code>does.</p>
<p>The parameters <code>kernel_size</code>, <code>stride</code>, <code>padding</code>, <code>dilation</code>can either be:</p>
<blockquote>
<ul>
<li>a single <code>int</code>&#x2013; in which case the same value is used for the height and
width dimension</li>
</ul>
</blockquote>
<p>&gt;</p>
<blockquote>
<ul>
<li>a <code>tuple</code>of two ints &#x2013; in which case, the first int is used for the
height dimension, and the second int for the width dimension</li>
</ul>
</blockquote>
<p>&gt;</p>
<p>&gt;</p>
<p>Parameters</p>
<ul>
<li><p><strong>kernel_size</strong> &#x2013; the size of the window to take a max over</p>
</li>
<li><p><strong>stride</strong> &#x2013; the stride of the window. Default value is <code>kernel_size</code></p>
</li>
<li><p><strong>padding</strong> &#x2013; implicit zero padding to be added on both sides</p>
</li>
<li><p><strong>dilation</strong> &#x2013; a parameter that controls the stride of elements in the window</p>
</li>
<li><p><strong>return_indices</strong> - &#x5982;&#x679C;<code>&#x771F;</code>&#xFF0C;&#x5C06;&#x8FD4;&#x56DE;&#x6700;&#x5927;&#x6307;&#x6570;&#x4E2D;&#xFF0C;&#x4EA7;&#x51FA;&#x4E00;&#x8D77;&#x3002;&#x6709;&#x7528;&#x7684; <code>torch.nn.MaxUnpool2d</code>&#x4EE5;&#x540E;</p>
</li>
<li><p><strong>ceil_mode</strong> &#x2013; when True, will use ceil instead of floor to compute the output shape</p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>&#x8F93;&#x5165;&#xFF1A; &#xFF08; N  &#xFF0C; C  &#xFF0C; H  i&#x7684; n&#x7684; &#xFF0C; W  i&#x7684; n&#x7684; &#xFF09; &#xFF08;N&#xFF0C;C&#xFF0C;H<em> {IN} &#xFF0C;W</em> {&#x5728;}&#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF0C; H  i&#x7684; n&#x7684; &#xFF0C; W  i&#x7684; n&#x7684; &#xFF09;</p>
</li>
<li><p>&#x8F93;&#x51FA;&#xFF1A; &#xFF08; N  &#xFF0C; C  &#xFF0C; H  O  U  T  &#xFF0C; W  O  U  T  &#xFF09; &#xFF08;N&#xFF0C;C&#xFF0C;H<em> {&#x51FA;}&#xFF0C;W</em> {&#x51FA;}&#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF0C; H  O  U  T  &#xFF0C; W  O  U  T  &#xFF09; &#xFF0C;&#x5176;&#x4E2D;</p>
</li>
</ul>
<p>Hout=&#x230A;Hin+2&#x2217;padding[0]&#x2212;dilation[0]&#xD7;(kernel<em>size[0]&#x2212;1)&#x2212;1stride[0]+1&#x230B;H</em>{out} =
\left\lfloor\frac{H_{in} + 2 * \text{padding[0]} - \text{dilation[0]} \times
(\text{kernel\_size[0]} - 1) - 1}{\text{stride[0]}} + 1\right\rfloor
Hout&#x200B;=&#x230A;stride[0]Hin&#x200B;+2&#x2217;padding[0]&#x2212;dilation[0]&#xD7;(kernel_size[0]&#x2212;1)&#x2212;1&#x200B;+1&#x230B;</p>
<p>Wout=&#x230A;Win+2&#x2217;padding[1]&#x2212;dilation[1]&#xD7;(kernel<em>size[1]&#x2212;1)&#x2212;1stride[1]+1&#x230B;W</em>{out} =
\left\lfloor\frac{W_{in} + 2 * \text{padding[1]} - \text{dilation[1]} \times
(\text{kernel\_size[1]} - 1) - 1}{\text{stride[1]}} + 1\right\rfloor
Wout&#x200B;=&#x230A;stride[1]Win&#x200B;+2&#x2217;padding[1]&#x2212;dilation[1]&#xD7;(kernel_size[1]&#x2212;1)&#x2212;1&#x200B;+1&#x230B;</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; # pool of square window of size=3, stride=2
&gt;&gt;&gt; m = nn.MaxPool2d(3, stride=2)
&gt;&gt;&gt; # pool of non-square window
&gt;&gt;&gt; m = nn.MaxPool2d((3, 2), stride=(2, 1))
&gt;&gt;&gt; input = torch.randn(20, 16, 50, 32)
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="maxpool3d">MaxPool3d</h3>
<p><em>class</em><code>torch.nn.``MaxPool3d</code>( <em>kernel_size</em> , <em>stride=None</em> , <em>padding=0</em> ,
<em>dilation=1</em> , <em>return_indices=False</em> , <em>ceil_mode=False</em>
)<a href="_modules/torch/nn/modules/pooling.html#MaxPool3d">[source]</a></p>
<p>&#x5E94;&#x7528;&#x4E86;3D&#x6700;&#x5927;&#x6C47;&#x96C6;&#x4E86;&#x591A;&#x4E2A;&#x8F93;&#x5165;&#x98DE;&#x673A;&#x7EC4;&#x6210;&#x7684;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x3002;</p>
<p>&#x5728;&#x6700;&#x7B80;&#x5355;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x6240;&#x8FF0;&#x5C42;&#x7684;&#x4E0E;&#x8F93;&#x5165;&#x5927;&#x5C0F; &#x7684;&#x8F93;&#x51FA;&#x503C;&#xFF08; N  C  &#xFF0C; d  &#xFF0C; H  &#xFF0C; W  &#xFF09; &#xFF08;N&#xFF0C;C&#xFF0C;d&#xFF0C;H&#xFF0C;W&#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF0C; d  &#xFF0C; H
&#xFF0C; W  &#xFF09; &#xFF0C;&#x8F93;&#x51FA; &#xFF08; N  C  &#xFF0C; d  O  U  T  [HT G98]  &#xFF0C; H  O  U  T  &#xFF0C; W  O  U  T  &#xFF09;
&#xFF08;N&#xFF0C;C&#xFF0C;D<em> {&#x51FA;}&#xFF0C;H</em> {&#x51FA;}&#xFF0C;W_ {&#x51FA;}&#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF0C; d  O  U  T  &#xFF0C; H  O  U  T  &#xFF0C; W  O  U  T
&#xFF09; &#x548C;<code>&#x200B;&#x200B;  kernel_size</code>&#xFF08; K  d  &#x137;  H  &#xFF0C; K  W  &#xFF09; &#xFF08;KD&#xFF0C;KH&#xFF0C;&#x5343;&#x74E6;&#xFF09; &#xFF08; K  d  K  H  &#xFF0C; K  W
&#xFF09; &#x53EF;&#x4EE5;&#x7CBE;&#x786E;&#x5730;&#x63CF;&#x8FF0;&#x4E3A;&#xFF1A;</p>
<p>out(Ni,Cj,d,h,w)=max&#x2061;k=0,&#x2026;,kD&#x2212;1max&#x2061;m=0,&#x2026;,kH&#x2212;1max&#x2061;n=0,&#x2026;,kW&#x2212;1input(Ni,Cj,stride[0]&#xD7;d+k,stride[1]&#xD7;h+m,stride[2]&#xD7;w+n)\begin{aligned}
\text{out}(N<em>i, C_j, d, h, w) ={} &amp; \max</em>{k=0, \ldots, kD-1} \max<em>{m=0,
\ldots, kH-1} \max</em>{n=0, \ldots, kW-1} \\ &amp; \text{input}(N_i, C_j,
\text{stride[0]} \times d + k, \text{stride[1]} \times h + m, \text{stride[2]}
\times w + n) \end{aligned}
out(Ni&#x200B;,Cj&#x200B;,d,h,w)=&#x200B;k=0,&#x2026;,kD&#x2212;1max&#x200B;m=0,&#x2026;,kH&#x2212;1max&#x200B;n=0,&#x2026;,kW&#x2212;1max&#x200B;input(Ni&#x200B;,Cj&#x200B;,stride[0]&#xD7;d+k,stride[1]&#xD7;h+m,stride[2]&#xD7;w+n)&#x200B;</p>
<p>If <code>padding</code>is non-zero, then the input is implicitly zero-padded on both
sides for <code>padding</code>number of points. <code>dilation</code>controls the spacing between
the kernel points. It is harder to describe, but this
<a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md" target="_blank">link</a> has
a nice visualization of what <code>dilation</code>does.</p>
<p>The parameters <code>kernel_size</code>, <code>stride</code>, <code>padding</code>, <code>dilation</code>can either be:</p>
<blockquote>
<ul>
<li>a single <code>int</code>&#x2013; in which case the same value is used for the depth,
height and width dimension</li>
</ul>
</blockquote>
<p>&gt;</p>
<blockquote>
<ul>
<li>a <code>tuple</code>of three ints &#x2013; in which case, the first int is used for the
depth dimension, the second int for the height dimension and the third int for
the width dimension</li>
</ul>
</blockquote>
<p>&gt;</p>
<p>&gt;</p>
<p>Parameters</p>
<ul>
<li><p><strong>kernel_size</strong> &#x2013; the size of the window to take a max over</p>
</li>
<li><p><strong>stride</strong> &#x2013; the stride of the window. Default value is <code>kernel_size</code></p>
</li>
<li><p><strong>&#x586B;&#x5145;</strong> - &#x9690;&#x542B;&#x96F6;&#x586B;&#x5145;&#x5728;&#x6240;&#x6709;&#x4E09;&#x4E2A;&#x4FA7;&#x88AB;&#x6DFB;&#x52A0;</p>
</li>
<li><p><strong>dilation</strong> &#x2013; a parameter that controls the stride of elements in the window</p>
</li>
<li><p><strong>return_indices</strong> - &#x5982;&#x679C;<code>&#x771F;</code>&#xFF0C;&#x5C06;&#x8FD4;&#x56DE;&#x6700;&#x5927;&#x6307;&#x6570;&#x4E2D;&#xFF0C;&#x4EA7;&#x51FA;&#x4E00;&#x8D77;&#x3002;&#x6709;&#x7528;&#x7684; <code>torch.nn.MaxUnpool3d</code>&#x4EE5;&#x540E;</p>
</li>
<li><p><strong>ceil_mode</strong> &#x2013; when True, will use ceil instead of floor to compute the output shape</p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>&#x8F93;&#x5165;&#xFF1A; &#xFF08; N  &#xFF0C; C  &#xFF0C; d  i&#x7684; n&#x7684; &#xFF0C; H  i&#x7684; n&#x7684; &#xFF0C; W  i&#x7684; n&#x7684; &#xFF09; &#xFF08;N&#xFF0C;C&#xFF0C;D<em> {IN}&#xFF0C;H</em> {IN}&#xFF0C;W_ {IN} &#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF0C; d  i&#x7684; n&#x7684; &#xFF0C; H  i&#x7684; n&#x7684; &#xFF0C; W  i&#x7684; n&#x7684; &#xFF09;</p>
</li>
<li><p>&#x8F93;&#x51FA;&#xFF1A; &#xFF08; N  &#xFF0C; C  &#xFF0C; d  O  U  T  &#xFF0C; H  O  U  T  &#xFF0C; W  O  U  T  &#xFF09; &#xFF08;N&#xFF0C;C&#xFF0C;D<em> {&#x51FA;}&#xFF0C;H</em> {&#x51FA;}&#xFF0C;W_ {&#x51FA;}&#xFF09; &#xFF08;  N  &#xFF0C; C  &#xFF0C; d  O  U  T  &#xFF0C; H  O  U  T  &#xFF0C; W  O  U  T  &#xFF09; &#xFF0C;&#x5176;&#x4E2D;</p>
</li>
</ul>
<p>Dout=&#x230A;Din+2&#xD7;padding[0]&#x2212;dilation[0]&#xD7;(kernel<em>size[0]&#x2212;1)&#x2212;1stride[0]+1&#x230B;D</em>{out} =
\left\lfloor\frac{D_{in} + 2 \times \text{padding}[0] - \text{dilation}[0]
\times (\text{kernel\_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor
Dout&#x200B;=&#x230A;stride[0]Din&#x200B;+2&#xD7;padding[0]&#x2212;dilation[0]&#xD7;(kernel_size[0]&#x2212;1)&#x2212;1&#x200B;+1&#x230B;</p>
<p>Hout=&#x230A;Hin+2&#xD7;padding[1]&#x2212;dilation[1]&#xD7;(kernel<em>size[1]&#x2212;1)&#x2212;1stride[1]+1&#x230B;H</em>{out} =
\left\lfloor\frac{H_{in} + 2 \times \text{padding}[1] - \text{dilation}[1]
\times (\text{kernel\_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor
Hout&#x200B;=&#x230A;stride[1]Hin&#x200B;+2&#xD7;padding[1]&#x2212;dilation[1]&#xD7;(kernel_size[1]&#x2212;1)&#x2212;1&#x200B;+1&#x230B;</p>
<p>Wout=&#x230A;Win+2&#xD7;padding[2]&#x2212;dilation[2]&#xD7;(kernel<em>size[2]&#x2212;1)&#x2212;1stride[2]+1&#x230B;W</em>{out} =
\left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] - \text{dilation}[2]
\times (\text{kernel\_size}[2] - 1) - 1}{\text{stride}[2]} + 1\right\rfloor
Wout&#x200B;=&#x230A;stride[2]Win&#x200B;+2&#xD7;padding[2]&#x2212;dilation[2]&#xD7;(kernel_size[2]&#x2212;1)&#x2212;1&#x200B;+1&#x230B;</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; # pool of square window of size=3, stride=2
&gt;&gt;&gt; m = nn.MaxPool3d(3, stride=2)
&gt;&gt;&gt; # pool of non-square window
&gt;&gt;&gt; m = nn.MaxPool3d((3, 2, 2), stride=(2, 1, 2))
&gt;&gt;&gt; input = torch.randn(20, 16, 50,44, 31)
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="maxunpool1d">MaxUnpool1d</h3>
<p><em>class</em><code>torch.nn.``MaxUnpool1d</code>( <em>kernel_size</em> , <em>stride=None</em> , <em>padding=0</em>
)<a href="_modules/torch/nn/modules/pooling.html#MaxUnpool1d">[source]</a></p>
<p>&#x8BA1;&#x7B97;&#x7684; <code>MaxPool1d</code>&#x7684;&#x5C40;&#x90E8;&#x9006;&#x3002;</p>
<p><code>MaxPool1d</code>&#x4E0D;&#x662F;&#x5B8C;&#x5168;&#x53EF;&#x9006;&#x7684;&#xFF0C;&#x56E0;&#x4E3A;&#x975E;&#x6781;&#x5927;&#x503C;&#x90FD;&#x5C06;&#x4E22;&#x5931;&#x3002;</p>
<p><code>MaxUnpool1d</code>&#x53D6;&#x5165;&#x4F5C;&#x4E3A;&#x8F93;&#x5165; <code>&#x8F93;&#x51FA;MaxPool1d</code>&#x5305;&#x62EC;&#x7684;&#x7D22;&#x5F15;&#x6781;&#x5927;&#x503C;&#xFF0C;&#x5E76;&#x8BA1;&#x7B97;&#x5176;&#x4E2D;&#x6240;&#x6709;&#x975E;&#x6781;&#x5927;&#x503C;&#x90FD;&#x8BBE;&#x7F6E;&#x4E3A;&#x96F6;&#x7684;&#x5C40;&#x90E8;&#x9006;&#x3002;</p>
<p>Note</p>
<p><code>MaxPool1d</code>
&#x53EF;&#x4EE5;&#x6620;&#x5C04;&#x591A;&#x4E2A;&#x8F93;&#x5165;&#x5927;&#x5C0F;&#xFF0C;&#x4EE5;&#x76F8;&#x540C;&#x7684;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x3002;&#x56E0;&#x6B64;&#xFF0C;&#x53CD;&#x6F14;&#x8FC7;&#x7A0B;&#x53EF;&#x4EE5;&#x5F97;&#x5230;&#x660E;&#x786E;&#x3002;&#x4E3A;&#x4E86;&#x9002;&#x5E94;&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#xFF0C;&#x53EF;&#x4EE5;&#x63D0;&#x4F9B;&#x6240;&#x9700;&#x7684;&#x8F93;&#x51FA;&#x5C3A;&#x5BF8;&#x4E3A;&#x524D;&#x5411;&#x547C;&#x53EB;&#x7684;&#x9644;&#x52A0;&#x81EA;&#x53D8;&#x91CF;<code>output_size</code>&#x3002;&#x770B;&#x5230;&#x8F93;&#x5165;&#x548C;&#x4E0B;&#x9762;&#x7684;&#x5B9E;&#x65BD;&#x4F8B;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>kernel_size</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#x6216;</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>&#x5143;&#x7EC4;</em> </a>&#xFF09; - &#x6700;&#x5927;&#x6C60;&#x7A97;&#x53E3;&#x7684;&#x5927;&#x5C0F;&#x3002;</p>
</li>
<li><p><strong>&#x6B65;&#x5E45;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#x6216;</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>&#x5143;&#x7EC4;</em> </a>&#xFF09; - &#x6700;&#x5927;&#x6C60;&#x7A97;&#x53E3;&#x7684;&#x6B65;&#x5E45;&#x3002;&#x5B83;&#x7684;&#x9ED8;&#x8BA4;&#x8BBE;&#x7F6E;&#x4E3A;<code>kernel_size</code>&#x3002;</p>
</li>
<li><p>&#x8FD9;&#x662F;&#x6DFB;&#x52A0;&#x5230;&#x8F93;&#x5165;&#x586B;&#x5145; - <strong>&#x586B;&#x5145;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#x6216;</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>&#x5143;&#x7EC4;</em> </a>&#xFF09;</p>
</li>
</ul>
<p>Inputs:</p>
<ul>
<li><p>&#x8F93;&#x5165;&#xFF1A;&#x5C06;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x53CD;&#x8F6C;</p>
</li>
<li><p>&#x6307;&#x6570;&#xFF1A;&#x7531;&#x7ED9;&#x51FA;&#x4E86;&#x7D22;&#x5F15;<code>MaxPool1d</code></p>
</li>
<li><p>output_size &#xFF08;&#x53EF;&#x9009;&#xFF09;&#xFF1A;&#x76EE;&#x6807;&#x8F93;&#x51FA;&#x5927;&#x5C0F;</p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>&#x8F93;&#x5165;&#xFF1A; &#xFF08; N  &#xFF0C; C  &#xFF0C; H  i&#x7684; n&#x7684; &#xFF09; &#xFF08;N&#xFF0C;C&#xFF0C;H_ {&#x5728;}&#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF0C; H  i&#x7684; n&#x7684; &#xFF09;</p>
</li>
<li><p>&#x8F93;&#x51FA;&#xFF1A; &#xFF08; N  &#xFF0C; C  &#xFF0C; H  O  U  T  &#xFF09; &#xFF08;N&#xFF0C;C&#xFF0C;H_ {&#x51FA;}&#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF0C; H  O  U  T  &#xFF09;  &#xFF0C;&#x5176;&#x4E2D;</p>
</li>
</ul>
<p>Hout=(Hin&#x2212;1)&#xD7;stride[0]&#x2212;2&#xD7;padding[0]+kernel<em>size[0]H</em>{out} = (H_{in} - 1)
\times \text{stride}[0] - 2 \times \text{padding}[0] + \text{kernel\_size}[0]
Hout&#x200B;=(Hin&#x200B;&#x2212;1)&#xD7;stride[0]&#x2212;2&#xD7;padding[0]+kernel_size[0]</p>
<p>&#x6216;&#x7531;<code>output_size</code>&#x5728;&#x547C;&#x53EB;&#x64CD;&#x4F5C;&#x5458;&#x7ED9;&#x5B9A;&#x7684;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; pool = nn.MaxPool1d(2, stride=2, return_indices=True)
&gt;&gt;&gt; unpool = nn.MaxUnpool1d(2, stride=2)
&gt;&gt;&gt; input = torch.tensor([[[1., 2, 3, 4, 5, 6, 7, 8]]])
&gt;&gt;&gt; output, indices = pool(input)
&gt;&gt;&gt; unpool(output, indices)
tensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.]]])

&gt;&gt;&gt; # Example showcasing the use of output_size
&gt;&gt;&gt; input = torch.tensor([[[1., 2, 3, 4, 5, 6, 7, 8, 9]]])
&gt;&gt;&gt; output, indices = pool(input)
&gt;&gt;&gt; unpool(output, indices, output_size=input.size())
tensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.,  0.]]])

&gt;&gt;&gt; unpool(output, indices)
tensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.]]])
</code></pre><h3 id="maxunpool2d">MaxUnpool2d</h3>
<p><em>class</em><code>torch.nn.``MaxUnpool2d</code>( <em>kernel_size</em> , <em>stride=None</em> , <em>padding=0</em>
)<a href="_modules/torch/nn/modules/pooling.html#MaxUnpool2d">[source]</a></p>
<p>&#x8BA1;&#x7B97;&#x7684; <code>MaxPool2d</code>&#x7684;&#x5C40;&#x90E8;&#x9006;&#x3002;</p>
<p><code>MaxPool2d</code>&#x4E0D;&#x662F;&#x5B8C;&#x5168;&#x53EF;&#x9006;&#x7684;&#xFF0C;&#x56E0;&#x4E3A;&#x975E;&#x6781;&#x5927;&#x503C;&#x90FD;&#x5C06;&#x4E22;&#x5931;&#x3002;</p>
<p><code>MaxUnpool2d</code>&#x53D6;&#x5165;&#x4F5C;&#x4E3A;&#x8F93;&#x5165; <code>&#x8F93;&#x51FA;MaxPool2d</code>&#x5305;&#x62EC;&#x7684;&#x7D22;&#x5F15;&#x6781;&#x5927;&#x503C;&#xFF0C;&#x5E76;&#x8BA1;&#x7B97;&#x5176;&#x4E2D;&#x6240;&#x6709;&#x975E;&#x6781;&#x5927;&#x503C;&#x90FD;&#x8BBE;&#x7F6E;&#x4E3A;&#x96F6;&#x7684;&#x5C40;&#x90E8;&#x9006;&#x3002;</p>
<p>Note</p>
<p><code>MaxPool2d</code>
&#x53EF;&#x4EE5;&#x6620;&#x5C04;&#x591A;&#x4E2A;&#x8F93;&#x5165;&#x5927;&#x5C0F;&#xFF0C;&#x4EE5;&#x76F8;&#x540C;&#x7684;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x3002;&#x56E0;&#x6B64;&#xFF0C;&#x53CD;&#x6F14;&#x8FC7;&#x7A0B;&#x53EF;&#x4EE5;&#x5F97;&#x5230;&#x660E;&#x786E;&#x3002;&#x4E3A;&#x4E86;&#x9002;&#x5E94;&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#xFF0C;&#x53EF;&#x4EE5;&#x63D0;&#x4F9B;&#x6240;&#x9700;&#x7684;&#x8F93;&#x51FA;&#x5C3A;&#x5BF8;&#x4E3A;&#x524D;&#x5411;&#x547C;&#x53EB;&#x7684;&#x9644;&#x52A0;&#x81EA;&#x53D8;&#x91CF;<code>output_size</code>&#x3002;&#x770B;&#x5230;&#x8F93;&#x5165;&#x548C;&#x4E0B;&#x9762;&#x7684;&#x5B9E;&#x65BD;&#x4F8B;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>kernel_size</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>or</em><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>tuple</em></a>) &#x2013; Size of the max pooling window.</p>
</li>
<li><p><strong>stride</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>or</em><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>tuple</em></a>) &#x2013; Stride of the max pooling window. It is set to <code>kernel_size</code>by default.</p>
</li>
<li><p><strong>padding</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>or</em><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>tuple</em></a>) &#x2013; Padding that was added to the input</p>
</li>
</ul>
<p>Inputs:</p>
<ul>
<li><p>input: the input Tensor to invert</p>
</li>
<li><p>&#x6307;&#x6570;&#xFF1A;&#x7531;&#x7ED9;&#x51FA;&#x4E86;&#x7D22;&#x5F15;<code>MaxPool2d</code></p>
</li>
<li><p>output_size (optional): the targeted output size</p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>Input: (N,C,Hin,Win)(N, C, H<em>{in}, W</em>{in})(N,C,Hin&#x200B;,Win&#x200B;)</p>
</li>
<li><p>Output: (N,C,Hout,Wout)(N, C, H<em>{out}, W</em>{out})(N,C,Hout&#x200B;,Wout&#x200B;) , where</p>
</li>
</ul>
<p>Hout=(Hin&#x2212;1)&#xD7;stride[0]&#x2212;2&#xD7;padding[0]+kernel<em>size[0]H</em>{out} = (H_{in} - 1)
\times \text{stride[0]} - 2 \times \text{padding[0]} + \text{kernel\_size[0]}
Hout&#x200B;=(Hin&#x200B;&#x2212;1)&#xD7;stride[0]&#x2212;2&#xD7;padding[0]+kernel_size[0]</p>
<p>Wout=(Win&#x2212;1)&#xD7;stride[1]&#x2212;2&#xD7;padding[1]+kernel<em>size[1]W</em>{out} = (W_{in} - 1)
\times \text{stride[1]} - 2 \times \text{padding[1]} + \text{kernel\_size[1]}
Wout&#x200B;=(Win&#x200B;&#x2212;1)&#xD7;stride[1]&#x2212;2&#xD7;padding[1]+kernel_size[1]</p>
<p>or as given by <code>output_size</code>in the call operator</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; pool = nn.MaxPool2d(2, stride=2, return_indices=True)
&gt;&gt;&gt; unpool = nn.MaxUnpool2d(2, stride=2)
&gt;&gt;&gt; input = torch.tensor([[[[ 1.,  2,  3,  4],
                            [ 5,  6,  7,  8],
                            [ 9, 10, 11, 12],
                            [13, 14, 15, 16]]]])
&gt;&gt;&gt; output, indices = pool(input)
&gt;&gt;&gt; unpool(output, indices)
tensor([[[[  0.,   0.,   0.,   0.],
          [  0.,   6.,   0.,   8.],
          [  0.,   0.,   0.,   0.],
          [  0.,  14.,   0.,  16.]]]])

&gt;&gt;&gt; # specify a different output size than input size
&gt;&gt;&gt; unpool(output, indices, output_size=torch.Size([1, 1, 5, 5]))
tensor([[[[  0.,   0.,   0.,   0.,   0.],
          [  6.,   0.,   8.,   0.,   0.],
          [  0.,   0.,   0.,  14.,   0.],
          [ 16.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.]]]])
</code></pre><h3 id="maxunpool3d">MaxUnpool3d</h3>
<p><em>class</em><code>torch.nn.``MaxUnpool3d</code>( <em>kernel_size</em> , <em>stride=None</em> , <em>padding=0</em>
)<a href="_modules/torch/nn/modules/pooling.html#MaxUnpool3d">[source]</a></p>
<p>&#x8BA1;&#x7B97;&#x7684; <code>MaxPool3d</code>&#x7684;&#x5C40;&#x90E8;&#x9006;&#x3002;</p>
<p><code>MaxPool3d</code>&#x4E0D;&#x662F;&#x5B8C;&#x5168;&#x53EF;&#x9006;&#x7684;&#xFF0C;&#x56E0;&#x4E3A;&#x975E;&#x6781;&#x5927;&#x503C;&#x90FD;&#x5C06;&#x4E22;&#x5931;&#x3002;<code>MaxUnpool3d</code>&#x53D6;&#x5165;&#x4F5C;&#x4E3A;&#x8F93;&#x5165; <code>&#x8F93;&#x51FA;MaxPool3d</code>
&#x5305;&#x62EC;&#x7684;&#x7D22;&#x5F15;&#x6781;&#x5927;&#x503C;&#xFF0C;&#x5E76;&#x8BA1;&#x7B97;&#x5176;&#x4E2D;&#x6240;&#x6709;&#x975E;&#x6781;&#x5927;&#x503C;&#x90FD;&#x8BBE;&#x7F6E;&#x4E3A;&#x96F6;&#x7684;&#x5C40;&#x90E8;&#x9006;&#x3002;</p>
<p>Note</p>
<p><code>MaxPool3d</code>
&#x53EF;&#x4EE5;&#x6620;&#x5C04;&#x591A;&#x4E2A;&#x8F93;&#x5165;&#x5927;&#x5C0F;&#xFF0C;&#x4EE5;&#x76F8;&#x540C;&#x7684;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x3002;&#x56E0;&#x6B64;&#xFF0C;&#x53CD;&#x6F14;&#x8FC7;&#x7A0B;&#x53EF;&#x4EE5;&#x5F97;&#x5230;&#x660E;&#x786E;&#x3002;&#x4E3A;&#x4E86;&#x9002;&#x5E94;&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#xFF0C;&#x53EF;&#x4EE5;&#x63D0;&#x4F9B;&#x6240;&#x9700;&#x7684;&#x8F93;&#x51FA;&#x5C3A;&#x5BF8;&#x4E3A;&#x524D;&#x5411;&#x547C;&#x53EB;&#x7684;&#x9644;&#x52A0;&#x81EA;&#x53D8;&#x91CF;<code>output_size</code>&#x3002;&#x8BF7;&#x53C2;&#x9605;&#x4E0B;&#x9762;&#x7684;&#x8F93;&#x5165;&#x90E8;&#x5206;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>kernel_size</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>or</em><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>tuple</em></a>) &#x2013; Size of the max pooling window.</p>
</li>
<li><p><strong>stride</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>or</em><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>tuple</em></a>) &#x2013; Stride of the max pooling window. It is set to <code>kernel_size</code>by default.</p>
</li>
<li><p><strong>padding</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>or</em><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>tuple</em></a>) &#x2013; Padding that was added to the input</p>
</li>
</ul>
<p>Inputs:</p>
<ul>
<li><p>input: the input Tensor to invert</p>
</li>
<li><p>&#x6307;&#x6570;&#xFF1A;&#x7531;&#x7ED9;&#x51FA;&#x4E86;&#x7D22;&#x5F15;<code>MaxPool3d</code></p>
</li>
<li><p>output_size (optional): the targeted output size</p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>Input: (N,C,Din,Hin,Win)(N, C, D<em>{in}, H</em>{in}, W_{in})(N,C,Din&#x200B;,Hin&#x200B;,Win&#x200B;)</p>
</li>
<li><p>Output: (N,C,Dout,Hout,Wout)(N, C, D<em>{out}, H</em>{out}, W_{out})(N,C,Dout&#x200B;,Hout&#x200B;,Wout&#x200B;) , where</p>
</li>
</ul>
<p>Dout=(Din&#x2212;1)&#xD7;stride[0]&#x2212;2&#xD7;padding[0]+kernel<em>size[0]D</em>{out} = (D_{in} - 1)
\times \text{stride[0]} - 2 \times \text{padding[0]} + \text{kernel\_size[0]}
Dout&#x200B;=(Din&#x200B;&#x2212;1)&#xD7;stride[0]&#x2212;2&#xD7;padding[0]+kernel_size[0]</p>
<p>Hout=(Hin&#x2212;1)&#xD7;stride[1]&#x2212;2&#xD7;padding[1]+kernel<em>size[1]H</em>{out} = (H_{in} - 1)
\times \text{stride[1]} - 2 \times \text{padding[1]} + \text{kernel\_size[1]}
Hout&#x200B;=(Hin&#x200B;&#x2212;1)&#xD7;stride[1]&#x2212;2&#xD7;padding[1]+kernel_size[1]</p>
<p>Wout=(Win&#x2212;1)&#xD7;stride[2]&#x2212;2&#xD7;padding[2]+kernel<em>size[2]W</em>{out} = (W_{in} - 1)
\times \text{stride[2]} - 2 \times \text{padding[2]} + \text{kernel\_size[2]}
Wout&#x200B;=(Win&#x200B;&#x2212;1)&#xD7;stride[2]&#x2212;2&#xD7;padding[2]+kernel_size[2]</p>
<p>or as given by <code>output_size</code>in the call operator</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; # pool of square window of size=3, stride=2
&gt;&gt;&gt; pool = nn.MaxPool3d(3, stride=2, return_indices=True)
&gt;&gt;&gt; unpool = nn.MaxUnpool3d(3, stride=2)
&gt;&gt;&gt; output, indices = pool(torch.randn(20, 16, 51, 33, 15))
&gt;&gt;&gt; unpooled_output = unpool(output, indices)
&gt;&gt;&gt; unpooled_output.size()
torch.Size([20, 16, 51, 33, 15])
</code></pre><h3 id="avgpool1d">AvgPool1d</h3>
<p><em>class</em><code>torch.nn.``AvgPool1d</code>( <em>kernel_size</em> , <em>stride=None</em> , <em>padding=0</em> ,
<em>ceil_mode=False</em> , <em>count_include_pad=True</em>
)<a href="_modules/torch/nn/modules/pooling.html#AvgPool1d">[source]</a></p>
<p>&#x9002;&#x7528;&#x5728;&#x51E0;&#x4E2A;&#x8F93;&#x5165;&#x5E73;&#x9762;&#x7EC4;&#x6210;&#x7684;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x7684;&#x5E73;&#x5747;1D&#x6C60;&#x3002;</p>
<p>&#x5728;&#x6700;&#x7B80;&#x5355;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x6240;&#x8FF0;&#x5C42;&#x7684;&#x4E0E;&#x8F93;&#x5165;&#x5927;&#x5C0F; &#x7684;&#x8F93;&#x51FA;&#x503C;&#xFF08; N  C  &#xFF0C; L  &#xFF09; &#xFF08;N&#xFF0C;C&#xFF0C;L&#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF0C; L  &#xFF09; &#xFF0C;&#x8F93;&#x51FA; &#xFF08; N  &#xFF0C; C  &#xFF0C;
L  O  U  T  &#xFF09; &#xFF08; N&#xFF0C;C&#xFF0C;L_ {&#x51FA;}&#xFF09; &#xFF08; N  C  &#xFF0C; L  O  U  T  &#xFF09; &#x548C;<code>kernel_size</code>K  K  K
&#x53EF;&#x4EE5;&#x7CBE;&#x786E;&#x5730;&#x63CF;&#x8FF0;&#x4E3A;&#xFF1A;</p>
<p>out(Ni,Cj,l)=1k&#x2211;m=0k&#x2212;1input(Ni,Cj,stride&#xD7;l+m)\text{out}(N<em>i, C_j, l) =
\frac{1}{k} \sum</em>{m=0}^{k-1} \text{input}(N_i, C_j, \text{stride} \times l +
m)out(Ni&#x200B;,Cj&#x200B;,l)=k1&#x200B;m=0&#x2211;k&#x2212;1&#x200B;input(Ni&#x200B;,Cj&#x200B;,stride&#xD7;l+m)</p>
<p>&#x5982;&#x679C;<code>&#x586B;&#x5145;</code>&#x662F;&#x975E;&#x96F6;&#xFF0C;&#x5219;&#x8F93;&#x5165;&#x662F;&#x9690;&#x5F0F;&#x5730;&#x5728;&#x4E24;&#x4FA7;&#x4E0A;&#x7528;&#x96F6;&#x586B;&#x5145;&#x4E3A;<code>&#x586B;&#x5145;</code>&#x70B9;&#x6570;&#x3002;</p>
<p>&#x53C2;&#x6570;<code>kernel_size</code>&#xFF0C;<code>&#x6B65;&#x5E45;</code>&#xFF0C;<code>&#x586B;&#x5145;</code>&#x53EF;&#x5404;&#x81EA;&#x4E3A;&#x4E00;&#x4E2A;<code>INT</code>&#x6216;&#x4E00;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x5143;&#x7EC4;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>kernel_size</strong> - &#x7A97;&#x53E3;&#x7684;&#x5927;&#x5C0F;</p>
</li>
<li><p><strong>stride</strong> &#x2013; the stride of the window. Default value is <code>kernel_size</code></p>
</li>
<li><p><strong>padding</strong> &#x2013; implicit zero padding to be added on both sides</p>
</li>
<li><p><strong>ceil_mode</strong> &#x2013; when True, will use ceil instead of floor to compute the output shape</p>
</li>
<li><p><strong>count_include_pad</strong> - &#x771F;&#x65F6;&#xFF0C;&#x5C06;&#x5305;&#x62EC;&#x5728;&#x5E73;&#x5747;&#x8BA1;&#x7B97;&#x8865;&#x96F6;</p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>Input: (N,C,Lin)(N, C, L_{in})(N,C,Lin&#x200B;)</p>
</li>
<li><p>Output: (N,C,Lout)(N, C, L_{out})(N,C,Lout&#x200B;) , where</p>
</li>
</ul>
<p>Lout=&#x230A;Lin+2&#xD7;padding&#x2212;kernel<em>sizestride+1&#x230B;L</em>{out} = \left\lfloor \frac{L_{in} +
2 \times \text{padding} - \text{kernel\_size}}{\text{stride}} +
1\right\rfloor Lout&#x200B;=&#x230A;strideLin&#x200B;+2&#xD7;padding&#x2212;kernel_size&#x200B;+1&#x230B;</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; # pool with window of size=3, stride=2
&gt;&gt;&gt; m = nn.AvgPool1d(3, stride=2)
&gt;&gt;&gt; m(torch.tensor([[[1.,2,3,4,5,6,7]]]))
tensor([[[ 2.,  4.,  6.]]])
</code></pre><h3 id="avgpool2d">AvgPool2d</h3>
<p><em>class</em><code>torch.nn.``AvgPool2d</code>( <em>kernel_size</em> , <em>stride=None</em> , <em>padding=0</em> ,
<em>ceil_mode=False</em> , <em>count_include_pad=True</em> , <em>divisor_override=None</em>
)<a href="_modules/torch/nn/modules/pooling.html#AvgPool2d">[source]</a></p>
<p>&#x9002;&#x7528;&#x5728;&#x51E0;&#x4E2A;&#x8F93;&#x5165;&#x5E73;&#x9762;&#x7EC4;&#x6210;&#x7684;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x7684;2D&#x5E73;&#x5747;&#x6C60;&#x3002;</p>
<p>In the simplest case, the output value of the layer with input size
(N,C,H,W)(N, C, H, W)(N,C,H,W) , output (N,C,Hout,Wout)(N, C, H<em>{out},
W</em>{out})(N,C,Hout&#x200B;,Wout&#x200B;) and <code>kernel_size</code>(kH,kW)(kH, kW)(kH,kW) can be
precisely described as:</p>
<p>out(Ni,Cj,h,w)=1kH&#x2217;kW&#x2211;m=0kH&#x2212;1&#x2211;n=0kW&#x2212;1input(Ni,Cj,stride[0]&#xD7;h+m,stride[1]&#xD7;w+n)out(N<em>i,
C_j, h, w) = \frac{1}{kH * kW} \sum</em>{m=0}^{kH-1} \sum_{n=0}^{kW-1} input(N_i,
C_j, stride[0] \times h + m, stride[1] \times w +
n)out(Ni&#x200B;,Cj&#x200B;,h,w)=kH&#x2217;kW1&#x200B;m=0&#x2211;kH&#x2212;1&#x200B;n=0&#x2211;kW&#x2212;1&#x200B;input(Ni&#x200B;,Cj&#x200B;,stride[0]&#xD7;h+m,stride[1]&#xD7;w+n)</p>
<p>If <code>padding</code>is non-zero, then the input is implicitly zero-padded on both
sides for <code>padding</code>number of points.</p>
<p>&#x53C2;&#x6570;<code>kernel_size</code>&#xFF0C;<code>&#x6B65;&#x5E45;</code>&#xFF0C;<code>&#x586B;&#x5145;</code>&#x53EF;&#x4EE5;&#x662F;&#xFF1A;</p>
<blockquote>
<ul>
<li>a single <code>int</code>&#x2013; in which case the same value is used for the height and
width dimension</li>
</ul>
</blockquote>
<p>&gt;</p>
<blockquote>
<ul>
<li>a <code>tuple</code>of two ints &#x2013; in which case, the first int is used for the
height dimension, and the second int for the width dimension</li>
</ul>
</blockquote>
<p>&gt;</p>
<p>&gt;</p>
<p>Parameters</p>
<ul>
<li><p><strong>kernel_size</strong> &#x2013; the size of the window</p>
</li>
<li><p><strong>stride</strong> &#x2013; the stride of the window. Default value is <code>kernel_size</code></p>
</li>
<li><p><strong>padding</strong> &#x2013; implicit zero padding to be added on both sides</p>
</li>
<li><p><strong>ceil_mode</strong> &#x2013; when True, will use ceil instead of floor to compute the output shape</p>
</li>
<li><p><strong>count_include_pad</strong> &#x2013; when True, will include the zero-padding in the averaging calculation</p>
</li>
<li><p><strong>divisor_override</strong> - &#x5982;&#x679C;&#x6307;&#x5B9A;&#x7684;&#x8BDD;&#xFF0C;&#x5B83;&#x5C06;&#x88AB;&#x7528;&#x4F5C;&#x9664;&#x6570;&#xFF0C;&#x5426;&#x5219;ATTR&#xFF1A; kernel_size &#x5C06;&#x7528;&#x4E8E;</p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>Input: (N,C,Hin,Win)(N, C, H<em>{in}, W</em>{in})(N,C,Hin&#x200B;,Win&#x200B;)</p>
</li>
<li><p>Output: (N,C,Hout,Wout)(N, C, H<em>{out}, W</em>{out})(N,C,Hout&#x200B;,Wout&#x200B;) , where</p>
</li>
</ul>
<p>Hout=&#x230A;Hin+2&#xD7;padding[0]&#x2212;kernel<em>size[0]stride[0]+1&#x230B;H</em>{out} =
\left\lfloor\frac{H_{in} + 2 \times \text{padding}[0] -
\text{kernel\_size}[0]}{\text{stride}[0]} + 1\right\rfloor
Hout&#x200B;=&#x230A;stride[0]Hin&#x200B;+2&#xD7;padding[0]&#x2212;kernel_size[0]&#x200B;+1&#x230B;</p>
<p>Wout=&#x230A;Win+2&#xD7;padding[1]&#x2212;kernel<em>size[1]stride[1]+1&#x230B;W</em>{out} =
\left\lfloor\frac{W_{in} + 2 \times \text{padding}[1] -
\text{kernel\_size}[1]}{\text{stride}[1]} + 1\right\rfloor
Wout&#x200B;=&#x230A;stride[1]Win&#x200B;+2&#xD7;padding[1]&#x2212;kernel_size[1]&#x200B;+1&#x230B;</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; # pool of square window of size=3, stride=2
&gt;&gt;&gt; m = nn.AvgPool2d(3, stride=2)
&gt;&gt;&gt; # pool of non-square window
&gt;&gt;&gt; m = nn.AvgPool2d((3, 2), stride=(2, 1))
&gt;&gt;&gt; input = torch.randn(20, 16, 50, 32)
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="avgpool3d">AvgPool3d</h3>
<p><em>class</em><code>torch.nn.``AvgPool3d</code>( <em>kernel_size</em> , <em>stride=None</em> , <em>padding=0</em> ,
<em>ceil_mode=False</em> , <em>count_include_pad=True</em> , <em>divisor_override=None</em>
)<a href="_modules/torch/nn/modules/pooling.html#AvgPool3d">[source]</a></p>
<p>&#x9002;&#x7528;&#x5728;&#x51E0;&#x4E2A;&#x8F93;&#x5165;&#x5E73;&#x9762;&#x7EC4;&#x6210;&#x7684;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x7684;&#x5E73;&#x5747;&#x4E09;&#x7EF4;&#x6C60;&#x3002;</p>
<p>In the simplest case, the output value of the layer with input size
(N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W) , output (N,C,Dout,Hout,Wout)(N, C,
D<em>{out}, H</em>{out}, W_{out})(N,C,Dout&#x200B;,Hout&#x200B;,Wout&#x200B;) and <code>kernel_size</code>
(kD,kH,kW)(kD, kH, kW)(kD,kH,kW) can be precisely described as:</p>
<p>out(Ni,Cj,d,h,w)=&#x2211;k=0kD&#x2212;1&#x2211;m=0kH&#x2212;1&#x2211;n=0kW&#x2212;1input(Ni,Cj,stride[0]&#xD7;d+k,stride[1]&#xD7;h+m,stride[2]&#xD7;w+n)kD&#xD7;kH&#xD7;kW\begin{aligned}
\text{out}(N<em>i, C_j, d, h, w) ={} &amp; \sum</em>{k=0}^{kD-1} \sum<em>{m=0}^{kH-1}
\sum</em>{n=0}^{kW-1} \\ &amp; \frac{\text{input}(N_i, C_j, \text{stride}[0] \times d</p>
<ul>
<li>k, \text{stride}[1] \times h + m, \text{stride}[2] \times w + n)} {kD \times
kH \times kW} \end{aligned}
out(Ni&#x200B;,Cj&#x200B;,d,h,w)=&#x200B;k=0&#x2211;kD&#x2212;1&#x200B;m=0&#x2211;kH&#x2212;1&#x200B;n=0&#x2211;kW&#x2212;1&#x200B;kD&#xD7;kH&#xD7;kWinput(Ni&#x200B;,Cj&#x200B;,stride[0]&#xD7;d+k,stride[1]&#xD7;h+m,stride[2]&#xD7;w+n)&#x200B;&#x200B;</li>
</ul>
<p>&#x5982;&#x679C;<code>&#x586B;&#x5145;</code>&#x662F;&#x975E;&#x96F6;&#xFF0C;&#x5219;&#x8F93;&#x5165;&#x662F;&#x9690;&#x5F0F;&#x5730;&#x5728;&#x6240;&#x6709;&#x4E09;&#x4E2A;&#x4FA7;&#x9762;&#x96F6;&#x586B;&#x5145;&#x4E3A;<code>&#x586B;&#x5145;</code>&#x6570;&#x91CF;&#x7684;&#x70B9;&#x3002;</p>
<p>&#x53C2;&#x6570;<code>kernel_size</code>&#xFF0C;<code>&#x6B65;&#x5E45;</code>&#x53EF;&#x4EE5;&#x662F;&#xFF1A;</p>
<blockquote>
<ul>
<li>a single <code>int</code>&#x2013; in which case the same value is used for the depth,
height and width dimension</li>
</ul>
</blockquote>
<p>&gt;</p>
<blockquote>
<ul>
<li>a <code>tuple</code>of three ints &#x2013; in which case, the first int is used for the
depth dimension, the second int for the height dimension and the third int for
the width dimension</li>
</ul>
</blockquote>
<p>&gt;</p>
<p>&gt;</p>
<p>Parameters</p>
<ul>
<li><p><strong>kernel_size</strong> &#x2013; the size of the window</p>
</li>
<li><p><strong>stride</strong> &#x2013; the stride of the window. Default value is <code>kernel_size</code></p>
</li>
<li><p><strong>padding</strong> &#x2013; implicit zero padding to be added on all three sides</p>
</li>
<li><p><strong>ceil_mode</strong> &#x2013; when True, will use ceil instead of floor to compute the output shape</p>
</li>
<li><p><strong>count_include_pad</strong> &#x2013; when True, will include the zero-padding in the averaging calculation</p>
</li>
<li><p><strong>divisor_override</strong> &#x2013; if specified, it will be used as divisor, otherwise attr:kernel_size will be used</p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>Input: (N,C,Din,Hin,Win)(N, C, D<em>{in}, H</em>{in}, W_{in})(N,C,Din&#x200B;,Hin&#x200B;,Win&#x200B;)</p>
</li>
<li><p>Output: (N,C,Dout,Hout,Wout)(N, C, D<em>{out}, H</em>{out}, W_{out})(N,C,Dout&#x200B;,Hout&#x200B;,Wout&#x200B;) , where</p>
</li>
</ul>
<p>Dout=&#x230A;Din+2&#xD7;padding[0]&#x2212;kernel<em>size[0]stride[0]+1&#x230B;D</em>{out} =
\left\lfloor\frac{D_{in} + 2 \times \text{padding}[0] -
\text{kernel\_size}[0]}{\text{stride}[0]} + 1\right\rfloor
Dout&#x200B;=&#x230A;stride[0]Din&#x200B;+2&#xD7;padding[0]&#x2212;kernel_size[0]&#x200B;+1&#x230B;</p>
<p>Hout=&#x230A;Hin+2&#xD7;padding[1]&#x2212;kernel<em>size[1]stride[1]+1&#x230B;H</em>{out} =
\left\lfloor\frac{H_{in} + 2 \times \text{padding}[1] -
\text{kernel\_size}[1]}{\text{stride}[1]} + 1\right\rfloor
Hout&#x200B;=&#x230A;stride[1]Hin&#x200B;+2&#xD7;padding[1]&#x2212;kernel_size[1]&#x200B;+1&#x230B;</p>
<p>Wout=&#x230A;Win+2&#xD7;padding[2]&#x2212;kernel<em>size[2]stride[2]+1&#x230B;W</em>{out} =
\left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] -
\text{kernel\_size}[2]}{\text{stride}[2]} + 1\right\rfloor
Wout&#x200B;=&#x230A;stride[2]Win&#x200B;+2&#xD7;padding[2]&#x2212;kernel_size[2]&#x200B;+1&#x230B;</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; # pool of square window of size=3, stride=2
&gt;&gt;&gt; m = nn.AvgPool3d(3, stride=2)
&gt;&gt;&gt; # pool of non-square window
&gt;&gt;&gt; m = nn.AvgPool3d((3, 2, 2), stride=(2, 1, 2))
&gt;&gt;&gt; input = torch.randn(20, 16, 50,44, 31)
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="fractionalmaxpool2d">FractionalMaxPool2d</h3>
<p><em>class</em><code>torch.nn.``FractionalMaxPool2d</code>( <em>kernel_size</em> , <em>output_size=None</em> ,
<em>output_ratio=None</em> , <em>return_indices=False</em> , <em>_random_samples=None</em>
)<a href="_modules/torch/nn/modules/pooling.html#FractionalMaxPool2d">[source]</a></p>
<p>&#x9002;&#x7528;&#x5728;&#x51E0;&#x4E2A;&#x8F93;&#x5165;&#x5E73;&#x9762;&#x7EC4;&#x6210;&#x7684;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x7684;2D&#x5206;&#x6570;&#x6700;&#x5927;&#x6C60;&#x3002;</p>
<p>&#x5206;&#x6570;MaxPooling&#x4E2D;&#x8BE6;&#x7EC6;&#x7EB8;&#x5F20;<a href="http://arxiv.org/abs/1412.6071" target="_blank">&#x5206;&#x6570;MaxPooling </a>&#x901A;&#x8FC7;&#x683C;&#x96F7;&#x5384;&#x59C6;&#x63CF;&#x8FF0;</p>
<p>&#x6700;&#x5927;-&#x6C60;&#x64CD;&#x4F5C;&#x5728;&#x65BD;&#x52A0; K  H  &#xD7; K  W  &#x7684;kH \&#x500D;&#x5343;&#x74E6; K  H  &#xD7; K  W
&#x533A;&#x57DF;&#x901A;&#x8FC7;&#x7531;&#x76EE;&#x6807;&#x8F93;&#x51FA;&#x5C3A;&#x5BF8;&#x51B3;&#x5B9A;&#x7684;&#x968F;&#x673A;&#x6B65;&#x957F;&#x3002;&#x7684;&#x8F93;&#x51FA;&#x7279;&#x5F81;&#x7684;&#x6570;&#x91CF;&#x7B49;&#x4E8E;&#x8F93;&#x5165;&#x5E73;&#x9762;&#x7684;&#x6570;&#x91CF;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>kernel_size</strong> - &#x7A97;&#x53E3;&#x7684;&#x5927;&#x5C0F;&#xFF0C;&#x4EE5;&#x91C7;&#x53D6;&#x6700;&#x5927;&#x8FC7;&#x6765;&#x3002;&#x53EF;&#x4EE5;&#x662F;&#x5355;&#x4E00;&#x7684;&#x6570;k&#xFF08;&#x5BF9;&#x4E8E;k X k&#x7684;&#x5E73;&#x65B9;&#x5185;&#x6838;&#xFF09;&#x6216;&#x5143;&#x7EC4;&#xFF08;KH&#xFF0C;&#x5343;&#x74E6;&#xFF09;</p>
</li>
<li><p><strong>output_size</strong> - &#x5F62;&#x5F0F;&#x54E6;X OW &#x7684;&#x56FE;&#x50CF;&#x7684;&#x76EE;&#x6807;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x3002;&#x53EF;&#x4EE5;&#x662F;&#x4E00;&#x4E2A;&#x5143;&#x7EC4;&#xFF08;OH&#xFF0C;OW&#xFF09;&#x6216;&#x6B63;&#x65B9;&#x5F62;&#x56FE;&#x50CF;&#x7684;&#x5355;&#x4E2A;&#x6570;&#x5B57;&#x5594;&#x54E6;X&#x54E6;</p>
</li>
<li><p><strong>output_ratio</strong> - &#x5982;&#x679C;&#x4E00;&#x4E2A;&#x4EBA;&#x5E0C;&#x671B;&#x6709;&#x4E00;&#x4E2A;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x4E3A;&#x8F93;&#x5165;&#x5927;&#x5C0F;&#x7684;&#x6BD4;&#x7387;&#xFF0C;&#x8FD9;&#x4E2A;&#x9009;&#x9879;&#x53EF;&#x4EE5;&#x7ED9;&#x51FA;&#x3002;&#x8FD9;&#x5FC5;&#x987B;&#x662F;&#x5728;&#x8303;&#x56F4;&#x5185;&#x7684;&#x6570;&#x6216;&#x5143;&#x7EC4;&#xFF08;0&#xFF0C;1&#xFF09;</p>
</li>
<li><p><strong>return_indices</strong> - &#x5982;&#x679C;<code>&#x771F;</code>&#xFF0C;&#x5C06;&#x8FD4;&#x56DE;&#x6307;&#x6570;&#x4E2D;&#xFF0C;&#x4EA7;&#x51FA;&#x4E00;&#x8D77;&#x3002;&#x6709;&#x7528;&#x4F20;&#x9012;&#x7ED9;<code>nn.MaxUnpool2d&#xFF08;&#xFF09;</code>&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x5047;</code></p>
</li>
</ul>
<p>&#x4F8B;&#x5B50;</p>
<pre><code>&gt;&gt;&gt; # pool of square window of size=3, and target output size 13x12
&gt;&gt;&gt; m = nn.FractionalMaxPool2d(3, output_size=(13, 12))
&gt;&gt;&gt; # pool of square window and target output size being half of input image size
&gt;&gt;&gt; m = nn.FractionalMaxPool2d(3, output_ratio=(0.5, 0.5))
&gt;&gt;&gt; input = torch.randn(20, 16, 50, 32)
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="lppool1d">LPPool1d</h3>
<p><em>class</em><code>torch.nn.``LPPool1d</code>( <em>norm_type</em> , <em>kernel_size</em> , <em>stride=None</em> ,
<em>ceil_mode=False</em>
)<a href="_modules/torch/nn/modules/pooling.html#LPPool1d">[source]</a></p>
<p>&#x9002;&#x7528;&#x5728;&#x51E0;&#x4E2A;&#x8F93;&#x5165;&#x5E73;&#x9762;&#x7EC4;&#x6210;&#x7684;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x7684;&#x529F;&#x7387;1D&#x5E73;&#x5747;&#x6C60;&#x3002;</p>
<p>&#x5728;&#x6BCF;&#x4E2A;&#x7A97;&#x53E3;&#x4E2D;&#xFF0C;&#x8BA1;&#x7B97;&#x51FA;&#x7684;&#x51FD;&#x6570;&#x662F;&#xFF1A;</p>
<p>f(X)=&#x2211;x&#x2208;Xxppf(X) = \sqrt[p]{\sum_{x \in X} x^{p}} f(X)=p&#x200B;x&#x2208;X&#x2211;&#x200B;xp&#x200B;</p>
<ul>
<li><p>&#x5728;P =  &#x221E; \ infty  &#x221E; &#xFF0C;&#x53EF;&#x4EE5;&#x5F97;&#x5230;&#x6700;&#x5927;&#x6C60;</p>
</li>
<li><p>&#x5728;p = 1&#x65F6;&#xFF0C;&#x53EF;&#x4EE5;&#x5F97;&#x5230;&#x8428;&#x59C6;&#x6C60;&#xFF08;&#x5176;&#x6B63;&#x6BD4;&#x4E8E;&#x5E73;&#x5747;&#x6C60;&#xFF09;</p>
</li>
</ul>
<p>Note</p>
<p>&#x5982;&#x679C;&#x603B;&#x548C;&#x81F3;p&#x7684;&#x529F;&#x7387;&#x4E3A;&#x96F6;&#x65F6;&#xFF0C;&#x8BE5;&#x51FD;&#x6570;&#x7684;&#x68AF;&#x5EA6;&#x6CA1;&#x6709;&#x5B9A;&#x4E49;&#x3002;&#x6B64;&#x5B9E;&#x73B0;&#x4F1A;&#x5728;&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x8BBE;&#x7F6E;&#x6E10;&#x53D8;&#x81F3;&#x96F6;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>kernel_size</strong> - &#x5355;&#x4E2A;int&#xFF0C;&#x7A97;&#x53E3;&#x7684;&#x5927;&#x5C0F;</p>
</li>
<li><p><strong>&#x6B65;&#x5E45;</strong> - &#x4E00;&#x4E2A;&#x5355;&#x4E00;&#x7684;&#x5728;&#xFF0C;&#x7A97;&#x53E3;&#x7684;&#x6B65;&#x5E45;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#x4E3A;<code>kernel_size</code></p>
</li>
<li><p><strong>ceil_mode</strong> &#x2013; when True, will use ceil instead of floor to compute the output shape</p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>Input: (N,C,Lin)(N, C, L_{in})(N,C,Lin&#x200B;)</p>
</li>
<li><p>Output: (N,C,Lout)(N, C, L_{out})(N,C,Lout&#x200B;) , where</p>
</li>
</ul>
<p>Lout=&#x230A;Lin+2&#xD7;padding&#x2212;kernel<em>sizestride+1&#x230B;L</em>{out} = \left\lfloor\frac{L_{in} + 2
\times \text{padding} - \text{kernel\_size}}{\text{stride}} + 1\right\rfloor
Lout&#x200B;=&#x230A;strideLin&#x200B;+2&#xD7;padding&#x2212;kernel_size&#x200B;+1&#x230B;</p>
<p>Examples::</p>
<pre><code>&gt;&gt;&gt; # power-2 pool of window of length 3, with stride 2.
&gt;&gt;&gt; m = nn.LPPool1d(2, 3, stride=2)
&gt;&gt;&gt; input = torch.randn(20, 16, 50)
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="lppool2d">LPPool2d</h3>
<p><em>class</em><code>torch.nn.``LPPool2d</code>( <em>norm_type</em> , <em>kernel_size</em> , <em>stride=None</em> ,
<em>ceil_mode=False</em>
)<a href="_modules/torch/nn/modules/pooling.html#LPPool2d">[source]</a></p>
<p>&#x9002;&#x7528;&#x5728;&#x51E0;&#x4E2A;&#x8F93;&#x5165;&#x5E73;&#x9762;&#x7EC4;&#x6210;&#x7684;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x7684;2D&#x529F;&#x7387;&#x5E73;&#x5747;&#x6C60;&#x3002;</p>
<p>On each window, the function computed is:</p>
<p>f(X)=&#x2211;x&#x2208;Xxppf(X) = \sqrt[p]{\sum_{x \in X} x^{p}} f(X)=p&#x200B;x&#x2208;X&#x2211;&#x200B;xp&#x200B;</p>
<ul>
<li><p>At p = &#x221E;\infty&#x221E; , one gets Max Pooling</p>
</li>
<li><p>&#x5728;p = 1&#x65F6;&#xFF0C;&#x53EF;&#x4EE5;&#x5F97;&#x5230;&#x8428;&#x59C6;&#x6C60;&#xFF08;&#x5176;&#x6B63;&#x6BD4;&#x4E8E;&#x5E73;&#x5747;&#x6C60;&#xFF09;</p>
</li>
</ul>
<p>The parameters <code>kernel_size</code>, <code>stride</code>can either be:</p>
<blockquote>
<ul>
<li>a single <code>int</code>&#x2013; in which case the same value is used for the height and
width dimension</li>
</ul>
</blockquote>
<p>&gt;</p>
<blockquote>
<ul>
<li>a <code>tuple</code>of two ints &#x2013; in which case, the first int is used for the
height dimension, and the second int for the width dimension</li>
</ul>
</blockquote>
<p>&gt;</p>
<p>&gt;</p>
<p>Note</p>
<p>If the sum to the power of p is zero, the gradient of this function is not
defined. This implementation will set the gradient to zero in this case.</p>
<p>Parameters</p>
<ul>
<li><p><strong>kernel_size</strong> &#x2013; the size of the window</p>
</li>
<li><p><strong>stride</strong> &#x2013; the stride of the window. Default value is <code>kernel_size</code></p>
</li>
<li><p><strong>ceil_mode</strong> &#x2013; when True, will use ceil instead of floor to compute the output shape</p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>Input: (N,C,Hin,Win)(N, C, H<em>{in}, W</em>{in})(N,C,Hin&#x200B;,Win&#x200B;)</p>
</li>
<li><p>Output: (N,C,Hout,Wout)(N, C, H<em>{out}, W</em>{out})(N,C,Hout&#x200B;,Wout&#x200B;) , where</p>
</li>
</ul>
<p>Hout=&#x230A;Hin+2&#xD7;padding[0]&#x2212;dilation[0]&#xD7;(kernel<em>size[0]&#x2212;1)&#x2212;1stride[0]+1&#x230B;H</em>{out} =
\left\lfloor\frac{H_{in} + 2 \times \text{padding}[0] - \text{dilation}[0]
\times (\text{kernel\_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor
Hout&#x200B;=&#x230A;stride[0]Hin&#x200B;+2&#xD7;padding[0]&#x2212;dilation[0]&#xD7;(kernel_size[0]&#x2212;1)&#x2212;1&#x200B;+1&#x230B;</p>
<p>Wout=&#x230A;Win+2&#xD7;padding[1]&#x2212;dilation[1]&#xD7;(kernel<em>size[1]&#x2212;1)&#x2212;1stride[1]+1&#x230B;W</em>{out} =
\left\lfloor\frac{W_{in} + 2 \times \text{padding}[1] - \text{dilation}[1]
\times (\text{kernel\_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor
Wout&#x200B;=&#x230A;stride[1]Win&#x200B;+2&#xD7;padding[1]&#x2212;dilation[1]&#xD7;(kernel_size[1]&#x2212;1)&#x2212;1&#x200B;+1&#x230B;</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; # power-2 pool of square window of size=3, stride=2
&gt;&gt;&gt; m = nn.LPPool2d(2, 3, stride=2)
&gt;&gt;&gt; # pool of non-square window of power 1.2
&gt;&gt;&gt; m = nn.LPPool2d(1.2, (3, 2), stride=(2, 1))
&gt;&gt;&gt; input = torch.randn(20, 16, 50, 32)
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="adaptivemaxpool1d">AdaptiveMaxPool1d</h3>
<p><em>class</em><code>torch.nn.``AdaptiveMaxPool1d</code>( <em>output_size</em> , <em>return_indices=False</em>
)<a href="_modules/torch/nn/modules/pooling.html#AdaptiveMaxPool1d">[source]</a></p>
<p>&#x9002;&#x7528;&#x5728;&#x51E0;&#x4E2A;&#x8F93;&#x5165;&#x5E73;&#x9762;&#x7EC4;&#x6210;&#x7684;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x7684;1D&#x81EA;&#x9002;&#x5E94;&#x6700;&#x5927;&#x6C60;&#x3002;</p>
<p>&#x8F93;&#x51FA;&#x5C3A;&#x5BF8;&#x662F;H&#xFF0C;&#x4EFB;&#x4F55;&#x8F93;&#x5165;&#x7684;&#x5927;&#x5C0F;&#x3002;&#x7684;&#x8F93;&#x51FA;&#x7279;&#x5F81;&#x7684;&#x6570;&#x91CF;&#x7B49;&#x4E8E;&#x8F93;&#x5165;&#x5E73;&#x9762;&#x7684;&#x6570;&#x91CF;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>output_size</strong> - &#x76EE;&#x6807;&#x8F93;&#x51FA;&#x53E3;&#x5F84;H</p>
</li>
<li><p><strong>return_indices</strong> - &#x5982;&#x679C;<code>&#x771F;</code>&#xFF0C;&#x5C06;&#x8FD4;&#x56DE;&#x6307;&#x6570;&#x4E2D;&#xFF0C;&#x4EA7;&#x51FA;&#x4E00;&#x8D77;&#x3002;&#x6709;&#x7528;&#x4F20;&#x9012;&#x7ED9;nn.MaxUnpool1d&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x5047;</code></p>
</li>
</ul>
<p>Examples</p>
<pre><code>&gt;&gt;&gt; # target output size of 5
&gt;&gt;&gt; m = nn.AdaptiveMaxPool1d(5)
&gt;&gt;&gt; input = torch.randn(1, 64, 8)
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="adaptivemaxpool2d">AdaptiveMaxPool2d</h3>
<p><em>class</em><code>torch.nn.``AdaptiveMaxPool2d</code>( <em>output_size</em> , <em>return_indices=False</em>
)<a href="_modules/torch/nn/modules/pooling.html#AdaptiveMaxPool2d">[source]</a></p>
<p>&#x9002;&#x7528;&#x5728;&#x51E0;&#x4E2A;&#x8F93;&#x5165;&#x5E73;&#x9762;&#x7EC4;&#x6210;&#x7684;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x7684;2D&#x81EA;&#x9002;&#x5E94;&#x6700;&#x5927;&#x6C60;&#x3002;</p>
<p>&#x8F93;&#x51FA;&#x662F;&#x5C3A;&#x5BF8;&#x9AD8;&#xD7;&#x5BBD;&#x7684;&#xFF0C;&#x5BF9;&#x4E8E;&#x4EFB;&#x4F55;&#x8F93;&#x5165;&#x5927;&#x5C0F;&#x3002;&#x7684;&#x8F93;&#x51FA;&#x7279;&#x5F81;&#x7684;&#x6570;&#x91CF;&#x7B49;&#x4E8E;&#x8F93;&#x5165;&#x5E73;&#x9762;&#x7684;&#x6570;&#x91CF;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>output_size</strong> - &#x7684;&#x5F62;&#x5F0F;&#x9AD8;x W&#x7684;&#x56FE;&#x50CF;&#x7684;&#x76EE;&#x6807;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x53EF;&#x4EE5;&#x662F;&#x4E00;&#x4E2A;&#x5143;&#x7EC4;&#xFF08;H&#xFF0C;W&#xFF09;&#x6216;&#x5BF9;&#x4E8E;&#x65B9;&#x5F62;&#x56FE;&#x50CF;&#x9AD8;x H.&#x5355;&#x4E2A;H H&#x548C;W&#x53EF;&#x4EE5;&#x662F;<code>INT</code>&#x6216;<code>&#x65E0;</code>&#x8FD9;&#x610F;&#x5473;&#x7740;&#x5927;&#x5C0F;&#x5C06;&#x662F;&#x76F8;&#x540C;&#x7684;&#xFF0C;&#x8F93;&#x5165;&#x7684;&#x3002;</p>
</li>
<li><p><strong>return_indices</strong> - &#x5982;&#x679C;<code>&#x771F;</code>&#xFF0C;&#x5C06;&#x8FD4;&#x56DE;&#x6307;&#x6570;&#x4E2D;&#xFF0C;&#x4EA7;&#x51FA;&#x4E00;&#x8D77;&#x3002;&#x6709;&#x7528;&#x4F20;&#x9012;&#x7ED9;nn.MaxUnpool2d&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x5047;</code></p>
</li>
</ul>
<p>Examples</p>
<pre><code>&gt;&gt;&gt; # target output size of 5x7
&gt;&gt;&gt; m = nn.AdaptiveMaxPool2d((5,7))
&gt;&gt;&gt; input = torch.randn(1, 64, 8, 9)
&gt;&gt;&gt; output = m(input)
&gt;&gt;&gt; # target output size of 7x7 (square)
&gt;&gt;&gt; m = nn.AdaptiveMaxPool2d(7)
&gt;&gt;&gt; input = torch.randn(1, 64, 10, 9)
&gt;&gt;&gt; output = m(input)
&gt;&gt;&gt; # target output size of 10x7
&gt;&gt;&gt; m = nn.AdaptiveMaxPool2d((None, 7))
&gt;&gt;&gt; input = torch.randn(1, 64, 10, 9)
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="adaptivemaxpool3d">AdaptiveMaxPool3d</h3>
<p><em>class</em><code>torch.nn.``AdaptiveMaxPool3d</code>( <em>output_size</em> , <em>return_indices=False</em>
)<a href="_modules/torch/nn/modules/pooling.html#AdaptiveMaxPool3d">[source]</a></p>
<p>&#x9002;&#x7528;&#x5728;&#x51E0;&#x4E2A;&#x8F93;&#x5165;&#x5E73;&#x9762;&#x7EC4;&#x6210;&#x7684;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x7684;3D&#x81EA;&#x9002;&#x5E94;&#x6700;&#x5927;&#x6C60;&#x3002;</p>
<p>&#x8F93;&#x51FA;&#x662F;&#x5C3A;&#x5BF8;d x&#x9AD8;x W&#x7684;&#xFF0C;&#x5BF9;&#x4E8E;&#x4EFB;&#x4F55;&#x8F93;&#x5165;&#x5927;&#x5C0F;&#x3002;&#x7684;&#x8F93;&#x51FA;&#x7279;&#x5F81;&#x7684;&#x6570;&#x91CF;&#x7B49;&#x4E8E;&#x8F93;&#x5165;&#x5E73;&#x9762;&#x7684;&#x6570;&#x91CF;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>output_size</strong> - &#x5F62;&#x5F0F;d&#xD7;&#x9AD8;&#xD7;W&#x7684;&#x56FE;&#x50CF;&#x7684;&#x76EE;&#x6807;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x53EF;&#x4EE5;&#x662F;&#x4E00;&#x4E2A;&#x5143;&#x7EC4;&#xFF08;d&#xFF0C;H&#xFF0C;W&#xFF09;&#x6216;&#x591A;&#x7EF4;&#x6570;&#x636E;&#x96C6;d X d X D. d&#x5355;&#x4E2A;d&#xFF0C; H&#x548C;W&#x53EF;&#x4EE5;&#x662F;<code>INT</code>&#x6216;<code>&#x65E0;</code>&#x8FD9;&#x610F;&#x5473;&#x7740;&#x5927;&#x5C0F;&#x5C06;&#x662F;&#x76F8;&#x540C;&#x7684;&#xFF0C;&#x8F93;&#x5165;&#x7684;&#x3002;</p>
</li>
<li><p><strong>return_indices</strong> - &#x5982;&#x679C;<code>&#x771F;</code>&#xFF0C;&#x5C06;&#x8FD4;&#x56DE;&#x6307;&#x6570;&#x4E2D;&#xFF0C;&#x4EA7;&#x51FA;&#x4E00;&#x8D77;&#x3002;&#x6709;&#x7528;&#x4F20;&#x9012;&#x7ED9;nn.MaxUnpool3d&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x5047;</code></p>
</li>
</ul>
<p>Examples</p>
<pre><code>&gt;&gt;&gt; # target output size of 5x7x9
&gt;&gt;&gt; m = nn.AdaptiveMaxPool3d((5,7,9))
&gt;&gt;&gt; input = torch.randn(1, 64, 8, 9, 10)
&gt;&gt;&gt; output = m(input)
&gt;&gt;&gt; # target output size of 7x7x7 (cube)
&gt;&gt;&gt; m = nn.AdaptiveMaxPool3d(7)
&gt;&gt;&gt; input = torch.randn(1, 64, 10, 9, 8)
&gt;&gt;&gt; output = m(input)
&gt;&gt;&gt; # target output size of 7x9x8
&gt;&gt;&gt; m = nn.AdaptiveMaxPool3d((7, None, None))
&gt;&gt;&gt; input = torch.randn(1, 64, 10, 9, 8)
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="adaptiveavgpool1d">AdaptiveAvgPool1d</h3>
<p><em>class</em><code>torch.nn.``AdaptiveAvgPool1d</code>( <em>output_size</em>
)<a href="_modules/torch/nn/modules/pooling.html#AdaptiveAvgPool1d">[source]</a></p>
<p>&#x9002;&#x7528;&#x5728;&#x51E0;&#x4E2A;&#x8F93;&#x5165;&#x5E73;&#x9762;&#x7EC4;&#x6210;&#x7684;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x7684;1D&#x81EA;&#x9002;&#x5E94;&#x5E73;&#x5747;&#x6C60;&#x3002;</p>
<p>The output size is H, for any input size. The number of output features is
equal to the number of input planes.</p>
<p>Parameters</p>
<p><strong>output_size</strong> &#x2013; the target output size H</p>
<p>Examples</p>
<pre><code>&gt;&gt;&gt; # target output size of 5
&gt;&gt;&gt; m = nn.AdaptiveAvgPool1d(5)
&gt;&gt;&gt; input = torch.randn(1, 64, 8)
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="adaptiveavgpool2d">AdaptiveAvgPool2d</h3>
<p><em>class</em><code>torch.nn.``AdaptiveAvgPool2d</code>( <em>output_size</em>
)<a href="_modules/torch/nn/modules/pooling.html#AdaptiveAvgPool2d">[source]</a></p>
<p>&#x9002;&#x7528;&#x5728;&#x51E0;&#x4E2A;&#x8F93;&#x5165;&#x5E73;&#x9762;&#x7EC4;&#x6210;&#x7684;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x7684;2D&#x81EA;&#x9002;&#x5E94;&#x5E73;&#x5747;&#x6C60;&#x3002;</p>
<p>The output is of size H x W, for any input size. The number of output features
is equal to the number of input planes.</p>
<p>Parameters</p>
<p><strong>output_size</strong> &#x2013; the target output size of the image of the form H x W. Can
be a tuple (H, W) or a single H for a square image H x H. H and W can be
either a <code>int</code>, or <code>None</code>which means the size will be the same as that of the
input.</p>
<p>Examples</p>
<pre><code>&gt;&gt;&gt; # target output size of 5x7
&gt;&gt;&gt; m = nn.AdaptiveAvgPool2d((5,7))
&gt;&gt;&gt; input = torch.randn(1, 64, 8, 9)
&gt;&gt;&gt; output = m(input)
&gt;&gt;&gt; # target output size of 7x7 (square)
&gt;&gt;&gt; m = nn.AdaptiveAvgPool2d(7)
&gt;&gt;&gt; input = torch.randn(1, 64, 10, 9)
&gt;&gt;&gt; output = m(input)
&gt;&gt;&gt; # target output size of 10x7
&gt;&gt;&gt; m = nn.AdaptiveMaxPool2d((None, 7))
&gt;&gt;&gt; input = torch.randn(1, 64, 10, 9)
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="adaptiveavgpool3d">AdaptiveAvgPool3d</h3>
<p><em>class</em><code>torch.nn.``AdaptiveAvgPool3d</code>( <em>output_size</em>
)<a href="_modules/torch/nn/modules/pooling.html#AdaptiveAvgPool3d">[source]</a></p>
<p>&#x9002;&#x7528;&#x5728;&#x51E0;&#x4E2A;&#x8F93;&#x5165;&#x5E73;&#x9762;&#x7EC4;&#x6210;&#x7684;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x7684;3D&#x81EA;&#x9002;&#x5E94;&#x5E73;&#x5747;&#x6C60;&#x3002;</p>
<p>The output is of size D x H x W, for any input size. The number of output
features is equal to the number of input planes.</p>
<p>Parameters</p>
<p><strong>output_size</strong> - &#x5F62;&#x5F0F;d&#xD7;&#x9AD8;&#xD7;W&#x7684;&#x76EE;&#x6807;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x53EF;&#x4EE5;&#x662F;&#x7528;&#x4E8E;&#x7ACB;&#x65B9;&#x4F53;d X d X D. d&#xFF0C;H&#x548C;&#x5143;&#x7EC4;&#xFF08;d&#xFF0C;H&#xFF0C;W&#xFF09;&#x6216;&#x5355;&#x6570;d W&#x53EF;&#x4EE5;&#x662F;&#x65E0;&#x8BBA;&#x662F;<code>INT</code>&#x6216;<code>&#x65E0;</code>&#x8FD9;&#x610F;&#x5473;&#x7740;&#x5927;&#x5C0F;&#x5C06;&#x662F;&#x76F8;&#x540C;&#x7684;&#xFF0C;&#x8F93;&#x5165;&#x7684;&#x3002;</p>
<p>Examples</p>
<pre><code>&gt;&gt;&gt; # target output size of 5x7x9
&gt;&gt;&gt; m = nn.AdaptiveAvgPool3d((5,7,9))
&gt;&gt;&gt; input = torch.randn(1, 64, 8, 9, 10)
&gt;&gt;&gt; output = m(input)
&gt;&gt;&gt; # target output size of 7x7x7 (cube)
&gt;&gt;&gt; m = nn.AdaptiveAvgPool3d(7)
&gt;&gt;&gt; input = torch.randn(1, 64, 10, 9, 8)
&gt;&gt;&gt; output = m(input)
&gt;&gt;&gt; # target output size of 7x9x8
&gt;&gt;&gt; m = nn.AdaptiveMaxPool3d((7, None, None))
&gt;&gt;&gt; input = torch.randn(1, 64, 10, 9, 8)
&gt;&gt;&gt; output = m(input)
</code></pre><h2 id="&#x586B;&#x5145;&#x5C42;">&#x586B;&#x5145;&#x5C42;</h2>
<h3 id="reflectionpad1d">ReflectionPad1d</h3>
<p><em>class</em><code>torch.nn.``ReflectionPad1d</code>( <em>padding</em>
)<a href="_modules/torch/nn/modules/padding.html#ReflectionPad1d">[source]</a></p>
<p>&#x710A;&#x76D8;&#x4F7F;&#x7528;&#x8F93;&#x5165;&#x8FB9;&#x754C;&#x7684;&#x53CD;&#x5C04;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x3002;</p>
<p>&#x5BF9;&#x4E8E; N &#x7EF4;&#x586B;&#x5145;&#xFF0C;&#x7528;<a href="nn.functional.html#torch.nn.functional.pad" title="torch.nn.functional.pad"> <code>torch.nn.functional.pad&#xFF08;&#xFF09;</code>
</a>&#x3002;</p>
<p>Parameters</p>
<p><strong>&#x586B;&#x5145;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in
Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>&#x5143;&#x7EC4;</em>
</a>&#xFF09;</p>
<ul>
<li>&#x586B;&#x5145;&#x7684;&#x5927;&#x5C0F;&#x3002;&#x5982;&#x679C;&#x662F; INT &#xFF0C;&#x4F7F;&#x7528;&#x5728;&#x6240;&#x6709;&#x8FB9;&#x754C;&#x76F8;&#x540C;&#x7684;&#x586B;&#x5145;&#x3002;&#x5982;&#x679C;2- &#x5143;&#x7EC4;&#xFF0C;&#x4F7F;&#x7528;&#xFF08; padding_left  \ {&#x6587;&#x672C;&#x586B;&#x5145;\ _Left}
padding_left  &#xFF0C; padding_right  \&#x6587;&#x672C;{&#x586B;&#x5145;\ _right}  padding_right  &#xFF09;</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>&#x8F93;&#x5165;&#xFF1A; &#xFF08; N  &#xFF0C; C  &#xFF0C; W  i&#x7684; n&#x7684; &#xFF09; &#xFF08;N&#xFF0C;C&#xFF0C;W_ {&#x5728;}&#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF0C; W  i&#x7684; n&#x7684; &#xFF09;</p>
</li>
<li><p>&#x8F93;&#x51FA;&#xFF1A; &#xFF08; N  &#xFF0C; C  &#xFF0C; W  O  U  T  &#xFF09; &#xFF08;N&#xFF0C;C&#xFF0C;W_ {&#x51FA;}&#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF0C; W  O  U  T  &#xFF09;  &#x5176;&#x4E2D;</p>
</li>
</ul>
<p>W  O  U  T  =  W  i&#x7684; n&#x7684; +  padding<em>left  +  padding_right  W</em> {&#x51FA;} = W_ {&#x5728;} +
\&#x6587;&#x672C;{&#x586B;&#x5145;\ _Left} + \&#x6587;&#x672C;{&#x586B;&#x5145;\ _right}  W  O  U  T  =  W  [HT G100]  i&#x7684; n&#x7684; +
padding_left  +  padding_right</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.ReflectionPad1d(2)
&gt;&gt;&gt; input = torch.arange(8, dtype=torch.float).reshape(1, 2, 4)
&gt;&gt;&gt; input
tensor([[[0., 1., 2., 3.],
         [4., 5., 6., 7.]]])
&gt;&gt;&gt; m(input)
tensor([[[2., 1., 0., 1., 2., 3., 2., 1.],
         [6., 5., 4., 5., 6., 7., 6., 5.]]])
&gt;&gt;&gt; # using different paddings for different sides
&gt;&gt;&gt; m = nn.ReflectionPad1d((3, 1))
&gt;&gt;&gt; m(input)
tensor([[[3., 2., 1., 0., 1., 2., 3., 2.],
         [7., 6., 5., 4., 5., 6., 7., 6.]]])
</code></pre><h3 id="reflectionpad2d">ReflectionPad2d</h3>
<p><em>class</em><code>torch.nn.``ReflectionPad2d</code>( <em>padding</em>
)<a href="_modules/torch/nn/modules/padding.html#ReflectionPad2d">[source]</a></p>
<p>Pads the input tensor using the reflection of the input boundary.</p>
<p>For N-dimensional padding, use
<a href="nn.functional.html#torch.nn.functional.pad" title="torch.nn.functional.pad"><code>torch.nn.functional.pad()</code></a>.</p>
<p>Parameters</p>
<p><strong>&#x586B;&#x5145;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in
Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>&#x5143;&#x7EC4;</em>
</a>&#xFF09;</p>
<ul>
<li>&#x586B;&#x5145;&#x7684;&#x5927;&#x5C0F;&#x3002;&#x5982;&#x679C;&#x662F; INT &#xFF0C;&#x4F7F;&#x7528;&#x5728;&#x6240;&#x6709;&#x8FB9;&#x754C;&#x76F8;&#x540C;&#x7684;&#x586B;&#x5145;&#x3002;&#x5982;&#x679C;4- &#x5143;&#x7EC4;&#xFF0C;&#x4F7F;&#x7528;&#xFF08; padding_left  \ {&#x6587;&#x672C;&#x586B;&#x5145;\ _Left}
padding_left  &#xFF0C; padding_right  \&#x6587;&#x672C;{&#x586B;&#x5145;\ _right}  padding_right  &#xFF0C; padding_top
\&#x6587;&#x672C;{&#x586B;&#x5145;\ _top}  padding_top  &#xFF0C; padding_bottom  \ {&#x6587;&#x672C;paddi&#x7EB3;&#x514B;\ _bottom}
padding_bottom  &#xFF09;</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>Input: (N,C,Hin,Win)(N, C, H<em>{in}, W</em>{in})(N,C,Hin&#x200B;,Win&#x200B;)</p>
</li>
<li><p>&#x8F93;&#x51FA;&#xFF1A; &#xFF08; N  &#xFF0C; C  &#xFF0C; H  O  U  T  &#xFF0C; W  O  U  T  &#xFF09; &#xFF08;N&#xFF0C;C&#xFF0C;H<em> {&#x51FA;}&#xFF0C;W</em> {&#x51FA;}&#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF0C; H  O  U  T  &#xFF0C; W  O  U  T  &#xFF09; &#x5176;&#x4E2D;</p>
</li>
</ul>
<p>H  O  U  T  =  H  i&#x7684; n&#x7684; +  padding<em>top  +  padding_bottom  H</em> {&#x51FA;} = H_ {&#x5728;} +
\&#x6587;&#x672C;{&#x586B;&#x5145;\ _top} + \&#x6587;&#x672C;{&#x586B;&#x5145;\ _bottom}  H  O  U  T  =  H  [HT G100]  i&#x7684; n&#x7684; +
padding_top  +  padding_bottom</p>
<p>Wout=Win+padding<em>left+padding_rightW</em>{out} = W_{in} + \text{padding\_left} +
\text{padding\_right}Wout&#x200B;=Win&#x200B;+padding_left+padding_right</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.ReflectionPad2d(2)
&gt;&gt;&gt; input = torch.arange(9, dtype=torch.float).reshape(1, 1, 3, 3)
&gt;&gt;&gt; input
tensor([[[[0., 1., 2.],
          [3., 4., 5.],
          [6., 7., 8.]]]])
&gt;&gt;&gt; m(input)
tensor([[[[8., 7., 6., 7., 8., 7., 6.],
          [5., 4., 3., 4., 5., 4., 3.],
          [2., 1., 0., 1., 2., 1., 0.],
          [5., 4., 3., 4., 5., 4., 3.],
          [8., 7., 6., 7., 8., 7., 6.],
          [5., 4., 3., 4., 5., 4., 3.],
          [2., 1., 0., 1., 2., 1., 0.]]]])
&gt;&gt;&gt; # using different paddings for different sides
&gt;&gt;&gt; m = nn.ReflectionPad2d((1, 1, 2, 0))
&gt;&gt;&gt; m(input)
tensor([[[[7., 6., 7., 8., 7.],
          [4., 3., 4., 5., 4.],
          [1., 0., 1., 2., 1.],
          [4., 3., 4., 5., 4.],
          [7., 6., 7., 8., 7.]]]])
</code></pre><h3 id="replicationpad1d">ReplicationPad1d</h3>
<p><em>class</em><code>torch.nn.``ReplicationPad1d</code>( <em>padding</em>
)<a href="_modules/torch/nn/modules/padding.html#ReplicationPad1d">[source]</a></p>
<p>&#x57AB;&#x4F7F;&#x7528;&#x8F93;&#x5165;&#x8FB9;&#x754C;&#x7684;&#x590D;&#x5236;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x3002;</p>
<p>For N-dimensional padding, use
<a href="nn.functional.html#torch.nn.functional.pad" title="torch.nn.functional.pad"><code>torch.nn.functional.pad()</code></a>.</p>
<p>Parameters</p>
<p><strong>padding</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>,</em><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python
v3.7\)" target="_blank">
<em>tuple</em></a>) &#x2013; the size of the padding. If is int, uses the same padding in all
boundaries. If a 2-tuple, uses (padding_left\text{padding\_left}padding_left
, padding_right\text{padding\_right}padding_right )</p>
<p>Shape:</p>
<ul>
<li><p>Input: (N,C,Win)(N, C, W_{in})(N,C,Win&#x200B;)</p>
</li>
<li><p>Output: (N,C,Wout)(N, C, W_{out})(N,C,Wout&#x200B;) where</p>
</li>
</ul>
<p>Wout=Win+padding<em>left+padding_rightW</em>{out} = W_{in} + \text{padding\_left} +
\text{padding\_right}Wout&#x200B;=Win&#x200B;+padding_left+padding_right</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.ReplicationPad1d(2)
&gt;&gt;&gt; input = torch.arange(8, dtype=torch.float).reshape(1, 2, 4)
&gt;&gt;&gt; input
tensor([[[0., 1., 2., 3.],
         [4., 5., 6., 7.]]])
&gt;&gt;&gt; m(input)
tensor([[[0., 0., 0., 1., 2., 3., 3., 3.],
         [4., 4., 4., 5., 6., 7., 7., 7.]]])
&gt;&gt;&gt; # using different paddings for different sides
&gt;&gt;&gt; m = nn.ReplicationPad1d((3, 1))
&gt;&gt;&gt; m(input)
tensor([[[0., 0., 0., 0., 1., 2., 3., 3.],
         [4., 4., 4., 4., 5., 6., 7., 7.]]])
</code></pre><h3 id="replicationpad2d">ReplicationPad2d</h3>
<p><em>class</em><code>torch.nn.``ReplicationPad2d</code>( <em>padding</em>
)<a href="_modules/torch/nn/modules/padding.html#ReplicationPad2d">[source]</a></p>
<p>Pads the input tensor using replication of the input boundary.</p>
<p>For N-dimensional padding, use
<a href="nn.functional.html#torch.nn.functional.pad" title="torch.nn.functional.pad"><code>torch.nn.functional.pad()</code></a>.</p>
<p>Parameters</p>
<p><strong>padding</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>,</em><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python
v3.7\)" target="_blank">
<em>tuple</em></a>) &#x2013; the size of the padding. If is int, uses the same padding in all
boundaries. If a 4-tuple, uses (padding_left\text{padding\_left}padding_left
, padding_right\text{padding\_right}padding_right ,
padding_top\text{padding\_top}padding_top ,
padding_bottom\text{padding\_bottom}padding_bottom )</p>
<p>Shape:</p>
<ul>
<li><p>Input: (N,C,Hin,Win)(N, C, H<em>{in}, W</em>{in})(N,C,Hin&#x200B;,Win&#x200B;)</p>
</li>
<li><p>Output: (N,C,Hout,Wout)(N, C, H<em>{out}, W</em>{out})(N,C,Hout&#x200B;,Wout&#x200B;) where</p>
</li>
</ul>
<p>Hout=Hin+padding<em>top+padding_bottomH</em>{out} = H_{in} + \text{padding\_top} +
\text{padding\_bottom}Hout&#x200B;=Hin&#x200B;+padding_top+padding_bottom</p>
<p>Wout=Win+padding<em>left+padding_rightW</em>{out} = W_{in} + \text{padding\_left} +
\text{padding\_right}Wout&#x200B;=Win&#x200B;+padding_left+padding_right</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.ReplicationPad2d(2)
&gt;&gt;&gt; input = torch.arange(9, dtype=torch.float).reshape(1, 1, 3, 3)
&gt;&gt;&gt; input
tensor([[[[0., 1., 2.],
          [3., 4., 5.],
          [6., 7., 8.]]]])
&gt;&gt;&gt; m(input)
tensor([[[[0., 0., 0., 1., 2., 2., 2.],
          [0., 0., 0., 1., 2., 2., 2.],
          [0., 0., 0., 1., 2., 2., 2.],
          [3., 3., 3., 4., 5., 5., 5.],
          [6., 6., 6., 7., 8., 8., 8.],
          [6., 6., 6., 7., 8., 8., 8.],
          [6., 6., 6., 7., 8., 8., 8.]]]])
&gt;&gt;&gt; # using different paddings for different sides
&gt;&gt;&gt; m = nn.ReplicationPad2d((1, 1, 2, 0))
&gt;&gt;&gt; m(input)
tensor([[[[0., 0., 1., 2., 2.],
          [0., 0., 1., 2., 2.],
          [0., 0., 1., 2., 2.],
          [3., 3., 4., 5., 5.],
          [6., 6., 7., 8., 8.]]]])
</code></pre><h3 id="replicationpad3d">ReplicationPad3d</h3>
<p><em>class</em><code>torch.nn.``ReplicationPad3d</code>( <em>padding</em>
)<a href="_modules/torch/nn/modules/padding.html#ReplicationPad3d">[source]</a></p>
<p>Pads the input tensor using replication of the input boundary.</p>
<p>For N-dimensional padding, use
<a href="nn.functional.html#torch.nn.functional.pad" title="torch.nn.functional.pad"><code>torch.nn.functional.pad()</code></a>.</p>
<p>Parameters</p>
<p><strong>&#x586B;&#x5145;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in
Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>&#x5143;&#x7EC4;</em>
</a>&#xFF09;</p>
<ul>
<li>&#x586B;&#x5145;&#x7684;&#x5927;&#x5C0F;&#x3002;&#x5982;&#x679C;&#x662F; INT &#xFF0C;&#x4F7F;&#x7528;&#x5728;&#x6240;&#x6709;&#x8FB9;&#x754C;&#x76F8;&#x540C;&#x7684;&#x586B;&#x5145;&#x3002;&#x5982;&#x679C;6- &#x5143;&#x7EC4;&#xFF0C;&#x4F7F;&#x7528;&#xFF08; padding_left  \ {&#x6587;&#x672C;&#x586B;&#x5145;\ _Left}
padding_left  &#xFF0C; padding_right  \&#x6587;&#x672C;{&#x586B;&#x5145;\ _right}  padding_right  &#xFF0C; padding_top
\&#x6587;&#x672C;{&#x586B;&#x5145;\ _top}  padding_top  &#xFF0C; padding_bottom  \ {&#x6587;&#x672C;paddi&#x7EB3;&#x514B;\ _bottom}
padding_bottom  &#xFF0C; padding_front  \&#x6587;&#x672C;{&#x586B;&#x5145;\ _front}  padding_front  &#xFF0C;
padding_back  \&#x6587;&#x672C;{&#x586B;&#x5145;\ _back}  padding_back  &#xFF09;</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>Input: (N,C,Din,Hin,Win)(N, C, D<em>{in}, H</em>{in}, W_{in})(N,C,Din&#x200B;,Hin&#x200B;,Win&#x200B;)</p>
</li>
<li><p>&#x8F93;&#x51FA;&#xFF1A; &#xFF08; N  &#xFF0C; C  &#xFF0C; d  O  U  T  &#xFF0C; H  O  U  T  &#xFF0C; W  O  U  T  &#xFF09; &#xFF08;N&#xFF0C;C&#xFF0C;D<em> {&#x51FA;}&#xFF0C;H</em> {&#x51FA;}&#xFF0C;W_ {&#x51FA;}&#xFF09; &#xFF08;  N  &#xFF0C; C  &#xFF0C; d  O  U  T  &#xFF0C; H  O  U  T  &#xFF0C; W  O  U  T  &#xFF09; &#x5176;&#x4E2D;</p>
</li>
</ul>
<p>d  O  U  T  =  d  i&#x7684; n&#x7684; +  padding<em>front  +  padding_back  D</em> {&#x51FA;} = D_ {&#x5728;} +
\&#x6587;&#x672C;{&#x586B;&#x5145;\ _front} + \&#x6587;&#x672C;{&#x586B;&#x5145;\ _back}  d  O  U  T  =  d  [HT G100]  i&#x7684; n&#x7684; +
padding_front  +  padding_back</p>
<p>Hout=Hin+padding<em>top+padding_bottomH</em>{out} = H_{in} + \text{padding\_top} +
\text{padding\_bottom}Hout&#x200B;=Hin&#x200B;+padding_top+padding_bottom</p>
<p>Wout=Win+padding<em>left+padding_rightW</em>{out} = W_{in} + \text{padding\_left} +
\text{padding\_right}Wout&#x200B;=Win&#x200B;+padding_left+padding_right</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.ReplicationPad3d(3)
&gt;&gt;&gt; input = torch.randn(16, 3, 8, 320, 480)
&gt;&gt;&gt; output = m(input)
&gt;&gt;&gt; # using different paddings for different sides
&gt;&gt;&gt; m = nn.ReplicationPad3d((3, 3, 6, 6, 1, 1))
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="zeropad2d">ZeroPad2d</h3>
<p><em>class</em><code>torch.nn.``ZeroPad2d</code>( <em>padding</em>
)<a href="_modules/torch/nn/modules/padding.html#ZeroPad2d">[source]</a></p>
<p>&#x96F6;&#x57AB;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x8FB9;&#x754C;&#x3002;</p>
<p>For N-dimensional padding, use
<a href="nn.functional.html#torch.nn.functional.pad" title="torch.nn.functional.pad"><code>torch.nn.functional.pad()</code></a>.</p>
<p>Parameters</p>
<p><strong>padding</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>,</em><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python
v3.7\)" target="_blank">
<em>tuple</em></a>) &#x2013; the size of the padding. If is int, uses the same padding in all
boundaries. If a 4-tuple, uses (padding_left\text{padding\_left}padding_left
, padding_right\text{padding\_right}padding_right ,
padding_top\text{padding\_top}padding_top ,
padding_bottom\text{padding\_bottom}padding_bottom )</p>
<p>Shape:</p>
<ul>
<li><p>Input: (N,C,Hin,Win)(N, C, H<em>{in}, W</em>{in})(N,C,Hin&#x200B;,Win&#x200B;)</p>
</li>
<li><p>Output: (N,C,Hout,Wout)(N, C, H<em>{out}, W</em>{out})(N,C,Hout&#x200B;,Wout&#x200B;) where</p>
</li>
</ul>
<p>Hout=Hin+padding<em>top+padding_bottomH</em>{out} = H_{in} + \text{padding\_top} +
\text{padding\_bottom}Hout&#x200B;=Hin&#x200B;+padding_top+padding_bottom</p>
<p>Wout=Win+padding<em>left+padding_rightW</em>{out} = W_{in} + \text{padding\_left} +
\text{padding\_right}Wout&#x200B;=Win&#x200B;+padding_left+padding_right</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.ZeroPad2d(2)
&gt;&gt;&gt; input = torch.randn(1, 1, 3, 3)
&gt;&gt;&gt; input
tensor([[[[-0.1678, -0.4418,  1.9466],
          [ 0.9604, -0.4219, -0.5241],
          [-0.9162, -0.5436, -0.6446]]]])
&gt;&gt;&gt; m(input)
tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000, -0.1678, -0.4418,  1.9466,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.9604, -0.4219, -0.5241,  0.0000,  0.0000],
          [ 0.0000,  0.0000, -0.9162, -0.5436, -0.6446,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]])
&gt;&gt;&gt; # using different paddings for different sides
&gt;&gt;&gt; m = nn.ZeroPad2d((1, 1, 2, 0))
&gt;&gt;&gt; m(input)
tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
          [ 0.0000, -0.1678, -0.4418,  1.9466,  0.0000],
          [ 0.0000,  0.9604, -0.4219, -0.5241,  0.0000],
          [ 0.0000, -0.9162, -0.5436, -0.6446,  0.0000]]]])
</code></pre><h3 id="constantpad1d">ConstantPad1d</h3>
<p><em>class</em><code>torch.nn.``ConstantPad1d</code>( <em>padding</em> , <em>value</em>
)<a href="_modules/torch/nn/modules/padding.html#ConstantPad1d">[source]</a></p>
<p>&#x5177;&#x6709;&#x6052;&#x5B9A;&#x503C;&#x57AB;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x7684;&#x754C;&#x9650;&#x3002;</p>
<p>For N-dimensional padding, use
<a href="nn.functional.html#torch.nn.functional.pad" title="torch.nn.functional.pad"><code>torch.nn.functional.pad()</code></a>.</p>
<p>Parameters</p>
<p><strong>&#x586B;&#x5145;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in
Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>&#x5143;&#x7EC4;</em>
</a>&#xFF09;</p>
<ul>
<li>&#x586B;&#x5145;&#x7684;&#x5927;&#x5C0F;&#x3002;&#x5982;&#x679C;&#x662F; INT &#xFF0C;&#x4F7F;&#x7528;&#x5728;&#x4E24;&#x4E2A;&#x8FB9;&#x754C;&#x76F8;&#x540C;&#x7684;&#x586B;&#x5145;&#x3002;&#x5982;&#x679C;2- &#x5143;&#x7EC4;&#xFF0C;&#x4F7F;&#x7528;&#xFF08; padding_left  \ {&#x6587;&#x672C;&#x586B;&#x5145;\ _Left}
padding_left  &#xFF0C; padding_right  \&#x6587;&#x672C;{&#x586B;&#x5145;\ _right}  padding_right  &#xFF09;</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>Input: (N,C,Win)(N, C, W_{in})(N,C,Win&#x200B;)</p>
</li>
<li><p>Output: (N,C,Wout)(N, C, W_{out})(N,C,Wout&#x200B;) where</p>
</li>
</ul>
<p>Wout=Win+padding<em>left+padding_rightW</em>{out} = W_{in} + \text{padding\_left} +
\text{padding\_right}Wout&#x200B;=Win&#x200B;+padding_left+padding_right</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.ConstantPad1d(2, 3.5)
&gt;&gt;&gt; input = torch.randn(1, 2, 4)
&gt;&gt;&gt; input
tensor([[[-1.0491, -0.7152, -0.0749,  0.8530],
         [-1.3287,  1.8966,  0.1466, -0.2771]]])
&gt;&gt;&gt; m(input)
tensor([[[ 3.5000,  3.5000, -1.0491, -0.7152, -0.0749,  0.8530,  3.5000,
           3.5000],
         [ 3.5000,  3.5000, -1.3287,  1.8966,  0.1466, -0.2771,  3.5000,
           3.5000]]])
&gt;&gt;&gt; m = nn.ConstantPad1d(2, 3.5)
&gt;&gt;&gt; input = torch.randn(1, 2, 3)
&gt;&gt;&gt; input
tensor([[[ 1.6616,  1.4523, -1.1255],
         [-3.6372,  0.1182, -1.8652]]])
&gt;&gt;&gt; m(input)
tensor([[[ 3.5000,  3.5000,  1.6616,  1.4523, -1.1255,  3.5000,  3.5000],
         [ 3.5000,  3.5000, -3.6372,  0.1182, -1.8652,  3.5000,  3.5000]]])
&gt;&gt;&gt; # using different paddings for different sides
&gt;&gt;&gt; m = nn.ConstantPad1d((3, 1), 3.5)
&gt;&gt;&gt; m(input)
tensor([[[ 3.5000,  3.5000,  3.5000,  1.6616,  1.4523, -1.1255,  3.5000],
         [ 3.5000,  3.5000,  3.5000, -3.6372,  0.1182, -1.8652,  3.5000]]])
</code></pre><h3 id="constantpad2d">ConstantPad2d</h3>
<p><em>class</em><code>torch.nn.``ConstantPad2d</code>( <em>padding</em> , <em>value</em>
)<a href="_modules/torch/nn/modules/padding.html#ConstantPad2d">[source]</a></p>
<p>Pads the input tensor boundaries with a constant value.</p>
<p>For N-dimensional padding, use
<a href="nn.functional.html#torch.nn.functional.pad" title="torch.nn.functional.pad"><code>torch.nn.functional.pad()</code></a>.</p>
<p>Parameters</p>
<p><strong>padding</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>,</em><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python
v3.7\)" target="_blank">
<em>tuple</em></a>) &#x2013; the size of the padding. If is int, uses the same padding in all
boundaries. If a 4-tuple, uses (padding_left\text{padding\_left}padding_left
, padding_right\text{padding\_right}padding_right ,
padding_top\text{padding\_top}padding_top ,
padding_bottom\text{padding\_bottom}padding_bottom )</p>
<p>Shape:</p>
<ul>
<li><p>Input: (N,C,Hin,Win)(N, C, H<em>{in}, W</em>{in})(N,C,Hin&#x200B;,Win&#x200B;)</p>
</li>
<li><p>Output: (N,C,Hout,Wout)(N, C, H<em>{out}, W</em>{out})(N,C,Hout&#x200B;,Wout&#x200B;) where</p>
</li>
</ul>
<p>Hout=Hin+padding<em>top+padding_bottomH</em>{out} = H_{in} + \text{padding\_top} +
\text{padding\_bottom}Hout&#x200B;=Hin&#x200B;+padding_top+padding_bottom</p>
<p>Wout=Win+padding<em>left+padding_rightW</em>{out} = W_{in} + \text{padding\_left} +
\text{padding\_right}Wout&#x200B;=Win&#x200B;+padding_left+padding_right</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.ConstantPad2d(2, 3.5)
&gt;&gt;&gt; input = torch.randn(1, 2, 2)
&gt;&gt;&gt; input
tensor([[[ 1.6585,  0.4320],
         [-0.8701, -0.4649]]])
&gt;&gt;&gt; m(input)
tensor([[[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],
         [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],
         [ 3.5000,  3.5000,  1.6585,  0.4320,  3.5000,  3.5000],
         [ 3.5000,  3.5000, -0.8701, -0.4649,  3.5000,  3.5000],
         [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],
         [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000]]])
&gt;&gt;&gt; # using different paddings for different sides
&gt;&gt;&gt; m = nn.ConstantPad2d((3, 0, 2, 1), 3.5)
&gt;&gt;&gt; m(input)
tensor([[[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000],
         [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000],
         [ 3.5000,  3.5000,  3.5000,  1.6585,  0.4320],
         [ 3.5000,  3.5000,  3.5000, -0.8701, -0.4649],
         [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000]]])
</code></pre><h3 id="constantpad3d">ConstantPad3d</h3>
<p><em>class</em><code>torch.nn.``ConstantPad3d</code>( <em>padding</em> , <em>value</em>
)<a href="_modules/torch/nn/modules/padding.html#ConstantPad3d">[source]</a></p>
<p>Pads the input tensor boundaries with a constant value.</p>
<p>For N-dimensional padding, use
<a href="nn.functional.html#torch.nn.functional.pad" title="torch.nn.functional.pad"><code>torch.nn.functional.pad()</code></a>.</p>
<p>Parameters</p>
<p><strong>padding</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>,</em><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python
v3.7\)" target="_blank">
<em>tuple</em></a>) &#x2013; the size of the padding. If is int, uses the same padding in all
boundaries. If a 6-tuple, uses (padding_left\text{padding\_left}padding_left
, padding_right\text{padding\_right}padding_right ,
padding_top\text{padding\_top}padding_top ,
padding_bottom\text{padding\_bottom}padding_bottom ,
padding_front\text{padding\_front}padding_front ,
padding_back\text{padding\_back}padding_back )</p>
<p>Shape:</p>
<ul>
<li><p>Input: (N,C,Din,Hin,Win)(N, C, D<em>{in}, H</em>{in}, W_{in})(N,C,Din&#x200B;,Hin&#x200B;,Win&#x200B;)</p>
</li>
<li><p>Output: (N,C,Dout,Hout,Wout)(N, C, D<em>{out}, H</em>{out}, W_{out})(N,C,Dout&#x200B;,Hout&#x200B;,Wout&#x200B;) where</p>
</li>
</ul>
<p>Dout=Din+padding<em>front+padding_backD</em>{out} = D_{in} + \text{padding\_front} +
\text{padding\_back}Dout&#x200B;=Din&#x200B;+padding_front+padding_back</p>
<p>Hout=Hin+padding<em>top+padding_bottomH</em>{out} = H_{in} + \text{padding\_top} +
\text{padding\_bottom}Hout&#x200B;=Hin&#x200B;+padding_top+padding_bottom</p>
<p>Wout=Win+padding<em>left+padding_rightW</em>{out} = W_{in} + \text{padding\_left} +
\text{padding\_right}Wout&#x200B;=Win&#x200B;+padding_left+padding_right</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.ConstantPad3d(3, 3.5)
&gt;&gt;&gt; input = torch.randn(16, 3, 10, 20, 30)
&gt;&gt;&gt; output = m(input)
&gt;&gt;&gt; # using different paddings for different sides
&gt;&gt;&gt; m = nn.ConstantPad3d((3, 3, 6, 6, 0, 1), 3.5)
&gt;&gt;&gt; output = m(input)
</code></pre><h2 id="&#x975E;&#x7EBF;&#x6027;&#x6FC0;&#x6D3B;&#xFF08;&#x52A0;&#x6743;&#x548C;&#xFF0C;&#x975E;&#x7EBF;&#x6027;&#xFF09;">&#x975E;&#x7EBF;&#x6027;&#x6FC0;&#x6D3B;&#xFF08;&#x52A0;&#x6743;&#x548C;&#xFF0C;&#x975E;&#x7EBF;&#x6027;&#xFF09;</h2>
<h3 id="elu">ELU</h3>
<p><em>class</em><code>torch.nn.``ELU</code>( <em>alpha=1.0</em> , <em>inplace=False</em>
)<a href="_modules/torch/nn/modules/activation.html#ELU">[source]</a></p>
<p>&#x9002;&#x7528;&#x9010;&#x5143;&#x7D20;&#x7684;&#x529F;&#x80FD;&#xFF1A;</p>
<p>ELU(x)=max&#x2061;(0,x)+min&#x2061;(0,&#x3B1;&#x2217;(exp&#x2061;(x)&#x2212;1))\text{ELU}(x) = \max(0,x) + \min(0,
\alpha * (\exp(x) - 1)) ELU(x)=max(0,x)+min(0,&#x3B1;&#x2217;(exp(x)&#x2212;1))</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x963F;&#x5C14;&#x6CD5;</strong> - &#x7684; &#x3B1; \&#x963F;&#x5C14;&#x6CD5; &#x3B1; &#x4E3A;ELU&#x5236;&#x5242;&#x503C;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;1.0</p>
</li>
<li><p><strong>&#x5C31;&#x5730;</strong> - &#x53EF;&#x4EE5;&#x4EFB;&#x9009;&#x5730;&#x6267;&#x884C;&#x64CD;&#x4F5C;&#x5C31;&#x5730;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x5047;</code></p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>&#x8F93;&#x5165;&#xFF1A; &#xFF08; N  &#xFF0C; <em>  &#xFF09; &#xFF08;N&#xFF0C;</em>&#xFF09; &#xFF08; N  &#xFF0C; <em>  &#xFF09; &#x5176;&#x4E2D; </em> &#x7684;&#x88C5;&#x7F6E;&#xFF0C;&#x4EFB;&#x610F;&#x6570;&#x91CF;&#x7684;&#x9644;&#x52A0;&#x7EF4;&#x5EA6;&#x7684;</p>
</li>
<li><p>&#x8F93;&#x51FA;&#xFF1A; &#xFF08; N  &#xFF0C; <em>  &#xFF09; &#xFF08;N&#xFF0C;</em>&#xFF09; &#xFF08; N  &#xFF0C; *  &#xFF09; &#xFF0C;&#x76F8;&#x540C;&#x5F62;&#x72B6;&#x7684;&#x8F93;&#x5165;</p>
</li>
</ul>
<p><img src="_images/ELU.png" alt="_images/ELU.png"></p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.ELU()
&gt;&gt;&gt; input = torch.randn(2)
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="hardshrink">Hardshrink</h3>
<p><em>class</em><code>torch.nn.``Hardshrink</code>( <em>lambd=0.5</em>
)<a href="_modules/torch/nn/modules/activation.html#Hardshrink">[source]</a></p>
<p>&#x9002;&#x7528;&#x786C;&#x6536;&#x7F29;&#x529F;&#x80FD;&#x5143;&#x7D20;&#x65B9;&#x9762;&#xFF1A;</p>
<p>HardShrink(x)={x, if x&gt;&#x3BB;x, if x&lt;&#x2212;&#x3BB;0, otherwise \text{HardShrink}(x) =
\begin{cases} x, &amp; \text{ if } x &gt; \lambda \\ x, &amp; \text{ if } x &lt; -\lambda
\\ 0, &amp; \text{ otherwise } \end{cases} HardShrink(x)=&#x23A9;&#x23AA;&#x23A8;&#x23AA;&#x23A7;&#x200B;x,x,0,&#x200B; if x&gt;&#x3BB; if
x&lt;&#x2212;&#x3BB; otherwise &#x200B;</p>
<p>Parameters</p>
<p><strong>lambd</strong> - &#x7684; &#x3BB; \&#x62C9;&#x59C6;&#x8FBE; &#x3BB; &#x4E3A;Hardshrink&#x5236;&#x5242;&#x503C;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;0.5</p>
<p>Shape:</p>
<ul>
<li><p>Input: (N,&#x2217;)(N, <em>)(N,&#x2217;) where </em> means, any number of additional dimensions</p>
</li>
<li><p>Output: (N,&#x2217;)(N, *)(N,&#x2217;) , same shape as the input</p>
</li>
</ul>
<p><img src="_images/Hardshrink.png" alt="_images/Hardshrink.png"></p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.Hardshrink()
&gt;&gt;&gt; input = torch.randn(2)
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="hardtanh">Hardtanh</h3>
<p><em>class</em><code>torch.nn.``Hardtanh</code>( <em>min_val=-1.0</em> , <em>max_val=1.0</em> , <em>inplace=False</em>
, <em>min_value=None</em> , <em>max_value=None</em>
)<a href="_modules/torch/nn/modules/activation.html#Hardtanh">[source]</a></p>
<p>&#x5E94;&#x7528;HardTanh&#x529F;&#x80FD;&#x9010;&#x5143;&#x7D20;</p>
<p>HardTanh&#x5B9A;&#x4E49;&#x4E3A;&#xFF1A;</p>
<p>HardTanh(x)={1 if x&gt;1&#x2212;1 if x&lt;&#x2212;1x otherwise \text{HardTanh}(x) = \begin{cases}
1 &amp; \text{ if } x &gt; 1 \\ -1 &amp; \text{ if } x &lt; -1 \\ x &amp; \text{ otherwise }
\\ \end{cases} HardTanh(x)=&#x23A9;&#x23AA;&#x23A8;&#x23AA;&#x23A7;&#x200B;1&#x2212;1x&#x200B; if x&gt;1 if x&lt;&#x2212;1 otherwise &#x200B;</p>
<p>&#x7EBF;&#x6027;&#x533A;&#x57DF; [ &#x7684;&#x8303;&#x56F4;&#x5185; -  1  &#xFF0C; 1  [1,1]  [ -  1  &#xFF0C; 1  ]  &#x53EF;&#x4EE5;&#x7528;&#x88AB;&#x8C03;&#x6574;<code>MIN_VAL</code>&#x548C;<code>MAX_VAL</code>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>MIN_VAL</strong> - &#x7EBF;&#x6027;&#x533A;&#x57DF;&#x8303;&#x56F4;&#x7684;&#x6700;&#x5C0F;&#x503C;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;-1</p>
</li>
<li><p><strong>MAX_VAL</strong> - &#x7EBF;&#x6027;&#x533A;&#x57DF;&#x8303;&#x56F4;&#x7684;&#x6700;&#x5927;&#x503C;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;1</p>
</li>
<li><p><strong>inplace</strong> &#x2013; can optionally do the operation in-place. Default: <code>False</code></p>
</li>
</ul>
<p>&#x5173;&#x952E;&#x5B57;&#x53C2;&#x6570;<code>MIN_VALUE</code>&#x548C;<code>MAX_VALUE</code>&#x5DF2;&#x7ECF;&#x88AB;&#x5F03;&#x7528;&#xFF0C;&#x53D6;&#x800C;&#x4EE3;&#x4E4B;&#x7684;<code>MIN_VAL</code>&#x548C;<code>MAX_VAL</code>&#x3002;</p>
<p>Shape:</p>
<ul>
<li><p>Input: (N,&#x2217;)(N, <em>)(N,&#x2217;) where </em> means, any number of additional dimensions</p>
</li>
<li><p>Output: (N,&#x2217;)(N, *)(N,&#x2217;) , same shape as the input</p>
</li>
</ul>
<p><img src="_images/Hardtanh.png" alt="_images/Hardtanh.png"></p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.Hardtanh(-2, 2)
&gt;&gt;&gt; input = torch.randn(2)
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="leakyrelu">LeakyReLU</h3>
<p><em>class</em><code>torch.nn.``LeakyReLU</code>( <em>negative_slope=0.01</em> , <em>inplace=False</em>
)<a href="_modules/torch/nn/modules/activation.html#LeakyReLU">[source]</a></p>
<p>Applies the element-wise function:</p>
<p>LeakyReLU(x)=max&#x2061;(0,x)+negative_slope&#x2217;min&#x2061;(0,x)\text{LeakyReLU}(x) = \max(0,
x) + \text{negative\_slope} * \min(0, x)
LeakyReLU(x)=max(0,x)+negative_slope&#x2217;min(0,x)</p>
<p>&#x8981;&#x4E48;</p>
<p>LeakyRELU(x)={x, if x&#x2265;0negative_slope&#xD7;x, otherwise \text{LeakyRELU}(x) =
\begin{cases} x, &amp; \text{ if } x \geq 0 \\ \text{negative\_slope} \times x,
&amp; \text{ otherwise } \end{cases} LeakyRELU(x)={x,negative_slope&#xD7;x,&#x200B; if x&#x2265;0
otherwise &#x200B;</p>
<p>Parameters</p>
<ul>
<li><p><strong>negative_slope</strong> - &#x63A7;&#x5236;&#x8D1F;&#x659C;&#x7387;&#x7684;&#x89D2;&#x5EA6;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;1E-2</p>
</li>
<li><p><strong>inplace</strong> &#x2013; can optionally do the operation in-place. Default: <code>False</code></p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>Input: (N,&#x2217;)(N, <em>)(N,&#x2217;) where </em> means, any number of additional dimensions</p>
</li>
<li><p>Output: (N,&#x2217;)(N, *)(N,&#x2217;) , same shape as the input</p>
</li>
</ul>
<p><img src="_images/LeakyReLU.png" alt="_images/LeakyReLU.png"></p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.LeakyReLU(0.1)
&gt;&gt;&gt; input = torch.randn(2)
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="logsigmoid">LogSigmoid</h3>
<p><em>class</em><code>torch.nn.``LogSigmoid</code><a href="_modules/torch/nn/modules/activation.html#LogSigmoid">[source]</a></p>
<p>Applies the element-wise function:</p>
<p>LogSigmoid(x)=log&#x2061;(11+exp&#x2061;(&#x2212;x))\text{LogSigmoid}(x) = \log\left(\frac{ 1 }{ 1</p>
<ul>
<li>\exp(-x)}\right) LogSigmoid(x)=log(1+exp(&#x2212;x)1&#x200B;)</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>Input: (N,&#x2217;)(N, <em>)(N,&#x2217;) where </em> means, any number of additional dimensions</p>
</li>
<li><p>Output: (N,&#x2217;)(N, *)(N,&#x2217;) , same shape as the input</p>
</li>
</ul>
<p><img src="_images/LogSigmoid.png" alt="_images/LogSigmoid.png"></p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.LogSigmoid()
&gt;&gt;&gt; input = torch.randn(2)
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="multiheadattention">MultiheadAttention</h3>
<p><em>class</em><code>torch.nn.``MultiheadAttention</code>( <em>embed_dim</em> , <em>num_heads</em> ,
<em>dropout=0.0</em> , <em>bias=True</em> , <em>add_bias_kv=False</em> , <em>add_zero_attn=False</em> ,
<em>kdim=None</em> , <em>vdim=None</em>
)<a href="_modules/torch/nn/modules/activation.html#MultiheadAttention">[source]</a></p>
<p>&#x5141;&#x8BB8;&#x6A21;&#x578B;&#x5171;&#x540C;&#x51FA;&#x5E2D;&#xFF0C;&#x4ECE;&#x4E0D;&#x540C;&#x7684;&#x8868;&#x793A;&#x5B50;&#x7A7A;&#x95F4;&#x7684;&#x4FE1;&#x606F;&#x3002;&#x89C1;&#x53C2;&#x8003;&#x6587;&#x732E;&#xFF1A;&#x6CE8;&#x610F;&#x662F;&#x6240;&#x6709;&#x4F60;&#x9700;&#x8981;</p>
<p>MultiHead(Q,K,V)=Concat(head1,&#x2026;,headh)WOwhereheadi=Attention(QWiQ,KWiK,VWiV)\text{MultiHead}(Q,
K, V) = \text{Concat}(head_1,\dots,head_h)W^O \text{where} head_i =
\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
MultiHead(Q,K,V)=Concat(head1&#x200B;,&#x2026;,headh&#x200B;)WOwhereheadi&#x200B;=Attention(QWiQ&#x200B;,KWiK&#x200B;,VWiV&#x200B;)</p>
<p>Parameters</p>
<ul>
<li><p><strong>embed_dim</strong> - &#x6A21;&#x578B;&#x7684;&#x603B;&#x5C3A;&#x5BF8;&#x3002;</p>
</li>
<li><p><strong>num_heads</strong> - &#x5E73;&#x884C;&#x6CE8;&#x610F;&#x5934;&#x3002;</p>
</li>
<li><p><strong>&#x6EE4;&#x9664;</strong> - &#x5173;&#x4E8E;attn_output_weights&#x4E00;&#x4E2A;&#x6F0F;&#x5931;&#x5C42;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;0.0&#x3002;</p>
</li>
<li><p><strong>&#x504F;&#x538B;</strong> - &#x52A0;&#x504F;&#x538B;&#x4F5C;&#x4E3A;&#x6A21;&#x5757;&#x53C2;&#x6570;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;true&#x3002;</p>
</li>
<li><p><strong>add_bias_kv</strong> - &#x5728;&#x660F;&#x6697;= 0&#x6DFB;&#x52A0;&#x504F;&#x7F6E;&#x7684;&#x952E;&#x548C;&#x503C;&#x7684;&#x5E8F;&#x5217;&#x3002;</p>
</li>
<li><p><strong>add_zero_attn</strong> - &#x5728;&#x660F;&#x6697;= 1&#x6DFB;&#x52A0;&#x65B0;&#x7684;&#x6279;&#x6B21;&#x96F6;&#x5230;&#x7684;&#x952E;&#x548C;&#x503C;&#x7684;&#x5E8F;&#x5217;&#x3002;</p>
</li>
<li><p><strong>kdim</strong> - &#x7684;&#x5173;&#x952E;&#x7279;&#x5F81;&#x603B;&#x6570;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;&#x65E0;&#x3002;</p>
</li>
<li><p><strong>VDIM</strong> - &#x7684;&#x5173;&#x952E;&#x7279;&#x5F81;&#x603B;&#x6570;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;&#x65E0;&#x3002;</p>
</li>
<li><p>[HTG0&#x6CE8;&#x610F; - &#x5982;&#x679C;kdim&#x548C;VDIM&#x90FD;&#x6CA1;&#x6709;&#xFF0C;&#x5B83;&#x4EEC;&#x5C06;&#x88AB;&#x8BBE;&#x7F6E;&#x4E3A;embed_dim&#x8FD9;&#x6837;</p>
</li>
<li><p><strong>&#x952E;&#xFF0C;&#x503C;&#x5177;&#x6709;&#x76F8;&#x540C;&#x6570;&#x76EE;&#x7684;&#x7279;&#x5F81;&#x3002;</strong> &#xFF08; <em>&#x67E5;&#x8BE2;</em> <em>&#xFF0C;</em> &#xFF09; - </p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)
&gt;&gt;&gt; attn_output, attn_output_weights = multihead_attn(query, key, value)
</code></pre><p><code>forward</code>( <em>query</em> , <em>key</em> , <em>value</em> , <em>key_padding_mask=None</em> ,
<em>need_weights=True</em> , <em>attn_mask=None</em>
)<a href="_modules/torch/nn/modules/activation.html#MultiheadAttention.forward">[source]</a></p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x952E;&#xFF0C;&#x503C;</strong> &#xFF08; <em>&#x67E5;&#x8BE2;</em> <em>&#xFF0C;[5 HTG&#xFF09; - &#x6620;&#x5C04;&#x7684;&#x67E5;&#x8BE2;&#x548C;&#x4E00;&#x7EC4;&#x952E; - &#x503C;&#x5BF9;&#x5230;&#x8F93;&#x51FA;&#x3002;&#x8BF7;&#x53C2;&#x9605;&#x201C;&#x6CE8;&#x610F;&#x662F;&#x6240;&#x6709;&#x4F60;&#x9700;&#x8981;&#x201D;&#x66F4;&#x591A;&#x7684;&#x7EC6;&#x8282;&#x3002;</em></p>
</li>
<li><p><strong>key_padding_mask</strong> - &#x5982;&#x679C;&#x63D0;&#x4F9B;&#x7684;&#x8BDD;&#xFF0C;&#x5728;&#x952E;&#x914D;&#x7F6E;&#x7684;&#x586B;&#x5145;&#x5143;&#x4EF6;&#x5C06;&#x88AB;&#x53D7;&#x77A9;&#x76EE;&#x5FFD;&#x7565;&#x3002;&#x8FD9;&#x662F;&#x4E00;&#x4E2A;&#x4E8C;&#x8FDB;&#x5236;&#x63A9;&#x7801;&#x3002;&#x5F53;&#x503C;&#x4E3A;True&#xFF0C;&#x5173;&#x6CE8;&#x5C42;&#x4E0A;&#x7684;&#x76F8;&#x5E94;&#x503C;&#x5C06;&#x5145;&#x6EE1;-INF&#x3002;</p>
</li>
<li><p><strong>need_weights</strong> - &#x8F93;&#x51FA;attn_output_weights&#x3002;</p>
</li>
<li><p><strong>attn_mask</strong> - &#x63A9;&#x6A21;&#xFF0C;&#x9632;&#x6B62;&#x6CE8;&#x610F;&#x67D0;&#x4E9B;&#x4F4D;&#x7F6E;&#x3002;&#x8FD9;&#x662F;&#x4E00;&#x79CD;&#x6DFB;&#x52A0;&#x5242;&#x63A9;&#x6A21;&#xFF08;&#x5373;&#xFF0C;&#x503C;&#x5C06;&#x88AB;&#x6DFB;&#x52A0;&#x5230;&#x5173;&#x6CE8;&#x5C42;&#xFF09;&#x3002;</p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>&#x8F93;&#x5165;&#xFF1A;</p>
</li>
<li><p>&#x67E5;&#x8BE2;&#xFF1A; &#xFF08; L  &#xFF0C; N  &#xFF0C; E  &#xFF09; &#xFF08;L&#xFF0C;N&#xFF0C;E&#xFF09; &#xFF08; L  &#xFF0C; N  &#xFF0C; E  &#xFF09; &#x5176;&#x4E2D;&#xFF0C;L&#x662F;&#x6240;&#x8FF0;&#x9776;&#x5E8F;&#x5217;&#x7684;&#x957F;&#x5EA6;&#xFF0C;N&#x662F;&#x6279;&#x91CF;&#x5927;&#x5C0F;&#xFF0C;E&#x662F;&#x5D4C;&#x5165;&#x7EF4;&#x6570;&#x3002;</p>
</li>
<li><p>&#x952E;&#xFF1A; &#xFF08; S  &#xFF0C; N  &#xFF0C; E  &#xFF09; &#xFF08;S&#xFF0C;N&#xFF0C;E&#xFF09; &#xFF08; S  &#xFF0C; N  &#xFF0C; E  &#xFF09; &#xFF0C;&#x5176;&#x4E2D;S&#x662F;&#x6E90;&#x5E8F;&#x5217;&#x957F;&#x5EA6;&#xFF0C;N&#x662F;&#x6279;&#x91CF;&#x5927;&#x5C0F;&#xFF0C;E&#x662F;&#x5D4C;&#x5165;&#x7EF4;&#x6570;&#x3002;</p>
</li>
<li><p>&#x503C;&#xFF1A; &#xFF08; S  &#xFF0C; N  &#xFF0C; E  &#xFF09; &#xFF08;S&#xFF0C;N&#xFF0C;E&#xFF09; &#xFF08; S  &#xFF0C; N  &#xFF0C; E  &#xFF09; &#x5176;&#x4E2D;&#xFF0C;S&#x662F;&#x6E90;&#x5E8F;&#x5217;&#x957F;&#x5EA6;&#xFF0C;N&#x662F;&#x6279;&#x91CF;&#x5927;&#x5C0F;&#xFF0C;E&#x662F;&#x5D4C;&#x5165;&#x7EF4;&#x6570;&#x3002;</p>
</li>
<li><p>key_padding_mask&#xFF1A; &#xFF08; N  &#xFF0C; S  &#xFF09; &#xFF08;N&#xFF0C;S&#xFF09; &#xFF08; N  &#xFF0C; S  &#xFF09; &#xFF0C;ByteTensor&#xFF0C;&#x5176;&#x4E2D;N&#x662F;&#x6279;&#x91CF;&#x5927;&#x5C0F;&#xFF0C;S&#x662F;&#x6E90;&#x5E8F;&#x5217;&#x957F;&#x5EA6;&#x3002;</p>
</li>
<li><p>attn_mask&#xFF1A; &#xFF08; L  &#xFF0C; S  &#xFF09; &#xFF08;L&#xFF0C;S&#xFF09; &#xFF08; L  &#xFF0C; S  &#xFF09; &#x5176;&#x4E2D;&#xFF0C;L&#x662F;&#x6240;&#x8FF0;&#x9776;&#x5E8F;&#x5217;&#x7684;&#x957F;&#x5EA6;&#xFF0C;S&#x662F;&#x6E90;&#x5E8F;&#x5217;&#x957F;&#x5EA6;&#x3002;</p>
</li>
<li><p>&#x8F93;&#x51FA;&#xFF1A;</p>
</li>
<li><p>attn_output&#xFF1A; &#xFF08; L  &#xFF0C; N  &#xFF0C; E  &#xFF09; &#xFF08;L&#xFF0C;N&#xFF0C;E&#xFF09; &#xFF08; L  &#xFF0C; N  &#xFF0C; E  &#xFF09; &#x5176;&#x4E2D;&#xFF0C;L&#x662F;&#x6240;&#x8FF0;&#x9776;&#x5E8F;&#x5217;&#x7684;&#x957F;&#x5EA6;&#xFF0C;N&#x662F;&#x6279;&#x91CF;&#x5927;&#x5C0F;&#xFF0C;E&#x662F;&#x5D4C;&#x5165;&#x7EF4;&#x6570;&#x3002;</p>
</li>
<li><p>attn_output_weights&#xFF1A; &#xFF08; N  &#xFF0C; L  &#xFF0C; S  &#xFF09; &#xFF08;N&#xFF0C;L&#xFF0C;S&#xFF09; &#xFF08; N  &#xFF0C; L  &#xFF0C; S  &#xFF09; &#x5176;&#x4E2D;&#xFF0C;N&#x662F;&#x6279;&#x91CF;&#x5927;&#x5C0F;&#xFF0C;L&#x662F;&#x6240;&#x8FF0;&#x9776;&#x5E8F;&#x5217;&#x7684;&#x957F;&#x5EA6;&#xFF0C;S&#x662F;&#x6E90;&#x5E8F;&#x5217;&#x957F;&#x5EA6;&#x3002;</p>
</li>
</ul>
<h3 id="prelu">PReLU</h3>
<p><em>class</em><code>torch.nn.``PReLU</code>( <em>num_parameters=1</em> , <em>init=0.25</em>
)<a href="_modules/torch/nn/modules/activation.html#PReLU">[source]</a></p>
<p>Applies the element-wise function:</p>
<p>PReLU(x)=max&#x2061;(0,x)+a&#x2217;min&#x2061;(0,x)\text{PReLU}(x) = \max(0,x) + a * \min(0,x)
PReLU(x)=max(0,x)+a&#x2217;min(0,x)</p>
<p>or</p>
<p>PReLU(x)={x, if x&#x2265;0ax, otherwise \text{PReLU}(x) = \begin{cases} x, &amp; \text{
if } x \geq 0 \\ ax, &amp; \text{ otherwise } \end{cases} PReLU(x)={x,ax,&#x200B; if x&#x2265;0
otherwise &#x200B;</p>
<p>&#x6B64;&#x5904; &#x4E00; &#x4E00; &#x4E00; &#x662F;&#x4E00;&#x4E2A;&#x53EF;&#x4EE5;&#x5B66;&#x4E60;&#x7684;&#x53C2;&#x6570;&#x3002;&#x5F53;&#x4E0D;&#x5E26;&#x53C2;&#x6570;&#x8C03;&#x7528;&#xFF0C; nn.PReLU&#xFF08;&#xFF09;&#x4F7F;&#x7528;&#x5355;&#x4E2A;&#x53C2;&#x6570; &#x4E00; A  &#x4E00; &#x6240;&#x6709;&#x8F93;&#x5165;&#x901A;&#x9053;&#x3002;&#x5982;&#x679C;&#x8C03;&#x7528;
nn.PReLU&#xFF08;nChannels&#xFF09;&#xFF0C;&#x5355;&#x72EC;&#x7684; &#x4E00; &#x4E00; &#x4E00; &#x7528;&#x4E8E;&#x6BCF;&#x4E2A;&#x8F93;&#x5165;&#x901A;&#x9053;&#x3002;</p>
<p>Note</p>
<p>&#x6743;&#x8870;&#x51CF;&#x4E0D;&#x5E94;&#x8BE5;&#x88AB;&#x7528;&#x6765;&#x5F53;&#x5B66;&#x4E60; &#x4E00; &#x4E00; A  &#x83B7;&#x5F97;&#x826F;&#x597D;&#x7684;&#x6027;&#x80FD;&#x3002;</p>
<p>Note</p>
<p>&#x660F;&#x6697;&#x7684;&#x901A;&#x9053;&#x8F93;&#x5165;&#x7684;&#x7B2C;&#x4E8C;&#x6697;&#x6DE1;&#x3002;&#x5F53;&#x8F93;&#x5165;&#x5177;&#x6709;&#x53D8;&#x6697;&amp; LT ; 2&#xFF0C;&#x90A3;&#x4E48;&#x5C31;&#x6CA1;&#x6709;&#x4FE1;&#x9053;&#x6697;&#x6DE1;&#x548C;&#x901A;&#x9053;= 1&#x7684;&#x6570;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>num_parameters</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x7684; &#x6570;&#x76EE;&#x7684; &#x4E00; &#x4E00; &#x5B66;&#x4E60;&#x3002;&#x867D;&#x7136;&#x91C7;&#x7528;int&#x4F5C;&#x4E3A;&#x8F93;&#x5165;&#xFF0C;&#x4EC5;&#x5B58;&#x5728;&#x4E24;&#x4E2A;&#x503C;&#x662F;&#x5408;&#x6CD5;&#x7684;&#xFF1A;1&#xFF0C;&#x6216;&#x901A;&#x9053;&#x4E2D;&#x7684;&#x8F93;&#x5165;&#x7684;&#x6570;&#x76EE;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;1</p>
</li>
<li><p><strong>INIT</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a>&#xFF09; - &#x7684; &#x4E00;[&#x521D;&#x59CB;&#x503C;HTG13]  &#x4E00; &#x4E00; [ HTG29&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;0.25</p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>Input: (N,&#x2217;)(N, <em>)(N,&#x2217;) where </em> means, any number of additional dimensions</p>
</li>
<li><p>Output: (N,&#x2217;)(N, *)(N,&#x2217;) , same shape as the input</p>
</li>
</ul>
<p>Variables</p>
<p><strong>&#x301C;PReLU.weight</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; -
&#x5F62;&#x72B6;&#x7684;&#x53EF;&#x5B66;&#x4E60;&#x6743;&#x91CD;&#xFF08;<code>num_parameters</code>&#xFF09;&#x3002;</p>
<p><img src="_images/PReLU.png" alt="_images/PReLU.png"></p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.PReLU()
&gt;&gt;&gt; input = torch.randn(2)
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="relu">RELU</h3>
<p><em>class</em><code>torch.nn.``ReLU</code>( <em>inplace=False</em>
)<a href="_modules/torch/nn/modules/activation.html#ReLU">[source]</a></p>
<p>&#x65BD;&#x52A0;&#x6574;&#x6D41;&#x7684;&#x7EBF;&#x6027;&#x5355;&#x5143;&#x51FD;&#x6570;&#x9010;&#x5143;&#x7D20;&#xFF1A;</p>
<p>RELU  &#xFF08; &#xD7; &#xFF09; =  MAX  &#x2061; &#xFF08; 0  &#xFF0C; &#xD7; &#xFF09; \&#x6587;&#x672C;{RELU}&#xFF08;X&#xFF09;= \ MAX&#xFF08;0&#xFF0C;x&#xFF09;&#x7684; RELU  &#xFF08; &#xD7; &#xFF09; =  MAX
&#xFF08; 0  &#xFF0C; X  &#xFF09;</p>
<p>Parameters</p>
<p><strong>inplace</strong> &#x2013; can optionally do the operation in-place. Default: <code>False</code></p>
<p>Shape:</p>
<ul>
<li><p>Input: (N,&#x2217;)(N, <em>)(N,&#x2217;) where </em> means, any number of additional dimensions</p>
</li>
<li><p>Output: (N,&#x2217;)(N, *)(N,&#x2217;) , same shape as the input</p>
</li>
</ul>
<p><img src="_images/ReLU.png" alt="_images/ReLU.png"></p>
<p>Examples:</p>
<pre><code>  &gt;&gt;&gt; m = nn.ReLU()
  &gt;&gt;&gt; input = torch.randn(2)
  &gt;&gt;&gt; output = m(input)


An implementation of CReLU - https://arxiv.org/abs/1603.05201

  &gt;&gt;&gt; m = nn.ReLU()
  &gt;&gt;&gt; input = torch.randn(2).unsqueeze(0)
  &gt;&gt;&gt; output = torch.cat((m(input),m(-input)))
</code></pre><h3 id="relu6">ReLU6</h3>
<p><em>class</em><code>torch.nn.``ReLU6</code>( <em>inplace=False</em>
)<a href="_modules/torch/nn/modules/activation.html#ReLU6">[source]</a></p>
<p>Applies the element-wise function:</p>
<p>ReLU6(x)=min&#x2061;(max&#x2061;(0,x),6)\text{ReLU6}(x) = \min(\max(0,x), 6)
ReLU6(x)=min(max(0,x),6)</p>
<p>Parameters</p>
<p><strong>inplace</strong> &#x2013; can optionally do the operation in-place. Default: <code>False</code></p>
<p>Shape:</p>
<ul>
<li><p>Input: (N,&#x2217;)(N, <em>)(N,&#x2217;) where </em> means, any number of additional dimensions</p>
</li>
<li><p>Output: (N,&#x2217;)(N, *)(N,&#x2217;) , same shape as the input</p>
</li>
</ul>
<p><img src="_images/ReLU6.png" alt="_images/ReLU6.png"></p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.ReLU6()
&gt;&gt;&gt; input = torch.randn(2)
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="rrelu">RReLU</h3>
<p><em>class</em><code>torch.nn.``RReLU</code>( <em>lower=0.125</em> , <em>upper=0.3333333333333333</em> ,
<em>inplace=False</em> )<a href="_modules/torch/nn/modules/activation.html#RReLU">[source]</a></p>
<p>&#x5E94;&#x7528;&#x968F;&#x673A;&#x6F0F;&#x6CC4;&#x6574;&#x6D41;&#x886C;&#x57AB;&#x5355;&#x5143;&#x7684;&#x529F;&#x80FD;&#xFF0C;&#x9010;&#x5143;&#x7D20;&#xFF0C;&#x5982;&#x5728;&#x8BBA;&#x6587;&#x4E2D;&#x63CF;&#x8FF0;&#xFF1A;</p>
<p><a href="https://arxiv.org/abs/1505.00853" target="_blank">&#x6574;&#x6D41;&#x7684;&#x6FC0;&#x6D3B;&#x7684;&#x5B9E;&#x8BC1;&#x8BC4;&#x4EF7;&#x5377;&#x79EF;&#x7F51;&#x7EDC;</a>&#x3002;</p>
<p>&#x8BE5;&#x51FD;&#x6570;&#x88AB;&#x5B9A;&#x4E49;&#x4E3A;&#xFF1A;</p>
<p>RReLU(x)={xif x&#x2265;0ax otherwise \text{RReLU}(x) = \begin{cases} x &amp; \text{if } x
\geq 0 \\ ax &amp; \text{ otherwise } \end{cases} RReLU(x)={xax&#x200B;if x&#x2265;0 otherwise
&#x200B;</p>
<p>&#x5176;&#x4E2D; &#x4E00; &#x4E00; &#x4E00; &#x88AB;&#x968F;&#x673A;&#x5730;&#x4ECE;&#x53D6;&#x6837;&#x7684;&#x5747;&#x5300;&#x5206;&#x5E03; U  &#xFF08; &#x4E0B; &#xFF0C; &#x4E0A; &#xFF09; \ mathcal &#x3010;U}&#xFF08;\&#x6587;&#x672C;{&#x4F4E;}&#xFF0C;\&#x6587;&#x672C;{&#x4E0A;&#x90E8;}&#xFF09; U  &#xFF08; &#x4E0B; &#xFF0C; &#x4E0A; &#xFF09;
&#x3002;</p>
<blockquote>
<p>&#x8BF7;&#x53C2;&#x9605;&#xFF1A;<a href="https://arxiv.org/pdf/1505.00853.pdf" target="_blank"> https://arxiv.org/pdf/1505.00853.pdf
</a></p>
</blockquote>
<p>Parameters</p>
<ul>
<li><p><strong>&#x4E0B;</strong> - &#x4E0B;&#x754C;&#x7684;&#x5747;&#x5300;&#x5206;&#x5E03;&#x7684;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A; 1  8  \&#x538B;&#x88C2;{1 } {8}  8  1 </p>
</li>
<li><p><strong>&#x4E0A;</strong> - &#x4E0A;&#x9650;&#x7684;&#x5747;&#x5300;&#x5206;&#x5E03;&#x7684;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A; 1  3  \&#x538B;&#x88C2;{1 } {3}  3  1 </p>
</li>
<li><p><strong>inplace</strong> &#x2013; can optionally do the operation in-place. Default: <code>False</code></p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>Input: (N,&#x2217;)(N, <em>)(N,&#x2217;) where </em> means, any number of additional dimensions</p>
</li>
<li><p>Output: (N,&#x2217;)(N, *)(N,&#x2217;) , same shape as the input</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.RReLU(0.1, 0.3)
&gt;&gt;&gt; input = torch.randn(2)
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="&#x4E5D;&#x8272;&#x9E7F;">&#x4E5D;&#x8272;&#x9E7F;</h3>
<p><em>class</em><code>torch.nn.``SELU</code>( <em>inplace=False</em>
)<a href="_modules/torch/nn/modules/activation.html#SELU">[source]</a></p>
<p>&#x5E94;&#x7528;&#x5143;&#x7D20;&#x65B9;&#x9762;&#xFF0C;&#x5982;&#xFF1A;</p>
<p>SELU(x)=scale&#x2217;(max&#x2061;(0,x)+min&#x2061;(0,&#x3B1;&#x2217;(exp&#x2061;(x)&#x2212;1)))\text{SELU}(x) = \text{scale} <em>
(\max(0,x) + \min(0, \alpha </em> (\exp(x) - 1)))
SELU(x)=scale&#x2217;(max(0,x)+min(0,&#x3B1;&#x2217;(exp(x)&#x2212;1)))</p>
<p>&#x4E0E; &#x3B1; =  1.6732632423543772848170429916717  \&#x963F;&#x5C14;&#x6CD5;=
1.6732632423543772848170429916717  &#x3B1; =  1  &#x3002;  6  7  3  2  6  3  2  4  2  3  5
4  3  7  7  2  8  4  8  1  7  0  4  2  9  9  1  6  7  1  7  &#x548C; &#x89C4;&#x6A21; =
1.0507009873554804934193349852946  \&#x6587;&#x672C;{&#x89C4;&#x6A21;} = 1.0507009873554804934193349852946
&#x89C4;&#x6A21; =  1  &#x3002;  0  5  0  7  0  0  9  8  7  3  5  5  4  8  0  4  9  3  4  1  9  3
3  4  9  8  5  2  9  4  6  &#x3002;</p>
<p>&#x66F4;&#x591A;&#x8BE6;&#x7EC6;&#x4FE1;&#x606F;&#x53EF;&#x5728;&#x672C;&#x6587;&#x4E2D;&#x627E;&#x5230;[&#x81EA;&#x6B63;&#x706B;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;<a href="https://arxiv.org/abs/1706.02515" target="_blank">HTG1&#x3002;</a></p>
<p>Parameters</p>
<p><strong>&#x5C31;&#x5730;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in
Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x53EF;&#x4EFB;&#x9009;&#x5730;&#x6267;&#x884C;&#x64CD;&#x4F5C;&#x5C31;&#x5730;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x5047;</code></p>
<p>Shape:</p>
<ul>
<li><p>Input: (N,&#x2217;)(N, <em>)(N,&#x2217;) where </em> means, any number of additional dimensions</p>
</li>
<li><p>Output: (N,&#x2217;)(N, *)(N,&#x2217;) , same shape as the input</p>
</li>
</ul>
<p><img src="_images/SELU.png" alt="_images/SELU.png"></p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.SELU()
&gt;&gt;&gt; input = torch.randn(2)
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="celu">CELU</h3>
<p><em>class</em><code>torch.nn.``CELU</code>( <em>alpha=1.0</em> , <em>inplace=False</em>
)<a href="_modules/torch/nn/modules/activation.html#CELU">[source]</a></p>
<p>Applies the element-wise function:</p>
<p>CELU(x)=max&#x2061;(0,x)+min&#x2061;(0,&#x3B1;&#x2217;(exp&#x2061;(x/&#x3B1;)&#x2212;1))\text{CELU}(x) = \max(0,x) + \min(0,
\alpha * (\exp(x/\alpha) - 1)) CELU(x)=max(0,x)+min(0,&#x3B1;&#x2217;(exp(x/&#x3B1;)&#x2212;1))</p>
<p>&#x66F4;&#x591A;&#x7EC6;&#x8282;&#x53EF;&#x4EE5;&#x5728;&#x6587;&#x732E;<a href="https://arxiv.org/abs/1704.07483" target="_blank">&#x7684;&#x8FDE;&#x7EED;&#x53EF;&#x5FAE;&#x6307;&#x6570;&#x76F4;&#x7EBF;&#x5355;&#x5143;</a>&#x4E2D;&#x627E;&#x5230;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x963F;&#x5C14;&#x6CD5;</strong> - &#x7684; &#x3B1; \&#x963F;&#x5C14;&#x6CD5; &#x3B1; &#x4E3A;CELU&#x5236;&#x5242;&#x503C;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;1.0</p>
</li>
<li><p><strong>inplace</strong> &#x2013; can optionally do the operation in-place. Default: <code>False</code></p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>Input: (N,&#x2217;)(N, <em>)(N,&#x2217;) where </em> means, any number of additional dimensions</p>
</li>
<li><p>Output: (N,&#x2217;)(N, *)(N,&#x2217;) , same shape as the input</p>
</li>
</ul>
<p><img src="_images/CELU.png" alt="_images/CELU.png"></p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.CELU()
&gt;&gt;&gt; input = torch.randn(2)
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="&#x4E59;&#x72B6;&#x7ED3;&#x80A0;">&#x4E59;&#x72B6;&#x7ED3;&#x80A0;</h3>
<p><em>class</em><code>torch.nn.``Sigmoid</code><a href="_modules/torch/nn/modules/activation.html#Sigmoid">[source]</a></p>
<p>Applies the element-wise function:</p>
<p>Sigmoid(x)=11+exp&#x2061;(&#x2212;x)\text{Sigmoid}(x) = \frac{1}{1 + \exp(-x)}
Sigmoid(x)=1+exp(&#x2212;x)1&#x200B;</p>
<p>Shape:</p>
<ul>
<li><p>Input: (N,&#x2217;)(N, <em>)(N,&#x2217;) where </em> means, any number of additional dimensions</p>
</li>
<li><p>Output: (N,&#x2217;)(N, *)(N,&#x2217;) , same shape as the input</p>
</li>
</ul>
<p><img src="_images/Sigmoid.png" alt="_images/Sigmoid.png"></p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.Sigmoid()
&gt;&gt;&gt; input = torch.randn(2)
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="softplus">Softplus</h3>
<p><em>class</em><code>torch.nn.``Softplus</code>( <em>beta=1</em> , <em>threshold=20</em>
)<a href="_modules/torch/nn/modules/activation.html#Softplus">[source]</a></p>
<p>Applies the element-wise function:</p>
<p>Softplus(x)=1&#x3B2;&#x2217;log&#x2061;(1+exp&#x2061;(&#x3B2;&#x2217;x))\text{Softplus}(x) = \frac{1}{\beta} * \log(1</p>
<ul>
<li>\exp(\beta * x)) Softplus(x)=&#x3B2;1&#x200B;&#x2217;log(1+exp(&#x3B2;&#x2217;x))</li>
</ul>
<p>SoftPlus&#x662F;&#x5149;&#x6ED1;&#x903C;&#x8FD1;RELU&#x529F;&#x80FD;&#xFF0C;&#x5E76;&#x4E14;&#x53EF;&#x4EE5;&#x7528;&#x4E8E;&#x7EA6;&#x675F;&#x7684;&#x673A;&#x5668;&#x7684;&#x8F93;&#x51FA;&#x4EE5;&#x59CB;&#x7EC8;&#x662F;&#x6B63;&#x7684;&#x3002;</p>
<p>&#x5BF9;&#x4E8E;&#x6570;&#x503C;&#x7A33;&#x5B9A;&#x6027;&#x7684;&#x6267;&#x884C;&#x6062;&#x590D;&#x5230;&#x7EBF;&#x6027;&#x51FD;&#x6570;&#x5BF9;&#x4E8E;&#x9AD8;&#x4E8E;&#x67D0;&#x4E2A;&#x503C;&#x7684;&#x8F93;&#x5165;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x7684;&#x3B2;</strong> - &#x7684; &#x3B2; \&#x7684;&#x3B2; &#x3B2; &#x4E3A;Softplus&#x5236;&#x5242;&#x503C;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;1</p>
</li>
<li><p><strong>&#x9608;</strong> - &#x9AD8;&#x4E8E;&#x6B64;&#x503C;&#x6062;&#x590D;&#x5230;&#x4E00;&#x4E2A;&#x7EBF;&#x6027;&#x51FD;&#x6570;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;20</p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>Input: (N,&#x2217;)(N, <em>)(N,&#x2217;) where </em> means, any number of additional dimensions</p>
</li>
<li><p>Output: (N,&#x2217;)(N, *)(N,&#x2217;) , same shape as the input</p>
</li>
</ul>
<p><img src="_images/Softplus.png" alt="_images/Softplus.png"></p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.Softplus()
&gt;&gt;&gt; input = torch.randn(2)
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="softshrink">Softshrink</h3>
<p><em>class</em><code>torch.nn.``Softshrink</code>( <em>lambd=0.5</em>
)<a href="_modules/torch/nn/modules/activation.html#Softshrink">[source]</a></p>
<p>&#x5E94;&#x7528;&#x8F6F;&#x6536;&#x7F29;&#x529F;&#x80FD;&#x7684;elementwise&#xFF1A;</p>
<p>SoftShrinkage(x)={x&#x2212;&#x3BB;, if x&gt;&#x3BB;x+&#x3BB;, if x&lt;&#x2212;&#x3BB;0, otherwise \text{SoftShrinkage}(x)
= \begin{cases} x - \lambda, &amp; \text{ if } x &gt; \lambda \\ x + \lambda, &amp;
\text{ if } x &lt; -\lambda \\ 0, &amp; \text{ otherwise } \end{cases}
SoftShrinkage(x)=&#x23A9;&#x23AA;&#x23A8;&#x23AA;&#x23A7;&#x200B;x&#x2212;&#x3BB;,x+&#x3BB;,0,&#x200B; if x&gt;&#x3BB; if x&lt;&#x2212;&#x3BB; otherwise &#x200B;</p>
<p>Parameters</p>
<p><strong>lambd</strong> - &#x7684; &#x3BB; \&#x62C9;&#x59C6;&#x8FBE; &#x3BB; &#x4E3A;Softshrink&#x5236;&#x5242;&#x503C;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;0.5</p>
<p>Shape:</p>
<ul>
<li><p>Input: (N,&#x2217;)(N, <em>)(N,&#x2217;) where </em> means, any number of additional dimensions</p>
</li>
<li><p>Output: (N,&#x2217;)(N, *)(N,&#x2217;) , same shape as the input</p>
</li>
</ul>
<p><img src="_images/Softshrink.png" alt="_images/Softshrink.png"></p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.Softshrink()
&gt;&gt;&gt; input = torch.randn(2)
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="softsign">Softsign</h3>
<p><em>class</em><code>torch.nn.``Softsign</code><a href="_modules/torch/nn/modules/activation.html#Softsign">[source]</a></p>
<p>Applies the element-wise function:</p>
<p>SoftSign(x)=x1+&#x2223;x&#x2223;\text{SoftSign}(x) = \frac{x}{ 1 + |x|} SoftSign(x)=1+&#x2223;x&#x2223;x&#x200B;</p>
<p>Shape:</p>
<ul>
<li><p>Input: (N,&#x2217;)(N, <em>)(N,&#x2217;) where </em> means, any number of additional dimensions</p>
</li>
<li><p>Output: (N,&#x2217;)(N, *)(N,&#x2217;) , same shape as the input</p>
</li>
</ul>
<p><img src="_images/Softsign.png" alt="_images/Softsign.png"></p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.Softsign()
&gt;&gt;&gt; input = torch.randn(2)
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="&#x53CC;&#x66F2;&#x6B63;&#x5207;">&#x53CC;&#x66F2;&#x6B63;&#x5207;</h3>
<p><em>class</em><code>torch.nn.``Tanh</code><a href="_modules/torch/nn/modules/activation.html#Tanh">[source]</a></p>
<p>Applies the element-wise function:</p>
<p>Tanh(x)=tanh&#x2061;(x)=ex&#x2212;e&#x2212;xex+e&#x2212;x\text{Tanh}(x) = \tanh(x) = \frac{e^x - e^{-x}}
{e^x + e^{-x}} Tanh(x)=tanh(x)=ex+e&#x2212;xex&#x2212;e&#x2212;x&#x200B;</p>
<p>Shape:</p>
<ul>
<li><p>Input: (N,&#x2217;)(N, <em>)(N,&#x2217;) where </em> means, any number of additional dimensions</p>
</li>
<li><p>Output: (N,&#x2217;)(N, *)(N,&#x2217;) , same shape as the input</p>
</li>
</ul>
<p><img src="_images/Tanh.png" alt="_images/Tanh.png"></p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.Tanh()
&gt;&gt;&gt; input = torch.randn(2)
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="tanhshrink">Tanhshrink</h3>
<p><em>class</em><code>torch.nn.``Tanhshrink</code><a href="_modules/torch/nn/modules/activation.html#Tanhshrink">[source]</a></p>
<p>Applies the element-wise function:</p>
<p>Tanhshrink(x)=x&#x2212;Tanh(x)\text{Tanhshrink}(x) = x - \text{Tanh}(x)
Tanhshrink(x)=x&#x2212;Tanh(x)</p>
<p>Shape:</p>
<ul>
<li><p>Input: (N,&#x2217;)(N, <em>)(N,&#x2217;) where </em> means, any number of additional dimensions</p>
</li>
<li><p>Output: (N,&#x2217;)(N, *)(N,&#x2217;) , same shape as the input</p>
</li>
</ul>
<p><img src="_images/Tanhshrink.png" alt="_images/Tanhshrink.png"></p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.Tanhshrink()
&gt;&gt;&gt; input = torch.randn(2)
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="&#x9608;&#x503C;">&#x9608;&#x503C;</h3>
<p><em>class</em><code>torch.nn.``Threshold</code>( <em>threshold</em> , <em>value</em> , <em>inplace=False</em>
)<a href="_modules/torch/nn/modules/activation.html#Threshold">[source]</a></p>
<p>&#x9608;&#x503C;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x3002;</p>
<p>&#x9608;&#x503C;&#x5B9A;&#x4E49;&#x4E3A;&#xFF1A;</p>
<p>y={x, if x&gt;thresholdvalue, otherwise y = \begin{cases} x, &amp;\text{ if } x &gt;
\text{threshold} \\ \text{value}, &amp;\text{ otherwise } \end{cases}
y={x,value,&#x200B; if x&gt;threshold otherwise &#x200B;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x9608;</strong> - &#x5728;&#x8BE5;&#x503C;&#x7684;&#x9608;&#x503C;</p>
</li>
<li><p><strong>&#x503C;</strong> - &#x8BE5;&#x503C;&#x4E0E;&#x66FF;&#x6362;</p>
</li>
<li><p><strong>inplace</strong> &#x2013; can optionally do the operation in-place. Default: <code>False</code></p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>Input: (N,&#x2217;)(N, <em>)(N,&#x2217;) where </em> means, any number of additional dimensions</p>
</li>
<li><p>Output: (N,&#x2217;)(N, *)(N,&#x2217;) , same shape as the input</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.Threshold(0.1, 20)
&gt;&gt;&gt; input = torch.randn(2)
&gt;&gt;&gt; output = m(input)
</code></pre><h2 id="&#x975E;&#x7EBF;&#x6027;&#x6FC0;&#x6D3B;&#xFF08;&#x5176;&#x4ED6;&#xFF09;">&#x975E;&#x7EBF;&#x6027;&#x6FC0;&#x6D3B;&#xFF08;&#x5176;&#x4ED6;&#xFF09;</h2>
<h3 id="softmin">Softmin</h3>
<p><em>class</em><code>torch.nn.``Softmin</code>( <em>dim=None</em>
)<a href="_modules/torch/nn/modules/activation.html#Softmin">[source]</a></p>
<p>&#x65BD;&#x52A0;Softmin&#x529F;&#x80FD;&#x7684;n&#x7EF4;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x91CD;&#x65B0;&#x7F29;&#x653E;&#x5B83;&#x4EEC;&#xFF0C;&#x4F7F;&#x5F97;&#x6240;&#x8FF0;n&#x7EF4;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x8C0E;&#x8A00;&#x7684;&#x8303;&#x56F4;&#x5728; [0,1] &#x548C;&#x603B;&#x548C;&#x4E3A;1&#x7684;&#x5143;&#x7D20;&#x3002;</p>
<p>Softmin&#x5B9A;&#x4E49;&#x4E3A;&#xFF1A;</p>
<p>Softmin(xi)=exp&#x2061;(&#x2212;xi)&#x2211;jexp&#x2061;(&#x2212;xj)\text{Softmin}(x_{i}) =
\frac{\exp(-x_i)}{\sum_j \exp(-x_j)} Softmin(xi&#x200B;)=&#x2211;j&#x200B;exp(&#x2212;xj&#x200B;)exp(&#x2212;xi&#x200B;)&#x200B;</p>
<p>Shape:</p>
<ul>
<li><p>&#x8F93;&#x5165;&#xFF1A; &#xFF08; <em>  &#xFF09; &#xFF08;</em>&#xFF09;  &#xFF08; <em>  &#xFF09; &#x5176;&#x4E2D; </em> &#x624B;&#x6BB5;&#xFF0C;&#x4EFB;&#x610F;&#x6570;&#x91CF;&#x7684;&#x9644;&#x52A0;&#x7EF4;&#x5EA6;&#x7684;</p>
</li>
<li><p>&#x8F93;&#x51FA;&#xFF1A; &#xFF08; <em>  &#xFF09; &#xFF08;</em>&#xFF09;  &#xFF08; *  &#xFF09; &#xFF0C;&#x76F8;&#x540C;&#x5F62;&#x72B6;&#x7684;&#x8F93;&#x5165;</p>
</li>
</ul>
<p>Parameters</p>
<p><strong>&#x6697;&#x6DE1;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in
Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x6CBF;&#x5176;Softmin&#x5C06;&#x88AB;&#x8BA1;&#x7B97;&#x7684;&#x5C3A;&#x5BF8;&#xFF08;&#x56E0;&#x6B64;&#x6CBF;&#x6697;&#x6DE1;&#x6BCF;&#x7247;&#x5C06;&#x603B;&#x7ED3;&#x4E3A;1&#xFF09;&#x3002;</p>
<p>Returns</p>
<p>&#x76F8;&#x540C;&#x7684;&#x5C3A;&#x5BF8;&#x548C;&#x5F62;&#x72B6;&#x4F5C;&#x4E3A;&#x8F93;&#x5165;&#x7684;&#x8303;&#x56F4;&#x5185;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x503C;[0&#xFF0C;1]</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.Softmin()
&gt;&gt;&gt; input = torch.randn(2, 3)
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="&#x4F7F;&#x7528;softmax">&#x4F7F;&#x7528;SoftMax</h3>
<p><em>class</em><code>torch.nn.``Softmax</code>( <em>dim=None</em>
)<a href="_modules/torch/nn/modules/activation.html#Softmax">[source]</a></p>
<p>&#x5E94;&#x7528;&#x4F7F;&#x7528;SoftMax&#x529F;&#x80FD;&#x7684;n&#x7EF4;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x91CD;&#x65B0;&#x7F29;&#x653E;&#x5B83;&#x4EEC;&#xFF0C;&#x4F7F;&#x5F97;&#x6240;&#x8FF0;n&#x7EF4;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x8C0E;&#x8A00;&#x5728;&#x8303;&#x56F4;[0,1]&#x548C;&#x603B;&#x548C;&#x4E3A;1&#x7684;&#x5143;&#x7D20;&#x3002;</p>
<p>&#x4F7F;&#x7528;SoftMax&#x5B9A;&#x4E49;&#x4E3A;&#xFF1A;</p>
<p>Softmax(xi)=exp&#x2061;(xi)&#x2211;jexp&#x2061;(xj)\text{Softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j
\exp(x_j)} Softmax(xi&#x200B;)=&#x2211;j&#x200B;exp(xj&#x200B;)exp(xi&#x200B;)&#x200B;</p>
<p>Shape:</p>
<ul>
<li><p>Input: (&#x2217;)(<em>)(&#x2217;) where </em> means, any number of additional dimensions</p>
</li>
<li><p>Output: (&#x2217;)(*)(&#x2217;) , same shape as the input</p>
</li>
</ul>
<p>Returns</p>
<p>&#x76F8;&#x540C;&#x7684;&#x5C3A;&#x5BF8;&#x548C;&#x5F62;&#x72B6;&#x4F5C;&#x4E3A;&#x8F93;&#x5165;&#x7684;&#x4E0E;&#x503C;&#x7684;&#x8303;&#x56F4;&#x5185;&#x7684;&#x5F20;&#x91CF;[0&#xFF0C;1]</p>
<p>Parameters</p>
<p><strong>&#x6697;&#x6DE1;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in
Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x6CBF;&#x5176;&#x4F7F;&#x7528;SoftMax&#x5C06;&#x88AB;&#x8BA1;&#x7B97;&#x7684;&#x5C3A;&#x5BF8;&#xFF08;&#x56E0;&#x6B64;&#x6CBF;&#x6697;&#x6DE1;&#x6BCF;&#x7247;&#x5C06;&#x603B;&#x7ED3;&#x4E3A;1&#xFF09;&#x3002;</p>
<p>Note</p>
<p>&#x6B64;&#x6A21;&#x5757;&#x4E0D;&#x4E0E;NLLLoss&#xFF0C;&#x5176;&#x9884;&#x8BA1;&#x4F7F;&#x7528;SoftMax&#x548C;&#x81EA;&#x8EAB;&#x4E4B;&#x95F4;&#x8981;&#x8BA1;&#x7B97;&#x65E5;&#x5FD7;&#x76F4;&#x63A5;&#x5DE5;&#x4F5C;&#x3002;&#x4F7F;&#x7528; LogSoftmax &#x4EE3;&#x66FF;&#xFF08;&#x5B83;&#x7684;&#x901F;&#x5EA6;&#x66F4;&#x5FEB;&#xFF0C;&#x5177;&#x6709;&#x66F4;&#x597D;&#x7684;&#x6570;&#x503C;&#x5C5E;&#x6027;&#xFF09;&#x3002;</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.Softmax(dim=1)
&gt;&gt;&gt; input = torch.randn(2, 3)
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="softmax2d">Softmax2d</h3>
<p><em>class</em><code>torch.nn.``Softmax2d</code><a href="_modules/torch/nn/modules/activation.html#Softmax2d">[source]</a></p>
<p>&#x9002;&#x7528;&#x4F7F;&#x7528;SoftMax&#x5728;&#x529F;&#x80FD;&#xFF0C;&#x6BCF;&#x4E2A;&#x7A7A;&#x95F4;&#x4F4D;&#x7F6E;&#x3002;</p>
<p>&#x5F53;&#x7ED9;&#x5B9A;&#x7684;<code>&#x7684;&#x56FE;&#x50CF;&#x9891;&#x9053; &#xD7; &#x9AD8;&#x5EA6; &#xD7; &#x5BBD;&#x5EA6;</code>&#xFF0C;&#x5B83;&#x5C06;&#x5E94;&#x7528;&#x4F7F;&#x7528;SoftMax &#x5230;&#x6BCF;&#x4E2A;&#x4F4D;&#x7F6E; &#xFF08; C  H  &#x4E00; n&#x7684; n&#x7684; E  L  S  &#xFF0C; H  i&#x7684; &#xFF0C;
W  [HTG51&#xFF1A;J  &#xFF09; &#xFF08;&#x9891;&#x9053;&#xFF0C;h_i&#xFF0C;w_j&#xFF09; &#xFF08; &#xE7;  H  &#x4E00; n&#x7684; n&#x7684; E  L  S  &#xFF0C; H  i&#x7684; &#xFF0C; W  [HTG131&#xFF1A;J  &#xFF09;</p>
<p>Shape:</p>
<ul>
<li><p>&#x8F93;&#x5165;&#xFF1A; &#xFF08; N  &#xFF0C; C  &#xFF0C; H  &#xFF0C; W  &#xFF09; &#xFF08;N&#xFF0C;C&#xFF0C;H&#xFF0C;W&#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF0C; H  &#xFF0C; W  &#xFF09; </p>
</li>
<li><p>&#x8F93;&#x51FA;&#xFF1A; &#xFF08; N  &#xFF0C; C  &#xFF0C; H  &#xFF0C; W  &#xFF09; &#xFF08;N&#xFF0C;C&#xFF0C;H&#xFF0C;W&#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF0C; H  &#xFF0C; W  &#xFF09;  &#xFF08;&#x76F8;&#x540C;&#x5F62;&#x72B6;&#x7684;&#x8F93;&#x5165;&#xFF09;</p>
</li>
</ul>
<p>Returns</p>
<p>a Tensor of the same dimension and shape as the input with values in the range
[0, 1]</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.Softmax2d()
&gt;&gt;&gt; # you softmax over the 2nd dimension
&gt;&gt;&gt; input = torch.randn(2, 3, 12, 13)
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="logsoftmax">LogSoftmax</h3>
<p><em>class</em><code>torch.nn.``LogSoftmax</code>( <em>dim=None</em>
)<a href="_modules/torch/nn/modules/activation.html#LogSoftmax">[source]</a></p>
<p>&#x5E94;&#x7528; &#x65E5;&#x5FD7; &#x2061; &#xFF08; &#x4F7F;&#x7528;SoftMax  &#xFF08; &#xD7; &#xFF09; &#xFF09; \&#x65E5;&#x5FD7;&#xFF08;\&#x6587;&#x672C;{&#x4F7F;&#x7528;SoftMax}&#xFF08;X&#xFF09;&#xFF09; LO  G  &#xFF08; &#x4F7F;&#x7528;SoftMax  &#xFF08; &#xD7; &#xFF09;
&#xFF09; &#x529F;&#x80FD;&#x7684;n&#x7EF4;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x3002;&#x6240;&#x8FF0;LogSoftmax&#x5236;&#x5242;&#x53EF;&#x4EE5;&#x88AB;&#x7B80;&#x5316;&#x4E3A;&#xFF1A;</p>
<p>LogSoftmax(xi)=log&#x2061;(exp&#x2061;(xi)&#x2211;jexp&#x2061;(xj))\text{LogSoftmax}(x_{i}) =
\log\left(\frac{\exp(x_i) }{ \sum_j \exp(x_j)} \right)
LogSoftmax(xi&#x200B;)=log(&#x2211;j&#x200B;exp(xj&#x200B;)exp(xi&#x200B;)&#x200B;)</p>
<p>Shape:</p>
<ul>
<li><p>Input: (&#x2217;)(<em>)(&#x2217;) where </em> means, any number of additional dimensions</p>
</li>
<li><p>Output: (&#x2217;)(*)(&#x2217;) , same shape as the input</p>
</li>
</ul>
<p>Parameters</p>
<p><strong>&#x6697;&#x6DE1;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in
Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x6CBF;&#x5176;LogSoftmax&#x5C06;&#x88AB;&#x8BA1;&#x7B97;&#x7684;&#x5C3A;&#x5BF8;&#x3002;</p>
<p>Returns</p>
<p>&#x76F8;&#x540C;&#x7684;&#x5C3A;&#x5BF8;&#x548C;&#x5F62;&#x72B6;&#x4F5C;&#x4E3A;&#x8F93;&#x5165;&#x7684;&#x4E0E;&#x503C;&#x7684;&#x8303;&#x56F4;&#x5185;&#x7684;&#x5F20;&#x91CF;[-INF&#xFF0C;0&#xFF09;</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.LogSoftmax()
&gt;&gt;&gt; input = torch.randn(2, 3)
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="adaptivelogsoftmaxwithloss">AdaptiveLogSoftmaxWithLoss</h3>
<p><em>class</em><code>torch.nn.``AdaptiveLogSoftmaxWithLoss</code>( <em>in_features</em> , <em>n_classes</em> ,
<em>cutoffs</em> , <em>div_value=4.0</em> , <em>head_bias=False</em>
)<a href="_modules/torch/nn/modules/adaptive.html#AdaptiveLogSoftmaxWithLoss">[source]</a></p>
<p>&#x4F5C;&#x4E3A;&#x7528;&#x4E8E;GPU&#x7684;&#x7531;&#x7231;&#x5FB7;&#x534E;&#x5893;&#xFF0C;&#x963F;&#x8292;Joulin&#x7A46;&#x65AF;&#x5854;&#x6CD5;&#x897F;&#x585E;&#xFF0C;&#x5927;&#x536B;Grangier&#x548C;&#x57C3;&#x5C14;&#x97E6;J&#xE9;gou&#x5728;<a href="https://arxiv.org/abs/1609.04309" target="_blank">&#x9AD8;&#x6548;SOFTMAX&#x8FD1;&#x4F3C;&#x63CF;&#x8FF0;&#x9AD8;&#x6548;SOFTMAX&#x8FD1;&#x4F3C;&#x3002;</a></p>
<p>&#x81EA;&#x9002;&#x5E94;SOFTMAX&#x662F;&#x4E0E;&#x4EA7;&#x91CF;&#x5927;&#x7A7A;&#x95F4;&#x7684;&#x57F9;&#x8BAD;&#x6A21;&#x5F0F;&#x8FD1;&#x4F3C;&#x7684;&#x7B56;&#x7565;&#x3002;&#x5F53;&#x6807;&#x7B7E;&#x5206;&#x5E03;&#x6781;&#x4E0D;&#x5E73;&#x8861;&#xFF0C;&#x4F8B;&#x5982;&#xFF0C;&#x5728;&#x81EA;&#x7136;&#x8BED;&#x8A00;&#x5EFA;&#x6A21;&#xFF0C;&#x8FD9;&#x91CC;&#x6240;&#x8BF4;&#x7684;&#x9891;&#x7387;&#x5206;&#x5E03;&#x5927;&#x81F4;&#x5982;&#x4E0B;<a href="https://en.wikipedia.org/wiki/Zipf%27s_law" target="_blank">&#x9F50;&#x666E;&#x592B;&#x5B9A;&#x5F8B;</a>&#x8FD9;&#x662F;&#x6700;&#x6709;&#x6548;&#x7684;&#x3002;</p>
<p>&#x81EA;&#x9002;&#x5E94;SOFTMAX&#x5212;&#x5206;&#x6807;&#x7B7E;&#x5206;&#x6210;&#x51E0;&#x4E2A;&#x96C6;&#x7FA4;&#xFF0C;&#x6839;&#x636E;&#x81EA;&#x5DF1;&#x7684;&#x9891;&#x7387;&#x3002;&#x8FD9;&#x4E9B;&#x96C6;&#x7FA4;&#x53EF;&#x4EE5;&#x5305;&#x542B;&#x4E0D;&#x540C;&#x6570;&#x91CF;&#x7684;&#x6BCF;&#x4E00;&#x4E2A;&#x76EE;&#x6807;&#x3002;&#x53E6;&#x5916;&#xFF0C;&#x5C06;&#x542B;&#x6709;&#x8F83;&#x4E0D;&#x9891;&#x7E41;&#x7684;&#x6807;&#x7B7E;&#x7684;&#x96C6;&#x7FA4;&#x5206;&#x914D;&#x4F4E;&#x7EF4;&#x7684;&#x5D4C;&#x5165;&#x5230;&#x8FD9;&#x4E9B;&#x6807;&#x7B7E;&#xFF0C;&#x4ECE;&#x800C;&#x52A0;&#x5FEB;&#x4E86;&#x8BA1;&#x7B97;&#x3002;&#x5BF9;&#x4E8E;&#x6BCF;&#x4E2A;minibatch&#xFF0C;&#x53EA;&#x4E3A;&#x5176;&#x4E2D;&#x81F3;&#x5C11;&#x4E00;&#x4E2A;&#x76EE;&#x6807;&#x662F;&#x672C;&#x7C07;&#x8FDB;&#x884C;&#x8BC4;&#x4F30;&#x3002;</p>
<p>&#x8FD9;&#x4E2A;&#x60F3;&#x6CD5;&#x662F;&#xFF0C;&#x8FD9;&#x662F;&#x7ECF;&#x5E38;&#x8BBF;&#x95EE;&#x7684;&#x96C6;&#x7FA4;&#xFF08;&#x5982;&#x7B2C;&#x4E00;&#x4E2A;&#xFF0C;&#x5305;&#x542B;&#x6700;&#x5E38;&#x89C1;&#x7684;&#x6807;&#x7B7E;&#xFF09;&#xFF0C;&#x4E5F;&#x5E94;&#x8BE5;&#x662F;&#x5EC9;&#x4EF7;&#x7684;&#x8BA1;&#x7B97; - &#x4E5F;&#x5C31;&#x662F;&#xFF0C;&#x5305;&#x542B;&#x5C11;&#x91CF;&#x5206;&#x914D;&#x7684;&#x6807;&#x7B7E;&#x3002;</p>
<p>&#x6211;&#x4EEC;&#x5F3A;&#x70C8;&#x5EFA;&#x8BAE;&#x60A8;&#x8003;&#x8651;&#x770B;&#x770B;&#x539F;&#x6765;&#x7684;&#x6587;&#x4EF6;&#x7684;&#x66F4;&#x591A;&#x7EC6;&#x8282;&#x3002;</p>
<ul>
<li><p><code>&#x622A;&#x65AD;&#x503C;</code>&#x5E94;&#x5728;&#x589E;&#x52A0;&#x987A;&#x5E8F;&#x6392;&#x5E8F;&#x6574;&#x6570;&#x7684;&#x6709;&#x5E8F;&#x5E8F;&#x5217;&#x3002;&#x5B83;&#x63A7;&#x5236;&#x96C6;&#x7FA4;&#x7684;&#x6570;&#x91CF;&#x548C;&#x76EE;&#x6807;&#x5206;&#x5272;&#x6210;&#x96C6;&#x7FA4;&#x3002;&#x4F8B;&#x5982;&#x8BBE;&#x7F6E;<code>&#x622A;&#x65AD;&#x503C; =  [10&#xFF0C; 100&#xFF0C; 1000]</code>&#x8868;&#x793A;&#x7B2C;&#x4E00; 10 &#x76EE;&#x6807;&#x5C06;&#x88AB;&#x5206;&#x914D;&#x5230;&#x6240;&#x8FF0;&#x81EA;&#x9002;&#x5E94;SOFTMAX&#x7684; &apos;&#x5934;&#x90E8;&apos;&#xFF0C;&#x76EE;&#x6807; 11&#xFF0C;12&#xFF0C;...&#xFF0C;100 &#x5C06;&#x88AB;&#x5206;&#x914D;&#x5230;&#x6240;&#x8FF0;&#x7B2C;&#x4E00;&#x7FA4;&#x96C6;&#xFF0C;&#x548C;&#x76EE;&#x6807; 101 &#xFF0C;102&#xFF0C;...&#xFF0C;1000 &#x5C06;&#x88AB;&#x5206;&#x914D;&#x5230;&#x6240;&#x8FF0;&#x7B2C;&#x4E8C;&#x7FA4;&#x96C6;&#xFF0C;&#x800C;&#x76EE;&#x6807; 1001&#xFF0C;1002&#xFF0C;...&#xFF0C;n_classes - 1 &#x5C06;&#x88AB;&#x5206;&#x914D;&#x5230;&#x6700;&#x540E;&#xFF0C;&#x7B2C;&#x4E09;&#x96C6;&#x7FA4;&#x3002;</p>
</li>
<li><p><code>div_value</code>&#x88AB;&#x7528;&#x6765;&#x8BA1;&#x7B97;&#x6BCF;&#x4E2A;&#x9644;&#x52A0;&#x7C07;&#x7684;&#x5927;&#x5C0F;&#xFF0C;&#x5B83;&#x88AB;&#x7ED9;&#x5B9A;&#x4E3A; &#x230A;  i&#x7684; n&#x7684; <em>  F  E  &#x4E00; T  U  R  E  S  d  i&#x7684; [HTG42&#x3011;v  </em>  [HTG46&#x3011;v &#x4E00; L  U  E  i&#x7684; d  &#xD7; &#x230B; \&#x5DE6;\ lfloor \&#x538B;&#x88C2;{&#x5728;\ <em>features} {DIV \ _value ^ {IDX}} \&#x53F3;\ rfloor  &#x230A; d  i&#x7684; [HTG101&#x3011;V  </em>  [HTG105&#x3011;V  &#x4E00; L  U  E  i&#x7684; d  &#xD7; i&#x7684; n&#x7684; _  F  E  &#x4E00; T  U  R  E  S  [HTG19 0]  &#x230B; &#xFF0C;&#x5176;&#x4E2D; i&#x7684; d  &#xD7; IDX  i&#x7684; d  &#xD7; &#x662F;&#x7FA4;&#x96C6;&#x7D22;&#x5F15;&#xFF08;&#x4E0E;&#x96C6;&#x7FA4;&#x4EE5;&#x8F83;&#x5C11;&#x5177;&#x6709;&#x8F83;&#x5927;&#x7D22;&#x5F15;&#x9891;&#x7E41;&#x5B57;&#x548C;&#x7D22;&#x5F15;&#x8D77;&#x59CB;&#x4ECE; 1  1  1  &#xFF09;&#x3002;</p>
</li>
<li><p><code>head_bias</code>&#x5982;&#x679C;&#x8BBE;&#x7F6E;&#x4E3A;True&#xFF0C;&#x589E;&#x52A0;&#x4E86;&#x4E00;&#x4E2A;&#x504F;&#x9879;&#x5230;&#x81EA;&#x9002;&#x5E94;SOFTMAX&#x7684;&#x201C;&#x5934;&#x201D;&#x3002;&#x6709;&#x5173;&#x8BE6;&#x7EC6;&#x4FE1;&#x606F;&#xFF0C;&#x8BF7;&#x53C2;&#x9605;&#x7EB8;&#x5F20;&#x3002;&#x8BBE;&#x7F6E;&#x4E3A;False&#x6B63;&#x5F0F;&#x6267;&#x884C;&#x3002;</p>
</li>
</ul>
<p>Warning</p>
<p>&#x4F5C;&#x4E3A;&#x8F93;&#x5165;&#x4F20;&#x9012;&#x7ED9;&#x6B64;&#x6A21;&#x5757;&#x7684;&#x6807;&#x7B7E;&#x5E94;&#x88AB;&#x5206;&#x7C7B;accoridng&#x81EA;&#x5DF1;&#x7684;&#x9891;&#x7387;&#x3002;&#x8FD9;&#x610F;&#x5473;&#x7740;&#xFF0C;&#x6700;&#x9891;&#x7E41;&#x7684;&#x6807;&#x7B7E;&#x5E94;&#x8BE5;&#x7531;&#x6307;&#x6570; 0 &#xFF0C;&#x548C;&#x81F3;&#x5C11;&#x9891;&#x7E41;&#x6807;&#x7B7E;&#x5E94;&#x8BE5;&#x7531;&#x6307;&#x6570;
n_classes&#x6765;&#x8868;&#x793A;&#x6765;&#x8868;&#x793A; - 1 &#x3002;</p>
<p>Note</p>
<p>&#x8FD9;&#x4E2A;&#x6A21;&#x5757;&#x8FD4;&#x56DE;<code>NamedTuple</code>&#x4E0E;<code>&#x8F93;&#x51FA;</code>&#x548C;<code>&#x635F;&#x5931;</code>&#x5B57;&#x6BB5;&#x3002;&#x8BE6;&#x60C5;&#x8BF7;&#x53C2;&#x89C1;&#x66F4;&#x591A;&#x6587;&#x6863;&#x3002;</p>
<p>Note</p>
<p>&#x4E3A;&#x4E86;&#x8BA1;&#x7B97;&#x5BF9;&#x6570;&#x6982;&#x7387;&#x7684;&#x6240;&#x6709;&#x7C7B;&#xFF0C;&#x5219;<code>log_prob</code>&#x53EF;&#x4EE5;&#x4F7F;&#x7528;&#x7684;&#x65B9;&#x6CD5;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p>&#x5728;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x7684;&#x7279;&#x5F81;&#x6570; - <strong>in_features</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09;</p>
</li>
<li><p>&#x5728;&#x6570;&#x636E;&#x96C6;&#x7684;&#x7C7B;&#x7684;&#x6570;&#x91CF; - <strong>n_classes</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09;</p>
</li>
<li><p><strong>&#x622A;&#x65AD;&#x503C;</strong> &#xFF08; <em>&#x5E8F;&#x53F7;</em> &#xFF09; - &#x7528;&#x4E8E;&#x5206;&#x914D;&#x7684;&#x76EE;&#x6807;&#xFF0C;&#x4EE5;&#x4ED6;&#x4EEC;&#x7684;&#x6C34;&#x6876;&#x4FDD;&#x9669;&#x4E1D;</p>
</li>
<li><p><strong>div_value</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x7528;&#x6765;&#x4F5C;&#x4E3A;&#x6307;&#x6570;&#x6765;&#x8BA1;&#x7B97;&#x96C6;&#x7FA4;&#x7684;&#x5927;&#x5C0F;&#x503C;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;4.0</p>
</li>
<li><p><strong>head_bias</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5982;&#x679C;<code>&#x771F;</code>&#xFF0C;&#x589E;&#x52A0;&#x4E86;&#x4E00;&#x4E2A;&#x504F;&#x9879;&#x5230;&#x81EA;&#x9002;&#x5E94;SOFTMAX&#x7684;&#x201C;&#x5934;&#x201D;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x5047;</code></p>
</li>
</ul>
<p>Returns</p>
<ul>
<li><p><strong>&#x8F93;&#x51FA;</strong> &#x662F;&#x5927;&#x5C0F;&#x7684;&#x5F20;&#x91CF;<code>N  [HTG5&#x542B;&#x6709;&#x8BA1;&#x7B97;&#x76EE;&#x6807;&#x6570;&#x6982;&#x7387;&#x5BF9;&#x4E8E;&#x6BCF;&#x4E2A;&#x5B9E;&#x65BD;&#x4F8B;</code></p>
</li>
<li><p><strong>&#x635F;&#x5931;</strong> &#x662F;&#x8868;&#x793A;&#x6240;&#x8BA1;&#x7B97;&#x7684;&#x8D1F;&#x5BF9;&#x6570;&#x4F3C;&#x7136;&#x635F;&#x8017;&#x7684;&#x6807;&#x91CF;</p>
</li>
</ul>
<p>Return type</p>
<p><code>NamedTuple</code>&#x4E0E;<code>&#x8F93;&#x51FA;</code>&#x548C;<code>&#x635F;&#x5931;</code>&#x5B57;&#x6BB5;</p>
<p>Shape:</p>
<ul>
<li><p>&#x8F93;&#x5165;&#xFF1A; &#xFF08; N  &#xFF0C; i&#x7684; n&#x7684; <em>  F  E  &#x4E00; T  U  [R  E  S  &#xFF09; &#xFF08;N&#xFF0C;&#x5728;\ _features&#xFF09; &#xFF08; N  &#xFF0C; i&#x7684; n&#x7684; </em>  F  E  &#x4E00; T  U  R  E  S  &#xFF09;</p>
</li>
<li><p>&#x76EE;&#x6807;&#xFF1A; &#xFF08; N  &#xFF09; &#xFF08;N&#xFF09;  &#xFF08; N  &#xFF09; &#x5176;&#x4E2D;&#xFF0C;&#x6BCF;&#x4E2A;&#x503C;&#x6EE1;&#x8DB3; 0  &amp; LT ;  =  T  &#x4E00; R  &#x514B; E  T  [ i&#x7684; &amp; LT ;  =  n&#x7684; <em>  C  L  &#x4E00; S  S  E  S  0 &amp; LT ; =&#x76EE;&#x6807;[I] &amp; LT ; = N \ _classes  0  [H TG97]  &amp; LT ;  =  T  &#x4E00; R  &#x514B; E  T  [  i&#x7684; &amp; LT ;  =  n&#x7684; </em>  C  &#x5347; &#x4E00; S  S  E  S </p>
</li>
<li><p>&#x8F93;&#x51FA;1&#xFF1A; &#xFF08; N  &#xFF09; &#xFF08;N&#xFF09;  &#xFF08; N  &#xFF09;</p>
</li>
<li><p>OUTPUT2&#xFF1A;<code>[HTG1&#x6807;&#x91CF;</code></p>
</li>
</ul>
<p><code>log_prob</code>( <em>input</em>
)<a href="_modules/torch/nn/modules/adaptive.html#AdaptiveLogSoftmaxWithLoss.log_prob">[source]</a></p>
<p>&#x8BA1;&#x7B97;&#x5BF9;&#x6570;&#x6982;&#x7387;&#x5BF9;&#x4E8E;&#x6240;&#x6709; n&#x7684; <em>  C  L  &#x4E00; S  S  E  S  n&#x7684;\ _classes  n&#x7684; </em>  C  L  &#x4E00;&#x4E2A; S  S  E  S</p>
<p>Parameters</p>
<p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x7684;&#x4F8B;&#x5B50;&#x7684;minibatch</p>
<p>Returns</p>
<p>&#x5BF9;&#x6570;&#x6982;&#x7387;&#x4E3A;&#x6BCF;&#x4E2A;&#x7C7B; C  C  C  &#x5728;&#x8303;&#x56F4; 0  &amp; LT ;  =  C  &amp; LT ;  =  n&#x7684; <em>  C  L  &#x4E00; S  S  E  S  0
&amp; LT ; = C &amp; LT ; = N \ _classes  0  &amp; LT ;  =  C  &amp; LT ;  =  n&#x7684; </em>  C  L  &#x4E00; S
S  E  S  &#xFF0C;&#x5176;&#x4E2D; n&#x7684; <em>  C  &#x5347; &#x4E00; S  S  E  S  n&#x7684;\ _classes  n&#x7684; </em>  C  L  &#x4E00; S  S  E  S
&#x7684;&#x53C2;&#x6570;&#x4F20;&#x9012;&#x4E3A;<code>AdaptiveLogSoftmaxWithLoss [HTG1 86]</code>&#x6784;&#x9020;&#x3002;</p>
<p>Shape:</p>
<ul>
<li><p>&#x8F93;&#x5165;&#xFF1A; &#xFF08; N  &#xFF0C; i&#x7684; n&#x7684; <em>  F  E  &#x4E00; T  U  [R  E  S  &#xFF09; &#xFF08;N&#xFF0C;&#x5728;\ _features&#xFF09; &#xFF08; N  &#xFF0C; i&#x7684; n&#x7684; </em>  F  E  &#x4E00; T  U  R  E  S  &#xFF09;</p>
</li>
<li><p>&#x8F93;&#x51FA;&#xFF1A; &#xFF08; N  &#xFF0C; n&#x7684; <em>  C  L  &#x4E00; S  S  E  S  &#xFF09; &#xFF08;N&#xFF0C;N- \ _classes&#xFF09; &#xFF08; N  &#xFF0C; n&#x7684; </em>  C  &#x5347; &#x4E00; S  S  E  S  &#xFF09;</p>
</li>
</ul>
<p><code>predict</code>( <em>input</em>
)<a href="_modules/torch/nn/modules/adaptive.html#AdaptiveLogSoftmaxWithLoss.predict">[source]</a></p>
<p>&#x8FD9;&#x7B49;&#x540C;&#x4E8E; self.log_pob&#xFF08;&#x8F93;&#x5165;&#xFF09;.argmax&#xFF08;&#x6697;= 1&#xFF09;&#xFF0C;&#x4F46;&#x662F;&#x5728;&#x67D0;&#x4E9B;&#x60C5;&#x51B5;&#x4E0B;&#x66F4;&#x4E3A;&#x6709;&#x6548;&#x3002;</p>
<p>Parameters</p>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; a
minibatch of examples</p>
<p>Returns</p>
<p>&#x7528;&#x5BF9;&#x4E8E;&#x6BCF;&#x4E2A;&#x5B9E;&#x65BD;&#x4F8B;&#x7684;&#x6982;&#x7387;&#x6700;&#x9AD8;&#x7684;&#x7C7B;</p>
<p>Return type</p>
<p>&#x8F93;&#x51FA;&#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor">&#x5F20;&#x91CF;</a>&#xFF09;</p>
<p>Shape:</p>
<ul>
<li><p>Input: (N,in_features)(N, in\_features)(N,in_features)</p>
</li>
<li><p>&#x8F93;&#x51FA;&#xFF1A; &#xFF08; N  &#xFF09; &#xFF08;N&#xFF09;  &#xFF08; N  &#xFF09;</p>
</li>
</ul>
<h2 id="&#x5F52;&#x4E00;&#x5316;&#x7684;&#x5C42;">&#x5F52;&#x4E00;&#x5316;&#x7684;&#x5C42;</h2>
<h3 id="batchnorm1d">BatchNorm1d</h3>
<p><em>class</em><code>torch.nn.``BatchNorm1d</code>( <em>num_features</em> , <em>eps=1e-05</em> , <em>momentum=0.1</em>
, <em>affine=True</em> , <em>track_running_stats=True</em>
)<a href="_modules/torch/nn/modules/batchnorm.html#BatchNorm1d">[source]</a></p>
<p>&#x9002;&#x7528;&#x6279;&#x6807;&#x51C6;&#x5316;&#x5728;2D&#x6216;&#x5982;&#x5728;&#x6587;&#x732E;<a href="https://arxiv.org/abs/1502.03167" target="_blank">&#x6279;&#x6807;&#x51C6;&#x5316;&#x63CF;&#x8FF0;3D&#x8F93;&#x5165;&#xFF08;&#x5E26;&#x6709;&#x53EF;&#x9009;&#x7684;&#x9644;&#x52A0;&#x7684;&#x4FE1;&#x9053;&#x5C3A;&#x5BF8;&#x7684;&#x5C0F;&#x6279;&#x91CF;&#x7684;1D&#x8F93;&#x5165;&#xFF09;&#xFF1A;&#x901A;&#x8FC7;&#x51CF;&#x5C11;&#x5185;&#x90E8;&#x534F;&#x53D8;&#x91CF;&#x79FB;&#x4F4D;</a>&#x52A0;&#x901F;&#x6DF1;&#x7F51;&#x7EDC;&#x8BAD;&#x7EC3;&#x3002;</p>
<p>y=x&#x2212;E[x]Var[x]+&#x3F5;&#x2217;&#x3B3;+&#x3B2;y = \frac{x - \mathrm{E}[x]}{\sqrt{\mathrm{Var}[x] +
\epsilon}} * \gamma + \betay=Var[x]+&#x3F5;&#x200B;x&#x2212;E[x]&#x200B;&#x2217;&#x3B3;+&#x3B2;</p>
<p>&#x7684;&#x5E73;&#x5747;&#x503C;&#x548C;&#x6807;&#x51C6;&#x504F;&#x5DEE;&#x662F;&#x6BCF;&#x4E2A;&#x5C3A;&#x5BF8;&#x6765;&#x8BA1;&#x7B97;&#x653E;&#x7F6E;&#x5728;&#x8FF7;&#x4F60;&#x6279;&#x6B21;&#x548C; &#x3B3; \&#x4F3D;&#x9A6C; &#x3B3; &#x548C; &#x3B2; \&#x7684;&#x3B2; &#x3B2; &#x662F;&#x5927;&#x5C0F;&#x7684;&#x53EF;&#x5B66;&#x4E60;&#x7684;&#x53C2;&#x6570;&#x77E2;&#x91CF; C &#xFF08;&#x5176;&#x4E2D; C
&#x662F;&#x8F93;&#x5165;&#x5927;&#x5C0F;&#xFF09;&#x3002;&#x9ED8;&#x8BA4;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x3B3;  \&#x4F3D;&#x9A6C; &#x7684; &#x7684;&#x5143;&#x7D20; &#x3B3; &#x88AB;&#x8BBE;&#x7F6E;&#x4E3A;1&#x548C; &#x7684;&#x5143;&#x7D20; &#x3B2; \&#x7684;&#x3B2; &#x3B2; &#x88AB;&#x8BBE;&#x7F6E;&#x4E3A;0&#x3002;</p>
<p>&#x6B64;&#x5916;&#xFF0C;&#x9ED8;&#x8BA4;&#x60C5;&#x51B5;&#xFF0C;&#x5728;&#x57F9;&#x8BAD;&#x8FC7;&#x7A0B;&#x4E2D;&#x8FD9;&#x5C42;&#x7EE7;&#x7EED;&#x8FD0;&#x884C;&#x5176;&#x8BA1;&#x7B97;&#x7684;&#x5747;&#x503C;&#x548C;&#x65B9;&#x5DEE;&#xFF0C;&#x7136;&#x540E;&#x518D;&#x8BC4;&#x4F30;&#x671F;&#x95F4;&#x7528;&#x4E8E;&#x6807;&#x51C6;&#x5316;&#x7684;&#x4F30;&#x8BA1;&#x3002;&#x6B63;&#x5728;&#x8FD0;&#x884C;&#x7684;&#x4F30;&#x8BA1;&#x662F;&#x4FDD;&#x6301;&#x4E86;&#x9ED8;&#x8BA4;&#x7684;<code>&#x52BF;&#x5934;</code>0.1&#x3002;</p>
<p>&#x5982;&#x679C;<code>track_running_stats</code>&#x8BBE;&#x7F6E;&#x4E3A;<code>&#x5047;</code>&#xFF0C;&#x8FD9;&#x4E00;&#x5C42;&#x5219;&#x4E0D;&#x4F1A;&#x7EE7;&#x7EED;&#x8FD0;&#x884C;&#x7684;&#x4F30;&#x8BA1;&#xFF0C;&#x548C;&#x6279;&#x91CF;&#x7EDF;&#x8BA1;&#x8FC7;&#x7A0B;&#x4E2D;&#x7684;&#x8BC4;&#x4F30;&#x65F6;&#x95F4;&#xFF0C;&#x800C;&#x4E0D;&#x662F;&#x4F5C;&#x4E3A;&#x597D;&#x3002;</p>
<p>Note</p>
<p>&#x6B64;<code>&#x52A8;&#x91CF;</code>&#x53C2;&#x6570;&#x662F;&#x4ECE;&#x4E00;&#x4E2A;&#x5728;&#x4F18;&#x5316;&#x5668;&#x4E2D;&#x4F7F;&#x7528;&#x7684;&#x7C7B;&#x548C;&#x52A8;&#x91CF;&#x7684;&#x5E38;&#x89C4;&#x6982;&#x5FF5;&#x4E0D;&#x540C;&#x3002;&#x5728;&#x6570;&#x5B66;&#x4E0A;&#xFF0C;&#x8FD9;&#x91CC;&#x8FD0;&#x884C;&#x7EDF;&#x8BA1;&#x6570;&#x636E;&#x7684;&#x66F4;&#x65B0;&#x89C4;&#x5219;&#x4E3A; &#xD7; ^  &#x65B0; =  &#xFF08; 1  -  &#x52A8;&#x91CF; &#xFF09;
&#xD7; &#xD7; ^  +  &#x52A8;&#x91CF; &#xD7; &#xD7; T  \&#x5E3D;&#x5B50;{X} _ \&#x6587;&#x672C;{&#x65B0;} =&#xFF08;1 - \&#x6587;&#x672C;{&#x52A8;&#x91CF;}&#xFF09;\&#x500D;\&#x5E3D;&#x5B50;{X} + \&#x6587;&#x672C;{&#x52A8;&#x91CF;} \&#x500D;X_T  &#xD7;
^  &#x65B0; =  &#xFF08; 1  -  &#x52A8;&#x91CF; &#xFF09; &#xD7; &#xD7; ^  +  &#x52A8;&#x91CF; &#xD7; &#xD7; T  &#xFF0C;&#x5176;&#x4E2D; &#xD7; ^  \&#x5E3D;&#x5B50;{X}  &#xD7; ^  &#x200B;&#x200B;  &#x662F;&#x4F30;&#x8BA1;&#x7684;&#x7EDF;&#x8BA1;&#x91CF;&#x548C;
X  T  X_T  &#xD7; T  &#x662F;&#x65B0;&#x7684;&#x89C2;&#x6D4B;&#x503C;&#x3002;</p>
<p>&#x56E0;&#x4E3A;&#x6279;&#x6807;&#x51C6;&#x5316;&#x662F;&#x5728; C &#x7EF4;&#x5B8C;&#x6210;&#x7684;&#xFF0C;&#x5728;&#xFF08;N&#xFF0C;L&#xFF09;&#x5207;&#x7247;&#x8BA1;&#x7B97;&#x7EDF;&#x8BA1;&#x6570;&#x636E;&#xFF0C;&#x8FD9;&#x662F;&#x5171;&#x540C;&#x7684;&#x672F;&#x8BED;&#x6765;&#x8C03;&#x7528;&#x8FD9;&#x4E2A;&#x65F6;&#x7A7A;&#x6279;&#x6807;&#x51C6;&#x5316;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>NUM_FEATURES</strong> -  C  C  C  &#x4ECE;&#x5927;&#x5C0F;&#x7684;&#x9884;&#x671F;&#x8F93;&#x5165; &#xFF08; N  &#xFF0C; C  &#xFF0C; L  &#xFF09; &#xFF08;N&#xFF0C;C&#xFF0C;L&#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF0C; L  &#xFF09; &#x6216; L  L  L  &#x4ECE;&#x7684;&#x8F93;&#x5165;&#x5927;&#x5C0F; &#xFF08; N  &#xFF0C; L  &#xFF09; &#xFF08;N&#xFF0C;L&#xFF09; &#xFF08; N  &#xFF0C; L  &#xFF09;</p>
</li>
<li><p><strong>EPS</strong> - &#x7684;&#x503C;&#x6DFB;&#x52A0;&#x5230;&#x5206;&#x6BCD;&#x6570;&#x503C;&#x7A33;&#x5B9A;&#x6027;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;1E-5</p>
</li>
<li><p><strong>&#x52A8;&#x91CF;</strong> - &#x7528;&#x4E8E;running_mean&#x548C;running_var&#x8BA1;&#x7B97;&#x7684;&#x503C;&#x3002;&#x53EF;&#x4EE5;&#x88AB;&#x8BBE;&#x7F6E;&#x4E3A;<code>&#x65E0; [HTG5&#x7528;&#x4E8E;&#x7D2F;&#x79EF;&#x79FB;&#x52A8;&#x5E73;&#x5747;&#xFF08;&#x5373;&#x7B80;&#x5355;&#x5E73;&#x5747;&#xFF09;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;0.1</code></p>
</li>
<li><p><strong>&#x4EFF;&#x5C04;</strong> - &#x4E00;&#x4E2A;&#x5E03;&#x5C14;&#x503C;&#xFF0C;&#x5F53;&#x8BBE;&#x7F6E;&#x4E3A;<code>&#x771F;</code>&#xFF0C;&#x8BE5;&#x6A21;&#x5757;&#x5177;&#x6709;&#x53EF;&#x5B66;&#x4E60;&#x7684;&#x4EFF;&#x5C04;&#x53C2;&#x6570;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x771F;</code></p>
</li>
<li><p><strong>track_running_stats</strong> - &#x5F53;&#x8BBE;&#x7F6E;&#x4E3A;<code>&#x771F;</code>&#xFF0C;&#x6B64;&#x6A21;&#x5757;&#x8DDF;&#x8E2A;&#x7684;&#x8FD0;&#x884C;&#x5747;&#x503C;&#x548C;&#x65B9;&#x5DEE;&#xFF0C;&#x548C;&#x4E00;&#x4E2A;&#x5E03;&#x5C14;&#x503C;&#xFF0C;&#x5F53;&#x8BBE;&#x7F6E;&#x4E3A;<code>&#x5047;</code>&#xFF0C;&#x8BE5;&#x6A21;&#x5757;&#x4E0D;&#x8DDF;&#x8E2A;&#x8FD9;&#x6837;&#x7684;&#x7EDF;&#x8BA1;&#x5E76;&#x59CB;&#x7EC8;&#x4F7F;&#x7528;&#x5728;&#x8BAD;&#x7EC3;&#x548C;eval&#x6A21;&#x5F0F;&#x6279;&#x6B21;&#x7684;&#x7EDF;&#x8BA1;&#x6570;&#x636E;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x771F;</code></p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>&#x8F93;&#x5165;&#xFF1A; &#xFF08; N  &#xFF0C; C  &#xFF09; &#xFF08;N&#xFF0C;C&#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF09; &#x6216; &#xFF08; N  &#xFF0C; C  &#xFF0C; L  &#xFF09; &#xFF08;N&#xFF0C;C&#xFF0C;L&#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF0C; L  &#xFF09;</p>
</li>
<li><p>&#x8F93;&#x51FA;&#xFF1A; &#xFF08; N  &#xFF0C; C  &#xFF09; &#xFF08;N&#xFF0C;C&#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF09; &#x6216; &#xFF08; N  &#xFF0C; C  &#xFF0C; L  &#xFF09; &#xFF08;N&#xFF0C;C&#xFF0C;L&#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF0C; L  &#xFF09; &#xFF08;&#x76F8;&#x540C;&#x5F62;&#x72B6;&#x7684;&#x8F93;&#x5165;&#xFF09;</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; # With Learnable Parameters
&gt;&gt;&gt; m = nn.BatchNorm1d(100)
&gt;&gt;&gt; # Without Learnable Parameters
&gt;&gt;&gt; m = nn.BatchNorm1d(100, affine=False)
&gt;&gt;&gt; input = torch.randn(20, 100)
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="batchnorm2d">BatchNorm2d</h3>
<p><em>class</em><code>torch.nn.``BatchNorm2d</code>( <em>num_features</em> , <em>eps=1e-05</em> , <em>momentum=0.1</em>
, <em>affine=True</em> , <em>track_running_stats=True</em>
)<a href="_modules/torch/nn/modules/batchnorm.html#BatchNorm2d">[source]</a></p>
<p>&#x9002;&#x7528;&#x4F5C;&#x4E3A;&#x7EB8;<a href="https://arxiv.org/abs/1502.03167" target="_blank">&#x6279;&#x6807;&#x51C6;&#x5316;&#x63CF;&#x8FF0;&#x5728;4D&#x8F93;&#x5165;&#x6279;&#x6807;&#x51C6;&#x5316;&#xFF08;&#x7528;&#x53E6;&#x5916;&#x7684;&#x901A;&#x9053;&#x5C3A;&#x5BF8;&#x7684;&#x5C0F;&#x6279;&#x91CF;&#x7684;2D&#x8F93;&#x5165;&#xFF09;&#xFF1A;&#x901A;&#x8FC7;&#x51CF;&#x5C11;&#x5185;&#x90E8;&#x534F;&#x53D8;&#x91CF;&#x79FB;&#x4F4D;</a>&#x52A0;&#x901F;&#x6DF1;&#x7F51;&#x7EDC;&#x8BAD;&#x7EC3;&#x3002;</p>
<p>y=x&#x2212;E[x]Var[x]+&#x3F5;&#x2217;&#x3B3;+&#x3B2;y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] +
\epsilon}} * \gamma + \betay=Var[x]+&#x3F5;&#x200B;x&#x2212;E[x]&#x200B;&#x2217;&#x3B3;+&#x3B2;</p>
<p>The mean and standard-deviation are calculated per-dimension over the mini-
batches and &#x3B3;\gamma&#x3B3; and &#x3B2;\beta&#x3B2; are learnable parameter vectors of size C
(where C is the input size). By default, the elements of &#x3B3;\gamma&#x3B3; are set to 1
and the elements of &#x3B2;\beta&#x3B2; are set to 0.</p>
<p>Also by default, during training this layer keeps running estimates of its
computed mean and variance, which are then used for normalization during
evaluation. The running estimates are kept with a default <code>momentum</code>of 0.1.</p>
<p>If <code>track_running_stats</code>is set to <code>False</code>, this layer then does not keep
running estimates, and batch statistics are instead used during evaluation
time as well.</p>
<p>Note</p>
<p>This <code>momentum</code>argument is different from one used in optimizer classes and
the conventional notion of momentum. Mathematically, the update rule for
running statistics here is x^new=(1&#x2212;momentum)&#xD7;x^+momentum&#xD7;xt\hat{x}_\text{new}
= (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times
x_tx^new&#x200B;=(1&#x2212;momentum)&#xD7;x^+momentum&#xD7;xt&#x200B; , where x^\hat{x}x^ is the estimated
statistic and xtx_txt&#x200B; is the new observed value.</p>
<p>&#x56E0;&#x4E3A;&#x6279;&#x6807;&#x51C6;&#x5316;&#x662F;&#x5728; C &#x7EF4;&#x5B8C;&#x6210;&#x7684;&#xFF0C;&#x5728;&#xFF08;N&#xFF0C;H&#xFF0C;W&#xFF09;&#x5207;&#x7247;&#x8BA1;&#x7B97;&#x7EDF;&#x8BA1;&#x6570;&#x636E;&#xFF0C;&#x8FD9;&#x662F;&#x5171;&#x540C;&#x7684;&#x672F;&#x8BED;&#x6765;&#x8C03;&#x7528;&#x8FD9;&#x4E2A;&#x7A7A;&#x95F4;&#x6279;&#x6807;&#x51C6;&#x5316;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>NUM_FEATURES</strong> -  C  C  C  &#x4ECE;&#x5927;&#x5C0F;&#x7684;&#x9884;&#x671F;&#x8F93;&#x5165; &#xFF08; N  &#xFF0C; C  &#xFF0C; H  &#xFF0C; W  &#xFF09; &#xFF08;N&#xFF0C;C&#xFF0C;H&#xFF0C;W&#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF0C; H  &#xFF0C; W  &#xFF09;</p>
</li>
<li><p><strong>eps</strong> &#x2013; a value added to the denominator for numerical stability. Default: 1e-5</p>
</li>
<li><p><strong>momentum</strong> &#x2013; the value used for the running_mean and running_var computation. Can be set to <code>None</code>for cumulative moving average (i.e. simple average). Default: 0.1</p>
</li>
<li><p><strong>affine</strong> &#x2013; a boolean value that when set to <code>True</code>, this module has learnable affine parameters. Default: <code>True</code></p>
</li>
<li><p><strong>track_running_stats</strong> &#x2013; a boolean value that when set to <code>True</code>, this module tracks the running mean and variance, and when set to <code>False</code>, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: <code>True</code></p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>Input: (N,C,H,W)(N, C, H, W)(N,C,H,W)</p>
</li>
<li><p>Output: (N,C,H,W)(N, C, H, W)(N,C,H,W) (same shape as input)</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; # With Learnable Parameters
&gt;&gt;&gt; m = nn.BatchNorm2d(100)
&gt;&gt;&gt; # Without Learnable Parameters
&gt;&gt;&gt; m = nn.BatchNorm2d(100, affine=False)
&gt;&gt;&gt; input = torch.randn(20, 100, 35, 45)
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="batchnorm3d">BatchNorm3d</h3>
<p><em>class</em><code>torch.nn.``BatchNorm3d</code>( <em>num_features</em> , <em>eps=1e-05</em> , <em>momentum=0.1</em>
, <em>affine=True</em> , <em>track_running_stats=True</em>
)<a href="_modules/torch/nn/modules/batchnorm.html#BatchNorm3d">[source]</a></p>
<p>&#x9002;&#x7528;&#x4F5C;&#x4E3A;&#x7EB8;<a href="https://arxiv.org/abs/1502.03167" target="_blank">&#x6279;&#x6807;&#x51C6;&#x5316;&#x63CF;&#x8FF0;&#x5728;5D&#x8F93;&#x5165;&#x6279;&#x6807;&#x51C6;&#x5316;&#xFF08;&#x7528;&#x53E6;&#x5916;&#x7684;&#x901A;&#x9053;&#x5C3A;&#x5BF8;&#x7684;&#x5C0F;&#x6279;&#x91CF;&#x7684;3D&#x8F93;&#x5165;&#xFF09;&#xFF1A;&#x901A;&#x8FC7;&#x51CF;&#x5C11;&#x5185;&#x90E8;&#x534F;&#x53D8;&#x91CF;&#x79FB;&#x4F4D;</a>&#x52A0;&#x901F;&#x6DF1;&#x7F51;&#x7EDC;&#x8BAD;&#x7EC3;&#x3002;</p>
<p>y=x&#x2212;E[x]Var[x]+&#x3F5;&#x2217;&#x3B3;+&#x3B2;y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] +
\epsilon}} * \gamma + \betay=Var[x]+&#x3F5;&#x200B;x&#x2212;E[x]&#x200B;&#x2217;&#x3B3;+&#x3B2;</p>
<p>The mean and standard-deviation are calculated per-dimension over the mini-
batches and &#x3B3;\gamma&#x3B3; and &#x3B2;\beta&#x3B2; are learnable parameter vectors of size C
(where C is the input size). By default, the elements of &#x3B3;\gamma&#x3B3; are set to 1
and the elements of &#x3B2;\beta&#x3B2; are set to 0.</p>
<p>Also by default, during training this layer keeps running estimates of its
computed mean and variance, which are then used for normalization during
evaluation. The running estimates are kept with a default <code>momentum</code>of 0.1.</p>
<p>If <code>track_running_stats</code>is set to <code>False</code>, this layer then does not keep
running estimates, and batch statistics are instead used during evaluation
time as well.</p>
<p>Note</p>
<p>This <code>momentum</code>argument is different from one used in optimizer classes and
the conventional notion of momentum. Mathematically, the update rule for
running statistics here is x^new=(1&#x2212;momentum)&#xD7;x^+momentum&#xD7;xt\hat{x}_\text{new}
= (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times
x_tx^new&#x200B;=(1&#x2212;momentum)&#xD7;x^+momentum&#xD7;xt&#x200B; , where x^\hat{x}x^ is the estimated
statistic and xtx_txt&#x200B; is the new observed value.</p>
<p>&#x56E0;&#x4E3A;&#x6279;&#x6807;&#x51C6;&#x5316;&#x662F;&#x5728; C &#x7EF4;&#xFF0C;&#x5728;&#x8BA1;&#x7B97;&#x7EDF;&#x8BA1;&#x6570;&#x636E;&#xFF08;N&#xFF0C;d&#xFF0C;H&#xFF0C;W&#xFF09;&#x5207;&#x7247;&#xFF0C;&#x5B83;&#x662F;&#x5E38;&#x89C1;&#x7684;&#x672F;&#x8BED;&#x505A;&#x8C03;&#x7528;&#x6B64;&#x4F53;&#x79EF;&#x6279;&#x6807;&#x51C6;&#x5316;&#x6216;&#x65F6;&#x7A7A;&#x6279;&#x6807;&#x51C6;&#x5316;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>NUM_FEATURES</strong> -  C  C  C  &#x4ECE;&#x5927;&#x5C0F;&#x7684;&#x9884;&#x671F;&#x8F93;&#x5165; &#xFF08; N  &#xFF0C; C  &#xFF0C; d  &#xFF0C; H  &#xFF0C; W  &#xFF09; &#xFF08;N&#xFF0C;C&#xFF0C;d&#xFF0C;H&#xFF0C;W &#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF0C; d  &#xFF0C; H  &#xFF0C; W  &#xFF09;</p>
</li>
<li><p><strong>eps</strong> &#x2013; a value added to the denominator for numerical stability. Default: 1e-5</p>
</li>
<li><p><strong>momentum</strong> &#x2013; the value used for the running_mean and running_var computation. Can be set to <code>None</code>for cumulative moving average (i.e. simple average). Default: 0.1</p>
</li>
<li><p><strong>affine</strong> &#x2013; a boolean value that when set to <code>True</code>, this module has learnable affine parameters. Default: <code>True</code></p>
</li>
<li><p><strong>track_running_stats</strong> &#x2013; a boolean value that when set to <code>True</code>, this module tracks the running mean and variance, and when set to <code>False</code>, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: <code>True</code></p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>&#x8F93;&#x5165;&#xFF1A; &#xFF08; N  &#xFF0C; C  &#xFF0C; d  &#xFF0C; H  &#xFF0C; W  &#xFF09; &#xFF08;N&#xFF0C;C&#xFF0C;d&#xFF0C;H&#xFF0C;W&#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF0C; d  &#xFF0C; H  &#xFF0C; W  &#xFF09;</p>
</li>
<li><p>&#x8F93;&#x51FA;&#xFF1A; &#xFF08; N  &#xFF0C; C  &#xFF0C; d  &#xFF0C; H  &#xFF0C; W  &#xFF09; &#xFF08;N&#xFF0C;C&#xFF0C;d&#xFF0C;H&#xFF0C;W&#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF0C; d  &#xFF0C; H  &#xFF0C; W  &#xFF09; &#xFF08;&#x76F8;&#x540C;&#x5F62;&#x72B6;&#x7684;&#x8F93;&#x5165;&#xFF09;</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; # With Learnable Parameters
&gt;&gt;&gt; m = nn.BatchNorm3d(100)
&gt;&gt;&gt; # Without Learnable Parameters
&gt;&gt;&gt; m = nn.BatchNorm3d(100, affine=False)
&gt;&gt;&gt; input = torch.randn(20, 100, 35, 45, 10)
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="groupnorm">GroupNorm</h3>
<p><em>class</em><code>torch.nn.``GroupNorm</code>( <em>num_groups</em> , <em>num_channels</em> , <em>eps=1e-05</em> ,
<em>affine=True</em>
)<a href="_modules/torch/nn/modules/normalization.html#GroupNorm">[source]</a></p>
<p>&#x5982;&#x5728;&#x6587;&#x732E;<a href="https://arxiv.org/abs/1803.08494" target="_blank">&#x7EC4;&#x89C4;&#x8303;&#x5316;</a>&#x4E2D;&#x63CF;&#x8FF0;&#x7684;&#x5E94;&#x7528;&#x7EC4;&#x89C4;&#x8303;&#x5316;&#x5728;&#x5C0F;&#x6279;&#x91CF;&#x7684;&#x8F93;&#x5165;&#x3002;</p>
<p>y=x&#x2212;E[x]Var[x]+&#x3F5;&#x2217;&#x3B3;+&#x3B2;y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] +
\epsilon}} * \gamma + \beta y=Var[x]+&#x3F5;&#x200B;x&#x2212;E[x]&#x200B;&#x2217;&#x3B3;+&#x3B2;</p>
<p>&#x8F93;&#x5165;&#x901A;&#x9053;&#x5206;&#x79BB;&#x6210;<code>NUM_GROUPS</code>&#x7EC4;&#xFF0C;&#x6BCF;&#x7EC4;&#x5305;&#x542B;<code>NUM_CHANNELS  /  NUM_GROUPS</code>&#x901A;&#x9053;&#x3002;&#x7684;&#x5E73;&#x5747;&#x503C;&#x548C;&#x6807;&#x51C6;&#x504F;&#x5DEE;&#x5728;&#x5404;&#x7EC4;&#x5206;&#x522B;&#x8BA1;&#x7B97;&#x3002;  &#x3B3; \&#x4F3D;&#x9A6C; &#x3B3; &#x548C; &#x3B2; \&#x7684;&#x3B2; &#x3B2; &#x662F;&#x53EF;&#x5B66;&#x4E60;&#x7684;&#x6BCF;&#x4E2A;&#x4FE1;&#x9053;&#x7684;&#x4EFF;&#x5C04;&#x53D8;&#x6362;&#x5927;&#x5C0F;&#x7684;&#x53C2;&#x6570;&#x77E2;&#x91CF;<code>NUM_CHANNELS</code>&#x5982;&#x679C;<code>&#x4EFF;&#x5C04;</code>&#x662F;<code>&#x771F;</code>&#x3002;</p>
<p>&#x8BE5;&#x5C42;&#x4F7F;&#x7528;&#x5728;&#x8BAD;&#x7EC3;&#x548C;&#x8BC4;&#x4EF7;&#x6A21;&#x5F0F;&#x4ECE;&#x8F93;&#x5165;&#x6570;&#x636E;&#x8BA1;&#x7B97;&#x7684;&#x7EDF;&#x8BA1;&#x4FE1;&#x606F;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>NUM_GROUPS</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x57FA;&#x56E2;&#x7684;&#x6570;&#x76EE;&#x7684;&#x4FE1;&#x9053;&#x5206;&#x79BB;&#x6210;</p>
</li>
<li><p><strong>NUM_CHANNELS</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x9884;&#x8BA1;&#x5728;&#x8F93;&#x5165;&#x4FE1;&#x9053;&#x6570;</p>
</li>
<li><p><strong>EPS</strong> - &#x7684;&#x503C;&#x6DFB;&#x52A0;&#x5230;&#x5206;&#x6BCD;&#x6570;&#x503C;&#x7A33;&#x5B9A;&#x6027;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;1E-5</p>
</li>
<li><p><strong>&#x4EFF;&#x5C04;</strong> - &#x4E00;&#x4E2A;&#x5E03;&#x5C14;&#x503C;&#xFF0C;&#x5F53;&#x8BBE;&#x7F6E;&#x4E3A;<code>&#x771F;</code>&#xFF0C;&#x8BE5;&#x6A21;&#x5757;&#x5177;&#x6709;&#x521D;&#x59CB;&#x5316;&#x4E3A;&#x4E00;&#xFF08;&#x7528;&#x4E8E;&#x6743;&#x91CD;&#xFF09;&#x548C;&#x96F6;&#x53EF;&#x5B66;&#x4E60;&#x6BCF;&#x4E2A;&#x4FE1;&#x9053;&#x7684;&#x4EFF;&#x5C04;&#x53C2;&#x6570;&#xFF08;&#x504F;&#x5DEE;&#xFF09;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x771F; [HTG9&#x3002;</code></p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>&#x8F93;&#x5165;&#xFF1A; &#xFF08; N  &#xFF0C; C  &#xFF0C; <em>  &#xFF09; &#xFF08;N&#xFF0C;C&#xFF0C;</em>&#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF0C; *  &#xFF09; &#x5176;&#x4E2D; C  =  NUM_CHANNELS  C = \&#x6587;&#x672C;{NUM \ _channels}  C  =  NUM_CHANNELS </p>
</li>
<li><p>&#x8F93;&#x51FA;&#xFF1A; &#xFF08; N  &#xFF0C; C  &#xFF0C; <em>  &#xFF09; &#xFF08;N&#xFF0C;C&#xFF0C;</em>&#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF0C; *  &#xFF09; &#xFF08;&#x76F8;&#x540C;&#x5F62;&#x72B6;&#x7684;&#x8F93;&#x5165;&#xFF09;</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; input = torch.randn(20, 6, 10, 10)
&gt;&gt;&gt; # Separate 6 channels into 3 groups
&gt;&gt;&gt; m = nn.GroupNorm(3, 6)
&gt;&gt;&gt; # Separate 6 channels into 6 groups (equivalent with InstanceNorm)
&gt;&gt;&gt; m = nn.GroupNorm(6, 6)
&gt;&gt;&gt; # Put all 6 channels into a single group (equivalent with LayerNorm)
&gt;&gt;&gt; m = nn.GroupNorm(1, 6)
&gt;&gt;&gt; # Activating the module
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="syncbatchnorm">SyncBatchNorm</h3>
<p><em>class</em><code>torch.nn.``SyncBatchNorm</code>( <em>num_features</em> , <em>eps=1e-05</em> ,
<em>momentum=0.1</em> , <em>affine=True</em> , <em>track_running_stats=True</em> ,
<em>process_group=None</em>
)<a href="_modules/torch/nn/modules/batchnorm.html#SyncBatchNorm">[source]</a></p>
<p>&#x9002;&#x7528;&#x5728;N&#x7EF4;&#x8F93;&#x5165;&#x6279;&#x6807;&#x51C6;&#x5316;&#xFF08;&#x4E00;&#x5C0F;&#x6279;&#x91CF;&#x7684;[N-2]&#x7528;&#x53E6;&#x5916;&#x7684;&#x901A;&#x9053;&#x5C3A;&#x5BF8;d&#x7684;&#x8F93;&#x5165;&#xFF09;&#xFF0C;&#x5982;&#x6587;&#x732E;<a href="https://arxiv.org/abs/1502.03167" target="_blank">&#x6279;&#x6807;&#x51C6;&#x5316;&#x63CF;&#x8FF0;&#xFF1A;&#x901A;&#x8FC7;&#x51CF;&#x5C11;&#x5185;&#x90E8;&#x534F;&#x53D8;&#x91CF;&#x79FB;&#x52A0;&#x901F;&#x6DF1;&#x7F51;&#x7EDC;&#x8BAD;&#x7EC3;</a>
&#x3002;</p>
<p>y=x&#x2212;E[x]Var[x]+&#x3F5;&#x2217;&#x3B3;+&#x3B2;y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] +
\epsilon}} * \gamma + \betay=Var[x]+&#x3F5;&#x200B;x&#x2212;E[x]&#x200B;&#x2217;&#x3B3;+&#x3B2;</p>
<p>&#x5E73;&#x5747;&#x503C;&#x548C;&#x6807;&#x51C6;&#x504F;&#x5DEE;&#x6BCF;&#x4E2A;&#x7EF4;&#x5EA6;&#x5728;&#x540C;&#x4E00;&#x8FC7;&#x7A0B;&#x7EC4;&#x7684;&#x6240;&#x6709;&#x5C0F;&#x6279;&#x91CF;&#x8BA1;&#x7B97;&#x3002;  &#x3B3; \&#x4F3D;&#x9A6C; &#x3B3; &#x548C; &#x3B2; \&#x7684;&#x3B2; &#x3B2; &#x662F;&#x5927;&#x5C0F;&#x7684;&#x53EF;&#x5B66;&#x4E60;&#x7684;&#x53C2;&#x6570;&#x77E2;&#x91CF; C &#xFF08;&#x5176;&#x4E2D; C
&#x88AB;&#x8F93;&#x5165;&#x5927;&#x5C0F;&#xFF09;&#x3002;&#x9ED8;&#x8BA4;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x3B3;  \&#x4F3D;&#x9A6C; &#x7684; &#x7684;&#x5143;&#x7D20; &#x3B3; &#x662F;&#x4ECE; &#x53D6;&#x6837; U  &#xFF08; 0  &#xFF0C; 1  &#xFF09; \ mathcal {U&#x3011;&#xFF08;0&#xFF0C;1&#xFF09; U  &#xFF08; 0  &#xFF0C;
1  &#xFF09; &#x548C; &#x7684;&#x5143;&#x7D20;&#x3B2; \&#x7684;&#x3B2; &#x3B2; &#x88AB;&#x8BBE;&#x7F6E;&#x4E3A;0&#x3002;</p>
<p>Also by default, during training this layer keeps running estimates of its
computed mean and variance, which are then used for normalization during
evaluation. The running estimates are kept with a default <code>momentum</code>of 0.1.</p>
<p>If <code>track_running_stats</code>is set to <code>False</code>, this layer then does not keep
running estimates, and batch statistics are instead used during evaluation
time as well.</p>
<p>Note</p>
<p>&#x6B64;<code>&#x52A8;&#x91CF;</code>&#x53C2;&#x6570;&#x662F;&#x4ECE;&#x4E00;&#x4E2A;&#x5728;&#x4F18;&#x5316;&#x5668;&#x4E2D;&#x4F7F;&#x7528;&#x7684;&#x7C7B;&#x548C;&#x52A8;&#x91CF;&#x7684;&#x5E38;&#x89C4;&#x6982;&#x5FF5;&#x4E0D;&#x540C;&#x3002;&#x5728;&#x6570;&#x5B66;&#x4E0A;&#xFF0C;&#x8FD9;&#x91CC;&#x8FD0;&#x884C;&#x7EDF;&#x8BA1;&#x6570;&#x636E;&#x7684;&#x66F4;&#x65B0;&#x89C4;&#x5219;&#x4E3A; &#xD7; ^  &#x65B0; =  &#xFF08; 1  -  &#x52A8;&#x91CF; &#xFF09;
&#xD7; &#xD7; ^  +  momemtum  &#xD7; &#xD7; T  \&#x5E3D;&#x5B50;{X} _ \&#x6587;&#x672C;{&#x65B0;} =&#xFF08;1 - \&#x6587;&#x672C;{&#x52A8;&#x91CF;}&#xFF09;\&#x500D;\&#x5E3D;&#x5B50;{X} +
\&#x6587;&#x672C;{momemtum} \&#x500D;X_T  &#xD7; ^  &#x65B0; =  &#xFF08; 1  -  &#x52A8;&#x91CF; &#xFF09; &#xD7; &#xD7; ^  +  momemtum  &#xD7; &#xD7; T  &#xFF0C;&#x5176;&#x4E2D; &#xD7;
^  \&#x5E3D;&#x5B50;{X}  &#xD7; ^  &#x200B;&#x200B;  &#x662F;&#x4F30;&#x8BA1;&#x7684;&#x7EDF;&#x8BA1;&#x91CF;&#x548C; X  T  X_T  &#xD7; T  &#x662F;&#x65B0;&#x7684;&#x89C2;&#x6D4B;&#x503C;&#x3002;</p>
<p>&#x56E0;&#x4E3A;&#x6279;&#x6807;&#x51C6;&#x5316;&#x662F;&#x5728; C &#x7EF4;&#x5B8C;&#x6210;&#x7684;&#xFF0C;&#x5728;&#xFF08;N&#xFF0C;+&#xFF09;&#x5207;&#x7247;&#x8BA1;&#x7B97;&#x7EDF;&#x8BA1;&#x6570;&#x636E;&#xFF0C;&#x8FD9;&#x662F;&#x5171;&#x540C;&#x7684;&#x672F;&#x8BED;&#x6765;&#x8C03;&#x7528;&#x8FD9;&#x4E2A;&#x4F53;&#x79EF;&#x6279;&#x6807;&#x51C6;&#x5316;&#x6216;&#x65F6;&#x7A7A;&#x6279;&#x6807;&#x51C6;&#x5316;&#x3002;</p>
<p>&#x76EE;&#x524D;SyncBatchNorm&#x4EC5;&#x652F;&#x6301;DistributedDataParallel&#x6BCF;&#x4E2A;&#x8FDB;&#x7A0B;&#x5355;GPU&#x3002;&#x4F7F;&#x7528;torch.nn.SyncBatchNorm.convert_sync_batchnorm&#xFF08;&#xFF09;&#x4E0E;DDP&#x5305;&#x88C5;&#x7F51;&#x524D;BatchNorm&#x5C42;&#x8F6C;&#x6362;&#x4E3A;SyncBatchNorm&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>NUM_FEATURES</strong> -  C  C  C  &#x4ECE;&#x5927;&#x5C0F;&#x7684;&#x9884;&#x671F;&#x8F93;&#x5165; &#xFF08; N  &#xFF0C; C  &#xFF0C; +  &#xFF09; &#xFF08;N&#xFF0C;C&#xFF0C;+&#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF0C; +  &#xFF09;</p>
</li>
<li><p><strong>eps</strong> &#x2013; a value added to the denominator for numerical stability. Default: 1e-5</p>
</li>
<li><p><strong>momentum</strong> &#x2013; the value used for the running_mean and running_var computation. Can be set to <code>None</code>for cumulative moving average (i.e. simple average). Default: 0.1</p>
</li>
<li><p><strong>affine</strong> &#x2013; a boolean value that when set to <code>True</code>, this module has learnable affine parameters. Default: <code>True</code></p>
</li>
<li><p><strong>track_running_stats</strong> &#x2013; a boolean value that when set to <code>True</code>, this module tracks the running mean and variance, and when set to <code>False</code>, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: <code>True</code></p>
</li>
<li><p><strong>process_group</strong> - &#x7EDF;&#x8BA1;&#x7684;&#x540C;&#x6B65;&#x6BCF;&#x4E2A;&#x8FDB;&#x7A0B;&#x7EC4;&#x5185;&#x53D1;&#x751F;&#x72EC;&#x7ACB;&#x3002;&#x9ED8;&#x8BA4;&#x884C;&#x4E3A;&#x662F;&#x5728;&#x6574;&#x4E2A;&#x4E16;&#x754C;&#x540C;&#x6B65;</p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>&#x8F93;&#x5165;&#xFF1A; &#xFF08; N  &#xFF0C; C  &#xFF0C; +  &#xFF09; &#xFF08;N&#xFF0C;C&#xFF0C;+&#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF0C; +  &#xFF09;</p>
</li>
<li><p>&#x8F93;&#x51FA;&#xFF1A; &#xFF08; N  &#xFF0C; C  &#xFF0C; +  &#xFF09; &#xFF08;N&#xFF0C;C&#xFF0C;+&#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF0C; +  &#xFF09; &#xFF08;&#x76F8;&#x540C;&#x5F62;&#x72B6;&#x7684;&#x8F93;&#x5165;&#xFF09;</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; # With Learnable Parameters
&gt;&gt;&gt; m = nn.SyncBatchNorm(100)
&gt;&gt;&gt; # creating process group (optional)
&gt;&gt;&gt; # process_ids is a list of int identifying rank ids.
&gt;&gt;&gt; process_group = torch.distributed.new_group(process_ids)
&gt;&gt;&gt; # Without Learnable Parameters
&gt;&gt;&gt; m = nn.BatchNorm3d(100, affine=False, process_group=process_group)
&gt;&gt;&gt; input = torch.randn(20, 100, 35, 45, 10)
&gt;&gt;&gt; output = m(input)

&gt;&gt;&gt; # network is nn.BatchNorm layer
&gt;&gt;&gt; sync_bn_network = nn.SyncBatchNorm.convert_sync_batchnorm(network, process_group)
&gt;&gt;&gt; # only single gpu per process is currently supported
&gt;&gt;&gt; ddp_sync_bn_network = torch.nn.parallel.DistributedDataParallel(
&gt;&gt;&gt;                         sync_bn_network,
&gt;&gt;&gt;                         device_ids=[args.local_rank],
&gt;&gt;&gt;                         output_device=args.local_rank)
</code></pre><p><em>classmethod</em><code>convert_sync_batchnorm</code>( <em>module</em> , <em>process_group=None</em>
)<a href="_modules/torch/nn/modules/batchnorm.html#SyncBatchNorm.convert_sync_batchnorm">[source]</a></p>
<p>&#x8F85;&#x52A9;&#x51FD;&#x6570;&#x6765;&#x5728;&#x6A21;&#x578B;&#x4E3A; torch.nn.SyncBatchNorm &#x5C42; torch.nn.BatchNormND &#x5C42;&#x8F6C;&#x6362;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x6A21;&#x5757;</strong> &#xFF08; <em>nn.Module</em> &#xFF09; - &#x5305;&#x542B;&#x6A21;&#x5757;</p>
</li>
<li><p><strong>process_group</strong> &#xFF08; <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5904;&#x7406;&#x7EC4;&#x8303;&#x56F4;&#x7684;&#x540C;&#x6B65;&#xFF0C;</p>
</li>
</ul>
<p>&#x9ED8;&#x8BA4;&#x662F;&#x6574;&#x4E2A;&#x4E16;&#x754C;</p>
<p>Returns</p>
<p>&#x539F;&#x59CB;&#x6A21;&#x5757;&#x4E0E;&#x8F6C;&#x5316; torch.nn.SyncBatchNorm &#x5C42;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; # Network with nn.BatchNorm layer
&gt;&gt;&gt; module = torch.nn.Sequential(
&gt;&gt;&gt;            torch.nn.Linear(20, 100),
&gt;&gt;&gt;            torch.nn.BatchNorm1d(100)
&gt;&gt;&gt;          ).cuda()
&gt;&gt;&gt; # creating process group (optional)
&gt;&gt;&gt; # process_ids is a list of int identifying rank ids.
&gt;&gt;&gt; process_group = torch.distributed.new_group(process_ids)
&gt;&gt;&gt; sync_bn_module = convert_sync_batchnorm(module, process_group)
</code></pre><h3 id="instancenorm1d">InstanceNorm1d</h3>
<p><em>class</em><code>torch.nn.``InstanceNorm1d</code>( <em>num_features</em> , <em>eps=1e-05</em> ,
<em>momentum=0.1</em> , <em>affine=False</em> , <em>track_running_stats=False</em>
)<a href="_modules/torch/nn/modules/instancenorm.html#InstanceNorm1d">[source]</a></p>
<p>&#x9002;&#x7528;&#x5B9E;&#x4F8B;&#x6B63;&#x5E38;&#x5316;&#x4E86;&#x4F5C;&#x4E3A;&#x5728;&#x7EB8;<a href="https://arxiv.org/abs/1607.08022" target="_blank">&#x5B9E;&#x4F8B;&#x89C4;&#x8303;&#x5316;&#x63CF;&#x8FF0;&#x7684;3D&#x8F93;&#x5165;&#xFF08;&#x5E26;&#x6709;&#x53EF;&#x9009;&#x7684;&#x9644;&#x52A0;&#x7684;&#x4FE1;&#x9053;&#x5C3A;&#x5BF8;&#x7684;&#x5C0F;&#x6279;&#x91CF;&#x7684;1D&#x8F93;&#x5165;&#xFF09;&#xFF1A;&#x7528;&#x4E8E;&#x5FEB;&#x901F;&#x7A0B;&#x5F0F;&#x5316;&#x5931;&#x8E2A;&#x7684;&#x6210;&#x5206;</a>&#x3002;</p>
<p>y=x&#x2212;E[x]Var[x]+&#x3F5;&#x2217;&#x3B3;+&#x3B2;y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] +
\epsilon}} * \gamma + \betay=Var[x]+&#x3F5;&#x200B;x&#x2212;E[x]&#x200B;&#x2217;&#x3B3;+&#x3B2;</p>
<p>&#x7684;&#x5E73;&#x5747;&#x503C;&#x548C;&#x6807;&#x51C6;&#x504F;&#x5DEE;&#x662F;&#x6BCF;&#x4E2A;&#x7EF4;&#x5EA6;&#x5206;&#x522B;&#x8BA1;&#x7B97;&#x7528;&#x4E8E;&#x5728;&#x5C0F;&#x6279;&#x91CF;&#x7684;&#x6BCF;&#x4E2A;&#x5BF9;&#x8C61;&#x3002;  &#x3B3; \&#x4F3D;&#x9A6C; &#x3B3; &#x548C; &#x3B2; \&#x7684;&#x3B2; &#x3B2; &#x662F;&#x5927;&#x5C0F;&#x7684;&#x53EF;&#x5B66;&#x4E60;&#x7684;&#x53C2;&#x6570;&#x77E2;&#x91CF; C &#xFF08;&#x5176;&#x4E2D; C
&#x88AB;&#x8F93;&#x5165;&#x5C3A;&#x5BF8;&#xFF09;&#x5982;&#x679C;<code>&#x4EFF;&#x5C04;</code>&#x662F;<code>&#x771F;</code>&#x3002;</p>
<p>&#x9ED8;&#x8BA4;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x8BE5;&#x5C42;&#x4F7F;&#x7528;&#x5728;&#x8BAD;&#x7EC3;&#x548C;&#x8BC4;&#x4EF7;&#x6A21;&#x5F0F;&#x4ECE;&#x8F93;&#x5165;&#x6570;&#x636E;&#x8BA1;&#x7B97;&#x5B9E;&#x4F8B;&#x7684;&#x7EDF;&#x8BA1;&#x6570;&#x636E;&#x3002;</p>
<p>&#x5982;&#x679C;<code>track_running_stats</code>&#x88AB;&#x8BBE;&#x5B9A;&#x4E3A;<code>&#x771F;</code>&#xFF0C;&#x5728;&#x8BAD;&#x7EC3;&#x671F;&#x95F4;&#x8BE5;&#x5C42;&#x4FDD;&#x6301;&#x8FD0;&#x884C;&#x800C;&#x5176;&#x8BA1;&#x7B97;&#x5747;&#x503C;&#x548C;&#x65B9;&#x5DEE;&#xFF0C;&#x7136;&#x540E;&#x5C06;&#x5176;&#x7528;&#x4E8E;&#x4F30;&#x8BA1;&#x8BC4;&#x4F30;&#x671F;&#x95F4;&#x6B63;&#x5E38;&#x5316;&#x3002;&#x6B63;&#x5728;&#x8FD0;&#x884C;&#x7684;&#x4F30;&#x8BA1;&#x662F;&#x4FDD;&#x6301;&#x4E86;&#x9ED8;&#x8BA4;&#x7684;<code>&#x52BF;&#x5934;</code>0.1&#x3002;</p>
<p>Note</p>
<p>This <code>momentum</code>argument is different from one used in optimizer classes and
the conventional notion of momentum. Mathematically, the update rule for
running statistics here is x^new=(1&#x2212;momentum)&#xD7;x^+momemtum&#xD7;xt\hat{x}_\text{new}
= (1 - \text{momentum}) \times \hat{x} + \text{momemtum} \times
x_tx^new&#x200B;=(1&#x2212;momentum)&#xD7;x^+momemtum&#xD7;xt&#x200B; , where x^\hat{x}x^ is the estimated
statistic and xtx_txt&#x200B; is the new observed value.</p>
<p>Note</p>
<p><code>InstanceNorm1d</code>&#x548C; <code>LayerNorm</code>&#x975E;&#x5E38;&#x76F8;&#x4F3C;&#xFF0C;&#x4F46;&#x6709;&#x4E00;&#x4E9B;&#x7EC6;&#x5FAE;&#x7684;&#x5DEE;&#x522B;&#x3002;<code>InstanceNorm1d</code>
&#x52A0;&#x5230;&#x7B49;&#x591A;&#x7EF4;&#x65F6;&#x95F4;&#x5E8F;&#x5217;&#x5F15;&#x5BFC;&#x6570;&#x636E;&#x7684;&#x6BCF;&#x4E2A;&#x4FE1;&#x9053;&#xFF0C;&#x4F46; <code>LayerNorm</code>&#x901A;&#x5E38;&#x65BD;&#x52A0;&#x5728;&#x6574;&#x4E2A;&#x6837;&#x672C;&#x5E76;&#x7ECF;&#x5E38;&#x5728;NLP&#x4EFB;&#x52A1;&#x3002; Additionaly&#xFF0C; <code>LayerNorm</code>&#x9002;&#x7528;&#x7684;elementwise&#x4EFF;&#x5C04;&#x53D8;&#x6362;&#xFF0C;&#x800C; <code>InstanceNorm1d</code>&#x901A;&#x5E38;&#x4E0D;&#x5E94;&#x7528;&#x4EFF;&#x5C04;&#x53D8;&#x6362;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>num_features</strong> &#x2013; CCC from an expected input of size (N,C,L)(N, C, L)(N,C,L) or LLL from input of size (N,L)(N, L)(N,L)</p>
</li>
<li><p><strong>eps</strong> &#x2013; a value added to the denominator for numerical stability. Default: 1e-5</p>
</li>
<li><p><strong>&#x52A8;&#x91CF;</strong> - &#x7528;&#x4E8E;running_mean&#x548C;running_var&#x8BA1;&#x7B97;&#x7684;&#x503C;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;0.1</p>
</li>
<li><p><strong>&#x4EFF;&#x5C04;</strong> - &#x4E3A;&#x5B8C;&#x6210;&#x4E00;&#x4E2A;&#x5E03;&#x5C14;&#x503C;&#xFF0C;&#x5F53;&#x8BBE;&#x7F6E;&#x4E3A;<code>&#x771F;</code>&#xFF0C;&#x8BE5;&#x6A21;&#x5757;&#x5177;&#x6709;&#x53EF;&#x5B66;&#x4E60;&#x4EFF;&#x5C04;&#x53C2;&#x6570;&#xFF0C;&#x521D;&#x59CB;&#x5316;&#x7684;&#x76F8;&#x540C;&#x7684;&#x65B9;&#x5F0F;&#x8FDB;&#x884C;&#x6279;&#x91CF;&#x6807;&#x51C6;&#x5316;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x5047; [HTG9&#x3002;</code></p>
</li>
<li><p><strong>track_running_stats</strong> - &#x5F53;&#x8BBE;&#x7F6E;&#x4E3A;<code>&#x771F;</code>&#xFF0C;&#x6B64;&#x6A21;&#x5757;&#x8DDF;&#x8E2A;&#x7684;&#x8FD0;&#x884C;&#x5747;&#x503C;&#x548C;&#x65B9;&#x5DEE;&#xFF0C;&#x548C;&#x4E00;&#x4E2A;&#x5E03;&#x5C14;&#x503C;&#xFF0C;&#x5F53;&#x8BBE;&#x7F6E;&#x4E3A;<code>&#x5047;</code>&#xFF0C;&#x8BE5;&#x6A21;&#x5757;&#x4E0D;&#x8DDF;&#x8E2A;&#x8FD9;&#x6837;&#x7684;&#x7EDF;&#x8BA1;&#x5E76;&#x59CB;&#x7EC8;&#x4F7F;&#x7528;&#x5728;&#x8BAD;&#x7EC3;&#x548C;eval&#x6A21;&#x5F0F;&#x6279;&#x6B21;&#x7684;&#x7EDF;&#x8BA1;&#x6570;&#x636E;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x5047;</code></p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>&#x8F93;&#x5165;&#xFF1A; &#xFF08; N  &#xFF0C; C  &#xFF0C; L  &#xFF09; &#xFF08;N&#xFF0C;C&#xFF0C;L&#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF0C; L  &#xFF09;</p>
</li>
<li><p>&#x8F93;&#x51FA;&#xFF1A; &#xFF08; N  &#xFF0C; C  &#xFF0C; L  &#xFF09; &#xFF08;N&#xFF0C;C&#xFF0C;L&#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF0C; L  &#xFF09; &#xFF08;&#x76F8;&#x540C;&#x5F62;&#x72B6;&#x7684;&#x8F93;&#x5165;&#xFF09;</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; # Without Learnable Parameters
&gt;&gt;&gt; m = nn.InstanceNorm1d(100)
&gt;&gt;&gt; # With Learnable Parameters
&gt;&gt;&gt; m = nn.InstanceNorm1d(100, affine=True)
&gt;&gt;&gt; input = torch.randn(20, 100, 40)
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="instancenorm2d">InstanceNorm2d</h3>
<p><em>class</em><code>torch.nn.``InstanceNorm2d</code>( <em>num_features</em> , <em>eps=1e-05</em> ,
<em>momentum=0.1</em> , <em>affine=False</em> , <em>track_running_stats=False</em>
)<a href="_modules/torch/nn/modules/instancenorm.html#InstanceNorm2d">[source]</a></p>
<p>&#x9002;&#x7528;&#x5B9E;&#x4F8B;&#x6B63;&#x5E38;&#x5316;&#x4E86;&#x4F5C;&#x4E3A;&#x5728;&#x7EB8;<a href="https://arxiv.org/abs/1607.08022" target="_blank">&#x5B9E;&#x4F8B;&#x89C4;&#x8303;&#x5316;&#x63CF;&#x8FF0;&#x7684;4D&#x8F93;&#x5165;&#xFF08;&#x7528;&#x53E6;&#x5916;&#x7684;&#x901A;&#x9053;&#x5C3A;&#x5BF8;&#x7684;&#x5C0F;&#x6279;&#x91CF;&#x7684;2D&#x8F93;&#x5165;&#xFF09;&#xFF1A;&#x7528;&#x4E8E;&#x5FEB;&#x901F;&#x7A0B;&#x5F0F;&#x5316;&#x5931;&#x8E2A;&#x7684;&#x6210;&#x5206;</a>&#x3002;</p>
<p>y=x&#x2212;E[x]Var[x]+&#x3F5;&#x2217;&#x3B3;+&#x3B2;y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] +
\epsilon}} * \gamma + \betay=Var[x]+&#x3F5;&#x200B;x&#x2212;E[x]&#x200B;&#x2217;&#x3B3;+&#x3B2;</p>
<p>The mean and standard-deviation are calculated per-dimension separately for
each object in a mini-batch. &#x3B3;\gamma&#x3B3; and &#x3B2;\beta&#x3B2; are learnable parameter
vectors of size C (where C is the input size) if <code>affine</code>is <code>True</code>.</p>
<p>By default, this layer uses instance statistics computed from input data in
both training and evaluation modes.</p>
<p>If <code>track_running_stats</code>is set to <code>True</code>, during training this layer keeps
running estimates of its computed mean and variance, which are then used for
normalization during evaluation. The running estimates are kept with a default
<code>momentum</code>of 0.1.</p>
<p>Note</p>
<p>This <code>momentum</code>argument is different from one used in optimizer classes and
the conventional notion of momentum. Mathematically, the update rule for
running statistics here is x^new=(1&#x2212;momentum)&#xD7;x^+momemtum&#xD7;xt\hat{x}_\text{new}
= (1 - \text{momentum}) \times \hat{x} + \text{momemtum} \times
x_tx^new&#x200B;=(1&#x2212;momentum)&#xD7;x^+momemtum&#xD7;xt&#x200B; , where x^\hat{x}x^ is the estimated
statistic and xtx_txt&#x200B; is the new observed value.</p>
<p>Note</p>
<p><code>InstanceNorm2d</code>&#x548C; <code>LayerNorm</code>&#x975E;&#x5E38;&#x76F8;&#x4F3C;&#xFF0C;&#x4F46;&#x6709;&#x4E00;&#x4E9B;&#x7EC6;&#x5FAE;&#x7684;&#x5DEE;&#x522B;&#x3002;<code>InstanceNorm2d</code>
&#x52A0;&#x5230;&#x50CF;RGB&#x56FE;&#x50CF;&#x5F15;&#x5BFC;&#x6570;&#x636E;&#x7684;&#x6BCF;&#x4E2A;&#x4FE1;&#x9053;&#xFF0C;&#x4F46; <code>LayerNorm</code>&#x901A;&#x5E38;&#x65BD;&#x52A0;&#x5728;&#x6574;&#x4E2A;&#x6837;&#x672C;&#x5E76;&#x7ECF;&#x5E38;&#x5728;NLP&#x4EFB;&#x52A1;&#x3002; Additionaly&#xFF0C; <code>LayerNorm</code>&#x9002;&#x7528;&#x7684;elementwise&#x4EFF;&#x5C04;&#x53D8;&#x6362;&#xFF0C;&#x800C; <code>InstanceNorm2d</code>&#x901A;&#x5E38;&#x4E0D;&#x5E94;&#x7528;&#x4EFF;&#x5C04;&#x53D8;&#x6362;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>num_features</strong> &#x2013; CCC from an expected input of size (N,C,H,W)(N, C, H, W)(N,C,H,W)</p>
</li>
<li><p><strong>eps</strong> &#x2013; a value added to the denominator for numerical stability. Default: 1e-5</p>
</li>
<li><p><strong>momentum</strong> &#x2013; the value used for the running_mean and running_var computation. Default: 0.1</p>
</li>
<li><p><strong>affine</strong> &#x2013; a boolean value that when set to <code>True</code>, this module has learnable affine parameters, initialized the same way as done for batch normalization. Default: <code>False</code>.</p>
</li>
<li><p><strong>track_running_stats</strong> &#x2013; a boolean value that when set to <code>True</code>, this module tracks the running mean and variance, and when set to <code>False</code>, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: <code>False</code></p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>Input: (N,C,H,W)(N, C, H, W)(N,C,H,W)</p>
</li>
<li><p>Output: (N,C,H,W)(N, C, H, W)(N,C,H,W) (same shape as input)</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; # Without Learnable Parameters
&gt;&gt;&gt; m = nn.InstanceNorm2d(100)
&gt;&gt;&gt; # With Learnable Parameters
&gt;&gt;&gt; m = nn.InstanceNorm2d(100, affine=True)
&gt;&gt;&gt; input = torch.randn(20, 100, 35, 45)
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="instancenorm3d">InstanceNorm3d</h3>
<p><em>class</em><code>torch.nn.``InstanceNorm3d</code>( <em>num_features</em> , <em>eps=1e-05</em> ,
<em>momentum=0.1</em> , <em>affine=False</em> , <em>track_running_stats=False</em>
)<a href="_modules/torch/nn/modules/instancenorm.html#InstanceNorm3d">[source]</a></p>
<p>&#x9002;&#x7528;&#x5B9E;&#x4F8B;&#x6B63;&#x5E38;&#x5316;&#x4E86;&#x4F5C;&#x4E3A;&#x5728;&#x7EB8;<a href="https://arxiv.org/abs/1607.08022" target="_blank">&#x5B9E;&#x4F8B;&#x89C4;&#x8303;&#x5316;&#x63CF;&#x8FF0;&#x7684;5D&#x8F93;&#x5165;&#xFF08;&#x7528;&#x53E6;&#x5916;&#x7684;&#x901A;&#x9053;&#x5C3A;&#x5BF8;&#x7684;&#x5C0F;&#x6279;&#x91CF;&#x7684;3D&#x8F93;&#x5165;&#xFF09;&#xFF1A;&#x7528;&#x4E8E;&#x5FEB;&#x901F;&#x7A0B;&#x5F0F;&#x5316;&#x5931;&#x8E2A;&#x7684;&#x6210;&#x5206;</a>&#x3002;</p>
<p>y=x&#x2212;E[x]Var[x]+&#x3F5;&#x2217;&#x3B3;+&#x3B2;y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] +
\epsilon}} * \gamma + \betay=Var[x]+&#x3F5;&#x200B;x&#x2212;E[x]&#x200B;&#x2217;&#x3B3;+&#x3B2;</p>
<p>&#x7684;&#x5E73;&#x5747;&#x503C;&#x548C;&#x6807;&#x51C6;&#x504F;&#x5DEE;&#x662F;&#x6BCF;&#x4E2A;&#x7EF4;&#x5EA6;&#x5206;&#x522B;&#x8BA1;&#x7B97;&#x7528;&#x4E8E;&#x5728;&#x5C0F;&#x6279;&#x91CF;&#x7684;&#x6BCF;&#x4E2A;&#x5BF9;&#x8C61;&#x3002;  &#x3B3; \&#x4F3D;&#x9A6C; &#x3B3; &#x548C; &#x3B2; \&#x7684;&#x3B2; &#x3B2; &#x662F;&#x5C3A;&#x5BF8;&#x4E3A;C&#x7684;&#x53EF;&#x5B66;&#x4E60;&#x7684;&#x53C2;&#x6570;&#x5411;&#x91CF;&#xFF08;&#x5176;&#x4E2D;C&#x662F;&#x8F93;&#x5165;&#x5927;&#x5C0F;&#xFF09;&#x5982;&#x679C;<code>&#x4EFF;&#x5C04;</code>&#x662F;<code>&#x771F;</code>&#x3002;</p>
<p>By default, this layer uses instance statistics computed from input data in
both training and evaluation modes.</p>
<p>If <code>track_running_stats</code>is set to <code>True</code>, during training this layer keeps
running estimates of its computed mean and variance, which are then used for
normalization during evaluation. The running estimates are kept with a default
<code>momentum</code>of 0.1.</p>
<p>Note</p>
<p>This <code>momentum</code>argument is different from one used in optimizer classes and
the conventional notion of momentum. Mathematically, the update rule for
running statistics here is x^new=(1&#x2212;momentum)&#xD7;x^+momemtum&#xD7;xt\hat{x}_\text{new}
= (1 - \text{momentum}) \times \hat{x} + \text{momemtum} \times
x_tx^new&#x200B;=(1&#x2212;momentum)&#xD7;x^+momemtum&#xD7;xt&#x200B; , where x^\hat{x}x^ is the estimated
statistic and xtx_txt&#x200B; is the new observed value.</p>
<p>Note</p>
<p><code>InstanceNorm3d</code>&#x548C; <code>LayerNorm</code>&#x975E;&#x5E38;&#x76F8;&#x4F3C;&#xFF0C;&#x4F46;&#x6709;&#x4E00;&#x4E9B;&#x7EC6;&#x5FAE;&#x7684;&#x5DEE;&#x522B;&#x3002;<code>InstanceNorm3d</code>
&#x52A0;&#x5230;&#x50CF;&#x7684;&#x4E09;&#x7EF4;&#x6A21;&#x578B;&#x4E0E;RGB&#x5F69;&#x8272;&#x5F15;&#x5BFC;&#x6570;&#x636E;&#x7684;&#x6BCF;&#x4E2A;&#x4FE1;&#x9053;&#xFF0C;&#x4F46; <code>LayerNorm</code>&#x901A;&#x5E38;&#x65BD;&#x52A0;&#x5728;&#x6574;&#x4E2A;&#x6837;&#x672C;&#x5E76;&#x7ECF;&#x5E38;&#x5728;NLP&#x4EFB;&#x52A1;&#x3002; Additionaly&#xFF0C; <code>LayerNorm</code>&#x9002;&#x7528;&#x7684;elementwise&#x4EFF;&#x5C04;&#x53D8;&#x6362;&#xFF0C;&#x800C; <code>InstanceNorm3d</code>&#x901A;&#x5E38;&#x4E0D;&#x5E94;&#x7528;&#x4EFF;&#x5C04;&#x53D8;&#x6362;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>num_features</strong> &#x2013; CCC from an expected input of size (N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W)</p>
</li>
<li><p><strong>eps</strong> &#x2013; a value added to the denominator for numerical stability. Default: 1e-5</p>
</li>
<li><p><strong>momentum</strong> &#x2013; the value used for the running_mean and running_var computation. Default: 0.1</p>
</li>
<li><p><strong>affine</strong> &#x2013; a boolean value that when set to <code>True</code>, this module has learnable affine parameters, initialized the same way as done for batch normalization. Default: <code>False</code>.</p>
</li>
<li><p><strong>track_running_stats</strong> &#x2013; a boolean value that when set to <code>True</code>, this module tracks the running mean and variance, and when set to <code>False</code>, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: <code>False</code></p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>Input: (N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W)</p>
</li>
<li><p>Output: (N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W) (same shape as input)</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; # Without Learnable Parameters
&gt;&gt;&gt; m = nn.InstanceNorm3d(100)
&gt;&gt;&gt; # With Learnable Parameters
&gt;&gt;&gt; m = nn.InstanceNorm3d(100, affine=True)
&gt;&gt;&gt; input = torch.randn(20, 100, 35, 45, 10)
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="layernorm">LayerNorm</h3>
<p><em>class</em><code>torch.nn.``LayerNorm</code>( <em>normalized_shape</em> , <em>eps=1e-05</em> ,
<em>elementwise_affine=True</em>
)<a href="_modules/torch/nn/modules/normalization.html#LayerNorm">[source]</a></p>
<p>&#x4F5C;&#x4E3A;&#x7EB8;&#x5F20;<a href="https://arxiv.org/abs/1607.06450" target="_blank">&#x56FE;&#x5C42;&#x89C4;&#x8303;&#x5316;</a>&#x4E0A;&#x8FF0;&#x9002;&#x7528;&#x56FE;&#x5C42;&#x89C4;&#x8303;&#x5316;&#x5728;&#x5C0F;&#x6279;&#x91CF;&#x7684;&#x8F93;&#x5165;&#x3002;</p>
<p>y=x&#x2212;E[x]Var[x]+&#x3F5;&#x2217;&#x3B3;+&#x3B2;y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] +
\epsilon}} * \gamma + \beta y=Var[x]+&#x3F5;&#x200B;x&#x2212;E[x]&#x200B;&#x2217;&#x3B3;+&#x3B2;</p>
<p>&#x7684;&#x5E73;&#x5747;&#x503C;&#x548C;&#x6807;&#x51C6;&#x504F;&#x5DEE;&#x90FD;&#x5728;&#x5176;&#x5FC5;&#x987B;&#x7531;<code>normalized_shape</code>&#x4E2D;&#x6307;&#x5B9A;&#x7684;&#x5F62;&#x72B6;&#x7684;&#x6700;&#x540E;&#x82E5;&#x5E72;&#x5C3A;&#x5BF8;&#x5206;&#x522B;&#x8BA1;&#x7B97;&#x3002;  &#x3B3; \&#x4F3D;&#x9A6C; &#x3B3; &#x548C; &#x3B2; \&#x7684;&#x3B2; &#x3B2;
&#x662F;&#x53EF;&#x5B66;&#x4E60;&#x7684;&#x4EFF;&#x5C04;&#x53D8;&#x6362;&#x7684;<code>&#x53C2;&#x6570;normalized_shape</code>&#x5982;&#x679C;<code>elementwise_affine</code>&#x662F;<code>&#x771F;</code>&#x3002;</p>
<p>Note</p>
<p>&#x4E0D;&#x50CF;&#x6279;&#x91CF;&#x89C4;&#x8303;&#x5316;&#x4E0E;&#x5B9E;&#x4F8B;&#x6807;&#x51C6;&#x5316;&#xFF0C;&#x5B83;&#x9002;&#x7528;&#x6807;&#x91CF;&#x6BD4;&#x4F8B;&#x548C;&#x504F;&#x538B;&#x7528;&#x4E8E;&#x4E0E;<code>&#x4EFF;&#x5C04;</code>&#x9009;&#x9879;&#xFF0C;&#x5C42;&#x6B63;&#x5E38;&#x5316;&#x5E94;&#x7528;&#x4E8E;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x89C4;&#x6A21;&#x548C;&#x504F;&#x538B;<code>[&#x6BCF;&#x4E2A;&#x6574;&#x4E2A;&#x4FE1;&#x9053;/&#x5E73;&#x9762;HTG5]
elementwise_affine</code>&#x3002;</p>
<p>This layer uses statistics computed from input data in both training and
evaluation modes.</p>
<p>Parameters</p>
<ul>
<li><strong>normalized_shape</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#x6216;</em> <a href="https://docs.python.org/3/library/stdtypes.html#list" title="\(in Python v3.7\)" target="_blank"> <em>&#x5217;&#x8868;</em> </a> <em>&#x6216;</em> <em>torch.Size</em> &#xFF09; - </li>
</ul>
<p>&#x4ECE;&#x5C3A;&#x5BF8;&#x7684;&#x671F;&#x671B;&#x7684;&#x8F93;&#x5165;&#x7684;&#x8F93;&#x5165;&#x5F62;&#x72B6;</p>
<p>[&#x2217;&#xD7;normalized_shape[0]&#xD7;normalized_shape[1]&#xD7;&#x2026;&#xD7;normalized_shape[&#x2212;1]][* \times
\text{normalized\_shape}[0] \times \text{normalized\_shape}[1] \times \ldots
\times \text{normalized\_shape}[-1]]
[&#x2217;&#xD7;normalized_shape[0]&#xD7;normalized_shape[1]&#xD7;&#x2026;&#xD7;normalized_shape[&#x2212;1]]</p>
<p>&#x5982;&#x679C;&#x4F7F;&#x7528;&#x4E00;&#x4E2A;&#x6574;&#x6570;&#xFF0C;&#x5B83;&#x88AB;&#x89C6;&#x4E3A;&#x4E00;&#x4E2A;&#x5355;&#x72EC;&#x5217;&#x8868;&#xFF0C;&#x5E76;&#x4E14;&#x8BE5;&#x6A21;&#x5757;&#x5C06;&#x5728;&#x6B63;&#x5E38;&#x5316;&#xFF0C;&#x9884;&#x8BA1;&#x5C06;&#x7279;&#x5B9A;&#x5927;&#x5C0F;&#x7684;&#x6700;&#x540E;&#x4E00;&#x4E2A;&#x7EF4;&#x5EA6;&#x3002;</p>
<ul>
<li><p><strong>eps</strong> &#x2013; a value added to the denominator for numerical stability. Default: 1e-5</p>
</li>
<li><p><strong>elementwise_affine</strong> - &#x4E00;&#x4E2A;&#x5E03;&#x5C14;&#x503C;&#xFF0C;&#x5F53;&#x8BBE;&#x7F6E;&#x4E3A;<code>&#x771F;</code>&#xFF0C;&#x8BE5;&#x6A21;&#x5757;&#x5177;&#x6709;&#x521D;&#x59CB;&#x5316;&#x4E3A;&#x4E00;&#xFF08;&#x7528;&#x4E8E;&#x6743;&#x91CD;&#xFF09;&#x548C;&#x96F6;&#x53EF;&#x5B66;&#x4E60;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x4EFF;&#x5C04;&#x53C2;&#x6570;&#xFF08;&#x504F;&#x5DEE;&#xFF09;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x771F; [HTG9&#x3002;</code></p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>&#x8F93;&#x5165;&#xFF1A; &#xFF08; N  &#xFF0C; <em>  &#xFF09; &#xFF08;N&#xFF0C;</em>&#xFF09; &#xFF08; N  &#xFF0C; *  &#xFF09;</p>
</li>
<li><p>&#x8F93;&#x51FA;&#xFF1A; &#xFF08; N  &#xFF0C; <em>  &#xFF09; &#xFF08;N&#xFF0C;</em>&#xFF09; &#xFF08; N  &#xFF0C; *  &#xFF09; &#xFF08;&#x76F8;&#x540C;&#x5F62;&#x72B6;&#x7684;&#x8F93;&#x5165;&#xFF09;</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; input = torch.randn(20, 5, 10, 10)
&gt;&gt;&gt; # With Learnable Parameters
&gt;&gt;&gt; m = nn.LayerNorm(input.size()[1:])
&gt;&gt;&gt; # Without Learnable Parameters
&gt;&gt;&gt; m = nn.LayerNorm(input.size()[1:], elementwise_affine=False)
&gt;&gt;&gt; # Normalize over last two dimensions
&gt;&gt;&gt; m = nn.LayerNorm([10, 10])
&gt;&gt;&gt; # Normalize over last dimension of size 10
&gt;&gt;&gt; m = nn.LayerNorm(10)
&gt;&gt;&gt; # Activating the module
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="localresponsenorm">LocalResponseNorm</h3>
<p><em>class</em><code>torch.nn.``LocalResponseNorm</code>( <em>size</em> , <em>alpha=0.0001</em> , <em>beta=0.75</em> ,
<em>k=1.0</em>
)<a href="_modules/torch/nn/modules/normalization.html#LocalResponseNorm">[source]</a></p>
<p>&#x9002;&#x7528;&#x5728;&#x51E0;&#x4E2A;&#x8F93;&#x5165;&#x5E73;&#x9762;&#xFF0C;&#x5176;&#x4E2D;&#x4FE1;&#x9053;&#x5360;&#x7528;&#x6240;&#x8FF0;&#x7B2C;&#x4E8C;&#x7EF4;&#x7EC4;&#x6210;&#x7684;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x54CD;&#x5E94;&#x7684;&#x672C;&#x5730;&#x5F52;&#x4E00;&#x5316;&#x3002;&#x9002;&#x7528;&#x8DE8;&#x6E20;&#x9053;&#x6B63;&#x5E38;&#x5316;&#x3002;</p>
<p>bc=ac(k+&#x3B1;n&#x2211;c&#x2032;=max&#x2061;(0,c&#x2212;n/2)min&#x2061;(N&#x2212;1,c+n/2)ac&#x2032;2)&#x2212;&#x3B2;b<em>{c} = a</em>{c}\left(k +
\frac{\alpha}{n} \sum<em>{c&apos;=\max(0,
c-n/2)}^{\min(N-1,c+n/2)}a</em>{c&apos;}^2\right)^{-\beta}
bc&#x200B;=ac&#x200B;&#x239D;&#x239B;&#x200B;k+n&#x3B1;&#x200B;c&#x2032;=max(0,c&#x2212;n/2)&#x2211;min(N&#x2212;1,c+n/2)&#x200B;ac&#x2032;2&#x200B;&#x23A0;&#x239E;&#x200B;&#x2212;&#x3B2;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x5927;&#x5C0F;</strong> - &#x7528;&#x4E8E;&#x6807;&#x51C6;&#x5316;&#x76F8;&#x90BB;&#x4FE1;&#x9053;&#x7684;&#x91CF;</p>
</li>
<li><p><strong>&#x963F;&#x5C14;&#x6CD5;</strong> - &#x4E58;&#x6CD5;&#x56E0;&#x5B50;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;0.0001</p>
</li>
<li><p><strong>&#x7684;&#x3B2;</strong> - &#x6307;&#x6570;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;0.75</p>
</li>
<li><p><strong>K</strong> - &#x52A0;&#x6CD5;&#x56E0;&#x5B50;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;1</p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>Input: (N,C,&#x2217;)(N, C, *)(N,C,&#x2217;)</p>
</li>
<li><p>Output: (N,C,&#x2217;)(N, C, *)(N,C,&#x2217;) (same shape as input)</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; lrn = nn.LocalResponseNorm(2)
&gt;&gt;&gt; signal_2d = torch.randn(32, 5, 24, 24)
&gt;&gt;&gt; signal_4d = torch.randn(16, 5, 7, 7, 7, 7)
&gt;&gt;&gt; output_2d = lrn(signal_2d)
&gt;&gt;&gt; output_4d = lrn(signal_4d)
</code></pre><h2 id="&#x590D;&#x53D1;&#x6027;&#x5C42;">&#x590D;&#x53D1;&#x6027;&#x5C42;</h2>
<h3 id="rnn">RNN</h3>
<p><em>class</em><code>torch.nn.``RNN</code>( <em>*args</em> , <em>**kwargs</em>
)<a href="_modules/torch/nn/modules/rnn.html#RNN">[source]</a></p>
<p>&#x9002;&#x7528;&#x7684;&#x591A;&#x5C42;&#x57C3;&#x5C14;&#x66FC;RNN&#x4E0E; T  &#x4E00; n&#x7684; H  &#x7684;tanh  T  &#x4E00; n&#x7684; H  &#x6216; R  E  L  U  RELU  R  E  L  U
&#x975E;&#x7EBF;&#x6027;&#x5230;&#x8F93;&#x5165;&#x5E8F;&#x5217;&#x3002;</p>
<p>&#x5728;&#x8F93;&#x5165;&#x5E8F;&#x5217;&#x4E2D;&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x4EF6;&#x4E2D;&#xFF0C;&#x6BCF;&#x4E2A;&#x5C42;&#x8BA1;&#x7B97;&#x4E0B;&#x9762;&#x7684;&#x51FD;&#x6570;&#xFF1A;</p>
<p>ht=tanh(Wihxt+bih+Whhh(t&#x2212;1)+bhh)h<em>t = \text{tanh}(W</em>{ih} x<em>t + b</em>{ih} + W<em>{hh}
h</em>{(t-1)} + b_{hh}) ht&#x200B;=tanh(Wih&#x200B;xt&#x200B;+bih&#x200B;+Whh&#x200B;h(t&#x2212;1)&#x200B;+bhh&#x200B;)</p>
<p>&#x5176;&#x4E2D; H  T  h<em>t  H  T  &#x662F;&#x5728;&#x65F6;&#x523B; T &#x9690;&#x85CF;&#x72B6;&#x6001;&#xFF0C; &#xD7; T  X_T  &#xD7; T  &#x662F;&#x5728;&#x65F6;&#x523B; T &#x7684;&#x8F93;&#x5165;&#xFF0C;&#x5E76; H  &#xFF08; T  -  1  &#xFF09;
[HTG135 1 H </em> {&#xFF08;T-1&#xFF09;}  H  &#xFF08; T  -  1  &#xFF09; &#x662F;&#x4EE5;&#x524D;&#x7684;&#x5C42;&#x4E2D;&#x7684;&#x65F6;&#x95F4;&#x7684;&#x9690;&#x85CF;&#x72B6;&#x6001; T-1 &#x6216;&#x5728;&#x65F6;&#x95F4;&#x7684;&#x521D;&#x59CB;&#x9690;&#x85CF;&#x72B6;&#x6001; 0  &#x3002;&#x5982;&#x679C;<code>&#x975E;&#x7EBF;&#x6027;</code>&#x662F;<code>&apos;RELU&apos;</code>&#xFF0C;&#x5219;&#x4F7F;&#x7528; RELU &#x800C;&#x975E;&#x7684;tanh  &#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input_size</strong> - &#x7684;&#x9884;&#x671F;&#x529F;&#x80FD;&#x5728;&#x8F93;&#x5165;&#xD7;&#x4E2A;&#x6570;</p>
</li>
<li><p><strong>hidden_&#x200B;&#x200B;size</strong> - &#x7684;&#x7279;&#x5F81;&#x5728;&#x9690;&#x85CF;&#x72B6;&#x6001; h&#x5C06;&#x6570;</p>
</li>
<li><p><strong>num_layers</strong> - &#x590D;&#x53D1;&#x5C42;&#x6570;&#x3002;&#x4F8B;&#x5982;&#xFF0C;&#x8BBE;&#x7F6E;<code>num_layers = 2</code>&#x5C06;&#x610F;&#x5473;&#x7740;&#x5806;&#x53E0;&#x4E24;&#x4E2A;RNNs&#x5728;&#x4E00;&#x8D77;&#x4EE5;&#x5F62;&#x6210;&#x5C42;&#x53E0;RNN &#xFF0C;&#x4E0E;&#x7B2C;&#x4E8C;RNN&#x53D6;&#x5165;&#x7B2C;&#x4E00;RNN&#x7684;&#x8F93;&#x51FA;&#x548C;&#x8BA1;&#x7B97;&#x6240;&#x8FF0;&#x6700;&#x540E;&#x7684;&#x7ED3;&#x679C;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;1</p>
</li>
<li><p><strong>&#x975E;&#x7EBF;&#x6027;</strong> - &#x975E;&#x7EBF;&#x6027;&#x4F7F;&#x7528;&#x3002;&#x53EF;&#x4EE5;&#x662F;<code>&apos;&#x7684;tanh&apos;</code>&#x6216;<code>&apos;RELU&apos;</code>&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&apos;&#x7684;tanh&apos;</code></p>
</li>
<li><p><strong>&#x504F;&#x538B;</strong> - &#x82E5;<code>&#x5047;</code>&#xFF0C;&#x5219;&#x8BE5;&#x5C42;&#x4E0D;&#x4F7F;&#x7528;&#x504F;&#x538B;&#x6743;&#x91CD; b_ih &#x548C; b_hh &#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x771F;</code></p>
</li>
<li><p><strong>batch_first</strong> - &#x82E5;<code>&#x771F;</code>&#xFF0C;&#x5219;&#x8F93;&#x5165;&#x548C;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x88AB;&#x8BBE;&#x7F6E;&#x4E3A;&#xFF08;&#x5206;&#x6279;&#xFF0C;SEQ&#xFF0C;&#x7279;&#x5F81;&#xFF09;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x5047;</code></p>
</li>
<li><p><strong>&#x5DEE;</strong> - &#x5982;&#x679C;&#x4E0D;&#x4E3A;&#x96F6;&#xFF0C;&#x4ECB;&#x7ECD;&#x7B49;&#x4E8E;<code>&#x6F0F;&#x5931;&#x4E00;&#x4E2A;&#x964D;&#x4E0A;&#x9664;&#x4E86;&#x6700;&#x540E;&#x5C42;&#x5404;RNN&#x5C42;&#x7684;&#x8F93;&#x51FA;&#x5C42;&#xFF0C;&#x7528;&#x5DEE;&#x6982;&#x7387;</code>&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;0</p>
</li>
<li><p><strong>&#x53CC;&#x5411;</strong> - &#x82E5;<code>&#x771F;</code>&#xFF0C;&#x6210;&#x4E3A;&#x53CC;&#x5411;RNN&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x5047;</code></p>
</li>
</ul>
<p>Inputs: input, h_0</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#x7684;&#x5F62;&#x72B6;&#xFF08;seq_len&#xFF0C;&#x5206;&#x6279;&#xFF0C;input_size&#xFF09;&#xFF1A;&#x5F20;&#x91CF;&#x5305;&#x542B;&#x8F93;&#x5165;&#x5E8F;&#x5217;&#x7684;&#x7279;&#x5F81;&#x3002;&#x8F93;&#x5165;&#x4E5F;&#x53EF;&#x4EE5;&#x662F;&#x586B;&#x5145;&#x53EF;&#x53D8;&#x957F;&#x5EA6;&#x5E8F;&#x5217;&#x3002;&#x53C2;&#x89C1; <code>torch.nn.utils.rnn.pack_padded_sequence&#xFF08;&#xFF09;</code>&#x6216; <code>torch.nn.utils.rnn.pack_sequence&#xFF08; &#xFF09;</code>&#x7684;&#x8BE6;&#x7EC6;&#x4FE1;&#x606F;&#x3002;</p>
</li>
<li><p><strong>H_0</strong> &#x7684;&#x5F62;&#x72B6;&#xFF08;num<em>layers * num_directions&#xFF0C;&#x5206;&#x6279;&#xFF0C;hidden</em>&#x200B;&#x200B;size&#xFF09;&#xFF1A;&#x5F20;&#x91CF;&#x5305;&#x542B;&#x6240;&#x8FF0;&#x521D;&#x59CB;&#x72B6;&#x6001;&#x9690;&#x85CF;&#x5728;&#x6279;&#x6B21;&#x4E2D;&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x4EF6;&#x3002;&#x9ED8;&#x8BA4;&#x4E3A;&#x96F6;&#xFF0C;&#x5982;&#x679C;&#x4E0D;&#x63D0;&#x4F9B;&#x3002;&#x5982;&#x679C;RNN&#x662F;&#x53CC;&#x5411;&#x7684;&#xFF0C;num_directions&#x5E94;&#x8BE5;&#x662F;2&#xFF0C;&#x5426;&#x5219;&#x5B83;&#x5E94;&#x8BE5;&#x662F;1&#x3002;</p>
</li>
</ul>
<p>Outputs: output, h_n</p>
<ul>
<li><strong>&#x8F93;&#x51FA;</strong> &#x5F62;&#x72B6;&#x7684;&#xFF08;seq<em>len&#xFF0C;&#x5206;&#x6279;&#xFF0C;num_directions * hidden</em>&#x200B;&#x200B;size&#xFF09;&#xFF1A;&#x5F20;&#x91CF;&#x542B;&#x6709;&#x6765;&#x81EA;RNN&#x7684;&#x6700;&#x540E;&#x4E00;&#x5C42;&#x7684;&#x8F93;&#x51FA;&#x7279;&#x5F81;&#xFF08; h_t &#xFF09;&#xFF0C;&#x5BF9;&#x4E8E;&#x6BCF;&#x4E00;&#x4E2A; T &#x3002;&#x5982;&#x679C; <code>torch.nn.utils.rnn.PackedSequence</code>&#x5DF2;&#x88AB;&#x7ED9;&#x5B9A;&#x4E3A;&#x8F93;&#x5165;&#xFF0C;&#x8F93;&#x51FA;&#x4E5F;&#x5C06;&#x662F;&#x4E00;&#x4E2A;&#x62E5;&#x6324;&#x7684;&#x5E8F;&#x5217;&#x3002;</li>
</ul>
<p>&#x5BF9;&#x4E8E;&#x89E3;&#x538B;&#x7F29;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x8BE5;&#x65B9;&#x5411;&#x53EF;&#x4F7F;&#x7528;<code>output.view&#x5206;&#x79BB;&#xFF08;seq_len&#xFF0C; &#x6279;&#x6B21;&#xFF0C; num_directions&#xFF0C; hidden_&#x200B;&#x200B;size&#xFF09;</code>&#xFF0C;&#x4E0E;&#x5411;&#x524D;&#x548C;&#x5411;&#x540E;&#x65B9;&#x5411;&#x4E3A; 0 &#x548C; 1 &#x5206;&#x522B;&#x3002;&#x7C7B;&#x4F3C;&#x5730;&#xFF0C;&#x65B9;&#x5411;&#x53EF;&#x4EE5;&#x5728;&#x5806;&#x79EF;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#x88AB;&#x5206;&#x79BB;&#x3002;</p>
<ul>
<li><strong>h_n</strong> &#x7684;&#x5F62;&#x72B6;&#xFF08;num<em>layers * num_directions&#xFF0C;&#x5206;&#x6279;&#xFF0C;hidden</em>&#x200B;&#x200B;size&#xFF09;&#xFF1A;&#x5F20;&#x91CF;&#x542B;&#x6709; T = seq_len &#x9690;&#x85CF;&#x72B6;&#x6001;&#x3002;</li>
</ul>
<p>&#x50CF; <em>&#x8F93;&#x51FA;</em> &#x65F6;&#xFF0C;&#x5C42;&#x53EF;&#x4F7F;&#x7528;<code>h_n.view&#xFF08;num_layers&#x5206;&#x79BB;&#xFF0C; num_directions&#xFF0C; &#x6279;&#x6B21;&#xFF0C; hidden_&#x200B;&#x200B;size &#xFF09;</code>&#x3002;</p>
<p>Shape:</p>
<ul>
<li><p>&#x8F93;&#x5165;1&#xFF1A; &#xFF08; L  &#xFF0C; N  &#xFF0C; H  i&#x7684; n&#x7684; &#xFF09; &#xFF08;L&#xFF0C;N&#xFF0C;H<em> {&#x5728;}&#xFF09; &#xFF08; L  &#xFF0C; N  &#xFF0C; H  i&#x7684; n&#x7684; &#xFF09; [HTG89&#x542B;&#x6709;&#x5F20;&#x91CF;&#x8F93;&#x5165;&#x529F;&#x80FD;&#xFF0C;&#x5176;&#x4E2D; H  i&#x7684; [HT G102] n&#x7684;  =  input_size  H</em> {IN} = \&#x6587;&#x672C;{&#x8F93;&#x5165;\ _size}  H  i&#x7684; n&#x7684; =  input_size  &#x548C; L &#x8868;&#x793A;&#x5E8F;&#x5217;&#x957F;&#x5EA6;&#x3002;</p>
</li>
<li><p>&#x8F93;&#x5165;2&#xFF1A; &#xFF08; S  &#xFF0C; N  &#xFF0C; H  O  U  T  &#xFF09; &#xFF08;S&#xFF0C;N&#xFF0C;H<em> {&#x51FA;}&#xFF09; &#xFF08; S  &#xFF0C; N  &#xFF0C; H  O  U  T  &#xFF09;  [HTG93&#x542B;&#x6709;&#x521D;&#x59CB;&#x9690;&#x85CF;&#x72B6;&#x6001;&#x5728;&#x6279;&#x6B21;&#x4E2D;&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x4EF6;&#x5F20;&#x91CF;&#x3002;  H  O  U  T  =  hidden</em>&#x200B;&#x200B;size  H<em> {&#x51FA;} = \&#x6587;&#x672C;{&#x9690;&#x85CF;\ _size}  H  O  U  T  =  hidden</em>&#x200B;&#x200B;size  &#x5982;&#x679C;&#x4E0D;&#x8BBE;&#x7F6E;&#x7F3A;&#x7701;&#x503C;&#x4E3A;&#x96F6;&#x3002;&#x5176;&#x4E2D; S  =  num_layers  <em>  num_directions  S = \&#x6587;&#x672C;{NUM \ _layers} </em> \&#x6587;&#x672C;{NUM \ _directions}  S  =  num_layers  *  num_directions  [HTG237&#x5982;&#x679C;RNN&#x662F;&#x53CC;&#x5411;&#x7684;&#xFF0C;num_directions&#x5E94;&#x8BE5;&#x662F;2&#xFF0C;&#x5426;&#x5219;&#x5B83;&#x5E94;&#x4E3A;1&#x3002;</p>
</li>
<li><p>&#x8F93;&#x51FA;1&#xFF1A; &#xFF08; L  &#xFF0C; N  &#xFF0C; H  &#x4E00; L  L  &#xFF09; &#xFF08;L&#xFF0C;N&#xFF0C;H<em> {&#x6240;&#x6709;}&#xFF09; &#xFF08; L  &#xFF0C; N  &#xFF0C; H  &#x4E00; L  L  &#xFF09;  &#x5176;&#x4E2D; H  &#x4E00;&#x4E2A;[H TG105]  L  L  =  num_directions  *  hidden</em>&#x200B;&#x200B;size  H<em> {&#x6240;&#x6709;} = \&#x6587;&#x672C;{NUM \ _directions} <em> \&#x6587;&#x672C;{&#x9690;&#x85CF;\ _size}  H  &#x4E00; L  L  =  num_directions  </em>  hidden</em>&#x200B;&#x200B;size  [HT G193] </p>
</li>
<li><p>OUTPUT2&#xFF1A; &#xFF08; S  &#xFF0C; N  &#xFF0C; H  O  U  T  &#xFF09; &#xFF08;S&#xFF0C;N&#xFF0C;H_ {&#x51FA;}&#xFF09; &#xFF08; S  &#xFF0C; N  &#xFF0C; H  O  U  T  &#xFF09;  &#x5305;&#x542B;&#x4E0B;&#x4E00;&#x4E2A;&#x9690;&#x85CF;&#x72B6;&#x6001;&#x5728;&#x6279;&#x6B21;&#x4E2D;&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x4EF6;&#x5F20;&#x91CF;</p>
</li>
</ul>
<p>Variables</p>
<ul>
<li><p><strong>&#x301C;RNN.weight_ih_l [k]&#x7684;</strong> - &#x7B2C;k&#x5C42;&#x7684;&#x53EF;&#x5B66;&#x4E60;&#x8F93;&#x5165;&#x9690;&#x85CF;&#x91CD;&#x91CF;&#xFF0C;&#x5F62;&#x72B6;&#xFF08;hidden<em>&#x200B;&#x200B;size&#xFF0C;input_size&#xFF09;&#x7684;&#x4E3A; K = 0  &#x3002;&#x5426;&#x5219;&#xFF0C;&#x8BE5;&#x5F62;&#x72B6;&#x662F;&#xFF08;hidden</em>&#x200B;&#x200B;size&#xFF0C;num<em>directions * hidden</em>&#x200B;&#x200B;size&#xFF09;</p>
</li>
<li><p><strong>&#x301C;RNN.weight_hh_l [k]&#x7684;</strong> - &#x7B2C;k&#x5C42;&#x7684;&#x53EF;&#x5B66;&#x4E60;&#x9690;&#x85CF;&#x7684;&#x6743;&#x91CD;&#xFF0C;&#x5F62;&#x72B6;&#x7684;&#xFF08;hidden<em>&#x200B;&#x200B;size&#xFF0C;hidden</em>&#x200B;&#x200B;size&#xFF09;</p>
</li>
<li><p><strong>&#x301C;RNN.bias_ih_l [k]&#x7684;</strong> - &#x7B2C;k&#x5C42;&#x7684;&#x53EF;&#x5B66;&#x4E60;&#x8F93;&#x5165;&#x9690;&#x85CF;&#x504F;&#x538B;&#xFF0C;&#x5F62;&#x72B6;&#x7684;&#xFF08;hidden_&#x200B;&#x200B;size&#xFF09;</p>
</li>
<li><p><strong>&#x301C;RNN.bias_hh_l [k]&#x7684;</strong> - &#x7B2C;k&#x5C42;&#x7684;&#x53EF;&#x5B66;&#x4E60;&#x9690;&#x85CF;&#x504F;&#x538B;&#xFF0C;&#x5F62;&#x72B6;&#x7684;&#xFF08;hidden_&#x200B;&#x200B;size&#xFF09;</p>
</li>
</ul>
<p>Note</p>
<p>&#x6240;&#x6709;&#x7684;&#x91CD;&#x91CF;&#x548C;&#x504F;&#x89C1;&#x4ECE; &#x521D;&#x59CB;&#x5316;U  &#xFF08; -  K  &#xFF0C; K  &#xFF09; \ mathcal&#x3010;U}&#xFF08; - \ SQRT {K}&#xFF0C;\ SQRT {K}&#xFF09; U  &#xFF08; -
K  &#xFF0C; K  &#xFF09; &#x5176;&#x4E2D; K  =  1  hidden<em>&#x200B;&#x200B;size  K = \&#x538B;&#x88C2;{1} {\&#x6587;&#x672C;{&#x9690;&#x85CF;\ _size}}  K  =
hidden</em>&#x200B;&#x200B;size  1  [HTG19 3]</p>
<p>Note</p>
<p>&#x5982;&#x679C;&#x6EE1;&#x8DB3;&#x4EE5;&#x4E0B;&#x6761;&#x4EF6;&#xFF1A;1&#xFF09;&#x4F7F;&#x80FD;cudnn&#xFF0C;2&#xFF09;&#x8F93;&#x5165;&#x7684;&#x6570;&#x636E;&#x662F;&#x5728;GPU&#x4E0A;3&#xFF09;&#x7684;&#x8F93;&#x5165;&#x6570;&#x636E;&#x5DF2;&#x7ECF;DTYPE <code>torch.float16</code>4&#xFF09;V100
GPU&#x65F6;&#xFF0C;5 &#xFF09;&#x8F93;&#x5165;&#x6570;&#x636E;&#x4E0D;&#x662F;&#x5728;<code>PackedSequence</code>&#x683C;&#x5F0F;&#x6301;&#x7EED;&#x7B97;&#x6CD5;&#x53EF;&#x7ECF;&#x9009;&#x62E9;&#x4EE5;&#x63D0;&#x9AD8;&#x6027;&#x80FD;&#x3002;</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; rnn = nn.RNN(10, 20, 2)
&gt;&gt;&gt; input = torch.randn(5, 3, 10)
&gt;&gt;&gt; h0 = torch.randn(2, 3, 20)
&gt;&gt;&gt; output, hn = rnn(input, h0)
</code></pre><h3 id="lstm">LSTM</h3>
<p><em>class</em><code>torch.nn.``LSTM</code>( <em>*args</em> , <em>**kwargs</em>
)<a href="_modules/torch/nn/modules/rnn.html#LSTM">[source]</a></p>
<p>&#x9002;&#x7528;&#x7684;&#x591A;&#x5C42;&#x957F;&#x77ED;&#x671F;&#x8BB0;&#x5FC6;&#xFF08;LSTM&#xFF09;RNN&#x5230;&#x8F93;&#x5165;&#x5E8F;&#x5217;&#x3002;</p>
<p>For each element in the input sequence, each layer computes the following
function:</p>
<p>it=&#x3C3;(Wiixt+bii+Whih(t&#x2212;1)+bhi)ft=&#x3C3;(Wifxt+bif+Whfh(t&#x2212;1)+bhf)gt=tanh&#x2061;(Wigxt+big+Whgh(t&#x2212;1)+bhg)ot=&#x3C3;(Wioxt+bio+Whoh(t&#x2212;1)+bho)ct=ft&#x2217;c(t&#x2212;1)+it&#x2217;gtht=ot&#x2217;tanh&#x2061;(ct)\begin{array}{ll}
\\ i<em>t = \sigma(W</em>{ii} x<em>t + b</em>{ii} + W<em>{hi} h</em>{(t-1)} + b<em>{hi}) \\ f_t =
\sigma(W</em>{if} x<em>t + b</em>{if} + W<em>{hf} h</em>{(t-1)} + b<em>{hf}) \\ g_t = \tanh(W</em>{ig}
x<em>t + b</em>{ig} + W<em>{hg} h</em>{(t-1)} + b<em>{hg}) \\ o_t = \sigma(W</em>{io} x<em>t + b</em>{io}</p>
<ul>
<li>W<em>{ho} h</em>{(t-1)} + b<em>{ho}) \\ c_t = f_t * c</em>{(t-1)} + i_t <em> g_t \\ h_t =
o_t </em> \tanh(c_t) \\ \end{array}
it&#x200B;=&#x3C3;(Wii&#x200B;xt&#x200B;+bii&#x200B;+Whi&#x200B;h(t&#x2212;1)&#x200B;+bhi&#x200B;)ft&#x200B;=&#x3C3;(Wif&#x200B;xt&#x200B;+bif&#x200B;+Whf&#x200B;h(t&#x2212;1)&#x200B;+bhf&#x200B;)gt&#x200B;=tanh(Wig&#x200B;xt&#x200B;+big&#x200B;+Whg&#x200B;h(t&#x2212;1)&#x200B;+bhg&#x200B;)ot&#x200B;=&#x3C3;(Wio&#x200B;xt&#x200B;+bio&#x200B;+Who&#x200B;h(t&#x2212;1)&#x200B;+bho&#x200B;)ct&#x200B;=ft&#x200B;&#x2217;c(t&#x2212;1)&#x200B;+it&#x200B;&#x2217;gt&#x200B;ht&#x200B;=ot&#x200B;&#x2217;tanh(ct&#x200B;)&#x200B;</li>
</ul>
<p>&#x5176;&#x4E2D; H  T  h<em>t  H  T  &#x662F;&#x5728;&#x65F6;&#x523B; T &#x9690;&#x85CF;&#x72B6;&#x6001;&#xFF0C; C  T  C_T  C  T  &#x662F;&#x5728;&#x65F6;&#x523B; T &#x5C0F;&#x533A;&#x72B6;&#x6001;&#xFF0C; &#xD7; T  X_T  &#xD7; T
&#x662F;&#x8F93;&#x5165;&#x65F6;&#x523B; T &#xFF0C; H  &#xFF08; T  -  1  &#xFF09; [HTG191 1 H </em> {&#xFF08;T-1&#xFF09;}  [HTG19 6]  H  &#xFF08; T  -  1  &#xFF09;
&#x662F;&#x7684;&#x9690;&#x85CF;&#x72B6;&#x6001;&#x5C42;&#x65F6;&#x523B; T-1 &#x6216;&#x65F6;&#x523B; 0 &#x521D;&#x59CB;&#x9690;&#x85CF;&#x72B6;&#x6001;&#xFF0C;&#x548C; i&#x7684; T  I_T  i&#x7684;&#x200B;&#x200B;  T  [HTG2 85]  &#xFF0C; F  T  F_T  F  T
&#xFF0C; &#x514B; T  G_T  &#x514B; [HT G383]  T  &#xFF0C; O  T  O_t&#x540C; O  T  &#x662F;&#x8F93;&#x5165;&#xFF0C;&#x5FD8;&#x8BB0;&#xFF0C;&#x7EC6;&#x80DE;&#xFF0C;&#x548C;&#x8F93;&#x51FA;&#x95E8;&#xFF0C;&#x5206;&#x522B;&#x3002;  &#x3C3; \&#x897F;&#x683C;&#x739B; &#x3C3;
&#x662F;S&#x5F62;&#x51FD;&#x6570;&#xFF0C;&#x5E76; <em>  </em>  *  &#x4E3A;Hadamard&#x4E58;&#x79EF;&#x3002;</p>
<p>&#x5728;&#x591A;&#x5C42;LSTM&#xFF0C;&#x8F93;&#x5165; &#xD7; T  &#xFF08; L  &#xFF09; &#xD7;^ {&#xFF08;L&#xFF09;} <em> T  &#xD7; T  &#xFF08; L  &#xFF09; &#x7684; L  L  L  &#x7B2C;&#x5C42;&#xFF08; L  &amp; GT ;  =
2  L &amp; GT ; = 2  L  &amp; GT ;  =  2  &#xFF09;&#x662F;&#x9690;&#x85CF;&#x72B6;&#x6001; H  T  &#xFF08; L  -  1  &#xFF09; H ^ {&#xFF08;L-1&#xFF09;} </em> T
H  T  &#xFF08; L  -  1  &#xFF09; &#x5148;&#x524D;&#x5C42;&#x4E58;&#x4EE5;&#x7684;&#x8131;&#x843D; &#x3B4; T  &#xFF08; L  -  1  &#xFF09; \&#x589E;&#x91CF;^ {&#xFF08;L-1&#xFF09;} <em> T  &#x3B4; &#x200B;&#x200B;  T  &#xFF08; L
-  1  &#xFF09; &#x5176;&#x4E2D;&#x5404;HTG316 ]  &#x3B4; T  &#xFF08; L  -  1  &#xFF09; \&#x589E;&#x91CF;^ {&#xFF08;L-1&#xFF09;} </em> T  &#x3B4; T  &#xFF08; L  -  1  &#xFF09;
&#x662F;&#x4F2F;&#x52AA;&#x5229;&#x968F;&#x673A;&#x53D8;&#x91CF;&#xFF0C;&#x5B83;&#x662F; 0  0  0  &#x7684;&#x6982;&#x7387;<code>&#x6F0F;&#x5931;</code>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input_size</strong> &#x2013; The number of expected features in the input x</p>
</li>
<li><p><strong>hidden_size</strong> &#x2013; The number of features in the hidden state h</p>
</li>
<li><p><strong>num_layers</strong> - &#x590D;&#x53D1;&#x5C42;&#x6570;&#x3002;&#x4F8B;&#x5982;&#xFF0C;&#x8BBE;&#x7F6E;<code>num_layers = 2</code>&#x5C06;&#x610F;&#x5473;&#x7740;&#x5806;&#x53E0;&#x4E24;&#x4E2A;LSTMs&#x5728;&#x4E00;&#x8D77;&#x4EE5;&#x5F62;&#x6210;&#x5C42;&#x53E0;LSTM &#xFF0C;&#x4E0E;&#x7B2C;&#x4E8C;LSTM&#x53D6;&#x5165;&#x8F93;&#x51FA;&#x7B2C;&#x4E00;LSTM&#x7684;&#x548C;&#x8BA1;&#x7B97;&#x7684;&#x6700;&#x540E;&#x7684;&#x7ED3;&#x679C;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;1</p>
</li>
<li><p><strong>bias</strong> &#x2013; If <code>False</code>, then the layer does not use bias weights b_ih and b_hh. Default: <code>True</code></p>
</li>
<li><p><strong>batch_first</strong> - &#x82E5;<code>&#x771F;</code>&#xFF0C;&#x5219;&#x8F93;&#x5165;&#x548C;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x88AB;&#x8BBE;&#x7F6E;&#x4E3A;&#xFF08;&#x6279;&#x6B21;&#xFF0C;SEQ&#xFF0C;&#x7279;&#x5F81;&#xFF09;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x5047;</code></p>
</li>
<li><p><strong>&#x5DEE;</strong> - &#x5982;&#x679C;&#x4E0D;&#x4E3A;&#x96F6;&#xFF0C;&#x4ECB;&#x7ECD;&#x7B49;&#x4E8E;<code>&#x6F0F;&#x5931;&#x4E00;&#x4E2A;&#x964D;&#x4E0A;&#x9664;&#x4E86;&#x6700;&#x540E;&#x5C42;&#x5404;LSTM&#x5C42;&#x7684;&#x8F93;&#x51FA;&#x5C42;&#xFF0C;&#x7528;&#x5DEE;&#x6982;&#x7387;</code>&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;0</p>
</li>
<li><p><strong>&#x53CC;&#x5411;</strong> - &#x82E5;<code>&#x771F;</code>&#xFF0C;&#x6210;&#x4E3A;&#x53CC;&#x5411;LSTM&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x5047;</code></p>
</li>
</ul>
<p>Inputs: input, (h_0, c_0)</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#x7684;&#x5F62;&#x72B6;&#xFF08;seq_len&#xFF0C;&#x5206;&#x6279;&#xFF0C;input_size&#xFF09;&#xFF1A;&#x5F20;&#x91CF;&#x5305;&#x542B;&#x8F93;&#x5165;&#x5E8F;&#x5217;&#x7684;&#x7279;&#x5F81;&#x3002;&#x8F93;&#x5165;&#x4E5F;&#x53EF;&#x4EE5;&#x662F;&#x586B;&#x5145;&#x53EF;&#x53D8;&#x957F;&#x5EA6;&#x5E8F;&#x5217;&#x3002;&#x53C2;&#x89C1; <code>torch.nn.utils.rnn.pack_padded_sequence&#xFF08;&#xFF09;</code>&#x6216; <code>torch.nn.utils.rnn.pack_sequence&#xFF08; &#xFF09;</code>&#x7684;&#x8BE6;&#x7EC6;&#x4FE1;&#x606F;&#x3002;</p>
</li>
<li><p><strong>H_0</strong> &#x7684;&#x5F62;&#x72B6;&#xFF08;num<em>layers * num_directions&#xFF0C;&#x5206;&#x6279;&#xFF0C;hidden</em>&#x200B;&#x200B;size&#xFF09;&#xFF1A;&#x5F20;&#x91CF;&#x5305;&#x542B;&#x6240;&#x8FF0;&#x521D;&#x59CB;&#x72B6;&#x6001;&#x9690;&#x85CF;&#x5728;&#x6279;&#x6B21;&#x4E2D;&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x4EF6;&#x3002;&#x5982;&#x679C;LSTM&#x662F;&#x53CC;&#x5411;&#x7684;&#xFF0C;num_directions&#x5E94;&#x8BE5;&#x662F;2&#xFF0C;&#x5426;&#x5219;&#x5B83;&#x5E94;&#x8BE5;&#x662F;1&#x3002;</p>
</li>
<li><p>&#x7684;&#x5F62;&#x72B6;&#xFF08;num<em>layers * num_directions&#xFF0C;&#x5206;&#x6279;&#xFF0C;hidden</em>&#x200B;&#x200B;size&#xFF09; <strong>C_0</strong> &#xFF1A;&#x5F20;&#x91CF;&#x5305;&#x542B;&#x6240;&#x8FF0;&#x521D;&#x59CB;&#x5C0F;&#x533A;&#x72B6;&#x6001;&#x4E3A;&#x6279;&#x4E2D;&#x6BCF;&#x4E2A;&#x5143;&#x4EF6;&#x3002;</p>
</li>
</ul>
<p>&#x5982;&#x679C;&#xFF08;H_0&#xFF0C;C_0&#xFF09;&#x4E0D;&#x8BBE;&#x7F6E;&#xFF0C;&#x4E8C;&#x8005; <strong>H_0</strong> &#x548C; <strong>C_0</strong> &#x9ED8;&#x8BA4;&#x4E3A;&#x96F6;&#x3002;</p>
<p>Outputs: output, (h_n, c_n)</p>
<ul>
<li><strong>&#x8F93;&#x51FA;</strong> &#x5F62;&#x72B6;&#x7684;&#xFF08;seq<em>len&#xFF0C;&#x5206;&#x6279;&#xFF0C;num_directions * hidden</em>&#x200B;&#x200B;size&#xFF09;&#xFF1A;&#x5F20;&#x91CF;&#x5305;&#x542B;&#x8F93;&#x51FA;&#x7279;&#x5F81;&#xFF08;h_t&#xFF09;&#x4ECE;LSTM&#x7684;&#x6700;&#x540E;&#x4E00;&#x5C42;&#xFF0C;&#x5BF9;&#x4E8E;&#x6BCF;&#x4E2A; T &#x3002;&#x5982;&#x679C; <code>torch.nn.utils.rnn.PackedSequence</code>&#x5DF2;&#x88AB;&#x7ED9;&#x5B9A;&#x4E3A;&#x8F93;&#x5165;&#xFF0C;&#x8F93;&#x51FA;&#x4E5F;&#x5C06;&#x662F;&#x4E00;&#x4E2A;&#x62E5;&#x6324;&#x7684;&#x5E8F;&#x5217;&#x3002;</li>
</ul>
<p>For the unpacked case, the directions can be separated using
<code>output.view(seq_len, batch, num_directions, hidden_size)</code>, with forward and
backward being direction 0 and 1 respectively. Similarly, the directions can
be separated in the packed case.</p>
<ul>
<li><strong>h_n</strong> of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len.</li>
</ul>
<p>&#x50CF; <em>&#x8F93;&#x51FA;</em> &#x65F6;&#xFF0C;&#x5C42;&#x53EF;&#x4F7F;&#x7528;<code>h_n.view&#xFF08;num_layers&#x5206;&#x79BB;&#xFF0C; num_directions&#xFF0C; &#x6279;&#x6B21;&#xFF0C; hidden_&#x200B;&#x200B;size &#xFF09;</code>&#x540C;&#x6837;&#x5730;&#xFF0C;&#x5BF9;&#x4E8E; <em>C_N</em> &#x3002;</p>
<ul>
<li><strong>C_N</strong> &#x7684;&#x5F62;&#x72B6;&#xFF08;num<em>layers * num_directions&#xFF0C;&#x5206;&#x6279;&#xFF0C;hidden</em>&#x200B;&#x200B;size&#xFF09;&#xFF1A;&#x5F20;&#x91CF;&#x542B;&#x6709; T = seq_len &#x5C0F;&#x533A;&#x72B6;&#x6001;&#x3002;</li>
</ul>
<p>Variables</p>
<ul>
<li><p><strong>&#x301C;LSTM.weight_ih_l [k]&#x7684;</strong> - &#x7684; k&#x662F;&#x53EF;&#x5B66;&#x4E60;&#x8F93;&#x5165;&#x9690;&#x85CF;&#x6743;&#x91CD; T  H  \&#x6587;&#x672C;{K} ^ {&#x7B2C;}  K  T  H  &#x5C42;&#xFF08;W<em>ii | W_if | W_ig | W_io&#xFF09;&#xFF0C;&#x5F62;&#x72B6;&#x7684;&#xFF08;4 * hidden</em>&#x200B;&#x200B;size&#xFF0C;input<em>size&#xFF09;&#x4E3A; K = 0 &#x3002;&#x5426;&#x5219;&#xFF0C;&#x8BE5;&#x5F62;&#x72B6;&#x662F;&#xFF08;4 * hidden</em>&#x200B;&#x200B;size&#xFF0C;num<em>directions * hidden</em>&#x200B;&#x200B;size&#xFF09;</p>
</li>
<li><p><strong>&#x301C;LSTM.weight_hh_l [k]&#x7684;</strong> - &#x7684; k&#x662F;&#x53EF;&#x5B66;&#x4E60;&#x9690;&#x85CF;&#x6743;&#x91CD; T  H  \&#x6587;&#x672C;{K} ^ {&#x7B2C;}  K  T  H  &#x5C42;&#xFF08;W<em>hi | W_hf | W_hg | W_ho&#xFF09;&#xFF0C;&#x5F62;&#x72B6;&#x7684;&#xFF08;4 * hidden</em>&#x200B;&#x200B;size&#xFF0C;hidden_&#x200B;&#x200B;size&#xFF09;</p>
</li>
<li><p><strong>&#x301C;LSTM.bias_ih_l [k]&#x7684;</strong> - &#x7684;&#x53EF;&#x5B66;&#x4E60;&#x8F93;&#x5165;&#x9690;&#x85CF;&#x504F;&#x7F6E; K  T  H  \&#x6587;&#x672C;{K} ^ {&#x7B2C;}  K  T  H  &#x5C42;&#x5F62;&#x72B6;&#x7684;&#xFF08;b<em>ii | b_if | | b_ig b_io&#xFF09;&#xFF0C;&#xFF08;4 * hidden</em>&#x200B;&#x200B;size&#xFF09;</p>
</li>
<li><p><strong>&#x301C;LSTM.bias_hh_l [k]&#x7684;</strong> - &#x7684;&#x53EF;&#x5B66;&#x4E60;&#x9690;&#x85CF;&#x504F;&#x538B; K  T  H  \&#x6587;&#x672C;{K} ^ {&#x7B2C;}  K  T  H  &#x5C42;&#x5F62;&#x72B6;&#x7684;&#xFF08;b<em>hi | b_hf | | b_hg b_ho&#xFF09;&#xFF0C;&#xFF08;4 * hidden</em>&#x200B;&#x200B;size&#xFF09;</p>
</li>
</ul>
<p>Note</p>
<p>All the weights and biases are initialized from U(&#x2212;k,k)\mathcal{U}(-\sqrt{k},
\sqrt{k})U(&#x2212;k&#x200B;,k&#x200B;) where k=1hidden_sizek =
\frac{1}{\text{hidden\_size}}k=hidden_size1&#x200B;</p>
<p>Note</p>
<p>If the following conditions are satisfied: 1) cudnn is enabled, 2) input data
is on the GPU 3) input data has dtype <code>torch.float16</code>4) V100 GPU is used, 5)
input data is not in <code>PackedSequence</code>format persistent algorithm can be
selected to improve performance.</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; rnn = nn.LSTM(10, 20, 2)
&gt;&gt;&gt; input = torch.randn(5, 3, 10)
&gt;&gt;&gt; h0 = torch.randn(2, 3, 20)
&gt;&gt;&gt; c0 = torch.randn(2, 3, 20)
&gt;&gt;&gt; output, (hn, cn) = rnn(input, (h0, c0))
</code></pre><h3 id="gru">GRU</h3>
<p><em>class</em><code>torch.nn.``GRU</code>( <em>*args</em> , <em>**kwargs</em>
)<a href="_modules/torch/nn/modules/rnn.html#GRU">[source]</a></p>
<p>&#x9002;&#x7528;&#x95E8;&#x63A7;&#x91CD;&#x590D;&#x5355;&#x5143;&#xFF08;GRU&#xFF09;RNN&#x5230;&#x8F93;&#x5165;&#x5E8F;&#x5217;&#x7684;&#x591A;&#x5C42;&#x3002;</p>
<p>For each element in the input sequence, each layer computes the following
function:</p>
<p>rt=&#x3C3;(Wirxt+bir+Whrh(t&#x2212;1)+bhr)zt=&#x3C3;(Wizxt+biz+Whzh(t&#x2212;1)+bhz)nt=tanh&#x2061;(Winxt+bin+rt&#x2217;(Whnh(t&#x2212;1)+bhn))ht=(1&#x2212;zt)&#x2217;nt+zt&#x2217;h(t&#x2212;1)\begin{array}{ll}
r<em>t = \sigma(W</em>{ir} x<em>t + b</em>{ir} + W<em>{hr} h</em>{(t-1)} + b<em>{hr}) \\ z_t =
\sigma(W</em>{iz} x<em>t + b</em>{iz} + W<em>{hz} h</em>{(t-1)} + b<em>{hz}) \\ n_t = \tanh(W</em>{in}
x<em>t + b</em>{in} + r<em>t * (W</em>{hn} h<em>{(t-1)}+ b</em>{hn})) \\ h<em>t = (1 - z_t) <em> n_t +
z_t </em> h</em>{(t-1)} \end{array}
rt&#x200B;=&#x3C3;(Wir&#x200B;xt&#x200B;+bir&#x200B;+Whr&#x200B;h(t&#x2212;1)&#x200B;+bhr&#x200B;)zt&#x200B;=&#x3C3;(Wiz&#x200B;xt&#x200B;+biz&#x200B;+Whz&#x200B;h(t&#x2212;1)&#x200B;+bhz&#x200B;)nt&#x200B;=tanh(Win&#x200B;xt&#x200B;+bin&#x200B;+rt&#x200B;&#x2217;(Whn&#x200B;h(t&#x2212;1)&#x200B;+bhn&#x200B;))ht&#x200B;=(1&#x2212;zt&#x200B;)&#x2217;nt&#x200B;+zt&#x200B;&#x2217;h(t&#x2212;1)&#x200B;&#x200B;</p>
<p>&#x5176;&#x4E2D; H  T  h<em>t  H  T  &#x662F;&#x5728;&#x65F6;&#x523B; T &#x9690;&#x85CF;&#x72B6;&#x6001;&#xFF0C; &#xD7; T  X_T  &#xD7; T  &#x662F;&#x5728;&#x65F6;&#x523B; T &#x7684;&#x8F93;&#x5165;&#xFF0C; H  &#xFF08; T  -  1  &#xFF09;
[HTG135 1 H </em> {&#xFF08;T-1&#xFF09;}  H  &#xFF08; T  -  1  &#xFF09; &#x662F;&#x8BE5;&#x5C42;&#x7684;&#x5728;&#x65F6;&#x95F4;&#x7684;&#x9690;&#x85CF;&#x72B6;&#x6001; T-1 &#x6216;&#x5728;&#x65F6;&#x95F4;&#x7684;&#x521D;&#x59CB;&#x9690;&#x85CF;&#x72B6;&#x6001; 0 &#x548C;[HTG19
0]  R  T  r_t  R  T  &#xFF0C; Z  &#x5428; z_t  Z  &#x200B;&#x200B;  T  [HT G288]  &#xFF0C; n&#x7684; T  N_T  n&#x7684; T
&#x662F;&#x590D;&#x4F4D;&#xFF0C;&#x66F4;&#x65B0;&#x548C;&#x65B0;&#x7684;&#x5927;&#x95E8;&#xFF0C;&#x5206;&#x522B;&#x3002;  &#x3C3; \&#x897F;&#x683C;&#x739B; &#x3C3; &#x662F;S&#x5F62;&#x51FD;&#x6570;&#xFF0C;&#x5E76; <em>  </em>  *  &#x4E3A;Hadamard&#x4E58;&#x79EF;&#x3002;</p>
<p>&#x5728;&#x591A;&#x5C42;GRU&#xFF0C;&#x8F93;&#x5165; &#xD7; T  &#xFF08; L  &#xFF09; &#xD7;^ {&#xFF08;L&#xFF09;} <em> T  &#xD7; T  &#xFF08; L  &#xFF09; &#x7684; L  L  L  &#x7B2C;&#x5C42;&#xFF08; L  &amp; GT ;  =  2
&#x5347;&amp; GT ; = 2  L  &amp; GT ;  =  2  &#xFF09;&#x662F;&#x9690;&#x85CF;&#x72B6;&#x6001; [HTG155 1 H  T  &#xFF08; L  -  1  &#xFF09; H ^ {&#xFF08;L-1&#xFF09;}
</em> T  H  T [HTG1 94]  &#xFF08; L  -  1  &#xFF09; &#x5148;&#x524D;&#x5C42;&#x4E58;&#x4EE5;&#x7684;&#x8131;&#x843D; &#x3B4; T  &#xFF08; L  -  1  &#xFF09; \&#x589E;&#x91CF;^ {&#xFF08;L-1&#xFF09;} <em>
T  &#x3B4; &#x200B;&#x200B;  T  &#xFF08; L  -  1  &#xFF09; &#x5176;&#x4E2D;&#x5404;HTG316 ]  &#x3B4; T  &#xFF08; L  -  1  &#xFF09; \&#x589E;&#x91CF;^ {&#xFF08;L-1&#xFF09;} </em> T  &#x3B4;
T  &#xFF08; L  -  1  &#xFF09; &#x662F;&#x4F2F;&#x52AA;&#x5229;&#x968F;&#x673A;&#x53D8;&#x91CF;&#xFF0C;&#x5B83;&#x662F; 0  0  0  &#x7684;&#x6982;&#x7387;<code>&#x5DEE;</code>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input_size</strong> &#x2013; The number of expected features in the input x</p>
</li>
<li><p><strong>hidden_size</strong> &#x2013; The number of features in the hidden state h</p>
</li>
<li><p><strong>num_layers</strong> - &#x590D;&#x53D1;&#x5C42;&#x6570;&#x3002;&#x4F8B;&#x5982;&#xFF0C;&#x8BBE;&#x7F6E;<code>num_layers = 2</code>&#x5C06;&#x610F;&#x5473;&#x7740;&#x5806;&#x53E0;&#x4E24;&#x4E2A;&#x8D8A;&#x51AC;&#x5728;&#x4E00;&#x8D77;&#x4EE5;&#x5F62;&#x6210;&#x5C42;&#x53E0;GRU &#xFF0C;&#x4E0E;&#x7B2C;&#x4E8C;GRU&#x53D6;&#x5165;&#x7B2C;&#x4E00;GRU&#x7684;&#x8F93;&#x51FA;&#x548C;&#x8BA1;&#x7B97;&#x6240;&#x8FF0;&#x6700;&#x540E;&#x7684;&#x7ED3;&#x679C;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;1</p>
</li>
<li><p><strong>bias</strong> &#x2013; If <code>False</code>, then the layer does not use bias weights b_ih and b_hh. Default: <code>True</code></p>
</li>
<li><p><strong>batch_first</strong> &#x2013; If <code>True</code>, then the input and output tensors are provided as (batch, seq, feature). Default: <code>False</code></p>
</li>
<li><p><strong>&#x5DEE;</strong> - &#x5982;&#x679C;&#x4E0D;&#x4E3A;&#x96F6;&#xFF0C;&#x4ECB;&#x7ECD;&#x4E86;&#x9664;&#x4E86;&#x6700;&#x540E;&#x5C42;&#x5404;GRU&#x5C42;&#x7684;&#x8F93;&#x51FA;&#x964D;&#x5C42;&#xFF0C;&#x7528;&#x5DEE;&#x6982;&#x7387;&#x7B49;&#x4E8E;<code>&#x5DEE;</code>&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;0</p>
</li>
<li><p><strong>&#x53CC;&#x5411;</strong> - &#x82E5;<code>&#x771F;</code>&#xFF0C;&#x6210;&#x4E3A;&#x53CC;&#x5411;&#x7684;GRU&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x5047;</code></p>
</li>
</ul>
<p>Inputs: input, h_0</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#x7684;&#x5F62;&#x72B6;&#xFF08;seq_len&#xFF0C;&#x5206;&#x6279;&#xFF0C;input_size&#xFF09;&#xFF1A;&#x5F20;&#x91CF;&#x5305;&#x542B;&#x8F93;&#x5165;&#x5E8F;&#x5217;&#x7684;&#x7279;&#x5F81;&#x3002;&#x8F93;&#x5165;&#x4E5F;&#x53EF;&#x4EE5;&#x662F;&#x586B;&#x5145;&#x53EF;&#x53D8;&#x957F;&#x5EA6;&#x5E8F;&#x5217;&#x3002;&#x53C2;&#x89C1; <code>&#x5BF9;&#x4E8E;&#x7EC6;&#x8282;torch.nn.utils.rnn.pack_padded_sequence&#xFF08;&#xFF09;</code>&#x3002;</p>
</li>
<li><p><strong>h_0</strong> of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided. If the RNN is bidirectional, num_directions should be 2, else it should be 1.</p>
</li>
</ul>
<p>Outputs: output, h_n</p>
<ul>
<li><strong>&#x8F93;&#x51FA;</strong> &#x5F62;&#x72B6;&#xFF08;seq<em>len&#xFF0C;&#x5206;&#x6279;&#xFF0C;num_directions * hidden</em>&#x200B;&#x200B;size&#xFF09;&#x7684;&#xFF1A;&#x5F20;&#x91CF;&#x5305;&#x542B;&#x8F93;&#x51FA;&#x7279;&#x5F81;&#x4ECE;GRU&#x7684;&#x6700;&#x540E;&#x4E00;&#x5C42;h<em>t&#xFF0C;&#x5BF9;&#x4E8E;&#x6BCF;&#x4E2A; T &#x3002;&#x5982;&#x679C; <code>torch.nn.utils.rnn.PackedSequence</code>&#x5DF2;&#x88AB;&#x7ED9;&#x5B9A;&#x4E3A;&#x8F93;&#x5165;&#xFF0C;&#x8F93;&#x51FA;&#x4E5F;&#x5C06;&#x662F;&#x4E00;&#x4E2A;&#x62E5;&#x6324;&#x7684;&#x5E8F;&#x5217;&#x3002;&#x5BF9;&#x4E8E;&#x89E3;&#x538B;&#x7F29;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x8BE5;&#x65B9;&#x5411;&#x53EF;&#x4F7F;&#x7528;`output.view&#x5206;&#x79BB;&#xFF08;seq_len&#xFF0C; &#x6279;&#x6B21;&#xFF0C; num_directions&#xFF0C; hidden</em>&#x200B;&#x200B;size&#xFF09; `&#xFF0C;&#x4E0E;&#x5411;&#x524D;&#x548C;&#x5411;&#x540E;&#x65B9;&#x5411;&#x4E3A; 0 &#x548C; 1 &#x5206;&#x522B;&#x3002;</li>
</ul>
<p>&#x7C7B;&#x4F3C;&#x5730;&#xFF0C;&#x65B9;&#x5411;&#x53EF;&#x4EE5;&#x5728;&#x5806;&#x79EF;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#x88AB;&#x5206;&#x79BB;&#x3002;</p>
<ul>
<li><strong>h_n</strong> &#x5F62;&#x72B6;&#xFF08;num<em>layers * num_directions&#xFF0C;&#x5206;&#x6279;&#xFF0C;hidden</em>&#x200B;&#x200B;size&#xFF09;&#x7684;&#xFF1A;&#x5F20;&#x91CF;&#x5305;&#x542B;&#x9690;&#x85CF;&#x72B6;&#x6001; T = seq_len </li>
</ul>
<p>Like <em>output</em> , the layers can be separated using <code>h_n.view(num_layers,
num_directions, batch, hidden_size)</code>.</p>
<p>Shape:</p>
<ul>
<li><p>Input1: (L,N,Hin)(L, N, H<em>{in})(L,N,Hin&#x200B;) tensor containing input features where Hin=input_sizeH</em>{in}=\text{input\_size}Hin&#x200B;=input_size and L represents a sequence length.</p>
</li>
<li><p>Input2: (S,N,Hout)(S, N, H<em>{out})(S,N,Hout&#x200B;) tensor containing the initial hidden state for each element in the batch. Hout=hidden_sizeH</em>{out}=\text{hidden\_size}Hout&#x200B;=hidden_size Defaults to zero if not provided. where S=num_layers&#x2217;num_directionsS=\text{num\_layers} * \text{num\_directions}S=num_layers&#x2217;num_directions If the RNN is bidirectional, num_directions should be 2, else it should be 1.</p>
</li>
<li><p>Output1: (L,N,Hall)(L, N, H<em>{all})(L,N,Hall&#x200B;) where Hall=num_directions&#x2217;hidden_sizeH</em>{all}=\text{num\_directions} * \text{hidden\_size}Hall&#x200B;=num_directions&#x2217;hidden_size</p>
</li>
<li><p>Output2: (S,N,Hout)(S, N, H_{out})(S,N,Hout&#x200B;) tensor containing the next hidden state for each element in the batch</p>
</li>
</ul>
<p>Variables</p>
<ul>
<li><p><strong>&#x301C;GRU.weight_ih_l [k]&#x7684;</strong> - &#x7684; k&#x662F;&#x53EF;&#x5B66;&#x4E60;&#x8F93;&#x5165;&#x9690;&#x85CF;&#x6743;&#x91CD; T  H  \&#x6587;&#x672C;{K} ^ {&#x7B2C;}  K  T  H  &#x5C42;&#xFF08;W<em>ir | W_iz | W_IN&#xFF09;&#xFF0C;&#x5F62;&#x72B6;&#xFF08;3 * hidden</em>&#x200B;&#x200B;size&#xFF0C;input<em>size&#xFF09;&#x4E3A;&#x4E2D;&#x7684;k = 0 &#x3002;&#x5426;&#x5219;&#xFF0C;&#x8BE5;&#x5F62;&#x72B6;&#x662F;&#xFF08;3 * hidden</em>&#x200B;&#x200B;size&#xFF0C;num<em>directions * hidden</em>&#x200B;&#x200B;size&#xFF09;</p>
</li>
<li><p><strong>&#x301C;GRU.weight_hh_l [k]&#x7684;</strong> - &#x7684; k&#x662F;&#x53EF;&#x5B66;&#x4E60;&#x9690;&#x85CF;&#x6743;&#x91CD; T  H  \&#x6587;&#x672C;{K} ^ {&#x7B2C;}  K  T  H  &#x5C42;&#xFF08;W<em>hr | W_hz | W_hn&#xFF09;&#xFF0C;&#x5F62;&#x72B6;&#x7684;&#xFF08;3 * hidden</em>&#x200B;&#x200B;size&#xFF0C;hidden_&#x200B;&#x200B;size&#xFF09;</p>
</li>
<li><p><strong>&#x301C;GRU.bias_ih_l [k]&#x7684;</strong> - &#x7684;&#x53EF;&#x5B66;&#x4E60;&#x8F93;&#x5165;&#x9690;&#x85CF;&#x504F;&#x7F6E; K  T  H  \&#x6587;&#x672C;{K} ^ {&#x7B2C;}  K  T  H  &#x5C42;&#x5F62;&#x72B6;&#x7684;&#xFF08;b<em>ir | | b_iz B_IN&#xFF09;&#xFF0C;&#xFF08;3 * hidden</em>&#x200B;&#x200B;size&#xFF09;</p>
</li>
<li><p><strong>&#x301C;GRU.bias_hh_l [k]&#x7684;</strong> - &#x7684;&#x53EF;&#x5B66;&#x4E60;&#x9690;&#x85CF;&#x504F;&#x538B; K  T  H  \&#x6587;&#x672C;{K} ^ {&#x7B2C;}  K  T  H  &#x5C42;&#x5F62;&#x72B6;&#x7684;&#xFF08;b<em>hr | | b_hz b_hn&#xFF09;&#xFF0C;&#xFF08;3 * hidden</em>&#x200B;&#x200B;size&#xFF09;</p>
</li>
</ul>
<p>Note</p>
<p>All the weights and biases are initialized from U(&#x2212;k,k)\mathcal{U}(-\sqrt{k},
\sqrt{k})U(&#x2212;k&#x200B;,k&#x200B;) where k=1hidden_sizek =
\frac{1}{\text{hidden\_size}}k=hidden_size1&#x200B;</p>
<p>Note</p>
<p>If the following conditions are satisfied: 1) cudnn is enabled, 2) input data
is on the GPU 3) input data has dtype <code>torch.float16</code>4) V100 GPU is used, 5)
input data is not in <code>PackedSequence</code>format persistent algorithm can be
selected to improve performance.</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; rnn = nn.GRU(10, 20, 2)
&gt;&gt;&gt; input = torch.randn(5, 3, 10)
&gt;&gt;&gt; h0 = torch.randn(2, 3, 20)
&gt;&gt;&gt; output, hn = rnn(input, h0)
</code></pre><h3 id="rnncell">RNNCell</h3>
<p><em>class</em><code>torch.nn.``RNNCell</code>( <em>input_size</em> , <em>hidden_size</em> , <em>bias=True</em> ,
<em>nonlinearity=&apos;tanh&apos;</em> )<a href="_modules/torch/nn/modules/rnn.html#RNNCell">[source]</a></p>
<p>&#x4E00;&#x4E2A;&#x57C3;&#x5C14;&#x66FC;RNN&#x7EC6;&#x80DE;&#x4E0E;&#x53CC;&#x66F2;&#x6B63;&#x5207;&#x6216;RELU&#x975E;&#x7EBF;&#x6027;&#x3002;</p>
<p>h&#x2032;=tanh&#x2061;(Wihx+bih+Whhh+bhh)h&apos; = \tanh(W<em>{ih} x + b</em>{ih} + W<em>{hh} h +
b</em>{hh})h&#x2032;=tanh(Wih&#x200B;x+bih&#x200B;+Whh&#x200B;h+bhh&#x200B;)</p>
<p>&#x5982;&#x679C;<code>&#x975E;&#x7EBF;&#x6027;</code>&#x662F;&#x201C;RELU&#x201D; &#xFF0C;&#x7136;&#x540E;RELU&#x4EE3;&#x66FF;&#x7684;tanh&#x7684;&#x4F7F;&#x7528;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input_size</strong> &#x2013; The number of expected features in the input x</p>
</li>
<li><p><strong>hidden_size</strong> &#x2013; The number of features in the hidden state h</p>
</li>
<li><p><strong>bias</strong> &#x2013; If <code>False</code>, then the layer does not use bias weights b_ih and b_hh. Default: <code>True</code></p>
</li>
<li><p><strong>nonlinearity</strong> &#x2013; The non-linearity to use. Can be either <code>&apos;tanh&apos;</code>or <code>&apos;relu&apos;</code>. Default: <code>&apos;tanh&apos;</code></p>
</li>
</ul>
<p>Inputs: input, hidden</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#x5F62;&#x72B6;&#x7684;&#xFF08;&#x5206;&#x6279;&#xFF0C;input_size&#xFF09;&#xFF1A;&#x5F20;&#x91CF;&#x542B;&#x6709;&#x8F93;&#x5165;&#x7279;&#x5F81;</p>
</li>
<li><p><strong>&#x9690;&#x85CF;&#x5F62;&#x72B6; &#xFF08;&#x5206;&#x6279;&#xFF0C;hidden_&#x200B;&#x200B;size&#xFF09;&#x7684;</strong>&#xFF1A;&#x5F20;&#x91CF;&#x5305;&#x542B;&#x6240;&#x8FF0;&#x521D;&#x59CB;&#x72B6;&#x6001;&#x9690;&#x85CF;&#x5728;&#x6279;&#x6B21;&#x4E2D;&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x4EF6;&#x3002;&#x9ED8;&#x8BA4;&#x4E3A;&#x96F6;&#xFF0C;&#x5982;&#x679C;&#x4E0D;&#x63D0;&#x4F9B;&#x3002;</p>
</li>
</ul>
<p>Outputs: h&#x2019;</p>
<ul>
<li><strong>H&#x201D;</strong> &#x5F62;&#x72B6;&#xFF08;&#x5206;&#x6279;&#xFF0C;hidden_&#x200B;&#x200B;size&#xFF09;&#x7684;&#xFF1A;&#x5F20;&#x91CF;&#x5305;&#x542B;&#x4E0B;&#x4E00;&#x9690;&#x85CF;&#x72B6;&#x6001;&#x4E3A;&#x6279;&#x4E2D;&#x6BCF;&#x4E2A;&#x5143;&#x4EF6;</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>&#x8F93;&#x5165;1&#xFF1A; &#xFF08; N  &#xFF0C; H  i&#x7684; n&#x7684; &#xFF09; &#xFF08;N&#xFF0C;H<em> {&#x5728;}&#xFF09; &#xFF08; N  &#xFF0C; H  i&#x7684; n&#x7684; &#xFF09;  [HTG79&#x542B;&#x6709;&#x5F20;&#x91CF;&#x8F93;&#x5165;&#x529F;&#x80FD;&#xFF0C;&#x5176;&#x4E2D; H  i&#x7684; n&#x7684; H</em> {IN}  [HT G102]  H  i&#x7684; n&#x7684; =  input_size </p>
</li>
<li><p>&#x8F93;&#x5165;2&#xFF1A; &#xFF08; N  &#xFF0C; H  O  U  T  &#xFF09; &#xFF08;N&#xFF0C;H<em> {&#x51FA;}&#xFF09; &#xFF08; N  &#xFF0C; H  O  U  T  &#xFF09; [HTG83&#x542B;&#x6709;&#x521D;&#x59CB;&#x9690;&#x85CF;&#x72B6;&#x6001;&#x6279;&#x4E2D;&#x6BCF;&#x4E2A;&#x5143;&#x4EF6;&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x4E2D; H  O  U  T  H</em> {&#x51FA;}  H  O  U  T  =  hidden_&#x200B;&#x200B;size &#x5982;&#x679C;&#x4E0D;&#x8BBE;&#x7F6E;&#x7F3A;&#x7701;&#x503C;&#x4E3A;&#x96F6;&#x3002;</p>
</li>
<li><p>&#x8F93;&#x51FA;&#xFF1A; &#xFF08; N  &#xFF0C; H  O  U  T  &#xFF09; &#xFF08;N&#xFF0C;H_ {&#x51FA;}&#xFF09; &#xFF08; N  &#xFF0C; H  O  U  T  &#xFF09; &#x5305;&#x542B;&#x4E0B;&#x4E00;&#x4E2A;&#x9690;&#x85CF;&#x72B6;&#x6001;&#x5728;&#x6279;&#x6B21;&#x4E2D;&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x4EF6;&#x5F20;&#x91CF;</p>
</li>
</ul>
<p>Variables</p>
<ul>
<li><p><strong>&#x301C;RNNCell.weight_ih</strong> - &#x7684;&#x53EF;&#x5B66;&#x4E60;&#x8F93;&#x5165;&#x9690;&#x85CF;&#x91CD;&#x91CF;&#xFF0C;&#x5F62;&#x72B6;&#x7684;&#xFF08;hidden_&#x200B;&#x200B;size&#xFF0C;input_size&#xFF09;</p>
</li>
<li><p><strong>&#x301C;RNNCell.weight_hh</strong> - &#x7684;&#x53EF;&#x5B66;&#x4E60;&#x9690;&#x85CF;&#x7684;&#x6743;&#x91CD;&#xFF0C;&#x5F62;&#x72B6;&#x7684;&#xFF08;hidden<em>&#x200B;&#x200B;size&#xFF0C;hidden</em>&#x200B;&#x200B;size&#xFF09;</p>
</li>
<li><p><strong>&#x301C;RNNCell.bias_ih</strong> - &#x7684;&#x53EF;&#x5B66;&#x4E60;&#x8F93;&#x5165;&#x9690;&#x85CF;&#x504F;&#x538B;&#xFF0C;&#x5F62;&#x72B6;&#x7684;&#xFF08;hidden_&#x200B;&#x200B;size&#xFF09;</p>
</li>
<li><p><strong>&#x301C;RNNCell.bias_hh</strong> - &#x7684;&#x53EF;&#x5B66;&#x4E60;&#x9690;&#x85CF;&#x504F;&#x538B;&#xFF0C;&#x5F62;&#x72B6;&#x7684;&#xFF08;hidden_&#x200B;&#x200B;size&#xFF09;</p>
</li>
</ul>
<p>Note</p>
<p>All the weights and biases are initialized from U(&#x2212;k,k)\mathcal{U}(-\sqrt{k},
\sqrt{k})U(&#x2212;k&#x200B;,k&#x200B;) where k=1hidden_sizek =
\frac{1}{\text{hidden\_size}}k=hidden_size1&#x200B;</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; rnn = nn.RNNCell(10, 20)
&gt;&gt;&gt; input = torch.randn(6, 3, 10)
&gt;&gt;&gt; hx = torch.randn(3, 20)
&gt;&gt;&gt; output = []
&gt;&gt;&gt; for i in range(6):
        hx = rnn(input[i], hx)
        output.append(hx)
</code></pre><h3 id="lstmcell">LSTMCell</h3>
<p><em>class</em><code>torch.nn.``LSTMCell</code>( <em>input_size</em> , <em>hidden_size</em> , <em>bias=True</em>
)<a href="_modules/torch/nn/modules/rnn.html#LSTMCell">[source]</a></p>
<p>&#x957F;&#x77ED;&#x671F;&#x8BB0;&#x5FC6;&#xFF08;LSTM&#xFF09;&#x7EC6;&#x80DE;&#x3002;</p>
<p>i=&#x3C3;(Wiix+bii+Whih+bhi)f=&#x3C3;(Wifx+bif+Whfh+bhf)g=tanh&#x2061;(Wigx+big+Whgh+bhg)o=&#x3C3;(Wiox+bio+Whoh+bho)c&#x2032;=f&#x2217;c+i&#x2217;gh&#x2032;=o&#x2217;tanh&#x2061;(c&#x2032;)\begin{array}{ll}
i = \sigma(W<em>{ii} x + b</em>{ii} + W<em>{hi} h + b</em>{hi}) \\ f = \sigma(W<em>{if} x +
b</em>{if} + W<em>{hf} h + b</em>{hf}) \\ g = \tanh(W<em>{ig} x + b</em>{ig} + W<em>{hg} h +
b</em>{hg}) \\ o = \sigma(W<em>{io} x + b</em>{io} + W<em>{ho} h + b</em>{ho}) \\ c&apos; = f <em> c +
i </em> g \\ h&apos; = o * \tanh(c&apos;) \\
\end{array}i=&#x3C3;(Wii&#x200B;x+bii&#x200B;+Whi&#x200B;h+bhi&#x200B;)f=&#x3C3;(Wif&#x200B;x+bif&#x200B;+Whf&#x200B;h+bhf&#x200B;)g=tanh(Wig&#x200B;x+big&#x200B;+Whg&#x200B;h+bhg&#x200B;)o=&#x3C3;(Wio&#x200B;x+bio&#x200B;+Who&#x200B;h+bho&#x200B;)c&#x2032;=f&#x2217;c+i&#x2217;gh&#x2032;=o&#x2217;tanh(c&#x2032;)&#x200B;</p>
<p>&#x5176;&#x4E2D; &#x3C3; \&#x897F;&#x683C;&#x739B; &#x3C3; &#x662F;S&#x5F62;&#x51FD;&#x6570;&#xFF0C;&#x5E76; <em>  </em>  *  &#x4E3A;Hadamard&#x4E58;&#x79EF;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input_size</strong> &#x2013; The number of expected features in the input x</p>
</li>
<li><p><strong>hidden_size</strong> &#x2013; The number of features in the hidden state h</p>
</li>
<li><p><strong>&#x504F;&#x538B;</strong> - &#x82E5;<code>&#x5047;</code>&#xFF0C;&#x5219;&#x8BE5;&#x5C42;&#x4E0D;&#x4F7F;&#x7528;&#x504F;&#x538B;&#x6743;&#x91CD; b_ih &#x548C; b_hh &#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x771F;</code></p>
</li>
</ul>
<p>Inputs: input, (h_0, c_0)</p>
<ul>
<li><p><strong>input</strong> of shape (batch, input_size): tensor containing input features</p>
</li>
<li><p><strong>H_0</strong> &#x5F62;&#x72B6;&#xFF08;&#x5206;&#x6279;&#xFF0C;hidden_&#x200B;&#x200B;size&#xFF09;&#x7684;&#xFF1A;&#x5F20;&#x91CF;&#x5305;&#x542B;&#x6240;&#x8FF0;&#x521D;&#x59CB;&#x72B6;&#x6001;&#x9690;&#x85CF;&#x5728;&#x6279;&#x6B21;&#x4E2D;&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x4EF6;&#x3002;</p>
</li>
<li><p><strong>C_0</strong> &#x5F62;&#x72B6;&#xFF08;&#x5206;&#x6279;&#xFF0C;hidden_&#x200B;&#x200B;size&#xFF09;&#x7684;&#xFF1A;&#x5F20;&#x91CF;&#x5305;&#x542B;&#x6240;&#x8FF0;&#x521D;&#x59CB;&#x5C0F;&#x533A;&#x72B6;&#x6001;&#x4E3A;&#x6279;&#x4E2D;&#x6BCF;&#x4E2A;&#x5143;&#x4EF6;&#x3002;</p>
</li>
</ul>
<p>If (h_0, c_0) is not provided, both <strong>h_0</strong> and <strong>c_0</strong> default to zero.</p>
<p>Outputs: (h_1, c_1)</p>
<ul>
<li><p><strong>H_1</strong> &#x7684;&#x5F62;&#x72B6;&#xFF08;&#x5206;&#x6279;&#xFF0C;hidden_&#x200B;&#x200B;size&#xFF09;&#xFF1A;&#x5F20;&#x91CF;&#x542B;&#x6709;&#x6279;&#x5904;&#x7406;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x4E0B;&#x4E00;&#x4E2A;&#x9690;&#x85CF;&#x72B6;&#x6001;</p>
</li>
<li><p><strong>C_1</strong> &#x5F62;&#x72B6;&#xFF08;&#x5206;&#x6279;&#xFF0C;hidden_&#x200B;&#x200B;size&#xFF09;&#x7684;&#xFF1A;&#x5F20;&#x91CF;&#x542B;&#x6709;&#x6279;&#x5904;&#x7406;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x4E0B;&#x4E00;&#x4E2A;&#x5C0F;&#x533A;&#x72B6;&#x6001;</p>
</li>
</ul>
<p>Variables</p>
<ul>
<li><p><strong>&#x301C;LSTMCell.weight_ih</strong> - &#x7684;&#x53EF;&#x5B66;&#x4E60;&#x8F93;&#x5165;&#x9690;&#x85CF;&#x91CD;&#x91CF;&#xFF0C;&#x5F62;&#x72B6;&#x7684;&#xFF08;4 * hidden_&#x200B;&#x200B;size&#xFF0C;input_size&#xFF09;</p>
</li>
<li><p><strong>&#x301C;LSTMCell.weight_hh</strong> - &#x7684;&#x53EF;&#x5B66;&#x4E60;&#x9690;&#x85CF;&#x7684;&#x6743;&#x91CD;&#xFF0C;&#x5F62;&#x72B6;&#x7684;&#xFF08;4 * hidden<em>&#x200B;&#x200B;size&#xFF0C;hidden</em>&#x200B;&#x200B;size&#xFF09;</p>
</li>
<li><p><strong>&#x301C;LSTMCell.bias_ih</strong> - &#x7684;&#x53EF;&#x5B66;&#x4E60;&#x8F93;&#x5165;&#x9690;&#x85CF;&#x504F;&#x538B;&#xFF0C;&#x5F62;&#x72B6;&#x7684;&#xFF08;4 * hidden_&#x200B;&#x200B;size&#xFF09;</p>
</li>
<li><p><strong>&#x301C;LSTMCell.bias_hh</strong> - &#x7684;&#x53EF;&#x5B66;&#x4E60;&#x9690;&#x85CF;&#x504F;&#x538B;&#xFF0C;&#x5F62;&#x72B6;&#x7684;&#xFF08;4 * hidden_&#x200B;&#x200B;size&#xFF09;</p>
</li>
</ul>
<p>Note</p>
<p>All the weights and biases are initialized from U(&#x2212;k,k)\mathcal{U}(-\sqrt{k},
\sqrt{k})U(&#x2212;k&#x200B;,k&#x200B;) where k=1hidden_sizek =
\frac{1}{\text{hidden\_size}}k=hidden_size1&#x200B;</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; rnn = nn.LSTMCell(10, 20)
&gt;&gt;&gt; input = torch.randn(6, 3, 10)
&gt;&gt;&gt; hx = torch.randn(3, 20)
&gt;&gt;&gt; cx = torch.randn(3, 20)
&gt;&gt;&gt; output = []
&gt;&gt;&gt; for i in range(6):
        hx, cx = rnn(input[i], (hx, cx))
        output.append(hx)
</code></pre><h3 id="grucell">GRUCell</h3>
<p><em>class</em><code>torch.nn.``GRUCell</code>( <em>input_size</em> , <em>hidden_size</em> , <em>bias=True</em>
)<a href="_modules/torch/nn/modules/rnn.html#GRUCell">[source]</a></p>
<p>&#x95E8;&#x63A7;&#x91CD;&#x590D;&#x5355;&#x5143;&#xFF08;GRU&#xFF09;&#x7EC6;&#x80DE;</p>
<p>r=&#x3C3;(Wirx+bir+Whrh+bhr)z=&#x3C3;(Wizx+biz+Whzh+bhz)n=tanh&#x2061;(Winx+bin+r&#x2217;(Whnh+bhn))h&#x2032;=(1&#x2212;z)&#x2217;n+z&#x2217;h\begin{array}{ll}
r = \sigma(W<em>{ir} x + b</em>{ir} + W<em>{hr} h + b</em>{hr}) \\ z = \sigma(W<em>{iz} x +
b</em>{iz} + W<em>{hz} h + b</em>{hz}) \\ n = \tanh(W<em>{in} x + b</em>{in} + r <em> (W<em>{hn} h +
b</em>{hn})) \\ h&apos; = (1 - z) </em> n + z * h
\end{array}r=&#x3C3;(Wir&#x200B;x+bir&#x200B;+Whr&#x200B;h+bhr&#x200B;)z=&#x3C3;(Wiz&#x200B;x+biz&#x200B;+Whz&#x200B;h+bhz&#x200B;)n=tanh(Win&#x200B;x+bin&#x200B;+r&#x2217;(Whn&#x200B;h+bhn&#x200B;))h&#x2032;=(1&#x2212;z)&#x2217;n+z&#x2217;h&#x200B;</p>
<p>where &#x3C3;\sigma&#x3C3; is the sigmoid function, and &#x2217;*&#x2217; is the Hadamard product.</p>
<p>Parameters</p>
<ul>
<li><p><strong>input_size</strong> &#x2013; The number of expected features in the input x</p>
</li>
<li><p><strong>hidden_size</strong> &#x2013; The number of features in the hidden state h</p>
</li>
<li><p><strong>bias</strong> &#x2013; If <code>False</code>, then the layer does not use bias weights b_ih and b_hh. Default: <code>True</code></p>
</li>
</ul>
<p>Inputs: input, hidden</p>
<ul>
<li><p><strong>input</strong> of shape (batch, input_size): tensor containing input features</p>
</li>
<li><p><strong>hidden</strong> of shape (batch, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided.</p>
</li>
</ul>
<p>Outputs: h&#x2019;</p>
<ul>
<li><strong>h&#x2019;</strong> of shape (batch, hidden_size): tensor containing the next hidden state for each element in the batch</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>Input1: (N,Hin)(N, H<em>{in})(N,Hin&#x200B;) tensor containing input features where HinH</em>{in}Hin&#x200B; = input_size</p>
</li>
<li><p>Input2: (N,Hout)(N, H<em>{out})(N,Hout&#x200B;) tensor containing the initial hidden state for each element in the batch where HoutH</em>{out}Hout&#x200B; = hidden_size Defaults to zero if not provided.</p>
</li>
<li><p>Output: (N,Hout)(N, H_{out})(N,Hout&#x200B;) tensor containing the next hidden state for each element in the batch</p>
</li>
</ul>
<p>Variables</p>
<ul>
<li><p><strong>&#x301C;GRUCell.weight_ih</strong> - &#x7684;&#x53EF;&#x5B66;&#x4E60;&#x8F93;&#x5165;&#x9690;&#x85CF;&#x91CD;&#x91CF;&#xFF0C;&#x5F62;&#x72B6;&#x7684;&#xFF08;3 * hidden_&#x200B;&#x200B;size&#xFF0C;input_size&#xFF09;</p>
</li>
<li><p><strong>&#x301C;GRUCell.weight_hh</strong> - &#x7684;&#x53EF;&#x5B66;&#x4E60;&#x9690;&#x85CF;&#x7684;&#x6743;&#x91CD;&#xFF0C;&#x5F62;&#x72B6;&#x7684;&#xFF08;3 * hidden<em>&#x200B;&#x200B;size&#xFF0C;hidden</em>&#x200B;&#x200B;size&#xFF09;</p>
</li>
<li><p><strong>&#x301C;GRUCell.bias_ih</strong> - &#x7684;&#x53EF;&#x5B66;&#x4E60;&#x8F93;&#x5165;&#x9690;&#x85CF;&#x504F;&#x538B;&#xFF0C;&#x5F62;&#x72B6;&#x7684;&#xFF08;3 * hidden_&#x200B;&#x200B;size&#xFF09;</p>
</li>
<li><p><strong>&#x301C;GRUCell.bias_hh</strong> - &#x7684;&#x53EF;&#x5B66;&#x4E60;&#x9690;&#x85CF;&#x504F;&#x538B;&#xFF0C;&#x5F62;&#x72B6;&#x7684;&#xFF08;3 * hidden_&#x200B;&#x200B;size&#xFF09;</p>
</li>
</ul>
<p>Note</p>
<p>All the weights and biases are initialized from U(&#x2212;k,k)\mathcal{U}(-\sqrt{k},
\sqrt{k})U(&#x2212;k&#x200B;,k&#x200B;) where k=1hidden_sizek =
\frac{1}{\text{hidden\_size}}k=hidden_size1&#x200B;</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; rnn = nn.GRUCell(10, 20)
&gt;&gt;&gt; input = torch.randn(6, 3, 10)
&gt;&gt;&gt; hx = torch.randn(3, 20)
&gt;&gt;&gt; output = []
&gt;&gt;&gt; for i in range(6):
        hx = rnn(input[i], hx)
        output.append(hx)
</code></pre><h2 id="&#x53D8;&#x538B;&#x5668;&#x5C42;">&#x53D8;&#x538B;&#x5668;&#x5C42;</h2>
<h3 id="&#x53D8;&#x538B;&#x5668;">&#x53D8;&#x538B;&#x5668;</h3>
<p><em>class</em><code>torch.nn.``Transformer</code>( <em>d_model=512</em> , <em>nhead=8</em> ,
<em>num_encoder_layers=6</em> , <em>num_decoder_layers=6</em> , <em>dim_feedforward=2048</em> ,
<em>dropout=0.1</em> , <em>custom_encoder=None</em> , <em>custom_decoder=None</em>
)<a href="_modules/torch/nn/modules/transformer.html#Transformer">[source]</a></p>
<p>&#x53D8;&#x538B;&#x5668;&#x6A21;&#x578B;&#x3002;&#x7528;&#x6237;&#x53EF;&#x4EE5;&#x6839;&#x636E;&#x9700;&#x8981;&#x4FEE;&#x6539;&#x5176;&#x5C5E;&#x6027;&#x3002;&#x8BE5;architechture&#x662F;&#x57FA;&#x4E8E;&#x7EB8;&#x201C;&#x6CE8;&#x610F;&#x662F;&#x6240;&#x6709;&#x4F60;&#x9700;&#x8981;&#x201D;&#x3002;&#x963F;&#x5E0C;&#x4EC0;&#x74E6;&#x65AF;&#x74E6;&#x5C3C;&#xFF0C;&#x8BFA;&#x59C6;Shazeer&#xFF0C;&#x5C3C;&#x57FA;&#x5E15;&#x5C14;&#x9A6C;&#x96C5;&#x5404;&#x5E03;Uszkoreit&#xFF0C;Llion&#x743C;&#x65AF;&#xFF0C;&#x827E;&#xD1;&#x6208;&#x9EA6;&#x65AF;&#xFF0C;&#x5362;&#x5361;&#x65AF;&#x51EF;&#x6CFD;&#xFF0C;&#x548C;Illia
Polosukhin&#x3002; 2017&#x6CE8;&#x610F;&#x529B;&#x662F;&#x4F60;&#x6240;&#x9700;&#x8981;&#x7684;&#x3002;&#x5728;&#x795E;&#x7ECF;&#x4FE1;&#x606F;&#x5904;&#x7406;&#x7CFB;&#x7EDF;&#x7684;&#x8FDB;&#x6B65;&#xFF0C;6000-6010&#x9875;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>d_model</strong> - &#x7684;&#x9884;&#x671F;&#x529F;&#x80FD;&#x5728;&#x7F16;&#x7801;&#x5668;/&#x89E3;&#x7801;&#x5668;&#x7684;&#x8F93;&#x5165;&#x7684;&#x6570;&#x91CF;&#xFF08;&#x7F3A;&#x7701;&#x503C;= 512&#xFF09;&#x3002;</p>
</li>
<li><p><strong>NHEAD</strong> - &#x7684;&#x5934;&#x5728;multiheadattention&#x6A21;&#x578B;&#x6570;&#xFF08;&#x9ED8;&#x8BA4;= 8&#xFF09;&#x3002;</p>
</li>
<li><p><strong>num_encoder_layers</strong> - &#x5728;&#x7F16;&#x7801;&#x5668;&#xFF08;&#x9ED8;&#x8BA4;= 6&#xFF09;&#x5B50;&#x7F16;&#x7801;&#x5668;&#x7684;&#x5C42;&#x7684;&#x6570;&#x76EE;&#x3002;</p>
</li>
<li><p><strong>num_decoder_layers</strong> - &#x5728;&#x89E3;&#x7801;&#x5668;&#xFF08;&#x9ED8;&#x8BA4;= 6&#xFF09;&#x5B50;&#x8BD1;&#x7801;&#x5668;&#x5C42;&#x7684;&#x6570;&#x91CF;&#x3002;</p>
</li>
<li><p><strong>dim_feedforward</strong> - &#x524D;&#x9988;&#x7F51;&#x7EDC;&#x6A21;&#x578B;&#xFF08;&#x7F3A;&#x7701;&#x503C;= 2048&#xFF09;&#x7684;&#x7EF4;&#x6570;&#x3002;</p>
</li>
<li><p><strong>&#x5DEE;</strong> - &#x6F0F;&#x5931;&#x503C;&#xFF08;&#x7F3A;&#x7701;&#x503C;= 0.1&#xFF09;&#x3002;</p>
</li>
<li><p><strong>custom_encoder</strong> - &#x5B9A;&#x5236;&#x7684;&#x7F16;&#x7801;&#x5668;&#xFF08;&#x9ED8;&#x8BA4;=&#x65E0;&#xFF09;&#x3002;</p>
</li>
<li><p><strong>custom_decoder</strong> - &#x5B9A;&#x5236;&#x89E3;&#x7801;&#x5668;&#xFF08;&#x9ED8;&#x8BA4;=&#x65E0;&#xFF09;&#x3002;</p>
</li>
</ul>
<p>Examples::</p>
<pre><code>&gt;&gt;&gt; transformer_model = nn.Transformer(src_vocab, tgt_vocab)
&gt;&gt;&gt; transformer_model = nn.Transformer(src_vocab, tgt_vocab, nhead=16, num_encoder_layers=12)
</code></pre><p><code>forward</code>( <em>src</em> , <em>tgt</em> , <em>src_mask=None</em> , <em>tgt_mask=None</em> ,
<em>memory_mask=None</em> , <em>src_key_padding_mask=None</em> , <em>tgt_key_padding_mask=None</em>
, <em>memory_key_padding_mask=None</em>
)<a href="_modules/torch/nn/modules/transformer.html#Transformer.forward">[source]</a></p>
<p>&#x91C7;&#x53D6;&#x5728;&#x4E0E;&#x8FC7;&#x7A0B;&#x63A9;&#x853D;&#x6E90;/&#x9776;&#x5E8F;&#x5217;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>SRC</strong> - &#xFF08;&#x5FC5;&#x9700;&#xFF09;&#x5E8F;&#x5217;&#x4E0E;&#x7F16;&#x7801;&#x5668;&#x3002;</p>
</li>
<li><p><strong>TGT</strong> - &#x7684;&#x987A;&#x5E8F;&#x5411;&#x89E3;&#x7801;&#x5668;&#xFF08;&#x5FC5;&#x9700;&#xFF09;&#x3002;</p>
</li>
<li><p><strong>src_mask</strong> - &#x4E3A;&#x5BF9;SRC&#x5E8F;&#x5217;&#xFF08;&#x4EFB;&#x9009;&#x7684;&#xFF09;&#x6DFB;&#x52A0;&#x5242;&#x63A9;&#x6A21;&#x3002;</p>
</li>
<li><p><strong>tgt_mask</strong> - &#x6DFB;&#x52A0;&#x5242;&#x63A9;&#x7801;TGT&#x5E8F;&#x5217;&#xFF08;&#x53EF;&#x9009;&#xFF09;&#x3002;</p>
</li>
<li><p><strong>memory_mask</strong> - &#x4E3A;&#x5BF9;&#x7F16;&#x7801;&#x5668;&#x8F93;&#x51FA;&#xFF08;&#x53EF;&#x9009;&#xFF09;&#x6DFB;&#x52A0;&#x5242;&#x63A9;&#x6A21;&#x3002;</p>
</li>
<li><p><strong>src_key_padding_mask</strong> - &#x5BF9;&#x4E8E;&#x6BCF;&#x6279;&#x6B21;&#xFF08;&#x53EF;&#x9009;&#xFF09;SRC&#x952E;ByteTensor&#x63A9;&#x6A21;&#x3002;</p>
</li>
<li><p><strong>tgt_key_padding_mask</strong> - &#x7684;ByteTensor&#x63A9;&#x6A21;&#x6BCF;&#x6279;&#xFF08;&#x53EF;&#x9009;&#xFF09;TGT&#x5BC6;&#x94A5;&#x3002;</p>
</li>
<li><p><strong>memory_key_padding_mask</strong> - &#x5BF9;&#x4E8E;&#x6BCF;&#x6279;&#x6B21;&#xFF08;&#x53EF;&#x9009;&#xFF09;&#x5B58;&#x50A8;&#x5668;&#x952E;ByteTensor&#x63A9;&#x6A21;&#x3002;</p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>SRC&#xFF1A; &#xFF08; S  &#xFF0C; N  &#xFF0C; E  &#xFF09; &#xFF08;S&#xFF0C;N&#xFF0C;E&#xFF09; &#xFF08; S  &#xFF0C; N  &#xFF0C; E  &#xFF09; &#x3002;</p>
</li>
<li><p>TGT&#xFF1A; &#xFF08; T  &#xFF0C; N  &#xFF0C; E  &#xFF09; &#xFF08;T&#xFF0C;N&#xFF0C;E&#xFF09; &#xFF08; T  &#xFF0C; N  &#xFF0C; E  &#xFF09; &#x3002;</p>
</li>
<li><p>src_mask&#xFF1A; &#xFF08; S  &#xFF0C; S  &#xFF09; &#xFF08;S&#xFF0C;S&#xFF09; &#xFF08; S  &#xFF0C; S  &#xFF09; &#x3002;</p>
</li>
<li><p>tgt_mask&#xFF1A; &#xFF08; T  &#xFF0C; T  &#xFF09; &#xFF08;T&#xFF0C;T&#xFF09; &#xFF08; T  &#xFF0C; T  &#xFF09; &#x3002;</p>
</li>
<li><p>memory_mask&#xFF1A; &#xFF08; T  &#xFF0C; S  &#xFF09; &#xFF08;T&#xFF0C;S&#xFF09; &#xFF08; T  &#xFF0C; S  &#xFF09; &#x3002;</p>
</li>
<li><p>src_key_padding_mask&#xFF1A; &#xFF08; N  &#xFF0C; S  &#xFF09; &#xFF08;N&#xFF0C;S&#xFF09; &#xFF08; N  &#xFF0C; S  &#xFF09; &#x3002;</p>
</li>
<li><p>tgt_key_padding_mask&#xFF1A; &#xFF08; N  &#xFF0C; T  &#xFF09; &#xFF08;N&#xFF0C;T&#xFF09; &#xFF08; N  &#xFF0C; T  &#xFF09; &#x3002;</p>
</li>
<li><p>memory_key_padding_mask&#xFF1A; &#xFF08; N  &#xFF0C; S  &#xFF09; &#xFF08;N&#xFF0C;S&#xFF09; &#xFF08; N  &#xFF0C; S  &#xFF09; &#x3002;</p>
</li>
</ul>
<p>&#x6CE8;&#x610F;&#xFF1A;[SRC / TGT /&#x5B58;&#x50A8;&#x5668;] _mask&#x5E94;&#x4E0E;&#x6D6E;&#x5B50;&#xFF08;&#x201C; -
INF&#x201D;&#xFF09;&#x88AB;&#x586B;&#x5145;&#x5728;&#x63A9;&#x853D;&#x4F4D;&#x7F6E;&#x548C;&#x6D6E;&#x52A8;&#xFF08;0.0&#xFF09;&#x5176;&#x4ED6;&#x3002;&#x8FD9;&#x4E9B;&#x9762;&#x5177;&#x786E;&#x4FDD;&#x4E86;&#x4F4D;&#x7F6E;&#x9884;&#x6D4B;&#x6211;&#x53EA;&#x53D6;&#x51B3;&#x4E8E;&#x4E1C;&#x7A97;&#x4E8B;&#x53D1;j&#x548C;&#x662F;&#x76F8;&#x540C;&#x7684;&#x6279;&#x5904;&#x7406;&#x4E2D;&#x7684;&#x6BCF;&#x4E2A;&#x5E8F;&#x5217;&#x5E94;&#x7528;&#x3002; [SRC / TGT
/&#x5B58;&#x50A8;&#x5668;] _key_padding_mask&#x5E94;&#x8BE5;&#x662F;&#x4E00;&#x4E2A;ByteTensor&#x5176;&#x4E2D;&#x771F;&#x503C;&#x5E94;&#x4E0E;&#x6D6E;&#x5B50;&#xFF08;&#x201C; -
INF&#x201D;&#xFF09;&#x88AB;&#x63A9;&#x853D;&#x7684;&#x4F4D;&#x7F6E;&#x503C;&#x548C;&#x5047;&#x503C;&#x5C06;&#x4FDD;&#x6301;&#x4E0D;&#x53D8;&#x3002;&#x8BE5;&#x63A9;&#x6A21;&#x53EF;&#x786E;&#x4FDD;&#x6CA1;&#x6709;&#x4FE1;&#x606F;&#x5C06;&#x4ECE;&#x4F4D;&#x7F6E;&#x91C7;&#x53D6;i&#x5982;&#x679C;&#x5B83;&#x88AB;&#x5C4F;&#x853D;&#xFF0C;&#x5E76;&#x4E14;&#x5177;&#x6709;&#x7528;&#x4E8E;&#x6279;&#x5904;&#x7406;&#x4E2D;&#x7684;&#x6BCF;&#x4E2A;&#x5E8F;&#x5217;&#x7684;&#x5355;&#x72EC;&#x7684;&#x63A9;&#x6A21;&#x3002;</p>
<ul>
<li>&#x8F93;&#x51FA;&#xFF1A; &#xFF08; T  &#xFF0C; N  &#xFF0C; E  &#xFF09; &#xFF08;T&#xFF0C;N&#xFF0C;E&#xFF09; &#xFF08; T  &#xFF0C; N  &#xFF0C; E  &#xFF09; &#x3002;</li>
</ul>
<p>&#x6CE8;&#x610F;&#xFF1A;&#x7531;&#x4E8E;&#x5728;&#x53D8;&#x538B;&#x5668;&#x6A21;&#x578B;&#x4E2D;&#x7684;&#x591A;&#x78C1;&#x5934;&#x5173;&#x6CE8;&#x67B6;&#x6784;&#x4E2D;&#xFF0C;&#x53D8;&#x538B;&#x5668;&#x7684;&#x8F93;&#x51FA;&#x5E8F;&#x5217;&#x7684;&#x957F;&#x5EA6;&#x662F;&#x4E00;&#x6837;&#x7684;&#x89E3;&#x7801;&#x5668;&#x7684;&#x8F93;&#x5165;&#x5E8F;&#x5217;&#xFF08;&#x5373;&#x76EE;&#x6807;&#xFF09;&#x7684;&#x957F;&#x5EA6;&#x3002;</p>
<p>&#x5176;&#x4E2D;&#xFF0C;S&#x662F;&#x6E90;&#x5E8F;&#x5217;&#x957F;&#x5EA6;&#xFF0C;T&#x662F;&#x6240;&#x8FF0;&#x9776;&#x5E8F;&#x5217;&#x7684;&#x957F;&#x5EA6;&#xFF0C;N&#x662F;&#x6279;&#x91CF;&#x5927;&#x5C0F;&#xFF0C;E&#x662F;&#x529F;&#x80FD;&#x53F7;&#x7801;</p>
<p>Examples</p>
<pre><code>&gt;&gt;&gt; output = transformer_model(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)
</code></pre><p><code>generate_square_subsequent_mask</code>( <em>sz</em>
)<a href="_modules/torch/nn/modules/transformer.html#Transformer.generate_square_subsequent_mask">[source]</a></p>
<p>&#x751F;&#x6210;&#x5E8F;&#x5217;&#x7684;&#x65B9;&#x5F62;&#x9762;&#x5177;&#x3002;&#x8499;&#x9762;&#x4F4D;&#x7F6E;&#x90FD;&#x5145;&#x6EE1;&#x4E86;&#x6D6E;&#x52A8;&#xFF08;&#x201C; - INF&#x201D;&#xFF09;&#x3002;&#x672A;&#x63A9;&#x853D;&#x7684;&#x4F4D;&#x7F6E;&#x88AB;&#x586B;&#x5145;&#x6709;&#x6D6E;&#x5B50;&#xFF08;0.0&#xFF09;&#x3002;</p>
<h3 id="transformerencoder">TransformerEncoder</h3>
<p><em>class</em><code>torch.nn.``TransformerEncoder</code>( <em>encoder_layer</em> , <em>num_layers</em> ,
<em>norm=None</em>
)<a href="_modules/torch/nn/modules/transformer.html#TransformerEncoder">[source]</a></p>
<p>TransformerEncoder&#x662F;N&#x4E2A;&#x7F16;&#x7801;&#x5668;&#x7684;&#x5C42;&#x7684;&#x53E0;&#x5C42;</p>
<p>Parameters</p>
<ul>
<li><p><strong>encoder_layer</strong> - &#xFF08;&#x5FC5;&#x9700;&#xFF09;TransformerEncoderLayer&#xFF08;&#xFF09;&#x7684;&#x7C7B;&#x7684;&#x5B9E;&#x4F8B;&#x3002;</p>
</li>
<li><p><strong>num_layers</strong> - &#x5728;&#x7F16;&#x7801;&#x5668;&#x4E2D;&#x7684;&#x5B50;&#x7F16;&#x7801;&#x5668;&#x7684;&#x5C42;&#x7684;&#x6570;&#x91CF;&#xFF08;&#x9700;&#x8981;&#xFF09;&#x3002;</p>
</li>
<li><p><strong>&#x89C4;&#x8303;</strong> - &#x5C42;&#x5F52;&#x4E00;&#x90E8;&#x4EF6;&#xFF08;&#x53EF;&#x9009;&#xFF09;&#x3002;</p>
</li>
</ul>
<p>Examples::</p>
<pre><code>&gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model, nhead)
&gt;&gt;&gt; transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)
</code></pre><p><code>forward</code>( <em>src</em> , <em>mask=None</em> , <em>src_key_padding_mask=None</em>
)<a href="_modules/torch/nn/modules/transformer.html#TransformerEncoder.forward">[source]</a></p>
<p>&#x901A;&#x8FC7;&#x4F9D;&#x6B21;endocder&#x5C42;&#x4F20;&#x9012;&#x8F93;&#x5165;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>SRC</strong> - &#xFF08;&#x5FC5;&#x9700;&#xFF09;&#x5E8F;&#x5217;&#x7ED9;&#x89E3;&#x7801;&#x5668;&#x3002;</p>
</li>
<li><p><strong>&#x63A9;&#x6A21;</strong> - &#x4E3A;&#x5BF9;SRC&#x5E8F;&#x5217;&#xFF08;&#x4EFB;&#x9009;&#x7684;&#xFF09;&#x63A9;&#x6A21;&#x3002;</p>
</li>
<li><p><strong>src_key_padding_mask</strong> - &#x5BF9;&#x4E8E;&#x6BCF;&#x6279;&#x6B21;&#xFF08;&#x53EF;&#x9009;&#xFF09;&#x5728;src&#x952E;&#x63A9;&#x6A21;&#x3002;</p>
</li>
</ul>
<p>Shape:</p>
<p>&#x770B;&#x5230;&#x53D8;&#x538B;&#x5668;&#x7C7B;&#x7684;&#x6587;&#x6863;&#x3002;</p>
<h3 id="transformerdecoder">TransformerDecoder</h3>
<p><em>class</em><code>torch.nn.``TransformerDecoder</code>( <em>decoder_layer</em> , <em>num_layers</em> ,
<em>norm=None</em>
)<a href="_modules/torch/nn/modules/transformer.html#TransformerDecoder">[source]</a></p>
<p>TransformerDecoder&#x662F;N&#x89E3;&#x7801;&#x5668;&#x7684;&#x5C42;&#x7684;&#x53E0;&#x5C42;</p>
<p>Parameters</p>
<ul>
<li><p><strong>decoder_layer</strong> - &#x7684;TransformerDecoderLayer&#xFF08;&#xFF09;&#x7684;&#x7C7B;&#x7684;&#x5B9E;&#x4F8B;&#xFF08;&#x5FC5;&#x9700;&#xFF09;&#x3002;</p>
</li>
<li><p><strong>num_layers</strong> - &#x5728;&#x89E3;&#x7801;&#x5668;&#x4E2D;&#x7684;&#x5B50;&#x8BD1;&#x7801;&#x5668;&#xFF0C;&#x5C42;&#x7684;&#x6570;&#x91CF;&#xFF08;&#x9700;&#x8981;&#xFF09;&#x3002;</p>
</li>
<li><p><strong>norm</strong> &#x2013; the layer normalization component (optional).</p>
</li>
</ul>
<p>Examples::</p>
<pre><code>&gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(d_model, nhead)
&gt;&gt;&gt; transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers)
</code></pre><p><code>forward</code>( <em>tgt</em> , <em>memory</em> , <em>tgt_mask=None</em> , <em>memory_mask=None</em> ,
<em>tgt_key_padding_mask=None</em> , <em>memory_key_padding_mask=None</em>
)<a href="_modules/torch/nn/modules/transformer.html#TransformerDecoder.forward">[source]</a></p>
<p>&#x901A;&#x8FC7;&#x4F9D;&#x6B21;&#x89E3;&#x7801;&#x5668;&#x5C42;&#x4F20;&#x9012;&#x7684;&#x8F93;&#x5165;&#xFF08;&#x548C;&#x63A9;&#x6A21;&#xFF09;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>tgt</strong> &#x2013; the sequence to the decoder (required).</p>
</li>
<li><p><strong>&#x5B58;&#x50A8;&#x5668;</strong> - &#x6765;&#x81EA;&#x7F16;&#x7801;&#x5668;&#x7684;&#x6700;&#x540E;&#x4E00;&#x5C42;&#x7684;&#x5E8F;&#x5217;&#xFF08;&#x5FC5;&#x9700;&#xFF09;&#x3002;</p>
</li>
<li><p><strong>tgt_mask</strong> - &#x63A9;&#x6A21;&#x4E3A;TGT&#x5E8F;&#x5217;&#xFF08;&#x53EF;&#x9009;&#xFF09;&#x3002;</p>
</li>
<li><p><strong>memory_mask</strong> - &#x4E3A;&#x5BF9;&#x5B58;&#x50A8;&#x5668;&#x5E8F;&#x5217;&#xFF08;&#x4EFB;&#x9009;&#x7684;&#xFF09;&#x63A9;&#x6A21;&#x3002;</p>
</li>
<li><p><strong>tgt_key_padding_mask</strong> - &#x63A9;&#x6A21;&#x6BCF;&#x6279;&#x6B21;&#xFF08;&#x53EF;&#x9009;&#xFF09;&#x5728;tgt&#x5BC6;&#x94A5;&#x3002;</p>
</li>
<li><p><strong>memory_key_padding_mask</strong> - &#x5BF9;&#x4E8E;&#x6BCF;&#x6279;&#x6B21;&#xFF08;&#x53EF;&#x9009;&#xFF09;&#x5B58;&#x50A8;&#x952E;&#x63A9;&#x6A21;&#x3002;</p>
</li>
</ul>
<p>Shape:</p>
<p>see the docs in Transformer class.</p>
<h3 id="transformerencoderlayer">TransformerEncoderLayer</h3>
<p><em>class</em><code>torch.nn.``TransformerEncoderLayer</code>( <em>d_model</em> , <em>nhead</em> ,
<em>dim_feedforward=2048</em> , <em>dropout=0.1</em>
)<a href="_modules/torch/nn/modules/transformer.html#TransformerEncoderLayer">[source]</a></p>
<p>TransformerEncoderLayer&#x7531;&#x81EA;&#x7ECF;&#x529E;&#x4EBA;&#x53CA;&#x524D;&#x9988;&#x7F51;&#x7EDC;&#x3002;&#x8BE5;&#x6807;&#x51C6;&#x7F16;&#x7801;&#x5C42;&#x662F;&#x57FA;&#x4E8E;&#x7EB8;&#x201C;&#x6CE8;&#x610F;&#x662F;&#x6240;&#x6709;&#x4F60;&#x9700;&#x8981;&#x201D;&#x3002;&#x963F;&#x5E0C;&#x4EC0;&#x74E6;&#x65AF;&#x74E6;&#x5C3C;&#xFF0C;&#x8BFA;&#x59C6;Shazeer&#xFF0C;&#x5C3C;&#x57FA;&#x5E15;&#x5C14;&#x9A6C;&#x96C5;&#x5404;&#x5E03;Uszkoreit&#xFF0C;Llion&#x743C;&#x65AF;&#xFF0C;&#x827E;&#xD1;&#x6208;&#x9EA6;&#x65AF;&#xFF0C;&#x5362;&#x5361;&#x65AF;&#x51EF;&#x6CFD;&#xFF0C;&#x548C;Illia
Polosukhin&#x3002; 2017&#x6CE8;&#x610F;&#x529B;&#x662F;&#x4F60;&#x6240;&#x9700;&#x8981;&#x7684;&#x3002;&#x5728;&#x795E;&#x7ECF;&#x4FE1;&#x606F;&#x5904;&#x7406;&#x7CFB;&#x7EDF;&#x7684;&#x8FDB;&#x6B65;&#xFF0C;6000-6010&#x9875;&#x3002;&#x7528;&#x6237;&#x53EF;&#x4EE5;&#x4FEE;&#x6539;&#x6216;&#x5E94;&#x7528;&#x7A0B;&#x5E8F;&#x4E2D;&#x4EE5;&#x4E0D;&#x540C;&#x7684;&#x65B9;&#x5F0F;&#x5B9E;&#x73B0;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>d_model</strong> - &#x5728;&#x8F93;&#x5165;&#x9884;&#x671F;&#x7279;&#x5F81;&#x7684;&#x6570;&#x91CF;&#xFF08;&#x9700;&#x8981;&#xFF09;&#x3002;</p>
</li>
<li><p><strong>NHEAD</strong> - &#xFF08;&#x5FC5;&#x9700;&#xFF09;&#x5728;multiheadattention&#x6A21;&#x578B;&#x5934;&#x7684;&#x6570;&#x76EE;&#x3002;</p>
</li>
<li><p><strong>dim_feedforward</strong> &#x2013; the dimension of the feedforward network model (default=2048).</p>
</li>
<li><p><strong>dropout</strong> &#x2013; the dropout value (default=0.1).</p>
</li>
</ul>
<p>Examples::</p>
<pre><code>&gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model, nhead)
</code></pre><p><code>forward</code>( <em>src</em> , <em>src_mask=None</em> , <em>src_key_padding_mask=None</em>
)<a href="_modules/torch/nn/modules/transformer.html#TransformerEncoderLayer.forward">[source]</a></p>
<p>&#x901A;&#x8FC7;endocder&#x5C42;&#x4F20;&#x9012;&#x8F93;&#x5165;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>SRC</strong> - &#xFF08;&#x5FC5;&#x9700;&#xFF09;&#x5E8F;&#x5217;&#x63D0;&#x4F9B;&#x7ED9;&#x7F16;&#x7801;&#x5668;&#x5C42;&#x3002;</p>
</li>
<li><p><strong>src_mask</strong> - &#x4E3A;&#x5BF9;SRC&#x5E8F;&#x5217;&#xFF08;&#x4EFB;&#x9009;&#x7684;&#xFF09;&#x63A9;&#x6A21;&#x3002;</p>
</li>
<li><p><strong>src_key_padding_mask</strong> &#x2013; the mask for the src keys per batch (optional).</p>
</li>
</ul>
<p>Shape:</p>
<p>see the docs in Transformer class.</p>
<h3 id="transformerdecoderlayer">TransformerDecoderLayer</h3>
<p><em>class</em><code>torch.nn.``TransformerDecoderLayer</code>( <em>d_model</em> , <em>nhead</em> ,
<em>dim_feedforward=2048</em> , <em>dropout=0.1</em>
)<a href="_modules/torch/nn/modules/transformer.html#TransformerDecoderLayer">[source]</a></p>
<p>TransformerDecoderLayer&#x7531;&#x81EA;&#x7ECF;&#x529E;&#x4EBA;&#xFF0C;&#x591A;&#x5934;&#x7ECF;&#x529E;&#x4EBA;&#x53CA;&#x524D;&#x9988;&#x7F51;&#x7EDC;&#x3002;&#x8BE5;&#x6807;&#x51C6;&#x89E3;&#x7801;&#x5668;&#x5C42;&#x662F;&#x57FA;&#x4E8E;&#x7EB8;&#x201C;&#x6CE8;&#x610F;&#x662F;&#x6240;&#x6709;&#x4F60;&#x9700;&#x8981;&#x201D;&#x3002;&#x963F;&#x5E0C;&#x4EC0;&#x74E6;&#x65AF;&#x74E6;&#x5C3C;&#xFF0C;&#x8BFA;&#x59C6;Shazeer&#xFF0C;&#x5C3C;&#x57FA;&#x5E15;&#x5C14;&#x9A6C;&#x96C5;&#x5404;&#x5E03;Uszkoreit&#xFF0C;Llion&#x743C;&#x65AF;&#xFF0C;&#x827E;&#xD1;&#x6208;&#x9EA6;&#x65AF;&#xFF0C;&#x5362;&#x5361;&#x65AF;&#x51EF;&#x6CFD;&#xFF0C;&#x548C;Illia
Polosukhin&#x3002; 2017&#x6CE8;&#x610F;&#x529B;&#x662F;&#x4F60;&#x6240;&#x9700;&#x8981;&#x7684;&#x3002;&#x5728;&#x795E;&#x7ECF;&#x4FE1;&#x606F;&#x5904;&#x7406;&#x7CFB;&#x7EDF;&#x7684;&#x8FDB;&#x6B65;&#xFF0C;6000-6010&#x9875;&#x3002;&#x7528;&#x6237;&#x53EF;&#x4EE5;&#x4FEE;&#x6539;&#x6216;&#x5E94;&#x7528;&#x7A0B;&#x5E8F;&#x4E2D;&#x4EE5;&#x4E0D;&#x540C;&#x7684;&#x65B9;&#x5F0F;&#x5B9E;&#x73B0;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>d_model</strong> &#x2013; the number of expected features in the input (required).</p>
</li>
<li><p><strong>nhead</strong> &#x2013; the number of heads in the multiheadattention models (required).</p>
</li>
<li><p><strong>dim_feedforward</strong> &#x2013; the dimension of the feedforward network model (default=2048).</p>
</li>
<li><p><strong>dropout</strong> &#x2013; the dropout value (default=0.1).</p>
</li>
</ul>
<p>Examples::</p>
<pre><code>&gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(d_model, nhead)
</code></pre><p><code>forward</code>( <em>tgt</em> , <em>memory</em> , <em>tgt_mask=None</em> , <em>memory_mask=None</em> ,
<em>tgt_key_padding_mask=None</em> , <em>memory_key_padding_mask=None</em>
)<a href="_modules/torch/nn/modules/transformer.html#TransformerDecoderLayer.forward">[source]</a></p>
<p>&#x901A;&#x8FC7;&#x89E3;&#x7801;&#x5668;&#x5C42;&#x4F20;&#x9012;&#x7684;&#x8F93;&#x5165;&#xFF08;&#x548C;&#x63A9;&#x6A21;&#xFF09;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>TGT</strong> - &#x7684;&#x987A;&#x5E8F;&#x5411;&#x89E3;&#x7801;&#x5668;&#x5C42;&#xFF08;&#x5FC5;&#x9700;&#xFF09;&#x3002;</p>
</li>
<li><p><strong>memory</strong> &#x2013; the sequnce from the last layer of the encoder (required).</p>
</li>
<li><p><strong>tgt_mask</strong> &#x2013; the mask for the tgt sequence (optional).</p>
</li>
<li><p><strong>memory_mask</strong> &#x2013; the mask for the memory sequence (optional).</p>
</li>
<li><p><strong>tgt_key_padding_mask</strong> &#x2013; the mask for the tgt keys per batch (optional).</p>
</li>
<li><p><strong>memory_key_padding_mask</strong> &#x2013; the mask for the memory keys per batch (optional).</p>
</li>
</ul>
<p>Shape:</p>
<p>see the docs in Transformer class.</p>
<h2 id="&#x7EBF;&#x6027;&#x5C42;">&#x7EBF;&#x6027;&#x5C42;</h2>
<h3 id="&#x8EAB;&#x4EFD;">&#x8EAB;&#x4EFD;</h3>
<p><em>class</em><code>torch.nn.``Identity</code>( <em>*args</em> , <em>**kwargs</em>
)<a href="_modules/torch/nn/modules/linear.html#Identity">[source]</a></p>
<p>&#x5360;&#x4F4D;&#x7B26;&#x7684;&#x8EAB;&#x4EFD;&#x64CD;&#x4F5C;&#x7B26;&#x662F;&#x53C2;&#x6570;&#x4E0D;&#x654F;&#x611F;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>ARGS</strong> - &#x4EFB;&#x4F55;&#x53C2;&#x6570;&#xFF08;&#x672A;&#x4F7F;&#x7528;&#xFF09;</p>
</li>
<li><p><strong>kwargs</strong> - &#x4EFB;&#x4F55;&#x5173;&#x952E;&#x5B57;&#x53C2;&#x6570;&#xFF08;&#x672A;&#x4F7F;&#x7528;&#xFF09;</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.Identity(54, unused_argument1=0.1, unused_argument2=False)
&gt;&gt;&gt; input = torch.randn(128, 20)
&gt;&gt;&gt; output = m(input)
&gt;&gt;&gt; print(output.size())
torch.Size([128, 20])
</code></pre><h3 id="&#x7EBF;&#x6027;">&#x7EBF;&#x6027;</h3>
<p><em>class</em><code>torch.nn.``Linear</code>( <em>in_features</em> , <em>out_features</em> , <em>bias=True</em>
)<a href="_modules/torch/nn/modules/linear.html#Linear">[source]</a></p>
<p>&#x9002;&#x7528;&#x7684;&#x7EBF;&#x6027;&#x53D8;&#x6362;&#xFF0C;&#x4EE5;&#x5C06;&#x8F93;&#x5165;&#x6570;&#x636E;&#xFF1A; Y  =  &#xD7; A  T  +  b  Y = XA ^ T + b  Y  =  &#xD7; A  T  +  b</p>
<p>Parameters</p>
<ul>
<li><p><strong>in_features</strong> - &#x6BCF;&#x4E2A;&#x8F93;&#x5165;&#x6837;&#x672C;&#x7684;&#x5927;&#x5C0F;</p>
</li>
<li><p><strong>out_features</strong> - &#x6BCF;&#x4E2A;&#x8F93;&#x51FA;&#x6837;&#x672C;&#x7684;&#x5927;&#x5C0F;</p>
</li>
<li><p><strong>&#x504F;&#x538B;</strong> - &#x5982;&#x679C;&#x8BBE;&#x7F6E;&#x4E3A;<code>&#x5047;</code>&#xFF0C;&#x8BE5;&#x5C42;&#x4E0D;&#x4F1A;&#x5B66;&#x6DFB;&#x52A0;&#x5242;&#x504F;&#x538B;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x771F;</code></p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>&#x8F93;&#x5165;&#xFF1A; &#xFF08; N  &#xFF0C; <em>  &#xFF0C; H  i&#x7684; n&#x7684; &#xFF09; &#xFF08;N&#xFF0C;</em>&#xFF0C;H<em> {&#x5728;}&#xFF09; &#xFF08; N  &#xFF0C; <em>  &#xFF0C; H  i&#x7684; n&#x7684; &#xFF09; &#x5176;&#x4E2D; </em>  <em>  </em>  &#x662F;&#x6307;&#x4EFB;&#x4F55;&#x6570;&#x91CF;&#x7684;&#x9644;&#x52A0;&#x7EF4;&#x5EA6;&#x548C; H  i&#x7684; n&#x7684; =  in_features  H</em> {IN} = \&#x6587;&#x672C;{&#x5728;\ _features}  H  i&#x7684; n&#x7684; =  in_features </p>
</li>
<li><p>&#x8F93;&#x51FA;&#xFF1A; &#xFF08; N  &#xFF0C; <em>  &#xFF0C; H  O  U  T  &#xFF09; &#xFF08;N&#xFF0C;</em>&#xFF0C;H<em> {&#x51FA;}&#xFF09; &#xFF08; N  &#xFF0C; *  &#xFF0C; H  O  U  T  &#xFF09;  &#x5176;&#x4E2D;&#x9664;&#x4E86;&#x6700;&#x540E;&#x5C3A;&#x5BF8;&#x662F;&#x76F8;&#x540C;&#x7684;&#x5F62;&#x72B6;&#x7684;&#x8F93;&#x5165;&#xFF0C;&#x5E76; H  O  U  T  =  out_features  H</em> {&#x51FA;} = \&#x6587;&#x672C;{&#x51FA;\ _features}  H  O  U  T  =  out_features  &#x3002;</p>
</li>
</ul>
<p>Variables</p>
<ul>
<li><p><strong>&#x301C;Linear.weight</strong> - &#x5F62;&#x72B6; &#xFF08; out_features&#x7684;&#x6A21;&#x5757;[&#x7684;&#x53EF;&#x5B66;&#x4E60;&#x6743;&#x91CD;HTG11 ] &#xFF0C; in_features  &#xFF09; &#xFF08;\&#x6587;&#x672C;{&#x51FA;\ _features}&#xFF0C;\&#x6587;&#x672C;{&#x5728;\ _features}&#xFF09; &#xFF08; out_features  &#xFF0C; in_features  &#xFF09; &#x3002;&#x7684;&#x503C;&#x662F;&#x4ECE; &#x521D;&#x59CB;&#x5316;U  &#xFF08; -  K  &#xFF0C; K  &#xFF09; \ mathcal&#x3010;U}&#xFF08; - \ SQRT { &#x137;}&#xFF0C;\ SQRT {K}&#xFF09; U  &#xFF08;  -  K  &#xFF0C; K  &#xFF09; &#xFF0C;&#x5176;&#x4E2D; K  =  1  in_features  K = \&#x538B;&#x88C2;{1} {\&#x6587;&#x672C;{&#x5728;\ _features}}  K  =  in_features  1 [HT G237] </p>
</li>
<li><p><strong>&#x301C;Linear.bias</strong> - &#x5F62;&#x72B6; &#xFF08; out_features&#x7684;&#x6A21;&#x5757;[&#x7684;&#x53EF;&#x5B66;&#x4E60;&#x504F;&#x538B;HTG11 ] &#xFF09; &#xFF08;\&#x6587;&#x672C;{&#x51FA;\ _features}&#xFF09; &#xFF08; out_features  &#xFF09; &#x3002;&#x5982;&#x679C;<code>&#x504F;&#x538B;</code>&#x662F;<code>&#x771F;</code>&#x65F6;&#xFF0C;&#x503C;&#x88AB;&#x521D;&#x59CB;&#x5316;&#x4ECE; U  &#xFF08; -  K  &#xFF0C; &#x137;  &#xFF09; \ mathcal&#x3010;U}&#xFF08; - \ SQRT {K}&#xFF0C;\ SQRT {K}&#xFF09; U  &#xFF08; -  K  &#xFF0C; [HT G132] K  &#xFF09; &#x5176;&#x4E2D; K  =  1  in_features  K = \&#x538B;&#x88C2;{1} {\&#x6587;&#x672C;{&#x5728;\ _features}}  K  =  in_features  1 </p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.Linear(20, 30)
&gt;&gt;&gt; input = torch.randn(128, 20)
&gt;&gt;&gt; output = m(input)
&gt;&gt;&gt; print(output.size())
torch.Size([128, 30])
</code></pre><h3 id="&#x53CC;&#x7EBF;&#x6027;">&#x53CC;&#x7EBF;&#x6027;</h3>
<p><em>class</em><code>torch.nn.``Bilinear</code>( <em>in1_features</em> , <em>in2_features</em> , <em>out_features</em>
, <em>bias=True</em> )<a href="_modules/torch/nn/modules/linear.html#Bilinear">[source]</a></p>
<p>&#x9002;&#x7528;&#x53CC;&#x7EBF;&#x6027;&#x53D8;&#x6362;&#x5230;&#x4F20;&#x5165;&#x6570;&#x636E;&#xFF1A; Y  =  &#xD7; 1  A  &#xD7; 2  +  b  Y = X_1&#x7532;X_2 + b  Y  =  &#xD7; 1  A  &#xD7; 2  +
b</p>
<p>Parameters</p>
<ul>
<li><p><strong>in1_features</strong> - &#x6BCF;&#x4E2A;&#x7B2C;&#x4E00;&#x8F93;&#x5165;&#x6837;&#x672C;&#x7684;&#x5927;&#x5C0F;</p>
</li>
<li><p><strong>in2_features</strong> - &#x6BCF;&#x4E2A;&#x7B2C;&#x4E8C;&#x8F93;&#x5165;&#x6837;&#x672C;&#x7684;&#x5927;&#x5C0F;</p>
</li>
<li><p><strong>out_features</strong> &#x2013; size of each output sample</p>
</li>
<li><p><strong>&#x504F;&#x538B;</strong> - &#x5982;&#x679C;&#x8BBE;&#x7F6E;&#x4E3A;False&#xFF0C;&#x8BE5;&#x5C42;&#x4E0D;&#x4F1A;&#x5B66;&#x6DFB;&#x52A0;&#x5242;&#x504F;&#x538B;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x771F;</code></p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>&#x8F93;&#x5165;1&#xFF1A; &#xFF08; N  &#xFF0C; <em>  &#xFF0C; H  i&#x7684; n&#x7684; 1  &#xFF09; &#xFF08;N&#xFF0C;</em>&#xFF0C;H<em> {IN1}&#xFF09; &#xFF08; N  &#xFF0C; *  &#xFF0C; H  i&#x7684; n&#x7684; 1  &#xFF09;  &#x5176;&#x4E2D; H  &#x4E00;&#x4E16; n&#x7684; 1  =  in1_features  H</em> {IN1} = \&#x6587;&#x672C;{IN1 \ _features}  H  i&#x7684; n&#x7684; 1  =  in1_features  &#x548C; <em>  </em>  * [HT G197]  &#x662F;&#x6307;&#x4EFB;&#x4F55;&#x6570;&#x91CF;&#x7684;&#x9644;&#x52A0;&#x7684;&#x7EF4;&#x5EA6;&#x3002;&#x4F46;&#x6240;&#x6709;&#x7684;&#x8F93;&#x5165;&#x7684;&#x6700;&#x540E;&#x4E00;&#x4E2A;&#x7EF4;&#x5EA6;&#x5E94;&#x8BE5;&#x662F;&#x76F8;&#x540C;&#x7684;&#x3002;</p>
</li>
<li><p>&#x8F93;&#x5165;2&#xFF1A; &#xFF08; N  &#xFF0C; <em>  &#xFF0C; H  i&#x7684; n&#x7684; 2  &#xFF09; &#xFF08;N&#xFF0C;</em>&#xFF0C;H<em> {&#x5E73;&#x65B9;&#x82F1;&#x5BF8;}&#xFF09; &#xFF08; N  &#xFF0C; *  &#xFF0C; H  i&#x7684; n&#x7684; 2  &#xFF09;  &#x5176;&#x4E2D; H  &#x4E00;&#x4E16; n&#x7684; 2  =  in2_features  H</em> {&#x5E73;&#x65B9;&#x82F1;&#x5BF8;} = \&#x6587;&#x672C;{&#x5E73;&#x65B9;&#x82F1;&#x5BF8;\ _features}  H  i&#x7684; n&#x7684; 2  =  in2_features  &#x3002;</p>
</li>
<li><p>&#x8F93;&#x51FA;&#xFF1A; &#xFF08; N  &#xFF0C; <em>  &#xFF0C; H  O  U  T  &#xFF09; &#xFF08;N&#xFF0C;</em>&#xFF0C;H<em> {&#x51FA;}&#xFF09; &#xFF08; N  &#xFF0C; *  &#xFF0C; H  O  U  T  &#xFF09;  &#x5176;&#x4E2D; H  &#xD8;  U  T  =  out_features  H</em> {&#x51FA;} = \&#x6587;&#x672C;{&#x51FA;\ _features}  H  O  U  T  =  out_features  &#x548C;&#x6240;&#x6709;&#xFF0C;&#x4F46;&#x6700;&#x540E;&#x4E00;&#x4E2A;&#x7EF4;&#x5EA6;&#x662F;&#x76F8;&#x540C;&#x7684;&#x5F62;&#x72B6;&#x7684;&#x8F93;&#x5165;&#x3002;</p>
</li>
</ul>
<p>Variables</p>
<ul>
<li><p><strong>&#x301C;Bilinear.weight</strong> - &#x5F62;&#x72B6; &#xFF08; out_features&#x7684;&#x6A21;&#x5757;[&#x7684;&#x53EF;&#x5B66;&#x4E60;&#x6743;&#x91CD;HTG11 ] &#xFF0C; in1_features  &#xFF0C; in2_features  &#xFF09; &#xFF08;\&#x6587;&#x672C;{&#x51FA;\ _features} &#xFF0C;\&#x6587;&#x672C;{IN1 \ _features}&#xFF0C;\&#x6587;&#x672C;{&#x5E73;&#x65B9;&#x82F1;&#x5BF8;\ _features}&#xFF09; &#xFF08; out_features  &#xFF0C; in1_features  &#xFF0C; in2_features  &#xFF09; &#x3002;&#x7684;&#x503C;&#x662F;&#x4ECE; &#x521D;&#x59CB;&#x5316;U  &#xFF08; -  K  &#xFF0C; K  &#xFF09; \ mathcal&#x3010;U}&#xFF08; - \ SQRT { &#x137;}&#xFF0C;\ SQRT {K}&#xFF09; U  &#xFF08;  -  K  &#xFF0C; K  [HTG1 55]  &#xFF09; &#xFF0C;&#x5176;&#x4E2D; &#x137;  =  1  in1_features  K = \&#x538B;&#x88C2;{1} {\&#x6587;&#x672C;{IN1 \ _features }}  K  =  in1_features  1  &#x200B;&#x200B; </p>
</li>
<li><p><strong>&#x301C;Bilinear.bias</strong> - &#x5F62;&#x72B6; &#xFF08; out_features&#x7684;&#x6A21;&#x5757;[&#x7684;&#x53EF;&#x5B66;&#x4E60;&#x504F;&#x538B;HTG11 ] &#xFF09; &#xFF08;\&#x6587;&#x672C;{&#x51FA;\ _features}&#xFF09; &#xFF08; out_features  &#xFF09; &#x3002;&#x5982;&#x679C;<code>&#x504F;&#x538B;</code>&#x662F;<code>&#x771F;</code>&#x65F6;&#xFF0C;&#x503C;&#x88AB;&#x521D;&#x59CB;&#x5316;&#x4ECE; U  &#xFF08; -  K  &#xFF0C; &#x137;  &#xFF09; \ mathcal&#x3010;U}&#xFF08; - \ SQRT {K}&#xFF0C;\ SQRT {K}&#xFF09; U  &#xFF08; -  K  &#xFF0C; [HT G132] K  &#xFF09; &#xFF0C;&#x5176;&#x4E2D; K  =  1  in1_features  K = \&#x538B;&#x88C2;{1} {\&#x6587;&#x672C;{IN1 \ _features}}  K  =  in1_features  [HTG22 4]  1 </p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.Bilinear(20, 30, 40)
&gt;&gt;&gt; input1 = torch.randn(128, 20)
&gt;&gt;&gt; input2 = torch.randn(128, 30)
&gt;&gt;&gt; output = m(input1, input2)
&gt;&gt;&gt; print(output.size())
torch.Size([128, 40])
</code></pre><h2 id="&#x6F0F;&#x5931;&#x5C42;">&#x6F0F;&#x5931;&#x5C42;</h2>
<h3 id="&#x964D;">&#x964D;</h3>
<p><em>class</em><code>torch.nn.``Dropout</code>( <em>p=0.5</em> , <em>inplace=False</em>
)<a href="_modules/torch/nn/modules/dropout.html#Dropout">[source]</a></p>
<p>&#x5728;&#x8BAD;&#x7EC3;&#x671F;&#x95F4;&#xFF0C;&#x968F;&#x673A;&#x5F52;&#x96F6;&#x4E00;&#x4E9B;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x4E0E;&#x6982;&#x7387;<code>P</code>&#x4F7F;&#x7528;&#x6837;&#x54C1;&#x4ECE;&#x8D1D;&#x52AA;&#x5229;&#x5206;&#x5E03;&#x5143;&#x4EF6;&#x3002;&#x6BCF;&#x4E2A;&#x901A;&#x9053;&#x5C06;&#x72EC;&#x7ACB;&#x4E8E;&#x6BCF;&#x524D;&#x884C;&#x8C03;&#x7528;&#x6E05;&#x96F6;&#x3002;</p>
<p>&#x8FD9;&#x5DF2;&#x88AB;&#x8BC1;&#x660E;&#x662F;&#x7528;&#x4E8E;&#x6B63;&#x5219;&#x5316;&#x548C;&#x9632;&#x6B62;&#x795E;&#x7ECF;&#x5143;&#x7684;&#x5171;&#x9002;&#x5E94;&#x901A;&#x8FC7;&#x9632;&#x6B62;&#x7279;&#x5F81;&#x68C0;&#x6D4B;&#x5668;&#x7684;&#x4E92;&#x76F8;&#x9002;&#x5E94;&#x7684;&#x6587;&#x4EF6;<a href="https://arxiv.org/abs/1207.0580" target="_blank">&#x63D0;&#x9AD8;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x4E2D;&#x63CF;&#x8FF0;&#x7684;&#x6709;&#x6548;&#x7684;&#x6280;&#x672F;&#x3002;</a></p>
<p>&#x6B64;&#x5916;&#xFF0C;&#x8F93;&#x51FA;&#x7531;&#x4E00;&#x4E2A;&#x56E0;&#x7D20; 1  1  [HTG12&#x7F29;&#x653E;] -  p  \&#x538B;&#x88C2;{1} {1-p}  1  -  p  1
[HTG83&#x8BAD;&#x7EC3;&#x671F;&#x95F4;&#x3002;&#x8FD9;&#x610F;&#x5473;&#x7740;&#xFF0C;&#x5728;&#x8BC4;&#x4F30;&#x671F;&#x95F4;&#x6A21;&#x5757;&#x7B80;&#x5355;&#x8BA1;&#x7B97;&#x6807;&#x8BC6;&#x529F;&#x80FD;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>P</strong> - &#x5143;&#x7D20;&#x7684;&#x6982;&#x7387;&#x5C06;&#x88AB;&#x5F52;&#x96F6;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;0.5</p>
</li>
<li><p><strong>&#x5C31;&#x5730;</strong> - &#x5982;&#x679C;&#x8BBE;&#x7F6E;&#x4E3A;<code>&#x771F;</code>&#xFF0C;&#x4F1A;&#x505A;&#x6B64;&#x64CD;&#x4F5C;&#x5C31;&#x5730;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x5047;</code></p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>&#x8F93;&#x5165;&#xFF1A; &#xFF08; <em>  &#xFF09; &#xFF08;</em>&#xFF09;  &#xFF08; *  &#xFF09; &#x3002;&#x8F93;&#x5165;&#x53EF;&#x4EE5;&#x662F;&#x4EFB;&#x4F55;&#x5F62;&#x72B6;&#x7684;</p>
</li>
<li><p>&#x8F93;&#x51FA;&#xFF1A; &#xFF08; <em>  &#xFF09; &#xFF08;</em>&#xFF09;  &#xFF08; *  &#xFF09; &#x3002;&#x8F93;&#x51FA;&#x662F;&#x76F8;&#x540C;&#x7684;&#x5F62;&#x72B6;&#x4F5C;&#x4E3A;&#x8F93;&#x5165;</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.Dropout(p=0.2)
&gt;&gt;&gt; input = torch.randn(20, 16)
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="dropout2d">Dropout2d</h3>
<p><em>class</em><code>torch.nn.``Dropout2d</code>( <em>p=0.5</em> , <em>inplace=False</em>
)<a href="_modules/torch/nn/modules/dropout.html#Dropout2d">[source]</a></p>
<p>&#x968F;&#x673A;&#x96F6;&#x51FA;&#x6574;&#x4E2A;&#x4FE1;&#x9053;&#xFF08;&#x4FE1;&#x9053;&#x662F;2D&#x7279;&#x5F81;&#x6620;&#x5C04;&#xFF0C;&#x4F8B;&#x5982;&#xFF0C; [HTG6&#xFF1A;J  [HTG9&#xFF1A;J  [HTG18&#xFF1A;J  &#x7684;&#x7B2C;&#x4FE1;&#x9053; i&#x7684; i&#x7684; i&#x7684; &#x5728;&#x6210;&#x6279;&#x8F93;&#x5165;&#x7B2C;&#x6837;&#x54C1;&#x662F;&#x4E8C;&#x7EF4;&#x5F20;&#x91CF;
&#x8F93;&#x5165; [ i&#x7684; &#xFF0C; [HTG62&#xFF1A;J  \&#x6587;&#x672C;{&#x8F93;&#x5165;} [I&#xFF0C;J]  &#x8F93;&#x5165; [  i&#x7684; &#xFF0C; [HTG88&#xFF1A;J  [H TG91]
&#xFF09;&#x3002;&#x6BCF;&#x4E2A;&#x4FE1;&#x9053;&#x5C06;&#x4E0E;&#x4F7F;&#x7528;&#x7684;&#x6837;&#x54C1;&#x4ECE;&#x4E00;&#x4E2A;&#x4F2F;&#x52AA;&#x5229;&#x5206;&#x5E03;&#x6982;&#x7387;<code>P</code>&#x72EC;&#x7ACB;&#x5730;&#x7F6E;&#x96F6;&#x5728;&#x6BCF;&#x4E00;&#x4E2A;&#x524D;&#x5411;&#x547C;&#x53EB;&#x3002;</p>
<p>&#x901A;&#x5E38;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x8F93;&#x5165;&#x6765;&#x81EA;<code>nn.Conv2d</code>&#x6A21;&#x5757;&#x3002;</p>
<p>&#x6B63;&#x5982;&#x5728;&#x8BBA;&#x6587;<a href="http://arxiv.org/abs/1411.4280" target="_blank">&#x9AD8;&#x6548;&#x5BF9;&#x8C61;&#x5B9A;&#x4F4D;&#x4F7F;&#x7528;&#x5377;&#x79EF;&#x7F51;&#x7EDC;</a>&#xFF0C;&#x5982;&#x679C;&#x7279;&#x5F81;&#x6620;&#x5C04;&#x5185;&#x7684;&#x76F8;&#x90BB;&#x50CF;&#x7D20;&#x662F;&#x5F3A;&#x76F8;&#x5173;&#x7684;&#x63CF;&#x8FF0;&#xFF08;&#x5982;&#x901A;&#x5E38;&#x5728;&#x65E9;&#x671F;&#x5377;&#x79EF;&#x5C42;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#xFF09;&#xFF0C;&#x90A3;&#x4E48;&#x72EC;&#x7ACB;&#x540C;&#x5206;&#x5E03;&#x8F8D;&#x5B66;&#x4E0D;&#x4F1A;&#x6B63;&#x89C4;&#x5316;&#x7684;&#x6FC0;&#x6D3B;&#x548C;&#x5426;&#x5219;&#x5C06;&#x53EA;&#x662F;&#x5BFC;&#x81F4;&#x4E00;&#x4E2A;&#x6709;&#x6548;&#x7684;&#x5B66;&#x4E60;&#x901F;&#x5EA6;&#x4E0B;&#x964D;&#x3002;</p>
<p>&#x5728;&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;<code>nn.Dropout2d&#xFF08;&#xFF09;</code>&#x5C06;&#x6709;&#x5229;&#x4E8E;&#x4FC3;&#x8FDB;&#x529F;&#x80FD;&#x7684;&#x5730;&#x56FE;&#x4E4B;&#x95F4;&#x7684;&#x72EC;&#x7ACB;&#x6027;&#xFF0C;&#x5E76;&#x5E94;&#x6539;&#x4E3A;&#x4F7F;&#x7528;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>P</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x7684;&#x5143;&#x7D20;&#x7684;&#x6982;&#x7387;&#x662F;&#x96F6;-ED&#x3002;</p>
</li>
<li><p><strong>&#x5C31;&#x5730;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5982;&#x679C;&#x8BBE;&#x5B9A;&#x4E3A;<code>&#x771F;</code>&#xFF0C;&#x4F1A;&#x505A;&#x5C31;&#x5730;&#x8FD9;&#x79CD;&#x64CD;&#x4F5C;</p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>Input: (N,C,H,W)(N, C, H, W)(N,C,H,W)</p>
</li>
<li><p>Output: (N,C,H,W)(N, C, H, W)(N,C,H,W) (same shape as input)</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.Dropout2d(p=0.2)
&gt;&gt;&gt; input = torch.randn(20, 16, 32, 32)
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="dropout3d">Dropout3d</h3>
<p><em>class</em><code>torch.nn.``Dropout3d</code>( <em>p=0.5</em> , <em>inplace=False</em>
)<a href="_modules/torch/nn/modules/dropout.html#Dropout3d">[source]</a></p>
<p>&#x968F;&#x673A;&#x96F6;&#x51FA;&#x6574;&#x4E2A;&#x4FE1;&#x9053;&#xFF08;&#x4FE1;&#x9053;&#x662F;3D&#x7279;&#x5F81;&#x5730;&#x56FE;&#xFF0C;&#x4F8B;&#x5982;&#xFF0C; [HTG6&#xFF1A;J  [HTG9&#xFF1A;J  [HTG18&#xFF1A;J  &#x7684;&#x7B2C;&#x4FE1;&#x9053; i&#x7684; i&#x7684; i&#x7684; &#x5728;&#x6210;&#x6279;&#x8F93;&#x5165;&#x7B2C;&#x6837;&#x54C1;&#x662F;&#x4E09;&#x7EF4;&#x5F20;&#x91CF;
&#x8F93;&#x5165; [ i&#x7684; &#xFF0C; [HTG62&#xFF1A;J  \&#x6587;&#x672C;{&#x8F93;&#x5165;} [I&#xFF0C;J]  &#x8F93;&#x5165; [  i&#x7684; &#xFF0C; [HTG88&#xFF1A;J  [H TG91]
&#xFF09;&#x3002;&#x6BCF;&#x4E2A;&#x4FE1;&#x9053;&#x5C06;&#x4E0E;&#x4F7F;&#x7528;&#x7684;&#x6837;&#x54C1;&#x4ECE;&#x4E00;&#x4E2A;&#x4F2F;&#x52AA;&#x5229;&#x5206;&#x5E03;&#x6982;&#x7387;<code>P</code>&#x72EC;&#x7ACB;&#x5730;&#x7F6E;&#x96F6;&#x5728;&#x6BCF;&#x4E00;&#x4E2A;&#x524D;&#x5411;&#x547C;&#x53EB;&#x3002;</p>
<p>&#x901A;&#x5E38;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x8F93;&#x5165;&#x6765;&#x81EA;<code>nn.Conv3d</code>&#x6A21;&#x5757;&#x3002;</p>
<p>As described in the paper <a href="http://arxiv.org/abs/1411.4280" target="_blank">Efficient Object Localization Using Convolutional
Networks</a> , if adjacent pixels within feature
maps are strongly correlated (as is normally the case in early convolution
layers) then i.i.d. dropout will not regularize the activations and will
otherwise just result in an effective learning rate decrease.</p>
<p>&#x5728;&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;<code>nn.Dropout3d&#xFF08;&#xFF09;</code>&#x5C06;&#x6709;&#x5229;&#x4E8E;&#x4FC3;&#x8FDB;&#x529F;&#x80FD;&#x7684;&#x5730;&#x56FE;&#x4E4B;&#x95F4;&#x7684;&#x72EC;&#x7ACB;&#x6027;&#xFF0C;&#x5E76;&#x5E94;&#x6539;&#x4E3A;&#x4F7F;&#x7528;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>P</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x7684;&#x5143;&#x7D20;&#x7684;&#x6982;&#x7387;&#x5C06;&#x88AB;&#x5F52;&#x96F6;&#x3002;</p>
</li>
<li><p><strong>inplace</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; If set to <code>True</code>, will do this operation in-place</p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>Input: (N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W)</p>
</li>
<li><p>Output: (N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W) (same shape as input)</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.Dropout3d(p=0.2)
&gt;&gt;&gt; input = torch.randn(20, 16, 4, 32, 32)
&gt;&gt;&gt; output = m(input)
</code></pre><h3 id="alphadropout">AlphaDropout</h3>
<p><em>class</em><code>torch.nn.``AlphaDropout</code>( <em>p=0.5</em> , <em>inplace=False</em>
)<a href="_modules/torch/nn/modules/dropout.html#AlphaDropout">[source]</a></p>
<p>&#x9002;&#x7528;&#x963F;&#x5C14;&#x6CD5;&#x5DEE;&#x8D85;&#x8FC7;&#x8F93;&#x5165;&#x3002;</p>
<p>&#x963F;&#x5C14;&#x6CD5;&#x5DEE;&#x662F;&#x4E00;&#x79CD;&#x5DEE;&#x7684;&#x7EF4;&#x6301;&#x81EA;&#x6B63;&#x706B;&#x8D22;&#x4EA7;&#x3002;&#x5BF9;&#x4E8E;&#x5177;&#x6709;&#x96F6;&#x5747;&#x503C;&#x548C;&#x5355;&#x4F4D;&#x6807;&#x51C6;&#x5DEE;&#x7684;&#x8F93;&#x5165;&#xFF0C;&#x963F;&#x5C14;&#x6CD5;&#x5DEE;&#x7684;&#x8F93;&#x51FA;&#x4FDD;&#x6301;&#x539F;&#x59CB;&#x5E73;&#x5747;&#x503C;&#x548C;&#x8F93;&#x5165;&#x7684;&#x6807;&#x51C6;&#x504F;&#x5DEE;&#x3002;&#x963F;&#x5C14;&#x6CD5;&#x964D;&#x53BB;&#x624B;&#x5728;&#x624B;&#x4E0E;&#x6D3B;&#x5316;&#x4E5D;&#x8272;&#x9E7F;&#x51FD;&#x6570;&#xFF0C;&#x8FD9;&#x786E;&#x4FDD;&#x4E86;&#x8F93;&#x51FA;&#x5177;&#x6709;&#x96F6;&#x5747;&#x503C;&#x548C;&#x5355;&#x4F4D;&#x6807;&#x51C6;&#x504F;&#x5DEE;&#x3002;</p>
<p>&#x5728;&#x8BAD;&#x7EC3;&#x671F;&#x95F4;&#xFF0C;&#x5B83;&#x968F;&#x673A;&#x63A9;&#x6A21;&#x4E00;&#x4E9B;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x4E0E;&#x6982;&#x7387; <em>&#x4F7F;&#x7528;&#x6837;&#x54C1;&#x4ECE;&#x4F2F;&#x52AA;&#x5229;&#x5206;&#x5E03;p</em> &#x7684;&#x5143;&#x7D20;&#x3002;&#x5230;&#x5C4F;&#x853D;&#x5143;&#x4EF6;&#x88AB;&#x968F;&#x673A;&#x5316;&#x5728;&#x6BCF;&#x4E2A;&#x524D;&#x5411;&#x547C;&#x53EB;&#xFF0C;&#x5E76;&#x7F29;&#x653E;&#x548C;&#x79FB;&#x52A8;&#x4EE5;&#x4FDD;&#x6301;&#x96F6;&#x5747;&#x503C;&#x548C;&#x5355;&#x4F4D;&#x6807;&#x51C6;&#x504F;&#x5DEE;&#x3002;</p>
<p>&#x5728;&#x8BC4;&#x4F30;&#x8FC7;&#x7A0B;&#x4E2D;&#x7684;&#x6A21;&#x5757;&#x7B80;&#x5355;&#x5730;&#x8BA1;&#x7B97;&#x4E00;&#x4E2A;&#x8EAB;&#x4EFD;&#x529F;&#x80FD;&#x3002;</p>
<p>More details can be found in the paper <a href="https://arxiv.org/abs/1706.02515" target="_blank">Self-Normalizing Neural
Networks</a> .</p>
<p>Parameters</p>
<ul>
<li><p><strong>P</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a>&#xFF09; - &#x7684;&#x5143;&#x7D20;&#x7684;&#x6982;&#x7387;&#x88AB;&#x4E22;&#x5F03;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;0.5</p>
</li>
<li><p><strong>inplace</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; If set to <code>True</code>, will do this operation in-place</p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>Input: (&#x2217;)(*)(&#x2217;) . Input can be of any shape</p>
</li>
<li><p>Output: (&#x2217;)(*)(&#x2217;) . Output is of the same shape as input</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.AlphaDropout(p=0.2)
&gt;&gt;&gt; input = torch.randn(20, 16)
&gt;&gt;&gt; output = m(input)
</code></pre><h2 id="&#x7A00;&#x758F;&#x5C42;">&#x7A00;&#x758F;&#x5C42;</h2>
<h3 id="&#x5D4C;&#x5165;">&#x5D4C;&#x5165;</h3>
<p><em>class</em><code>torch.nn.``Embedding</code>( <em>num_embeddings</em> , <em>embedding_dim</em> ,
<em>padding_idx=None</em> , <em>max_norm=None</em> , <em>norm_type=2.0</em> ,
<em>scale_grad_by_freq=False</em> , <em>sparse=False</em> , <em>_weight=None</em>
)<a href="_modules/torch/nn/modules/sparse.html#Embedding">[source]</a></p>
<p>&#x5B58;&#x50A8;&#x4E00;&#x4E2A;&#x56FA;&#x5B9A;&#x5B57;&#x5178;&#x548C;&#x5C3A;&#x5BF8;&#x7684;&#x5D4C;&#x5165;&#x7B80;&#x5355;&#x7684;&#x67E5;&#x627E;&#x8868;&#x3002;</p>
<p>&#x8BE5;&#x6A21;&#x5757;&#x901A;&#x5E38;&#x7528;&#x4E8E;&#x5B58;&#x50A8;&#x5B57;&#x7684;&#x5D4C;&#x5165;&#xFF0C;&#x5E76;&#x4F7F;&#x7528;&#x7D22;&#x5F15;&#x8FDB;&#x884C;&#x68C0;&#x7D22;&#x3002;&#x8F93;&#x5165;&#x5230;&#x6A21;&#x5757;&#x662F;&#x6307;&#x6570;&#x5217;&#x8868;&#xFF0C;&#x5E76;&#x4E14;&#x8F93;&#x51FA;&#x662F;&#x5BF9;&#x5E94;&#x7684;&#x5B57;&#x7684;&#x5D4C;&#x5165;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>num_embeddings</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x7684;&#x5D4C;&#x5165;&#x7684;&#x8BCD;&#x5178;&#x7684;&#x5927;&#x5C0F;</p>
</li>
<li><p><strong>embedding_dim</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x5404;&#x5D4C;&#x5165;&#x77E2;&#x91CF;&#x7684;&#x5927;&#x5C0F;</p>
</li>
<li><p><strong>padding_idx</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5982;&#x679C;&#x7ED9;&#x5B9A;&#x7684;&#xFF0C;&#x57AB;&#x5728;&#x4E0E;&#x5D4C;&#x5165;&#x77E2;&#x91CF;&#x8F93;&#x51FA;<code>padding_idx</code>&#xFF08;&#x521D;&#x59CB;&#x5316;&#x4E3A;&#x96F6;&#xFF09;&#x6BCF;&#x5F53;&#x9047;&#x5230;&#x7684;&#x7D22;&#x5F15;&#x3002;</p>
</li>
<li><p><strong>max_norm</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5982;&#x679C;&#x7ED9;&#x5B9A;&#x7684;&#xFF0C;&#x5177;&#x6709;&#x8303;&#x6570;&#x5927;&#x4E8E;&#x5404;&#x5D4C;&#x5165;&#x77E2;&#x91CF;<code>max_norm</code>&#x88AB;&#x91CD;&#x65B0;&#x5F52;&#x4E00;&#x5316;&#xFF0C;&#x4EE5;&#x5177;&#x6709;&#x89C4;&#x8303;<code>max_norm</code>&#x3002;</p>
</li>
<li><p><strong>norm_type</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x7684;p&#x8303;&#x6570;&#x7684;p&#x6765;&#x8BA1;&#x7B97;&#x7528;&#x4E8E;<code>max_norm</code>&#x9009;&#x9879;&#x3002;&#x9ED8;&#x8BA4;<code>2</code>&#x3002;</p>
</li>
<li><p><strong>scale_grad_by_freq</strong> &#xFF08; <em>&#x5E03;&#x5C14;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5982;&#x679C;&#x7ED9;&#x51FA;&#xFF0C;&#x8FD9;&#x5C06;&#x901A;&#x8FC7;&#x7684;&#x8BDD;&#x9891;&#x7387;&#x5728;&#x5FAE;&#x578B;&#x9006;&#x6269;&#x5C55;&#x68AF;&#x5EA6;&#x6279;&#x91CF;&#x3002;&#x9ED8;&#x8BA4;&#x7684;<code>&#x5047; [HTG11&#x3002;</code></p>
</li>
<li><p><strong>&#x7A00;&#x758F;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5982;&#x679C;<code>&#x771F;</code>&#xFF0C;&#x68AF;&#x5EA6;WRT <code>&#x91CD;&#x91CF;</code>&#x77E9;&#x9635;&#x5C06;&#x662F;&#x7A00;&#x758F;&#x5F20;&#x91CF;&#x3002;&#x8BF7;&#x53C2;&#x9605;&#x6709;&#x5173;&#x7A00;&#x758F;&#x68AF;&#x5EA6;&#x66F4;&#x591A;&#x7EC6;&#x8282;&#x8BF4;&#x660E;&#x3002;</p>
</li>
</ul>
<p>Variables</p>
<p><strong>&#x301C;Embedding.weight</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; -
&#x5F62;&#x72B6;&#x7684;&#x6A21;&#x5757;&#x7684;&#x53EF;&#x5B66;&#x4E60;&#x6743;&#x91CD;&#xFF08;num_embeddings&#xFF0C;embedding_dim&#xFF09;&#x4ECE; &#x521D;&#x59CB;&#x5316; N  &#xFF08; 0  &#xFF0C; 1  &#xFF09; \ mathcal
{N}&#xFF08;0&#xFF0C;1&#xFF09; N  &#xFF08; 0  &#xFF0C; 1  &#xFF09;</p>
<p>Shape:</p>
<ul>
<li><p>&#x8F93;&#x5165;&#xFF1A; &#xFF08; <em>  &#xFF09; &#xFF08;</em>&#xFF09;  &#xFF08; *  &#xFF09; &#xFF0C;&#x4EFB;&#x610F;&#x5F62;&#x72B6;&#x7684;LongTensor&#x5305;&#x542B;&#x7684;&#x7D22;&#x5F15;&#x6765;&#x63D0;&#x53D6;</p>
</li>
<li><p>&#x8F93;&#x51FA;&#xFF1A; &#xFF08; <em>  &#xFF0C; H  &#xFF09; &#xFF08;</em>&#xFF0C;H&#xFF09; &#xFF08; <em>  &#xFF0C; H  &#xFF09; &#xFF0C;&#x5176;&#x4E2D; </em> &#x662F;&#x8F93;&#x5165;&#x5F62;&#x72B6;&#x548C; H  =  embedding_dim  H = \&#x6587;&#x672C;{&#x5D4C;&#x5165;\ _dim}  H  =  embedding_dim </p>
</li>
</ul>
<p>Note</p>
<p>&#x8BF7;&#x8BB0;&#x4F4F;&#xFF0C;&#x53EA;&#x6709;&#x4F18;&#x5316;&#x7684;&#x6570;&#x91CF;&#x6709;&#x9650;&#x652F;&#x6301;&#x7A00;&#x758F;&#x68AF;&#x5EA6;&#xFF1A;&#x76EE;&#x524D;&#x5B83;&#x7684;<code>optim.SGD</code>&#xFF08; CUDA &#x548C; CPU &#xFF09;&#xFF0C;<code>optim.SparseAdam</code>&#xFF08;
CUDA &#x548C; CPU &#xFF09;&#x548C;<code>optim.Adagrad</code>&#xFF08; CPU &#xFF09;</p>
<p>Note</p>
<p>&#x4E0E;<code>padding_idx</code>&#x7EC4;&#xFF0C;&#x5728;<code>&#x7684;&#x5D4C;&#x5165;&#x77E2;&#x91CF;padding_idx</code>&#x88AB;&#x521D;&#x59CB;&#x5316;&#x4E3A;&#x5168;&#x96F6;&#x3002;&#x7136;&#x800C;&#xFF0C;&#x6CE8;&#x610F;&#xFF0C;&#x8FD9;&#x8F7D;&#x4F53;&#x53EF;&#x968F;&#x540E;&#x4F7F;&#x7528;&#x5B9A;&#x5236;&#x521D;&#x59CB;&#x5316;&#x65B9;&#x6CD5;&#x6765;&#x4FEE;&#x9970;&#xFF0C;&#x4F8B;&#x5982;&#xFF0C;&#x4ECE;&#x800C;&#x6539;&#x53D8;&#x7528;&#x4E8E;&#x57AB;&#x7684;&#x8F93;&#x51FA;&#x77E2;&#x91CF;&#x3002;&#x4ECE; <code>&#x5D4C;&#x5165;</code>&#x672C;&#x77E2;&#x91CF;&#x68AF;&#x5EA6;&#x59CB;&#x7EC8;&#x4E3A;&#x96F6;&#x3002;</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; # an Embedding module containing 10 tensors of size 3
&gt;&gt;&gt; embedding = nn.Embedding(10, 3)
&gt;&gt;&gt; # a batch of 2 samples of 4 indices each
&gt;&gt;&gt; input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])
&gt;&gt;&gt; embedding(input)
tensor([[[-0.0251, -1.6902,  0.7172],
         [-0.6431,  0.0748,  0.6969],
         [ 1.4970,  1.3448, -0.9685],
         [-0.3677, -2.7265, -0.1685]],

        [[ 1.4970,  1.3448, -0.9685],
         [ 0.4362, -0.4004,  0.9400],
         [-0.6431,  0.0748,  0.6969],
         [ 0.9124, -2.3616,  1.1151]]])


&gt;&gt;&gt; # example with padding_idx
&gt;&gt;&gt; embedding = nn.Embedding(10, 3, padding_idx=0)
&gt;&gt;&gt; input = torch.LongTensor([[0,2,0,5]])
&gt;&gt;&gt; embedding(input)
tensor([[[ 0.0000,  0.0000,  0.0000],
         [ 0.1535, -2.0309,  0.9315],
         [ 0.0000,  0.0000,  0.0000],
         [-0.1655,  0.9897,  0.0635]]])
</code></pre><p><em>classmethod</em><code>from_pretrained</code>( <em>embeddings</em> , <em>freeze=True</em> ,
<em>padding_idx=None</em> , <em>max_norm=None</em> , <em>norm_type=2.0</em> ,
<em>scale_grad_by_freq=False</em> , <em>sparse=False</em>
)<a href="_modules/torch/nn/modules/sparse.html#Embedding.from_pretrained">[source]</a></p>
<p>&#x521B;&#x5EFA;&#x4E00;&#x4E2A;&#x4ECE;&#x7ED9;&#x5B9A;&#x7684;2&#x7EF4;FloatTensor&#x5D4C;&#x5165;&#x5B9E;&#x4F8B;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x7684;&#x5D4C;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - FloatTensor&#x542B;&#x6709;&#x7528;&#x4E8E;&#x5D4C;&#x5165;&#x6743;&#x91CD;&#x3002;&#x7B2C;&#x4E00;&#x7EF4;&#x5EA6;&#x88AB;&#x4F20;&#x9012;&#x7ED9;&#x5D4C;&#x5165;&#x4E3A;<code>num_embeddings</code>&#xFF0C;&#x7B2C;&#x4E8C;&#x4E3A;<code>embedding_dim</code>&#x3002;</p>
</li>
<li><p><strong>&#x51BB;&#x7ED3;</strong> &#xFF08; <em>&#x5E03;&#x5C14;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5982;&#x679C;<code>&#x771F;</code>&#x65F6;&#xFF0C;&#x5F20;&#x91CF;&#x4E0D;&#x5728;&#x5B66;&#x4E60;&#x8FC7;&#x7A0B;&#x4E2D;&#x5F97;&#x5230;&#x66F4;&#x65B0;&#x3002;&#x7B49;&#x4EF7;&#x4E8E;<code>embedding.weight.requires_grad  =  &#x5047;</code>&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x771F;</code></p>
</li>
<li><p><strong>padding_idx</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x53C2;&#x89C1;&#x6A21;&#x5757;&#x7684;&#x521D;&#x59CB;&#x5316;&#x6587;&#x6863;&#x3002;</p>
</li>
<li><p><strong>max_norm</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x53C2;&#x89C1;&#x6A21;&#x5757;&#x7684;&#x521D;&#x59CB;&#x5316;&#x6587;&#x6863;&#x3002;</p>
</li>
<li><p><strong>norm_type</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x53C2;&#x89C1;&#x6A21;&#x5757;&#x7684;&#x521D;&#x59CB;&#x5316;&#x6587;&#x6863;&#x3002;&#x9ED8;&#x8BA4;<code>2</code>&#x3002;</p>
</li>
<li><p><strong>scale_grad_by_freq</strong> &#xFF08; <em>&#x5E03;&#x5C14;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x53C2;&#x89C1;&#x6A21;&#x5757;&#x7684;&#x521D;&#x59CB;&#x5316;&#x6587;&#x6863;&#x3002;&#x9ED8;&#x8BA4;&#x7684;<code>&#x5047; [HTG11&#x3002;</code></p>
</li>
<li><p><strong>&#x7A00;&#x758F;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x53C2;&#x89C1;&#x6A21;&#x5757;&#x7684;&#x521D;&#x59CB;&#x5316;&#x6587;&#x6863;&#x3002;</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; # FloatTensor containing pretrained weights
&gt;&gt;&gt; weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])
&gt;&gt;&gt; embedding = nn.Embedding.from_pretrained(weight)
&gt;&gt;&gt; # Get embeddings for index 1
&gt;&gt;&gt; input = torch.LongTensor([1])
&gt;&gt;&gt; embedding(input)
tensor([[ 4.0000,  5.1000,  6.3000]])
</code></pre><h3 id="embeddingbag">EmbeddingBag</h3>
<p><em>class</em><code>torch.nn.``EmbeddingBag</code>( <em>num_embeddings</em> , <em>embedding_dim</em> ,
<em>max_norm=None</em> , <em>norm_type=2.0</em> , <em>scale_grad_by_freq=False</em> , <em>mode=&apos;mean&apos;</em>
, <em>sparse=False</em> , <em>_weight=None</em>
)<a href="_modules/torch/nn/modules/sparse.html#EmbeddingBag">[source]</a></p>
<p>&#x8BA1;&#x7B97;&#x7684;&#x7684;&#x5D4C;&#x5165;&#x7684;&#x201C;&#x888B;&#x201D;&#xFF0C;&#x548C;&#x6216;&#x88C5;&#x7F6E;&#x6CA1;&#x6709;&#x5B9E;&#x4F8B;&#x7684;&#x4E2D;&#x95F4;&#x7684;&#x5D4C;&#x5165;&#x3002;</p>
<p>&#x6052;&#x5B9A;&#x957F;&#x5EA6;&#x7684;&#x888B;&#x548C;&#x65E0;<code>per_sample_weights</code>&#xFF0C;&#x8FD9;&#x4E2A;&#x7C7B;</p>
<blockquote>
<ul>
<li>&#x4E0E;<code>&#x6A21;&#x5F0F;= &#x201C;&#x603B;&#x548C;&#x201D;</code>&#x7B49;&#x4E8E; <code>`&#x63A5;&#x7740;</code>torch&#x5D4C;&#x5165; `&#x3002;&#x603B;&#x548C;&#xFF08;&#x6697;= 0&#xFF09;</li>
</ul>
</blockquote>
<p>&gt;</p>
<blockquote>
<ul>
<li>&#x63A5;&#x7740;<code>torch&#x4E0E;</code>&#x6A21;&#x5F0F;= &#x201C;&#x610F;&#x6307;&#x201D; <code>&#x7B49;&#x4E8E;</code>&#x5D4C;&#x5165; <code>&#x3002;&#x5E73;&#x5747;&#xFF08;&#x6697;&#x6DE1;= 0&#xFF09;</code></li>
</ul>
</blockquote>
<p>&gt;</p>
<blockquote>
<ul>
<li>&#x4E0E;<code>&#x6A21;&#x5F0F;= &#x201C;&#x6700;&#x5927;&#x201D;</code>&#x7B49;&#x4E8E; <code>`&#x63A5;&#x7740;</code>torch&#x5D4C;&#x5165; `&#x3002;&#x6700;&#x5927;&#xFF08;&#x6697;= 0&#xFF09; &#x3002;</li>
</ul>
</blockquote>
<p>&gt;</p>
<p>&gt;</p>
<p>&#x7136;&#x800C;&#xFF0C; <code>EmbeddingBag</code>&#x505A;&#x82B1;&#x8D39;&#x66F4;&#x591A;&#x65F6;&#x95F4;&#x548C;&#x5B58;&#x50A8;&#x5668;&#x4E0D;&#x662F;&#x4F7F;&#x7528;&#x8FD9;&#x4E9B;&#x64CD;&#x4F5C;&#x4E2D;&#x7684;&#x4E00;&#x4E2A;&#x94FE;&#x9AD8;&#x6548;&#x3002;</p>
<p>EmbeddingBag&#x8FD8;&#x652F;&#x6301;&#x6BCF;&#x4E2A;&#x6837;&#x54C1;&#x91CD;&#x91CF;&#x4F5C;&#x4E3A;&#x53C2;&#x6570;&#x4F20;&#x9012;&#x7ED9;&#x76F4;&#x4F20;&#x3002;&#x8FD9;&#x4E2A;&#x7F29;&#x653E;&#x5D4C;&#x5165;&#x7684;&#x8F93;&#x51FA;&#x4F5C;&#x4E3A;&#x7531;<code>&#x6A21;&#x5F0F;</code>&#x4E2D;&#x6307;&#x5B9A;&#x6267;&#x884C;&#x7684;&#x52A0;&#x6743;&#x8FD8;&#x539F;&#x4E4B;&#x524D;&#x3002;&#x5982;&#x679C;<code>per_sample_weights``&#x901A;&#x8FC7;&#xFF0C;&#x4EC5;&#x652F;&#x6301;</code>&#x6A21;&#x5F0F; <code>&#x662F;</code>&#x201C;&#x603B;&#x548C;&#x201D; <code>&#xFF0C;&#x5176;&#x4E2D;&#x6839;&#x636E;</code>per_sample_weights
`&#x8BA1;&#x7B97;&#x7684;&#x52A0;&#x6743;&#x548C;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>num_embeddings</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a>) &#x2013; size of the dictionary of embeddings</p>
</li>
<li><p><strong>embedding_dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a>) &#x2013; the size of each embedding vector</p>
</li>
<li><p><strong>max_norm</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>float</em></a> <em>,</em> <em>optional</em> ) &#x2013; If given, each embedding vector with norm larger than <code>max_norm</code>is renormalized to have norm <code>max_norm</code>.</p>
</li>
<li><p><strong>norm_type</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>float</em></a> <em>,</em> <em>optional</em> ) &#x2013; The p of the p-norm to compute for the <code>max_norm</code>option. Default <code>2</code>.</p>
</li>
<li><p><strong>scale_grad_by_freq</strong> &#xFF08; <em>&#x5E03;&#x5C14;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5982;&#x679C;&#x7ED9;&#x5B9A;&#x7684;&#xFF0C;&#x8FD9;&#x5C06;&#x901A;&#x8FC7;&#x7684;&#x8BDD;&#x9891;&#x7387;&#x5728;&#x5FAE;&#x578B;&#x9006;&#x6269;&#x5C55;&#x68AF;&#x5EA6;&#x6279;&#x91CF;&#x3002;&#x9ED8;&#x8BA4;&#x7684;<code>&#x5047; [HTG11&#x3002;&#x6CE8;&#x610F;&#xFF1A;&#x4E0D;&#x652F;&#x6301;&#x6B64;&#x9009;&#x9879;&#x65F6;</code>&#x6A21;&#x5F0F;= &#x201C;MAX&#x201D; [HTG15&#x3002;``</p>
</li>
<li><p><strong>&#x6A21;&#x5F0F;</strong> &#xFF08; <em>&#x4E32;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - <code>&#x201C;&#x603B;&#x548C;&#x201D;</code>&#xFF0C;<code>&#x201C;&#x7684;&#x610F;&#x601D;&#x662F;&#x201D;</code>&#x6216;<code>&#x201C;&#x6700;&#x5927;&#x201D;</code>&#x3002;&#x6307;&#x5B9A;&#x8981;&#x964D;&#x4F4E;&#x888B;&#x7684;&#x65B9;&#x5F0F;&#x3002; <code>&#x201C;&#x603B;&#x548C;&#x201D;</code>&#x8BA1;&#x7B97;&#x7684;&#x52A0;&#x6743;&#x548C;&#xFF0C;&#x4EE5;<code>per_sample_weights</code>&#x8003;&#x8651;&#x5728;&#x5185;&#x3002; <code>&#x201C;&#x7684;&#x610F;&#x601D;&#x662F;&#x201D;</code>&#x8BA1;&#x7B97;&#x5728;&#x888B;&#x4E2D;&#x7684;&#x503C;&#x7684;&#x5E73;&#x5747;&#x503C;&#xFF0C;<code>&#x201C;&#x6700;&#x5927;&#x201D;</code>&#x8BA1;&#x7B97;&#x5728;&#x6BCF;&#x4E2A;&#x888B;&#x4E2D;&#x7684;&#x6700;&#x5927;&#x503C;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x201C;&#x7684;&#x610F;&#x601D;&#x662F;&#x201D;</code></p>
</li>
<li><p><strong>&#x7A00;&#x758F;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5982;&#x679C;<code>&#x771F;</code>&#xFF0C;&#x68AF;&#x5EA6;WRT <code>&#x91CD;&#x91CF;</code>&#x77E9;&#x9635;&#x5C06;&#x662F;&#x7A00;&#x758F;&#x5F20;&#x91CF;&#x3002;&#x8BF7;&#x53C2;&#x9605;&#x6709;&#x5173;&#x7A00;&#x758F;&#x68AF;&#x5EA6;&#x66F4;&#x591A;&#x7EC6;&#x8282;&#x8BF4;&#x660E;&#x3002;&#x6CE8;&#x610F;&#xFF1A;&#x4E0D;&#x652F;&#x6301;&#x6B64;&#x9009;&#x9879;&#x65F6;<code>&#x6A21;&#x5F0F;= &#x201C;MAX&#x201D; [HTG21&#x3002;</code></p>
</li>
</ul>
<p>Variables</p>
<p><strong>EmbeddingBag.weight</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09;&#x301C;</p>
<ul>
<li>&#xFF08;num_embeddings&#xFF0C;embedding_dim&#xFF09;&#x4ECE;&#x521D;&#x59CB;&#x5316;&#x5F62;&#x72B6;&#x7684;&#x6A21;&#x5757;&#x7684;&#x53EF;&#x5B66;&#x4E60;&#x6743;&#x91CD; N  &#xFF08; 0  &#xFF0C; 1  &#xFF09; \ mathcal
{N}&#xFF08;0&#xFF0C;1&#xFF09; N  &#xFF08; 0  &#xFF0C; 1  &#xFF09; &#x3002;</li>
</ul>
<p>Inputs: <code>input</code>(LongTensor), <code>offsets</code>(LongTensor, optional), and</p>
<p><code>per_index_weights</code>&#xFF08;&#x5F20;&#x91CF;&#xFF0C;&#x53EF;&#x9009;&#xFF09;</p>
<ul>
<li>&#x5982;&#x679C;<code>&#x8F93;&#x5165;</code>&#x662F;&#x5F62;&#x72B6;&#x7684;2D &#xFF08;B&#xFF0C;N&#xFF09;</li>
</ul>
<p>&#x5B83;&#x4F1A;&#x88AB;&#x89C6;&#x4E3A;<code>B</code>&#x888B;&#xFF08;&#x5E8F;&#x5217;&#xFF09;&#x5404;&#x56FA;&#x5B9A;&#x957F;&#x5EA6;&#x7684;<code>N</code>&#xFF0C;&#x8FD9;&#x5C06;&#x8FD4;&#x56DE;<code>B</code>&#x503C;&#x5728;&#x67D0;&#x79CD;&#x7A0B;&#x5EA6;&#x4E0A;&#x53D6;&#x51B3;&#x4E8E;<code>&#x6A21;&#x5F0F;</code>&#x805A;&#x5408;&#x3002; <code>&#x504F;&#x79FB;</code>&#x88AB;&#x5FFD;&#x7565;&#xFF0C;&#x5E76;&#x4E14;&#x9700;&#x8981;&#x4E3A;<code>&#x65E0;</code>&#x5728;&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;&#x3002;</p>
<ul>
<li>&#x5982;&#x679C;<code>&#x8F93;&#x5165;</code>&#x662F;&#x5F62;&#x72B6;&#x7684;1D &#xFF08;N&#xFF09;</li>
</ul>
<p>&#x5B83;&#x4F1A;&#x88AB;&#x89C6;&#x4E3A;&#x591A;&#x4E2A;&#x888B;&#xFF08;&#x5E8F;&#x5217;&#xFF09;&#x7684;&#x7EA7;&#x8054;&#x3002; <code>&#x504F;&#x79FB;</code>&#x9700;&#x8981;&#x4E3A;&#x542B;&#x6709;<code>&#x8F93;&#x5165;</code>&#x6BCF;&#x4E2A;&#x888B;&#x5B50;&#x7684;&#x8D77;&#x59CB;&#x7D22;&#x5F15;&#x4F4D;&#x7F6E;&#x7684;1D&#x5F20;&#x91CF;&#x3002;&#x56E0;&#x6B64;&#xFF0C;&#x5BF9;&#x4E8E;<code>&#x504F;&#x79FB;</code>&#x5F62;&#x72B6;&#x7684;&#xFF08;B&#xFF09;&#xFF0C;<code>&#x8F93;&#x5165;</code>&#x5C06;&#x88AB;&#x89C6;&#x4E3A;&#x5177;&#x6709;<code>B</code>&#x888B;&#x3002;&#x7A7A;&#x888B;&#x901A;&#x8FC7;&#x96F6;&#x586B;&#x5145;&#xFF08;&#x5373;&#xFF0C;&#x5177;&#x6709;&#x957F;&#x5EA6;&#x4E3A;0&#x7684;&#xFF09;&#x5C06;&#x5DF2;&#x7ECF;&#x8FD4;&#x56DE;&#x5411;&#x91CF;&#x3002;</p>
<p>per_sample_weights (Tensor, optional): a tensor of float / double weights, or
None</p>
<p>&#x4EE5;&#x6307;&#x793A;&#x6240;&#x6709;&#x7684;&#x6743;&#x91CD;&#x5E94;&#x53D6;&#x4E3A;<code>1</code>&#x3002;&#x5982;&#x679C;&#x5DF2;&#x6307;&#x5B9A;&#xFF0C;<code>per_sample_weights</code>&#x5FC5;&#x987B;&#x5177;&#x6709;&#x5B8C;&#x5168;&#x76F8;&#x540C;&#x7684;&#x5F62;&#x72B6;&#x4F5C;&#x4E3A;&#x8F93;&#x5165;&#xFF0C;&#x88AB;&#x89C6;&#x4E3A;&#x5177;&#x6709;&#x76F8;&#x540C;&#x7684;<code>&#x504F;&#x79FB;</code>&#xFF0C;&#x5982;&#x679C;&#x8FD9;&#x4E9B;&#x90FD;&#x6CA1;&#x6709;<code>&#x65E0;</code>&#x3002;&#x4EC5;&#x652F;&#x6301;<code>&#x6A21;&#x5F0F;= &apos;&#x603B;&#x548C;&apos;</code>&#x3002;</p>
<p>&#x8F93;&#x51FA;&#x5F62;&#x72B6;&#xFF1A;&#xFF08;B&#xFF0C;embedding_dim&#xFF09;</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; # an Embedding module containing 10 tensors of size 3
&gt;&gt;&gt; embedding_sum = nn.EmbeddingBag(10, 3, mode=&apos;sum&apos;)
&gt;&gt;&gt; # a batch of 2 samples of 4 indices each
&gt;&gt;&gt; input = torch.LongTensor([1,2,4,5,4,3,2,9])
&gt;&gt;&gt; offsets = torch.LongTensor([0,4])
&gt;&gt;&gt; embedding_sum(input, offsets)
tensor([[-0.8861, -5.4350, -0.0523],
        [ 1.1306, -2.5798, -1.0044]])
</code></pre><p><em>classmethod</em><code>from_pretrained</code>( <em>embeddings</em> , <em>freeze=True</em> , <em>max_norm=None</em>
, <em>norm_type=2.0</em> , <em>scale_grad_by_freq=False</em> , <em>mode=&apos;mean&apos;</em> ,
<em>sparse=False</em>
)<a href="_modules/torch/nn/modules/sparse.html#EmbeddingBag.from_pretrained">[source]</a></p>
<p>&#x4ECE;&#x7ED9;&#x5B9A;&#x7684;2&#x7EF4;FloatTensor&#x521B;&#x5EFA;EmbeddingBag&#x5B9E;&#x4F8B;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x7684;&#x5D4C;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - FloatTensor&#x542B;&#x6709;&#x7528;&#x4E8E;EmbeddingBag&#x6743;&#x91CD;&#x3002;&#x7B2C;&#x4E00;&#x7EF4;&#x5EA6;&#x88AB;&#x4F20;&#x9012;&#x7ED9;EmbeddingBag&#x4E3A;&#x201C;num_embeddings&#x201D;&#xFF0C;&#x7B2C;&#x4E8C;&#x4E3A;&#x201C;embedding_dim&#x201D;&#x3002;</p>
</li>
<li><p><strong>&#x51BB;&#x7ED3;</strong> &#xFF08; <em>&#x5E03;&#x5C14;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5982;&#x679C;<code>&#x771F;</code>&#x65F6;&#xFF0C;&#x5F20;&#x91CF;&#x4E0D;&#x5728;&#x5B66;&#x4E60;&#x8FC7;&#x7A0B;&#x4E2D;&#x5F97;&#x5230;&#x66F4;&#x65B0;&#x3002;&#x7B49;&#x4EF7;&#x4E8E;<code>embeddingbag.weight.requires_grad  =  &#x5047;</code>&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x771F;</code></p>
</li>
<li><p><strong>max_norm</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x53C2;&#x89C1;&#x6A21;&#x5757;&#x7684;&#x521D;&#x59CB;&#x5316;&#x6587;&#x6863;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x65E0;</code></p>
</li>
<li><p><strong>norm_type</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>float</em></a> <em>,</em> <em>optional</em> ) &#x2013; See module initialization documentation. Default <code>2</code>.</p>
</li>
<li><p><strong>scale_grad_by_freq</strong> ( <em>boolean</em> <em>,</em> <em>optional</em> ) &#x2013; See module initialization documentation. Default <code>False</code>.</p>
</li>
<li><p><strong>&#x6A21;&#x5F0F;</strong> &#xFF08; <em>&#x4E32;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x53C2;&#x89C1;&#x6A21;&#x5757;&#x7684;&#x521D;&#x59CB;&#x5316;&#x6587;&#x6863;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x201C;&#x7684;&#x610F;&#x601D;&#x662F;&#x201D;</code></p>
</li>
<li><p><strong>&#x7A00;&#x758F;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x53C2;&#x89C1;&#x6A21;&#x5757;&#x7684;&#x521D;&#x59CB;&#x5316;&#x6587;&#x6863;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x5047; [HTG13&#x3002;</code></p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; # FloatTensor containing pretrained weights
&gt;&gt;&gt; weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])
&gt;&gt;&gt; embeddingbag = nn.EmbeddingBag.from_pretrained(weight)
&gt;&gt;&gt; # Get embeddings for index 1
&gt;&gt;&gt; input = torch.LongTensor([[1, 0]])
&gt;&gt;&gt; embeddingbag(input)
tensor([[ 2.5000,  3.7000,  4.6500]])
</code></pre><h2 id="&#x8DDD;&#x79BB;&#x51FD;&#x6570;">&#x8DDD;&#x79BB;&#x51FD;&#x6570;</h2>
<h3 id="&#x4F59;&#x5F26;&#x76F8;&#x4F3C;&#x6027;">&#x4F59;&#x5F26;&#x76F8;&#x4F3C;&#x6027;</h3>
<p><em>class</em><code>torch.nn.``CosineSimilarity</code>( <em>dim=1</em> , <em>eps=1e-08</em>
)<a href="_modules/torch/nn/modules/distance.html#CosineSimilarity">[source]</a></p>
<p>&#x8FD4;&#x56DE;&#x4E4B;&#x95F4;&#x7684;&#x4F59;&#x5F26;&#x76F8;&#x4F3C;&#x5EA6; &#xD7; 1  X_1  &#xD7; 1  &#x548C; &#xD7; 2  X_2  &#xD7; 2  &#xFF0C;&#x6CBF;&#x7740;&#x660F;&#x6697;&#x8BA1;&#x7B97;&#x3002;</p>
<p>similarity=x1&#x22C5;x2max&#x2061;(&#x2225;x1&#x2225;2&#x22C5;&#x2225;x2&#x2225;2,&#x3F5;).\text{similarity} = \dfrac{x_1 \cdot
x_2}{\max(\Vert x_1 \Vert _2 \cdot \Vert x_2 \Vert _2, \epsilon)}.
similarity=max(&#x2225;x1&#x200B;&#x2225;2&#x200B;&#x22C5;&#x2225;x2&#x200B;&#x2225;2&#x200B;,&#x3F5;)x1&#x200B;&#x22C5;x2&#x200B;&#x200B;.</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x6697;&#x6DE1;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x7EF4;&#x5176;&#x4E2D;&#x4F59;&#x5F26;&#x76F8;&#x4F3C;&#x5EA6;&#x8FDB;&#x884C;&#x8BA1;&#x7B97;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;1</p>
</li>
<li><p><strong>EPS</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5C0F;&#x503C;&#x7531;&#x96F6;&#x907F;&#x514D;&#x5206;&#x88C2;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;1E-8</p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>&#x8F93;&#x5165;1&#xFF1A; &#xFF08; <em>  1  d  &#xFF0C; </em>  2  &#xFF09; &#xFF08;\ ast_1&#xFF0C;d&#xFF0C;\ ast_2&#xFF09; &#xFF08; <em>  1  &#xFF0C; d  &#xFF0C; </em>  2  &#xFF09; &#x5176;&#x4E2D;d&#x662F;&#x5728;&#x4F4D;&#x7F6E;&#x6697;&#x6DE1;</p>
</li>
<li><p>&#x8F93;&#x5165;2&#xFF1A; &#xFF08; <em>  1  d  &#xFF0C; </em>  2  &#xFF09; &#xFF08;\ ast_1&#xFF0C;d&#xFF0C;\ ast_2&#xFF09; &#xFF08; <em>  1  &#xFF0C; d  &#xFF0C; </em>  2  &#xFF09; &#xFF0C;&#x76F8;&#x540C;&#x5F62;&#x72B6;&#x7684;&#x8F93;&#x5165;1</p>
</li>
<li><p>&#x8F93;&#x51FA;&#xFF1A; &#xFF08; <em>  1  </em>  2  &#xFF09; &#xFF08;\ ast_1&#xFF0C;\ ast_2&#xFF09; &#xFF08; <em>  1  &#xFF0C; </em>  2  &#xFF09;</p>
</li>
</ul>
<p>Examples::</p>
<pre><code>&gt;&gt;&gt; input1 = torch.randn(100, 128)
&gt;&gt;&gt; input2 = torch.randn(100, 128)
&gt;&gt;&gt; cos = nn.CosineSimilarity(dim=1, eps=1e-6)
&gt;&gt;&gt; output = cos(input1, input2)
</code></pre><h3 id="pairwisedistance">PairwiseDistance</h3>
<p><em>class</em><code>torch.nn.``PairwiseDistance</code>( <em>p=2.0</em> , <em>eps=1e-06</em> , <em>keepdim=False</em>
)<a href="_modules/torch/nn/modules/distance.html#PairwiseDistance">[source]</a></p>
<p>&#x8BA1;&#x7B97;&#x4E4B;&#x95F4;&#x7684;&#x8F7D;&#x4F53; [HTG7&#x3011;V  1  [HTG13&#x5206;&#x6279;&#x6210;&#x5BF9;&#x8DDD;&#x79BB;] V_1  [HTG23&#x3011;v  1  &#xFF0C; v  2  V_2  [HTG77 &#x3011;v  2
&#x4F7F;&#x7528; &#x7684;p&#x8303;&#x6570;&#xFF1A;</p>
<p>&#x2225;x&#x2225;p=(&#x2211;i=1n&#x2223;xi&#x2223;p)1/p.\Vert x \Vert <em>p = \left( \sum</em>{i=1}^n \vert x_i \vert ^
p \right) ^ {1/p}. &#x2225;x&#x2225;p&#x200B;=(i=1&#x2211;n&#x200B;&#x2223;xi&#x200B;&#x2223;p)1/p.</p>
<p>Parameters</p>
<ul>
<li><p><strong>P</strong> &#xFF08; <em>&#x771F;&#x5B9E;</em> &#xFF09; - &#x8303;&#x6570;&#x5EA6;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;2</p>
</li>
<li><p><strong>EPS</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5C0F;&#x503C;&#x7531;&#x96F6;&#x907F;&#x514D;&#x5206;&#x88C2;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;1E-6</p>
</li>
<li><p><strong>keepdim</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x786E;&#x5B9A;&#x662F;&#x5426;&#x8981;&#x4FDD;&#x6301;&#x5411;&#x91CF;&#x7EF4;&#x5EA6;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;false</p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>&#x8F93;&#x5165;1&#xFF1A; &#xFF08; N  &#xFF0C; d  &#xFF09; &#xFF08;N&#xFF0C;d&#xFF09; &#xFF08; N  &#xFF0C; d  &#xFF09; &#x5176;&#x4E2D; d =&#x8F7D;&#x4F53;&#x5C3A;&#x5BF8;</p>
</li>
<li><p>&#x8F93;&#x5165;2&#xFF1A; &#xFF08; N  &#xFF0C; d  &#xFF09; &#xFF08;N&#xFF0C;d&#xFF09; &#xFF08; N  &#xFF0C; d  &#xFF09; &#xFF0C;&#x76F8;&#x540C;&#x5F62;&#x72B6;&#x7684;&#x8F93;&#x5165;1</p>
</li>
<li><p>&#x8F93;&#x51FA;&#xFF1A; &#xFF08; N  &#xFF09; &#xFF08;N&#xFF09;  &#xFF08; N  &#xFF09; &#x3002;&#x5982;&#x679C;<code>keepdim</code>&#x662F;<code>&#x771F;</code>&#xFF0C;&#x7136;&#x540E; &#xFF08; N  &#xFF0C; 1  &#xFF09; &#xFF08;N&#xFF0C;1&#xFF09; &#xFF08; N  &#xFF0C; 1  &#xFF09; &#x3002;</p>
</li>
</ul>
<p>Examples::</p>
<pre><code>&gt;&gt;&gt; pdist = nn.PairwiseDistance(p=2)
&gt;&gt;&gt; input1 = torch.randn(100, 128)
&gt;&gt;&gt; input2 = torch.randn(100, 128)
&gt;&gt;&gt; output = pdist(input1, input2)
</code></pre><h2 id="&#x635F;&#x5931;&#x51FD;&#x6570;">&#x635F;&#x5931;&#x51FD;&#x6570;</h2>
<h3 id="l1loss">L1Loss</h3>
<p><em>class</em><code>torch.nn.``L1Loss</code>( <em>size_average=None</em> , <em>reduce=None</em> ,
<em>reduction=&apos;mean&apos;</em> )<a href="_modules/torch/nn/modules/loss.html#L1Loss">[source]</a></p>
<p>&#x521B;&#x5EFA;&#x5728;&#x8F93;&#x5165; &#xD7; &#xD7;&#x5404;&#x5143;&#x4EF6;&#x4E4B;&#x95F4;&#x6D4B;&#x91CF;&#x7684;&#x5E73;&#x5747;&#x7EDD;&#x5BF9;&#x8BEF;&#x5DEE;&#xFF08;MAE&#xFF09;&#x7684;&#x6807;&#x51C6; &#xD7; &#x548C;&#x76EE;&#x6807; Y  Y  Y  &#x3002;</p>
<p>&#x672A;&#x8FD8;&#x539F;&#x7684;&#xFF08;&#x5373;&#xFF0C;&#x5177;&#x6709;<code>&#x8FD8;&#x539F;</code>&#x8BBE;&#x7F6E;&#x4E3A; <code>&apos;&#x65E0;&apos;&#xFF09;&#x635F;&#x8017;&#x53EF;&#x4EE5;&#x88AB;&#x63CF;&#x8FF0;&#x4E3A;&#xFF1A;</code></p>
<p>&#x2113;(x,y)=L={l1,&#x2026;,lN}&#x22A4;,ln=&#x2223;xn&#x2212;yn&#x2223;,\ell(x, y) = L = \{l_1,\dots,l_N\}^\top,
\quad l_n = \left| x_n - y_n \right|, &#x2113;(x,y)=L={l1&#x200B;,&#x2026;,lN&#x200B;}&#x22A4;,ln&#x200B;=&#x2223;xn&#x200B;&#x2212;yn&#x200B;&#x2223;,</p>
<p>&#x5176;&#x4E2D; N  N  N  &#x662F;&#x6279;&#x91CF;&#x5927;&#x5C0F;&#x3002;&#x5982;&#x679C;<code>&#x8FD8;&#x539F;</code>&#x4E0D;&#x662F;<code>&apos;&#x65E0;&apos;</code>&#xFF08;&#x9ED8;&#x8BA4;<code>&apos;&#x5E73;&#x5747;&apos;</code>&#xFF09;&#xFF0C;&#x7136;&#x540E;&#xFF1A;</p>
<p>&#x2113;(x,y)={mean&#x2061;(L),if reduction=&#x2019;mean&#x2019;;sum&#x2061;(L),if reduction=&#x2019;sum&#x2019;.\ell(x, y) =
\begin{cases} \operatorname{mean}(L), &amp; \text{if reduction} =
\text{&apos;mean&apos;;}\\ \operatorname{sum}(L), &amp; \text{if reduction} = \text{&apos;sum&apos;.}
\end{cases} &#x2113;(x,y)={mean(L),sum(L),&#x200B;if reduction=&#x2019;mean&#x2019;;if reduction=&#x2019;sum&#x2019;.&#x200B;</p>
<p>&#xD7; &#xD7; &#xD7; &#x548C; Y  Y  Y  &#x662F;&#x4EFB;&#x610F;&#x5F62;&#x72B6;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x603B;&#x7684; n&#x7684; n&#x7684; n&#x7684; &#x6BCF;&#x4E2A;&#x5143;&#x4EF6;&#x3002;</p>
<p>&#x6C42;&#x548C;&#x64CD;&#x4F5C;&#x4ECD;&#x7136;&#x5DE5;&#x4F5C;&#x5728;&#x6240;&#x6709;&#x5143;&#x7D20;&#xFF0C;&#x5E76;&#x9664;&#x4EE5; n&#x7684; n&#x7684; n&#x7684; &#x3002;</p>
<p>&#x9664;&#x4EE5; n&#x7684; n&#x7684; n&#x7684; &#x53EF;&#x907F;&#x514D;&#x5982;&#x679C;&#x4E00;&#x4E2A;&#x96C6;<code>&#x8FD8;&#x539F; =  &apos;&#x548C;&apos;</code>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>size_average</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5DF2;&#x8FC7;&#x65F6;&#xFF08;&#x89C1;<code>&#x8FD8;&#x539F;</code>&#xFF09;&#x3002;&#x9ED8;&#x8BA4;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x635F;&#x5931;&#x5E73;&#x5747;&#x8D85;&#x8FC7;&#x6279;&#x4E2D;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x635F;&#x5931;&#x3002;&#x8BF7;&#x6CE8;&#x610F;&#xFF0C;&#x5BF9;&#x4E8E;&#x4E00;&#x4E9B;&#x635F;&#x5931;&#xFF0C;&#x6709;&#x6BCF;&#x4E2A;&#x6837;&#x54C1;&#x7684;&#x591A;&#x4E2A;&#x5143;&#x7D20;&#x3002;&#x5982;&#x679C;&#x8BE5;&#x5B57;&#x6BB5;<code>size_average</code>&#x88AB;&#x8BBE;&#x5B9A;&#x4E3A;<code>&#x5047;</code>&#x65F6;&#xFF0C;&#x635F;&#x5931;&#x4EE3;&#x66FF;&#x6C42;&#x548C;&#x6BCF;&#x4E2A;minibatch&#x3002;&#x5F53;&#x51CF;&#x5C11;&#x662F;<code>&#x5047;</code>&#x5FFD;&#x7565;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x771F;</code></p>
</li>
<li><p><strong>&#x51CF;&#x5C11;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5DF2;&#x8FC7;&#x65F6;&#xFF08;&#x89C1;<code>&#x8FD8;&#x539F;</code>&#xFF09;&#x3002;&#x9ED8;&#x8BA4;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x635F;&#x8017;&#x8FDB;&#x884C;&#x5E73;&#x5747;&#x6216;&#x6C42;&#x548C;&#x89C2;&#x6D4B;&#x4E3A;&#x89C6;<code>size_average</code>&#x6BCF;&#x4E2A;minibatch&#x3002;&#x5F53;<code>&#x51CF;&#x5C11;</code>&#x662F;<code>&#x5047;</code>&#xFF0C;&#x8FD4;&#x56DE;&#x6BCF;&#x6279;&#x5143;&#x4EF6;&#x7684;&#x635F;&#x8017;&#xFF0C;&#x800C;&#x4E0D;&#x662F;&#x5E76;&#x5FFD;&#x7565;<code>size_average</code>&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x771F;</code></p>
</li>
<li><p><strong>&#x8FD8;&#x539F;</strong> &#xFF08; <em>&#x4E32;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x6307;&#x5B9A;&#x8FD8;&#x539F;&#x5E94;&#x7528;&#x5230;&#x8F93;&#x51FA;&#xFF1A;<code>&apos;&#x65E0;&apos;</code>| <code>&apos;&#x7684;&#x610F;&#x601D;&#x662F;&apos;</code>| <code>&apos;&#x548C;&apos;</code>&#x3002; <code>&apos;&#x65E0;&apos;</code>&#xFF1A;&#x4E0D;&#x964D;&#x4F4E;&#x5C06;&#x88AB;&#x5E94;&#x7528;&#xFF0C;<code>&apos;&#x610F;&#x5473;&apos;</code>&#xFF1A;&#x5C06;&#x8F93;&#x51FA;&#x7684;&#x603B;&#x548C;&#x5C06;&#x901A;&#x8FC7;&#x7684;&#x6570;&#x91CF;&#x6765;&#x5212;&#x5206;&#x5728;&#x8F93;&#x51FA;&#x4E2D;&#xFF0C;<code>&apos;&#x548C;&apos;</code>&#x5143;&#x7D20;&#xFF1A;&#x8F93;&#x51FA;&#x5C06;&#x88AB;&#x7D2F;&#x52A0;&#x3002;&#x6CE8;&#x610F;&#xFF1A;<code>size_average</code>&#x548C;<code>&#x51CF;&#x5C11;</code>&#x5904;&#x4E8E;&#x88AB;&#x6DD8;&#x6C70;&#xFF0C;&#x5E76;&#x4E14;&#x5728;&#x6B64;&#x540C;&#x65F6;&#xFF0C;&#x6307;&#x5B9A;&#x662F;&#x8FD9;&#x4E24;&#x4E2A;&#x53C2;&#x6570;&#x7684;&#x4E2A;&#x6570;&#x5C06;&#x8986;&#x76D6;<code>&#x8FD8;&#x539F;</code>&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&apos;&#x5E73;&#x5747;&apos;</code></p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>&#x8F93;&#x5165;&#xFF1A; &#xFF08; N  &#xFF0C; <em>  &#xFF09; &#xFF08;N&#xFF0C;</em>&#xFF09; &#xFF08; N  &#xFF0C; <em>  &#xFF09; &#x5176;&#x4E2D; </em>  <em>  </em>  &#x624B;&#x6BB5;&#xFF0C;&#x4EFB;&#x610F;&#x6570;&#x91CF;&#x7684;&#x9644;&#x52A0;&#x7EF4;&#x5EA6;&#x7684;</p>
</li>
<li><p>&#x76EE;&#x6807;&#xFF1A; &#xFF08; N  &#xFF0C; <em>  &#xFF09; &#xFF08;N&#xFF0C;</em>&#xFF09; &#xFF08; N  &#xFF0C; *  &#xFF09; &#xFF0C;&#x76F8;&#x540C;&#x5F62;&#x72B6;&#x7684;&#x8F93;&#x5165;</p>
</li>
<li><p>&#x8F93;&#x51FA;&#xFF1A;&#x6807;&#x91CF;&#x3002;&#x5982;&#x679C;<code>&#x8FD8;&#x539F;</code>&#x662F;<code>&apos;&#x65E0;&apos;</code>&#xFF0C;&#x7136;&#x540E; &#xFF08; N  &#xFF0C; <em>  &#xFF09; &#xFF08;N&#xFF0C;</em>&#xFF09; &#xFF08; N  &#xFF0C; *  &#xFF09; &#xFF0C;&#x76F8;&#x540C;&#x5F62;&#x72B6;&#x7684;&#x8F93;&#x5165;</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; loss = nn.L1Loss()
&gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True)
&gt;&gt;&gt; target = torch.randn(3, 5)
&gt;&gt;&gt; output = loss(input, target)
&gt;&gt;&gt; output.backward()
</code></pre><h3 id="mseloss">MSELoss</h3>
<p><em>class</em><code>torch.nn.``MSELoss</code>( <em>size_average=None</em> , <em>reduce=None</em> ,
<em>reduction=&apos;mean&apos;</em> )<a href="_modules/torch/nn/modules/loss.html#MSELoss">[source]</a></p>
<p>&#x521B;&#x5EFA;&#x5728;&#x8F93;&#x5165; &#xD7; [HTG9&#x5404;&#x5143;&#x4EF6;&#x4E4B;&#x95F4;&#x6D4B;&#x91CF;&#x5747;&#x65B9;&#x8BEF;&#x5DEE;&#xFF08;&#x5E73;&#x65B9;L2&#x8303;&#x6570;&#xFF09;&#x7684;&#x6807;&#x51C6;]&#xD7;  &#xD7; &#x548C;&#x76EE;&#x6807; Y  Y  Y  &#x3002;</p>
<p>The unreduced (i.e. with <code>reduction</code>set to <code>&apos;none&apos;</code>) loss can be described
as:</p>
<p>&#x2113;(x,y)=L={l1,&#x2026;,lN}&#x22A4;,ln=(xn&#x2212;yn)2,\ell(x, y) = L = \{l_1,\dots,l_N\}^\top,
\quad l_n = \left( x_n - y_n \right)^2, &#x2113;(x,y)=L={l1&#x200B;,&#x2026;,lN&#x200B;}&#x22A4;,ln&#x200B;=(xn&#x200B;&#x2212;yn&#x200B;)2,</p>
<p>where NNN is the batch size. If <code>reduction</code>is not <code>&apos;none&apos;</code>(default
<code>&apos;mean&apos;</code>), then:</p>
<p>&#x2113;(x,y)={mean&#x2061;(L),if reduction=&#x2019;mean&#x2019;;sum&#x2061;(L),if reduction=&#x2019;sum&#x2019;.\ell(x, y) =
\begin{cases} \operatorname{mean}(L), &amp; \text{if reduction} =
\text{&apos;mean&apos;;}\\ \operatorname{sum}(L), &amp; \text{if reduction} = \text{&apos;sum&apos;.}
\end{cases} &#x2113;(x,y)={mean(L),sum(L),&#x200B;if reduction=&#x2019;mean&#x2019;;if reduction=&#x2019;sum&#x2019;.&#x200B;</p>
<p>xxx and yyy are tensors of arbitrary shapes with a total of nnn elements each.</p>
<p>The sum operation still operates over all the elements, and divides by nnn .</p>
<p>The division by nnn can be avoided if one sets <code>reduction = &apos;sum&apos;</code>.</p>
<p>Parameters</p>
<ul>
<li><p><strong>size_average</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field <code>size_average</code>is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></p>
</li>
<li><p><strong>reduce</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code>is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></p>
</li>
<li><p><strong>reduction</strong> ( <em>string</em> <em>,</em> <em>optional</em> ) &#x2013; Specifies the reduction to apply to the output: <code>&apos;none&apos;</code>| <code>&apos;mean&apos;</code>| <code>&apos;sum&apos;</code>. <code>&apos;none&apos;</code>: no reduction will be applied, <code>&apos;mean&apos;</code>: the sum of the output will be divided by the number of elements in the output, <code>&apos;sum&apos;</code>: the output will be summed. Note: <code>size_average</code>and <code>reduce</code>are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: <code>&apos;mean&apos;</code></p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>Input: (N,&#x2217;)(N, <em>)(N,&#x2217;) where &#x2217;</em>&#x2217; means, any number of additional dimensions</p>
</li>
<li><p>Target: (N,&#x2217;)(N, *)(N,&#x2217;) , same shape as the input</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; loss = nn.MSELoss()
&gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True)
&gt;&gt;&gt; target = torch.randn(3, 5)
&gt;&gt;&gt; output = loss(input, target)
&gt;&gt;&gt; output.backward()
</code></pre><h3 id="crossentropyloss">CrossEntropyLoss</h3>
<p><em>class</em><code>torch.nn.``CrossEntropyLoss</code>( <em>weight=None</em> , <em>size_average=None</em> ,
<em>ignore_index=-100</em> , <em>reduce=None</em> , <em>reduction=&apos;mean&apos;</em>
)<a href="_modules/torch/nn/modules/loss.html#CrossEntropyLoss">[source]</a></p>
<p>&#x8BE5;&#x6807;&#x51C6;&#x7ED3;&#x5408;<code>nn.LogSoftmax&#xFF08;&#xFF09;</code>&#x548C;<code>nn.NLLLoss&#xFF08;&#xFF09;</code>&#x5728;&#x4E00;&#x4E2A;&#x5355;&#x72EC;&#x7684;&#x7C7B;&#x3002;</p>
<p>&#x8BAD;&#x7EC3;&#x4E0E; C &#x7C7B;&#x5206;&#x7C7B;&#x95EE;&#x9898;&#x65F6;&#x662F;&#x6709;&#x7528;&#x7684;&#x3002;&#x5982;&#x679C;&#x63D0;&#x4F9B;&#x7684;&#x8BDD;&#xFF0C;&#x53EF;&#x9009;&#x7684;&#x53C2;&#x6570;<code>&#x91CD;&#x91CF;</code>&#x5E94;&#x8BE5;&#x662F;&#x4E00;&#x4E2A;1D &#x5F20;&#x91CF;&#x91CD;&#x91CF;&#x5206;&#x914D;&#x5230;&#x6BCF;&#x4E2A;&#x7C7B;&#x3002;&#x5F53;&#x4F60;&#x6709;&#x4E00;&#x4E2A;&#x4E0D;&#x5E73;&#x8861;&#x7684;&#x8BAD;&#x7EC3;&#x96C6;&#x8FD9;&#x662F;&#x7279;&#x522B;&#x6709;&#x7528;&#x3002;</p>
<p>&#x7684;&#x8F93;&#x5165;&#x9884;&#x8BA1;&#x5305;&#x542B;&#x751F;&#x7684;&#xFF0C;&#x975E;&#x5F52;&#x4E00;&#x5316;&#x7684;&#x5206;&#x6570;&#x4E3A;&#x6BCF;&#x4E2A;&#x7C7B;&#x3002;</p>
<p>&#x8F93;&#x5165;&#x5FC5;&#x987B;&#x662F;&#x5C3A;&#x5BF8;&#x7684;&#x5F20;&#x91CF;&#x4E3A; &#xFF08; M  I  n&#x7684; i&#x7684; b  &#x4E00; T  C  [HTG26 1 H &#xFF0C; C  &#xFF09; &#xFF08;minibatch&#xFF0C;C&#xFF09; &#xFF08; M  i&#x7684;
n&#x7684; i&#x7684; b  &#x4E00; T  C  H  &#xFF0C; C  &#xFF09; &#x6216; &#xFF08; M  i&#x7684; n&#x7684; i&#x7684; b  &#x4E00; T  C  H [HTG1 01] &#xFF0C; C  &#xFF0C; d  1
&#xFF0C; d  2  &#xFF0C; &#x3002;  &#x3002;  &#x3002;  &#xFF0C; d  K  &#xFF09; &#xFF08;minibatch&#xFF0C;C&#xFF0C;D_1&#xFF0C; D_2&#xFF0C;...&#xFF0C;d_K&#xFF09; &#xFF08; M  i&#x7684; n&#x7684; i&#x7684; b  &#x4E00;
T  C  H  &#xFF0C; C  &#xFF0C; d  1  &#xFF0C; d  [H TG223] 2  &#xFF0C; &#x3002;  &#x3002;  &#x3002;  &#xFF0C; d  K  &#x200B;&#x200B;  &#xFF09; &#x4E0E; K  &#x2265; 1  &#x137;\
GEQ 1  &#x137;  &#x2265; 1  &#x4E3A; K &#x7EF4;&#x60C5;&#x51B5;&#x4E0B;&#xFF08;&#x540E;&#x8FF0;&#xFF09;&#x3002;</p>
<p>&#x8BE5;&#x6807;&#x51C6;&#x9700;&#x8981;&#x4E00;&#x4E2A;&#x7C7B;&#x6307;&#x6570;&#x5728;&#x8303;&#x56F4; [ 0  &#xFF0C; C  -  1  [0&#xFF0C;C-1]  [ 0  &#xFF0C; C  -  1  &#x4F5C;&#x4E3A;&#x7528;&#x4E8E;&#x5927;&#x5C0F; minibatch
[&#x7684;&#x4E00;&#x7EF4;&#x5F20;&#x91CF;&#x7684;&#x6BCF;&#x4E2A;&#x503C;&#x7684;&#x76EE;&#x6807;;]&#x5982;&#x679C; ignore_index &#x88AB;&#x6307;&#x5B9A;&#x65F6;&#xFF0C;&#x8BE5;&#x6807;&#x51C6;&#x4E5F;&#x63A5;&#x53D7;&#x8FD9;&#x4E2A;&#x7C7B;&#x7D22;&#x5F15;&#xFF08;&#x6B64;&#x7D22;&#x5F15;&#x53EF;&#x4EE5;&#x4E0D;&#x4E00;&#x5B9A;&#x662F;&#x5728;&#x7C7B;&#x8303;&#x56F4;&#x5185;&#xFF09;&#x3002;</p>
<p>&#x635F;&#x5931;&#x53EF;&#x4EE5;&#x88AB;&#x63CF;&#x8FF0;&#x4E3A;&#xFF1A;</p>
<p>loss(x,class)=&#x2212;log&#x2061;(exp&#x2061;(x[class])&#x2211;jexp&#x2061;(x[j]))=&#x2212;x[class]+log&#x2061;(&#x2211;jexp&#x2061;(x[j]))\text{loss}(x,
class) = -\log\left(\frac{\exp(x[class])}{\sum_j \exp(x[j])}\right) =
-x[class] + \log\left(\sum_j \exp(x[j])\right)
loss(x,class)=&#x2212;log(&#x2211;j&#x200B;exp(x[j])exp(x[class])&#x200B;)=&#x2212;x[class]+log(j&#x2211;&#x200B;exp(x[j]))</p>
<p>&#x6216;&#x5728;<code>&#x91CD;&#x91CF;</code>&#x53C2;&#x6570;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#x88AB;&#x6307;&#x5B9A;&#x7684;&#xFF1A;</p>
<p>loss(x,class)=weight<a href="&#x2212;x[class]+log&#x2061;(&#x2211;jexp&#x2061;(x[j]">class</a>))\text{loss}(x, class)
= weight[class] \left(-x[class] + \log\left(\sum_j \exp(x[j])\right)\right)
loss(x,class)=weight<a href="&#x2212;x[class]+log(j&#x2211;&#x200B;exp(x[j]">class</a>))</p>
<p>&#x8FD9;&#x4E9B;&#x635F;&#x5931;&#x662F;&#x6574;&#x4E2A;&#x89C2;&#x6D4B;&#x5E73;&#x5747;&#x6BCF;&#x4E2A;minibatch&#x3002;</p>
<p>&#x4E5F;&#x53EF;&#x4EE5;&#x7528;&#x4E8E;&#x66F4;&#x9AD8;&#x7684;&#x5C3A;&#x5BF8;&#x7684;&#x8F93;&#x5165;&#xFF0C;&#x5982;2D&#x56FE;&#x50CF;&#xFF0C;&#x901A;&#x8FC7;&#x63D0;&#x4F9B;&#x5C3A;&#x5BF8; &#xFF08; M [&#x7684;&#x8F93;&#x5165;HTG9]  i&#x7684; n&#x7684; i&#x7684; b  &#x4E00; T  C  H  &#xFF0C; C  &#xFF0C; d  1
&#xFF0C; d  2  &#xFF0C; &#x3002; &#x3002; &#xFF0C; d  K  &#xFF09; &#xFF08;minibatch&#xFF0C;C&#xFF0C;D_1 &#xFF0C;D_2&#xFF0C;...&#xFF0C;d_K&#xFF09; &#xFF08; M  I  n&#x7684; i&#x7684; b  &#x4E00; T  C
[HTG92 1 H  &#xFF0C; C  &#xFF0C; d  1  &#xFF0C; d  2  &#xFF0C; &#x3002;  &#x3002;  &#x3002;  &#xFF0C; d  K  &#xFF09; &#x4E0E; K  &#x2265; 1  &#x137;\ GEQ 1  &#x137;  &#x2265;
1  &#xFF0C;&#x5176;&#x4E2D; K  K  &#x200B;&#x200B; K  &#x662F;&#x7EF4;&#x6570;&#xFF0C;&#x548C;&#x9002;&#x5F53;&#x5F62;&#x72B6;&#x7684;&#x76EE;&#x6807;&#xFF08;&#x89C1;&#x4E0B;&#x6587;&#xFF09;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x91CD;&#x91CF;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x7ED9;&#x6BCF;&#x4E2A;&#x7C7B;&#x7684;&#x624B;&#x52A8;&#x91CD;&#x65B0;&#x7F29;&#x653E;&#x6743;&#x91CD;&#x3002;&#x5982;&#x679C;&#x7ED9;&#x5B9A;&#x7684;&#xFF0C;&#x5FC5;&#x987B;&#x662F;&#x5C3A;&#x5BF8;&#x2103;&#x7684;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>size_average</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field <code>size_average</code>is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></p>
</li>
<li><p><strong>ignore_index</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x6307;&#x5B9A;&#x5C06;&#x88AB;&#x5FFD;&#x7565;&#xFF0C;&#x5E76;&#x4E14;&#x4E0D;&#x5411;&#x76EE;&#x6807;&#x503C;&#x8F93;&#x5165;&#x68AF;&#x5EA6;&#x3002;&#x5F53;<code>size_average</code>&#x662F;<code>&#x771F;</code>&#xFF0C;&#x635F;&#x5931;&#x5E73;&#x5747;&#x8D85;&#x8FC7;&#x975E;&#x5FFD;&#x7565;&#x7684;&#x76EE;&#x6807;&#x3002;</p>
</li>
<li><p><strong>reduce</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code>is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></p>
</li>
<li><p><strong>reduction</strong> ( <em>string</em> <em>,</em> <em>optional</em> ) &#x2013; Specifies the reduction to apply to the output: <code>&apos;none&apos;</code>| <code>&apos;mean&apos;</code>| <code>&apos;sum&apos;</code>. <code>&apos;none&apos;</code>: no reduction will be applied, <code>&apos;mean&apos;</code>: the sum of the output will be divided by the number of elements in the output, <code>&apos;sum&apos;</code>: the output will be summed. Note: <code>size_average</code>and <code>reduce</code>are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: <code>&apos;mean&apos;</code></p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>&#x8F93;&#x5165;&#xFF1A; &#xFF08; N  &#xFF0C; C  &#xFF09; &#xFF08;N&#xFF0C;C&#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF09; &#x5176;&#x4E2D; C =&#x53F7;&#x7801;&#x7C7B;&#x6216; &#xFF08; N  &#xFF0C; &#xE7;  &#xFF0C; d  1  &#xFF0C; d  2  &#xFF0C; &#x3002; &#x3002; &#x3002; &#xFF0C; d  K  &#xFF09; &#xFF08;N&#xFF0C;C&#xFF0C;D_1&#xFF0C;D_2&#xFF0C;...&#xFF0C;d_K&#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF0C; d  1  &#xFF0C; d  2  &#xFF0C; &#x3002;  &#x3002;  &#x3002;  &#xFF0C; d  K  &#xFF09; &#x4E0E; K  &#x2265; 1  &#x137;\ GEQ 1  &#x137;  &#x2265; 1  &#x5982;&#x7684;&#x60C5;&#x51B5;&#x4E0B;K &#x7EF4;&#x635F;&#x5931;&#x3002;</p>
</li>
<li><p>&#x76EE;&#x6807;&#xFF1A; &#xFF08; N  &#xFF09; &#xFF08;N&#xFF09;  &#xFF08; N  &#xFF09; &#x5176;&#x4E2D;&#x6BCF;&#x4E2A;&#x503C;&#x662F; 0  &#x2264; &#x76EE;&#x6807; [ i&#x7684; &#x2264; C  -  1  0 \&#x5F53;&#x91CF;\&#x6587;&#x672C;{&#x76EE;&#x6807;} [I] \&#x5F53;&#x91CF;C-1  0  &#x2264; &#x76EE;&#x6807; [  i&#x7684; &#x2264; C  -  1  &#x6216; &#xFF08; N  &#xFF0C; d  1  &#xFF0C; d  2  &#xFF0C; &#x3002;  &#x3002;  &#x3002;  &#xFF0C; d  K  &#xFF09; &#xFF08;N&#xFF0C;D_1&#xFF0C;D_2&#xFF0C; ...&#xFF0C;d_K&#xFF09; &#xFF08; N  &#xFF0C; d  1  &#xFF0C; d  2  &#xFF0C; &#x3002; [HTG246  &#x3002;  &#xFF0C; d  K  &#x200B;&#x200B;  &#xFF09; &#x4E0E; K  &#x2265; 1  &#x137;\ GEQ 1  &#x137;  &#x2265; 1  &#x5728;K&#x7EF4;&#x635F;&#x5931;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#x3002;</p>
</li>
<li><p>&#x8F93;&#x51FA;&#xFF1A;&#x6807;&#x91CF;&#x3002;&#x5982;&#x679C;<code>&#x8FD8;&#x539F;</code>&#x662F;<code>&apos;&#x65E0;&apos;</code>&#xFF0C;&#x7136;&#x540E;&#x76F8;&#x540C;&#x7684;&#x5C3A;&#x5BF8;&#x4E3A;&#x76EE;&#x6807;&#xFF1A; &#xFF08; N  &#xFF09; &#xFF08;N&#xFF09; &#xFF08; N  &#xFF09; &#x6216; &#xFF08; N  &#xFF0C; d  1  &#xFF0C; d  2  &#xFF0C; &#x3002; &#x3002;  &#x3002; &#xFF0C; d  K  &#xFF09; &#xFF08;N&#xFF0C;D_1&#xFF0C;D_2&#xFF0C;...&#xFF0C;d_K&#xFF09; &#xFF08; N  &#xFF0C;[HT G99]  d  1  &#xFF0C; d  2  &#xFF0C; &#x3002;  &#x3002;  &#x3002;  &#xFF0C; d  K  &#xFF09; &#x4E0E; K  &#x2265; 1  &#x137;\ GEQ 1  &#x137;  &#x2265; 1  &#x5728;K&#x7EF4;&#x635F;&#x5931;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#x3002;</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; loss = nn.CrossEntropyLoss()
&gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True)
&gt;&gt;&gt; target = torch.empty(3, dtype=torch.long).random_(5)
&gt;&gt;&gt; output = loss(input, target)
&gt;&gt;&gt; output.backward()
</code></pre><h3 id="ctcloss">CTCLoss</h3>
<p><em>class</em><code>torch.nn.``CTCLoss</code>( <em>blank=0</em> , <em>reduction=&apos;mean&apos;</em> ,
<em>zero_infinity=False</em>
)<a href="_modules/torch/nn/modules/loss.html#CTCLoss">[source]</a></p>
<p>&#x8BE5;&#x8054;&#x7ED3;&#x989E;&#x5206;&#x7C7B;&#x635F;&#x5931;&#x3002;</p>
<p>&#x8BA1;&#x7B97;&#x8FDE;&#x7EED;&#x7684;&#xFF08;&#x4E0D;&#x5206;&#x6BB5;&#xFF09;&#x7684;&#x65F6;&#x95F4;&#x5E8F;&#x5217;&#x548C;&#x9776;&#x5E8F;&#x5217;&#x4E4B;&#x95F4;&#x7684;&#x635F;&#x8017;&#x3002;
CTCLoss&#x603B;&#x7ED3;&#x4EE5;&#x4E0A;&#x8F93;&#x5165;&#x7684;&#x53EF;&#x80FD;&#x7684;&#x5BF9;&#x51C6;&#x76EE;&#x6807;&#x7684;&#x6982;&#x7387;&#xFF0C;&#x4EA7;&#x751F;&#x5176;&#x662F;&#x53EF;&#x5FAE;&#x5206;&#x7684;&#x76F8;&#x5BF9;&#x4E8E;&#x6BCF;&#x4E2A;&#x8F93;&#x5165;&#x8282;&#x70B9;&#x7684;&#x635F;&#x8017;&#x503C;&#x3002;&#x8F93;&#x5165;&#x5230;&#x76EE;&#x6807;&#x7684;&#x53D6;&#x5411;&#x88AB;&#x5047;&#x5B9A;&#x4E3A;&#x201C;&#x591A;&#x5230;&#x4E00;&#x201D;&#xFF0C;&#x8FD9;&#x9650;&#x5236;&#x4E86;&#x9776;&#x5E8F;&#x5217;&#x7684;&#x957F;&#x5EA6;&#xFF0C;&#x4F7F;&#x5F97;&#x5B83;&#x5FC5;&#x987B;&#x662F;
&#x2264; \&#x5F53;&#x91CF; &#x2264; &#x8F93;&#x5165;&#x957F;&#x5EA6;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x7A7A;&#x767D;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x7A7A;&#x767D;&#x6807;&#x7B7E;&#x3002;&#x9ED8;&#x8BA4; 0  0  0  &#x3002;</p>
</li>
<li><p><strong>&#x8FD8;&#x539F;</strong> &#xFF08; <em>&#x4E32;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x6307;&#x5B9A;&#x8FD8;&#x539F;&#x5E94;&#x7528;&#x5230;&#x8F93;&#x51FA;&#xFF1A;<code>&apos;&#x65E0;&apos;</code>| <code>&apos;&#x7684;&#x610F;&#x601D;&#x662F;&apos;</code>| <code>&apos;&#x548C;&apos;</code>&#x3002; <code>&apos;&#x65E0;&apos;</code>&#xFF1A;&#x4E0D;&#x964D;&#x4F4E;&#x5C06;&#x88AB;&#x5E94;&#x7528;&#xFF0C;<code>&apos;&#x610F;&#x5473;&apos;</code>&#xFF1A;&#x8F93;&#x51FA;&#x635F;&#x8017;&#x5C06;&#x7531;&#x76EE;&#x6807;&#x957F;&#x5EA6;&#xFF0C;&#x7136;&#x540E;&#x88AB;&#x5212;&#x5206;&#x5E73;&#x5747;&#x8D85;&#x8FC7;&#x8BE5;&#x6279;&#x6B21;&#x53D6;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&apos;&#x5E73;&#x5747;&apos;</code></p>
</li>
<li><p><strong>zero_infinity</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x662F;&#x5426;&#x4E3A;&#x96F6;&#x65E0;&#x9650;&#x635F;&#x5931;&#x548C;&#x76F8;&#x5173;&#x8054;&#x7684;&#x68AF;&#x5EA6;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x5047;</code>&#x4E3B;&#x8981;&#x662F;&#x5F53;&#x8F93;&#x5165;&#x592A;&#x77ED;&#xFF0C;&#x65E0;&#x6CD5;&#x5BF9;&#x51C6;&#x76EE;&#x6807;&#x51FA;&#x73B0;&#x65E0;&#x9650;&#x635F;&#x5931;&#x3002;</p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>Log_probs&#xFF1A;&#x7684;&#x5F20;&#x91CF;&#x5927;&#x5C0F; &#xFF08; T  &#xFF0C; N  &#xFF0C; C  &#xFF09; &#xFF08;T&#xFF0C;N&#xFF0C;C&#xFF09; &#xFF08; T  &#xFF0C; N  &#xFF0C; C  &#xFF09; &#xFF0C;&#x5176;&#x4E2D; T  =  &#x8F93;&#x5165;&#x957F;&#x5EA6; T = \&#x6587;&#x672C;{&#x8F93;&#x5165;&#x957F;&#x5EA6;}  T  =  &#x8F93;&#x5165;&#x957F;&#x5EA6; &#xFF0C; N  =  &#x6279;&#x91CF;&#x5927;&#x5C0F; N = \&#x6587;&#x672C;{&#x6279;&#x91CF;&#x5927;&#x5C0F;}  N  =  &#x6279;&#x91CF;&#x5927;&#x5C0F; &#x548C; &#xE7;  =  &#x7684;&#x7C7B;&#xFF08;&#x5305;&#x62EC;&#x7A7A;&#x767D;&#xFF09; C = \&#x6587;&#x672C;{&#x7684;&#x7C7B;&#xFF08;&#x5305;&#x62EC;&#x576F;&#x4EF6;&#xFF09;&#x6570;}&#x6570; C  =  &#x7684;&#x7C7B;&#x53F7;&#x7801;&#xFF08;&#x5305;&#x62EC;&#x7A7A;&#x683C;&#xFF09; &#x3002;&#x7684;&#x8F93;&#x51FA;&#x7684;&#x53D6;&#x5BF9;&#x6570;&#x6982;&#x7387;&#xFF08;&#x4F8B;&#x5982;&#xFF0C;&#x7528;<a href="nn.functional.html#torch.nn.functional.log_softmax" title="torch.nn.functional.log_softmax"> <code>&#x83B7;&#x5F97;torch.nn.functional.log_softmax&#xFF08;&#xFF09;</code></a>&#xFF09;&#x3002;</p>
</li>
<li><p>&#x76EE;&#x6807;&#xFF1A;&#x5927;&#x5C0F; &#xFF08; N  &#xFF0C; S&#x7684;&#x5F20;&#x91CF; &#xFF09; &#xFF08;N&#xFF0C;S&#xFF09; &#xFF08; N  &#xFF0C; S  &#xFF09; &#x6216; &#xFF08; &#x603B;&#x7ED3; &#x2061; &#xFF08; target_lengths  &#xFF09; &#xFF09; &#xFF08;\ operatorname {&#x603B;&#x548C;}&#xFF08;\&#x6587;&#x672C;{&#x76EE;&#x6807;\ _lengths}&#xFF09;&#xFF09; &#xFF08; S  U  M  &#xFF08; target_lengths  &#xFF09; &#xFF09; &#xFF0C;&#x5176;&#x4E2D;[H TG96]  N  =  &#x6279;&#x91CF;&#x5927;&#x5C0F; N = \&#x6587;&#x672C;{&#x6279;&#x91CF;&#x5927;&#x5C0F;}  N  =  &#x6279;&#x91CF;&#x5927;&#x5C0F; &#x548C; S  =  &#x6700;&#x5927;&#x76EE;&#x6807;&#x957F;&#x5EA6;&#xFF0C;&#x5982;&#x679C;&#x5F62;&#x72B6;&#x662F; &#xFF08; N  &#xFF0C; S  &#xFF09; S = \&#x6587;&#x672C;{&#x6700;&#x5927;&#x76EE;&#x6807;&#x957F;&#x5EA6;&#xFF0C;&#x5982;&#x679C;&#x5F62;&#x72B6;&#x4E3A;}&#xFF08;N&#xFF0C;S&#xFF09; S  =  &#x6700;&#x5927;&#x76EE;&#x6807;&#x957F;&#x5EA6;&#xFF0C;&#x5982;&#x679C;&#x5F62;&#x72B6;&#x662F; &#xFF08; N  &#xFF0C; S  &#xFF09; &#x3002;&#x5B83;&#x4EE3;&#x8868;&#x4E86;&#x9776;&#x5E8F;&#x5217;&#x3002;&#x5728;&#x9776;&#x5E8F;&#x5217;&#x4E2D;&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x662F;&#x4E00;&#x4E2A;&#x7C7B;&#x7684;&#x7D22;&#x5F15;&#x3002;&#x548C;&#x76EE;&#x6807;&#x7D22;&#x5F15;&#x4E0D;&#x80FD;&#x4E3A;&#x7A7A;&#xFF08;&#x7F3A;&#x7701;&#x503C;= 0&#xFF09;&#x3002;&#x5728; &#xFF08; N  &#xFF0C; S  &#xFF09; &#xFF08;N&#xFF0C;S&#xFF09; &#xFF08; N  &#xFF0C; S  &#xFF09; &#x5F62;&#x5F0F;&#xFF0C;&#x76EE;&#x6807;&#x88AB;&#x586B;&#x5145;&#x5230;&#x6700;&#x957F;&#x7684;&#x5E8F;&#x5217;&#x7684;&#x957F;&#x5EA6;&#xFF0C;&#x5E76;&#x5806;&#x53E0;&#x3002;&#x5728; &#xFF08; &#x603B;&#x548C; &#x2061; &#xFF08; target_lengths  &#xFF09; &#xFF09; &#xFF08;\ operatorname {&#x603B;&#x548C;}&#xFF08;\&#x6587;&#x672C;{&#x76EE;&#x6807;\ _lengths}&#xFF09;&#xFF09; &#x200B;&#x200B;  &#xFF08; S  U  M  &#xFF08; target_lengths  &#xFF09; &#xFF09; &#x7684;&#x5F62;&#x5F0F;&#x4E2D;&#xFF0C;&#x76EE;&#x6807;&#x662F;&#x5047;&#x5B9A;&#x4E3A;&#x672A;&#x586B;&#x5145;&#x7684;&#x548C;1&#x540D;&#x7EF4;&#x4E2D;&#x4E32;&#x8054;&#x3002;</p>
</li>
<li><p>Input_lengths&#xFF1A;&#x5143;&#x7EC4;&#x6216;&#x7684;&#x5927;&#x5C0F; &#xFF08; N  &#xFF09;&#x5F20;&#x91CF; &#xFF08;N&#xFF09; &#xFF08; N  &#xFF09; &#xFF0C;&#x5176;&#x4E2D; N  =  &#x6279;&#x6B21;&#x5927;&#x5C0F; N = \&#x6587;&#x672C;{&#x6279;&#x91CF;&#x5927;&#x5C0F;}  N  =  &#x6279;&#x91CF;&#x5927;&#x5C0F; &#x3002;&#x5B83;&#x8868;&#x793A;&#x7684;&#x8F93;&#x5165;&#x957F;&#x5EA6;&#xFF08;&#x6BCF;&#x4E00;&#x4E2A;&#x90FD;&#x5FC5;&#x987B; &#x2264; T  \&#x5F53;&#x91CF;T  &#x2264; T  &#xFF09;&#x3002;&#x548C;&#x957F;&#x5EA6;&#x4E3A;&#x6BCF;&#x4E2A;&#x5E8F;&#x5217;&#xFF0C;&#x4EE5;&#x5B9E;&#x73B0;&#x8BE5;&#x5E8F;&#x5217;&#x88AB;&#x586B;&#x5145;&#x5230;&#x957F;&#x5EA6;&#x76F8;&#x7B49;&#x7684;&#x5047;&#x8BBE;&#x4E0B;&#x63A9;&#x853D;&#x6307;&#x5B9A;&#x3002;</p>
</li>
<li><p>Target_lengths&#xFF1A;&#x5143;&#x7EC4;&#x6216;&#x7684;&#x5927;&#x5C0F; &#xFF08; N  &#xFF09;&#x5F20;&#x91CF; &#xFF08;N&#xFF09; &#xFF08; N  &#xFF09; &#xFF0C;&#x5176;&#x4E2D; N  =  &#x6279;&#x6B21;&#x5927;&#x5C0F; N = \&#x6587;&#x672C;{&#x6279;&#x91CF;&#x5927;&#x5C0F;}  N  =  &#x6279;&#x91CF;&#x5927;&#x5C0F; &#x3002;&#x5B83;&#x4EE3;&#x8868;&#x4E86;&#x76EE;&#x6807;&#x7684;&#x957F;&#x5EA6;&#x3002;&#x957F;&#x5EA6;&#x5BF9;&#x4E8E;&#x6BCF;&#x4E2A;&#x5E8F;&#x5217;&#xFF0C;&#x4EE5;&#x5B9E;&#x73B0;&#x8BE5;&#x5E8F;&#x5217;&#x88AB;&#x586B;&#x5145;&#x5230;&#x957F;&#x5EA6;&#x76F8;&#x7B49;&#x7684;&#x5047;&#x8BBE;&#x4E0B;&#x63A9;&#x853D;&#x6307;&#x5B9A;&#x3002;&#x5982;&#x679C;&#x76EE;&#x6807;&#x5F62;&#x72B6;&#x4E3A; &#xFF08; N  &#xFF0C; S  &#xFF09; &#xFF08;N&#xFF0C;S&#xFF09; &#xFF08; N  &#xFF0C; S  &#xFF09; &#xFF0C;target_lengths&#x662F;&#x6709;&#x6548;&#x7684;&#x505C;&#x6B62;&#x6307;&#x6570; S  n&#x7684; S_N  S  n&#x7684; [H TG167]  &#x5BF9;&#x4E8E;&#x6BCF;&#x4E2A;&#x9776;&#x5E8F;&#x5217;&#xFF0C;&#x4ECE;&#x800C;&#x4F7F;&#x5F97;<code>TARGET_N  =  &#x76EE;&#x6807;[N&#xFF0C;0&#xFF1A;S_N]  [HTG177&#x7528;&#x4E8E;&#x6279;&#x5904;&#x7406;&#x4E2D;&#x7684;&#x6BCF;&#x4E2A;&#x76EE;&#x6807;&#x3002;&#x957F;&#x5EA6;&#x6BCF;&#x4E00;&#x4E2A;&#x90FD;&#x5FC5;&#x987B; &#x2264; S  \&#x5F53;&#x91CF;S  &#x2264; S  [HTG211&#x5982;&#x679C;&#x76EE;&#x6807;&#x88AB;&#x7ED9;&#x5B9A;&#x4E3A;&#x4E00;&#x7EF4;&#x5F20;&#x91CF;&#x662F;&#x5355;&#x4E2A;&#x76EE;&#x6807;&#x7684;&#x7EA7;&#x8054;&#xFF0C;&#x6240;&#x8FF0;target_lengths&#x5FC5;&#x987B;&#x52A0;&#x8D77;&#x6765;&#x5F20;&#x91CF;&#x7684;&#x603B;&#x957F;&#x5EA6;&#x3002;</code></p>
</li>
<li><p>&#x8F93;&#x51FA;&#xFF1A;&#x6807;&#x91CF;&#x3002;&#x5982;&#x679C;<code>&#x8FD8;&#x539F;</code>&#x662F;<code>&apos;&#x65E0;&apos;</code>&#xFF0C;&#x7136;&#x540E; &#xFF08; N  &#xFF09; &#xFF08;N&#xFF09; &#xFF08; N  &#xFF09; &#xFF0C;&#x5176;&#x4E2D; N  =  &#x6279;&#x91CF;&#x5927;&#x5C0F; N = \&#x6587;&#x672C;{&#x6279;&#x91CF;&#x5927;&#x5C0F;}  N  =  &#x6279;&#x91CF;&#x5927;&#x5C0F; &#x3002;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; T = 50      # Input sequence length
&gt;&gt;&gt; C = 20      # Number of classes (including blank)
&gt;&gt;&gt; N = 16      # Batch size
&gt;&gt;&gt; S = 30      # Target sequence length of longest target in batch
&gt;&gt;&gt; S_min = 10  # Minimum target length, for demonstration purposes
&gt;&gt;&gt;
&gt;&gt;&gt; # Initialize random batch of input vectors, for *size = (T,N,C)
&gt;&gt;&gt; input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()
&gt;&gt;&gt;
&gt;&gt;&gt; # Initialize random batch of targets (0 = blank, 1:C = classes)
&gt;&gt;&gt; target = torch.randint(low=1, high=C, size=(N, S), dtype=torch.long)
&gt;&gt;&gt;
&gt;&gt;&gt; input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)
&gt;&gt;&gt; target_lengths = torch.randint(low=S_min, high=S, size=(N,), dtype=torch.long)
&gt;&gt;&gt; ctc_loss = nn.CTCLoss()
&gt;&gt;&gt; loss = ctc_loss(input, target, input_lengths, target_lengths)
&gt;&gt;&gt; loss.backward()
</code></pre><p>Reference:</p>
<p>A. Graves&#x7B49;&#x4EBA;&#xFF1A;&#x8054;&#x7ED3;&#x989E;&#x5206;&#x7C7B;&#xFF1A;&#x4E0E;&#x56DE;&#x5F52;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x7684;&#x6807;&#x6CE8;&#x672A;&#x5206;&#x6BB5;&#x5E8F;&#x5217;&#x6570;&#x636E;&#xFF1A;<a href="https://www.cs.toronto.edu/~graves/icml_2006.pdf" target="_blank">
https://www.cs.toronto.edu/~graves/icml_2006.pdf
</a></p>
<p>Note</p>
<p>&#x4E3A;&#x4E86;&#x4F7F;&#x7528;CuDNN&#x65F6;&#xFF0C;&#x5FC5;&#x987B;&#x6EE1;&#x8DB3;&#x4EE5;&#x4E0B;&#x6761;&#x4EF6;&#xFF1A;HTG0] &#x76EE;&#x6807; &#x5FC5;&#x987B;&#x5728;&#x7EA7;&#x8054;&#x683C;&#x5F0F;&#xFF0C;&#x6240;&#x6709;<code>input_lengths</code>&#x5FC5;&#x987B; T &#x3002;  B  L  &#x4E00; n&#x7684; &#x137;
=  0  &#x7A7A;&#x767D;= 0  b  L  &#x4E00; n&#x7684; K  =  0  &#xFF0C;<code>target_lengths</code>&#x2264; 256  \&#x5F53;&#x91CF;256  &#x2264; 2  5  6
&#xFF0C;&#x6574;&#x6570;&#x53C2;&#x6570;&#x5FC5;&#x987B;&#x662F;D&#x578B;&#x7EC6;&#x80DE;&#x7684;<code>torch.int32</code>&#x3002;</p>
<p>&#x5E38;&#x89C4;&#x5B9E;&#x73B0;&#x4F7F;&#x7528;&#xFF08;&#x591A;&#x89C1;&#x4E8E;PyTorch&#xFF09; torch.long  D&#x578B;&#x3002;</p>
<p>Note</p>
<p>In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at a
performance cost) by setting <code>torch.backends.cudnn.deterministic = True</code>.
Please see the notes on <a href="notes/randomness.html">Reproducibility</a> for
background.</p>
<h3 id="nllloss">NLLLoss</h3>
<p><em>class</em><code>torch.nn.``NLLLoss</code>( <em>weight=None</em> , <em>size_average=None</em> ,
<em>ignore_index=-100</em> , <em>reduce=None</em> , <em>reduction=&apos;mean&apos;</em>
)<a href="_modules/torch/nn/modules/loss.html#NLLLoss">[source]</a></p>
<p>&#x8D1F;&#x5BF9;&#x6570;&#x4F3C;&#x7136;&#x7684;&#x635F;&#x5931;&#x3002;&#x91CD;&#x8981;&#x7684;&#x662F;&#x8981;&#x57F9;&#x517B;&#x5177;&#x6709; C &#x7C7B;&#x5206;&#x7C7B;&#x95EE;&#x9898;&#x662F;&#x6709;&#x7528;&#x7684;&#x3002;</p>
<p>&#x5982;&#x679C;&#x63D0;&#x4F9B;&#x7684;&#x8BDD;&#xFF0C;&#x53EF;&#x9009;&#x7684;&#x53C2;&#x6570;<code>&#x91CD;&#x91CF;</code>&#x5E94;&#x8BE5;&#x662F;&#x5F20;&#x91CF;1D&#x91CD;&#x91CF;&#x5206;&#x914D;&#x7ED9;&#x6BCF;&#x4E2A;&#x7C7B;&#x3002;&#x5F53;&#x4F60;&#x6709;&#x4E00;&#x4E2A;&#x4E0D;&#x5E73;&#x8861;&#x7684;&#x8BAD;&#x7EC3;&#x96C6;&#x8FD9;&#x662F;&#x7279;&#x522B;&#x6709;&#x7528;&#x3002;</p>
<p>&#x7684;&#x8F93;&#x5165;&#x901A;&#x8FC7;&#x524D;&#x5411;&#x547C;&#x53EB;&#x7ED9;&#x51FA;&#x9884;&#x8BA1;&#x5305;&#x542B;&#x6BCF;&#x4E2A;&#x7C7B;&#x7684;&#x5BF9;&#x6570;&#x6982;&#x7387;&#x3002; &#x8F93;&#x5165;&#x5FC5;&#x987B;&#x662F;&#x5C3A;&#x5BF8;&#x7684;&#x5F20;&#x91CF;&#x4E3A; &#xFF08; M  I  n&#x7684; i&#x7684; b  &#x4E00; T  C  [HTG28 1 H &#xFF0C; C
&#xFF09; &#xFF08;minibatch&#xFF0C;C&#xFF09; &#xFF08; M  i&#x7684; n&#x7684; i&#x7684; b  &#x4E00; T  C  H  &#xFF0C; C  &#xFF09; &#x6216; &#xFF08; M  i&#x7684; n&#x7684; i&#x7684; b  &#x4E00; T  C
H  &#xFF0C; C  &#xFF0C; d  1  &#xFF0C; d  2  &#xFF0C; &#x3002;  &#x3002;  &#x3002;  &#xFF0C; d  K  &#xFF09; &#xFF08;minibatch&#xFF0C;C&#xFF0C;D_1&#xFF0C; D_2&#xFF0C;...&#xFF0C;d_K&#xFF09; &#xFF08;
M  i&#x7684; n&#x7684; i&#x7684; b  &#x4E00; T  C  H  &#xFF0C; C  &#xFF0C; d  1  &#xFF0C; d  [H TG225] 2  &#xFF0C; &#x3002;  &#x3002;  &#x3002;  &#xFF0C; d  K  &#x200B;&#x200B;
&#xFF09; &#x4E0E; K  &#x2265; 1  &#x137;\ GEQ 1  &#x137;  &#x2265; 1  &#x4E3A; K &#x7EF4;&#x60C5;&#x51B5;&#x4E0B;&#xFF08;&#x540E;&#x8FF0;&#xFF09;&#x3002;</p>
<p>&#x5728;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x4E2D;&#x83B7;&#x53D6;&#x6570;&#x7684;&#x6982;&#x7387;&#x662F;&#x5F88;&#x5BB9;&#x6613;&#x5728;&#x4F60;&#x7684;&#x7F51;&#x7EDC;&#x7684;&#x6700;&#x540E;&#x4E00;&#x5C42;&#x6DFB;&#x52A0; LogSoftmax &#x5C42;&#x6765;&#x5B9E;&#x73B0;&#x3002;&#x60A8;&#x53EF;&#x4EE5;&#x4F7F;&#x7528; CrossEntropyLoss
&#x76F8;&#x53CD;&#xFF0C;&#x5982;&#x679C;&#x4F60;&#x4E0D;&#x559C;&#x6B22;&#x6DFB;&#x52A0;&#x989D;&#x5916;&#x7684;&#x5C42;&#x3002;</p>
<p>&#x7684;&#x76EE;&#x6807;&#xFF0C;&#x8FD9;&#x4E00;&#x635F;&#x5931;&#x9884;&#x8BA1;&#x5E94;&#x8BE5;&#x662F;&#x5728;&#x8303;&#x56F4; [ 0 [&#x4E00;&#x7C7B;&#x7D22;&#x5F15;HTG11] &#xFF0C; C  -  1  [0&#xFF0C;C-1]  [ 0  &#xFF0C; C  -  1  ]  &#x5176;&#x4E2D; C
=&#x73ED;&#x6570; ;&#x5982;&#x679C; ignore_index &#x88AB;&#x6307;&#x5B9A;&#x65F6;&#xFF0C;&#x8FD9;&#x4E2A;&#x635F;&#x5931;&#x4E5F;&#x63A5;&#x53D7;&#x8FD9;&#x4E2A;&#x7C7B;&#x522B;&#x7684;&#x7D22;&#x5F15;&#xFF08;&#x6B64;&#x7D22;&#x5F15;&#x53EF;&#x4EE5;&#x4E0D;&#x4E00;&#x5B9A;&#x662F;&#x5728;&#x7C7B;&#x8303;&#x56F4;&#x5185;&#xFF09;&#x3002;</p>
<p>The unreduced (i.e. with <code>reduction</code>set to <code>&apos;none&apos;</code>) loss can be described
as:</p>
<p>&#x2113;(x,y)=L={l1,&#x2026;,lN}&#x22A4;,ln=&#x2212;wynxn,yn,wc=weight[c]&#x22C5;1{c=&#x338;ignore<em>index},\ell(x, y) =
L = \{l_1,\dots,l_N\}^\top, \quad l_n = - w</em>{y<em>n} x</em>{n,y<em>n}, \quad w</em>{c} =
\text{weight}[c] \cdot \mathbb{1}\{c \not= \text{ignore\_index}\},
&#x2113;(x,y)=L={l1&#x200B;,&#x2026;,lN&#x200B;}&#x22A4;,ln&#x200B;=&#x2212;wyn&#x200B;&#x200B;xn,yn&#x200B;&#x200B;,wc&#x200B;=weight[c]&#x22C5;1{c&#xE020;&#x200B;=ignore_index},</p>
<p>&#x5176;&#x4E2D; N  N  N  &#x662F;&#x6279;&#x91CF;&#x5927;&#x5C0F;&#x3002;&#x5982;&#x679C;<code>&#x8FD8;&#x539F;</code>&#x4E0D;&#x662F;<code>&apos;&#x65E0;&apos;</code>&#xFF08;&#x9ED8;&#x8BA4;<code>&apos;&#x5E73;&#x5747;&apos;</code>&#xFF09;&#xFF0C;&#x7136;&#x540E;</p>
<p>&#x2113;(x,y)={&#x2211;n=1N1&#x2211;n=1Nwynln,if reduction=&#x2019;mean&#x2019;;&#x2211;n=1Nln,if
reduction=&#x2019;sum&#x2019;.\ell(x, y) = \begin{cases} \sum<em>{n=1}^N \frac{1}{\sum</em>{n=1}^N
w<em>{y_n}} l_n, &amp; \text{if reduction} = \text{&apos;mean&apos;;}\\ \sum</em>{n=1}^N l_n, &amp;
\text{if reduction} = \text{&apos;sum&apos;.} \end{cases}
&#x2113;(x,y)={&#x2211;n=1N&#x200B;&#x2211;n=1N&#x200B;wyn&#x200B;&#x200B;1&#x200B;ln&#x200B;,&#x2211;n=1N&#x200B;ln&#x200B;,&#x200B;if reduction=&#x2019;mean&#x2019;;if
reduction=&#x2019;sum&#x2019;.&#x200B;</p>
<p>&#x4E5F;&#x53EF;&#x4EE5;&#x7528;&#x4E8E;&#x66F4;&#x9AD8;&#x7684;&#x5C3A;&#x5BF8;&#x7684;&#x8F93;&#x5165;&#xFF0C;&#x5982;2D&#x56FE;&#x50CF;&#xFF0C;&#x901A;&#x8FC7;&#x63D0;&#x4F9B;&#x5C3A;&#x5BF8; &#xFF08; M [&#x7684;&#x8F93;&#x5165;HTG9]  i&#x7684; n&#x7684; i&#x7684; b  &#x4E00; T  C  H  &#xFF0C; C  &#xFF0C; d  1
&#xFF0C; d  2  &#xFF0C; &#x3002; &#x3002; &#xFF0C; d  K  &#xFF09; &#xFF08;minibatch&#xFF0C;C&#xFF0C;D_1 &#xFF0C;D_2&#xFF0C;...&#xFF0C;d_K&#xFF09; &#xFF08; M  I  n&#x7684; i&#x7684; b  &#x4E00; T  C
[HTG92 1 H  &#xFF0C; C  &#xFF0C; d  1  &#xFF0C; d  2  &#xFF0C; &#x3002;  &#x3002;  &#x3002;  &#xFF0C; d  K  &#xFF09; &#x4E0E; K  &#x2265; 1  &#x137;\ GEQ 1  &#x137;  &#x2265;
1  &#xFF0C;&#x5176;&#x4E2D; K  K  &#x200B;&#x200B; K  &#x662F;&#x7EF4;&#x6570;&#xFF0C;&#x548C;&#x9002;&#x5F53;&#x5F62;&#x72B6;&#x7684;&#x76EE;&#x6807;&#xFF08;&#x89C1;&#x4E0B;&#x6587;&#xFF09;&#x3002;&#x5728;&#x56FE;&#x50CF;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x5B83;&#x8BA1;&#x7B97;&#x6BCF;&#x50CF;&#x7D20;NLL&#x635F;&#x5931;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x91CD;&#x91CF;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x7ED9;&#x6BCF;&#x4E2A;&#x7C7B;&#x7684;&#x624B;&#x52A8;&#x91CD;&#x65B0;&#x7F29;&#x653E;&#x6743;&#x91CD;&#x3002;&#x5982;&#x679C;&#x7ED9;&#x5B9A;&#x7684;&#xFF0C;&#x5B83;&#x5FC5;&#x987B;&#x662F;&#x5C3A;&#x5BF8; C &#x7684;&#x5F20;&#x91CF;&#x3002;&#x5426;&#x5219;&#xFF0C;&#x5C06;&#x88AB;&#x89C6;&#x4E3A;&#x6709;&#xFF0C;&#x5982;&#x679C;&#x6240;&#x6709;&#x7684;&#x4EBA;&#x3002;</p>
</li>
<li><p><strong>size_average</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field <code>size_average</code>is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></p>
</li>
<li><p><strong>ignore_index</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x6307;&#x5B9A;&#x5C06;&#x88AB;&#x5FFD;&#x7565;&#xFF0C;&#x5E76;&#x4E14;&#x4E0D;&#x5411;&#x76EE;&#x6807;&#x503C;&#x8F93;&#x5165;&#x68AF;&#x5EA6;&#x3002;&#x5F53;<code>size_average</code>&#x662F;<code>&#x771F;</code>&#xFF0C;&#x635F;&#x5931;&#x5E73;&#x5747;&#x8D85;&#x8FC7;&#x975E;&#x5FFD;&#x7565;&#x7684;&#x76EE;&#x6807;&#x3002;</p>
</li>
<li><p><strong>reduce</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code>is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></p>
</li>
<li><p><strong>reduction</strong> ( <em>string</em> <em>,</em> <em>optional</em> ) &#x2013; Specifies the reduction to apply to the output: <code>&apos;none&apos;</code>| <code>&apos;mean&apos;</code>| <code>&apos;sum&apos;</code>. <code>&apos;none&apos;</code>: no reduction will be applied, <code>&apos;mean&apos;</code>: the sum of the output will be divided by the number of elements in the output, <code>&apos;sum&apos;</code>: the output will be summed. Note: <code>size_average</code>and <code>reduce</code>are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: <code>&apos;mean&apos;</code></p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>Input: (N,C)(N, C)(N,C) where C = number of classes, or (N,C,d1,d2,...,dK)(N, C, d_1, d_2, ..., d_K)(N,C,d1&#x200B;,d2&#x200B;,...,dK&#x200B;) with K&#x2265;1K \geq 1K&#x2265;1 in the case of K-dimensional loss.</p>
</li>
<li><p>Target: (N)(N)(N) where each value is 0&#x2264;targets[i]&#x2264;C&#x2212;10 \leq \text{targets}[i] \leq C-10&#x2264;targets[i]&#x2264;C&#x2212;1 , or (N,d1,d2,...,dK)(N, d_1, d_2, ..., d_K)(N,d1&#x200B;,d2&#x200B;,...,dK&#x200B;) with K&#x2265;1K \geq 1K&#x2265;1 in the case of K-dimensional loss.</p>
</li>
<li><p>&#x8F93;&#x51FA;&#xFF1A;&#x6807;&#x91CF;&#x3002;&#x5982;&#x679C;<code>&#x8FD8;&#x539F;</code>&#x662F;<code>&apos;&#x65E0;&apos;</code>&#xFF0C;&#x7136;&#x540E;&#x76F8;&#x540C;&#x7684;&#x5C3A;&#x5BF8;&#x4E3A;&#x76EE;&#x6807;&#xFF1A; &#xFF08; N  &#xFF09; &#xFF08;N&#xFF09; &#xFF08; N  &#xFF09; &#x6216; &#xFF08; N  &#xFF0C; d  1  &#xFF0C; d  2  &#xFF0C; &#x3002; &#x3002;  &#x3002; &#xFF0C; d  K  &#xFF09; &#xFF08;N&#xFF0C;D_1&#xFF0C;D_2&#xFF0C;...&#xFF0C;d_K&#xFF09; &#xFF08; N  &#xFF0C;[HT G99]  d  1  &#xFF0C; d  2  &#xFF0C; &#x3002;  &#x3002;  &#x3002;  &#xFF0C; d  K  &#xFF09; &#x4E0E; K  &#x2265; 1  &#x137;\ GEQ 1  &#x137;  &#x2265; 1  &#x5728;K&#x7EF4;&#x635F;&#x5931;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#x3002;</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.LogSoftmax(dim=1)
&gt;&gt;&gt; loss = nn.NLLLoss()
&gt;&gt;&gt; # input is of size N x C = 3 x 5
&gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True)
&gt;&gt;&gt; # each element in target has to have 0 &lt;= value &lt; C
&gt;&gt;&gt; target = torch.tensor([1, 0, 4])
&gt;&gt;&gt; output = loss(m(input), target)
&gt;&gt;&gt; output.backward()
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; # 2D loss example (used, for example, with image inputs)
&gt;&gt;&gt; N, C = 5, 4
&gt;&gt;&gt; loss = nn.NLLLoss()
&gt;&gt;&gt; # input is of size N x C x height x width
&gt;&gt;&gt; data = torch.randn(N, 16, 10, 10)
&gt;&gt;&gt; conv = nn.Conv2d(16, C, (3, 3))
&gt;&gt;&gt; m = nn.LogSoftmax(dim=1)
&gt;&gt;&gt; # each element in target has to have 0 &lt;= value &lt; C
&gt;&gt;&gt; target = torch.empty(N, 8, 8, dtype=torch.long).random_(0, C)
&gt;&gt;&gt; output = loss(m(conv(data)), target)
&gt;&gt;&gt; output.backward()
</code></pre><h3 id="poissonnllloss">PoissonNLLLoss</h3>
<p><em>class</em><code>torch.nn.``PoissonNLLLoss</code>( <em>log_input=True</em> , <em>full=False</em> ,
<em>size_average=None</em> , <em>eps=1e-08</em> , <em>reduce=None</em> , <em>reduction=&apos;mean&apos;</em>
)<a href="_modules/torch/nn/modules/loss.html#PoissonNLLLoss">[source]</a></p>
<p>&#x8D1F;&#x5BF9;&#x6570;&#x4F3C;&#x7136;&#x635F;&#x5931;&#x4E0E;&#x76EE;&#x6807;&#x7684;&#x6CCA;&#x677E;&#x5206;&#x5E03;&#x3002;</p>
<p>The loss can be described as:</p>
<p>target&#x223C;Poisson(input)loss(input,target)=input&#x2212;target&#x2217;log&#x2061;(input)+log&#x2061;(target!)\text{target}
\sim \mathrm{Poisson}(\text{input}) \text{loss}(\text{input}, \text{target}) =
\text{input} - \text{target} * \log(\text{input}) +
\log(\text{target!})target&#x223C;Poisson(input)loss(input,target)=input&#x2212;target&#x2217;log(input)+log(target!)</p>
<p>&#x7684;&#x6700;&#x540E;&#x4E00;&#x9879;&#x53EF;&#x88AB;&#x7701;&#x7565;&#x6216;&#x7528;&#x65AF;&#x7279;&#x6797;&#x5F0F;&#x8FD1;&#x4F3C;&#x3002;&#x7528;&#x4E8E;&#x9776;&#x7684;&#x8FD1;&#x4F3C;&#x503C;&#x8D85;&#x8FC7;1.&#x5BF9;&#x4E8E;&#x76EE;&#x6807;&#x5C0F;&#x4E8E;&#x6216;&#x7B49;&#x4E8E;1&#x4E2A;&#x96F6;&#x6DFB;&#x52A0;&#x5230;&#x635F;&#x5931;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>log_input</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5982;&#x679C;<code>&#x771F;</code>&#x635F;&#x8017;&#x88AB;&#x8BA1;&#x7B97;&#x4E3A; EXP  &#x2061; &#xFF08; &#x8F93;&#x5165; &#xFF09; -  &#x76EE;&#x6807; <em>  &#x8F93;&#x5165; \ EXP&#xFF08;\&#x6587;&#x672C;{&#x8F93;&#x5165;}&#xFF09; - \&#x6587;&#x672C;{&#x76EE;&#x6807;} </em> \&#x6587;&#x672C;{&#x8F93;&#x5165;}  EXP  &#xFF08; &#x8F93;&#x5165; &#xFF09; -  &#x76EE;&#x6807; <em>  &#x8F93;&#x5165; &#x65F6;&#xFF0C;&#x5982;&#x679C;<code>&#x5047;</code>&#x635F;&#x5931; &#x8F93;&#x5165; -  &#x76EE;&#x6807; </em>  &#x65E5;&#x5FD7; &#x2061; &#xFF08; &#x8F93;&#x5165; +  EPS  &#xFF09;  \&#x6587;&#x672C;{&#x8F93;&#x5165;} - \&#x6587;&#x672C;{&#x76EE;&#x6807;} <em> \&#x65E5;&#x5FD7;&#xFF08;\&#x6587;&#x672C;{&#x8F93;&#x5165;} + \&#x6587;&#x672C;{EPS}&#xFF09; &#x8F93;&#x5165; -  &#x76EE;&#x6807; </em>  LO  G  &#xFF08; &#x8F93;&#x5165; +  EPS  &#xFF09; &#x3002;</p>
</li>
<li><p><strong>&#x5145;&#x6EE1;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - </p>
</li>
</ul>
<p>&#x662F;&#x5426;&#x8BA1;&#x7B97;&#x5168;&#x90E8;&#x635F;&#x5931;&#xFF0C;&#x6211;&#x3002;&#x5373;&#x6DFB;&#x52A0;&#x7684;&#x65AF;&#x7279;&#x6797;&#x8FD1;&#x4F3C;&#x672F;&#x8BED;</p>
<p>target&#x2217;log&#x2061;(target)&#x2212;target+0.5&#x2217;log&#x2061;(2&#x3C0;target).\text{target}*\log(\text{target})</p>
<ul>
<li><p>\text{target} + 0.5 * \log(2\pi\text{target}).
target&#x2217;log(target)&#x2212;target+0.5&#x2217;log(2&#x3C0;target).</p>
<ul>
<li><p><strong>size_average</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field <code>size_average</code>is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></p>
</li>
<li><p><strong>EPS</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5C0F;&#x503C;&#xFF0C;&#x4EE5;&#x907F;&#x514D;&#x7684; [&#x8BC4;&#x4EF7;HTG12]  &#x65E5;&#x5FD7; &#x2061; &#xFF08; 0  &#xFF09; \&#x65E5;&#x5FD7;&#xFF08;0&#xFF09; LO  G  &#xFF08; 0  &#xFF09; &#x65F6;<code>log_input  =  &#x5047;</code>&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;1E-8</p>
</li>
<li><p><strong>reduce</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code>is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></p>
</li>
<li><p><strong>reduction</strong> ( <em>string</em> <em>,</em> <em>optional</em> ) &#x2013; Specifies the reduction to apply to the output: <code>&apos;none&apos;</code>| <code>&apos;mean&apos;</code>| <code>&apos;sum&apos;</code>. <code>&apos;none&apos;</code>: no reduction will be applied, <code>&apos;mean&apos;</code>: the sum of the output will be divided by the number of elements in the output, <code>&apos;sum&apos;</code>: the output will be summed. Note: <code>size_average</code>and <code>reduce</code>are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: <code>&apos;mean&apos;</code></p>
</li>
</ul>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; loss = nn.PoissonNLLLoss()
&gt;&gt;&gt; log_input = torch.randn(5, 2, requires_grad=True)
&gt;&gt;&gt; target = torch.randn(5, 2)
&gt;&gt;&gt; output = loss(log_input, target)
&gt;&gt;&gt; output.backward()
</code></pre><p>Shape:</p>
<ul>
<li><p>Input: (N,&#x2217;)(N, <em>)(N,&#x2217;) where &#x2217;</em>&#x2217; means, any number of additional dimensions</p>
</li>
<li><p>Target: (N,&#x2217;)(N, *)(N,&#x2217;) , same shape as the input</p>
</li>
<li><p>&#x8F93;&#x51FA;&#xFF1A;&#x5728;&#x9ED8;&#x8BA4;&#x60C5;&#x51B5;&#x4E0B;&#x6807;&#x3002;&#x5982;&#x679C;<code>&#x8FD8;&#x539F;</code>&#x662F;<code>&apos;&#x65E0;&apos;</code>&#xFF0C;&#x7136;&#x540E; &#xFF08; N  &#xFF0C; <em>  &#xFF09; &#xFF08;N&#xFF0C;</em>&#xFF09; &#xFF08; N  &#xFF0C; *  &#xFF09; &#xFF0C;&#x76F8;&#x540C;&#x7684;&#x5F62;&#x72B6;&#x4F5C;&#x4E3A;&#x8F93;&#x5165;</p>
</li>
</ul>
<h3 id="kldivloss">KLDivLoss</h3>
<p><em>class</em><code>torch.nn.``KLDivLoss</code>( <em>size_average=None</em> , <em>reduce=None</em> ,
<em>reduction=&apos;mean&apos;</em> )<a href="_modules/torch/nn/modules/loss.html#KLDivLoss">[source]</a></p>
<p>&#x5728;<a href="https://en.wikipedia.org/wiki/Kullback-Leibler_divergence" target="_blank">&#x76F8;&#x5BF9;&#x71B5;</a>&#x635F;&#x5931;</p>
<p>KL&#x6563;&#x5EA6;&#x662F;&#x8FDE;&#x7EED;&#x5206;&#x5E03;&#x7684;&#x6709;&#x7528;&#x8DDD;&#x79BB;&#x5EA6;&#x91CF;&#x548C;&#x8FC7;&#xFF08;&#x79BB;&#x6563;&#x91C7;&#x6837;&#xFF09;&#x8FDE;&#x7EED;&#x8F93;&#x51FA;&#x5206;&#x5E03;&#x7684;&#x7A7A;&#x95F4;&#x6267;&#x884C;&#x76F4;&#x63A5;&#x56DE;&#x5F52;&#x65F6;&#x662F;&#x7ECF;&#x5E38;&#x6709;&#x76CA;&#x7684;&#x3002;</p>
<p>&#x4E0E; <code>NLLLoss</code>&#x65F6;&#xFF0C;&#x8F93;&#x5165;&#x4E2D;&#x7ED9;&#x51FA;&#x9884;&#x8BA1;&#x5305;&#x542B; <em>&#x5BF9;&#x6570;&#x6982;&#x7387;</em> &#xFF0C;&#x5E76;&#x4E14;&#x4E0D;&#x9650;&#x5236;&#x4E8E;2D&#x5F20;&#x91CF;&#x3002;&#x5404;&#x9879;&#x6307;&#x6807;&#x5747;&#x7ED9;&#x5B9A;&#x4E3A; <em>&#x6982;&#x7387;</em> &#xFF08;&#x5373;&#x6CA1;&#x6709;&#x53D6;&#x5BF9;&#x6570;&#xFF09;&#x3002;</p>
<p>&#x8BE5;&#x6807;&#x51C6;&#x9700;&#x8981;&#x4E00;&#x4E2A;&#x76EE;&#x6807; &#x5F20;&#x91CF;&#x76F8;&#x540C;&#x5C3A;&#x5BF8;&#x4E0E;&#x8F93;&#x5165; &#x5F20;&#x91CF;&#x7684;&#x3002;</p>
<p>The unreduced (i.e. with <code>reduction</code>set to <code>&apos;none&apos;</code>) loss can be described
as:</p>
<p>l(x,y)=L={l1,&#x2026;,lN},ln=yn&#x22C5;(log&#x2061;yn&#x2212;xn)l(x,y) = L = \{ l_1,\dots,l_N \}, \quad
l_n = y_n \cdot \left( \log y_n - x_n \right)
l(x,y)=L={l1&#x200B;,&#x2026;,lN&#x200B;},ln&#x200B;=yn&#x200B;&#x22C5;(logyn&#x200B;&#x2212;xn&#x200B;)</p>
<p>&#x5176;&#x4E2D;&#xFF0C;&#x7D22;&#x5F15; N  N  N  &#x8DE8;&#x8D8A;<code>&#x8F93;&#x5165;</code>&#x7684;&#x6240;&#x6709;&#x5C3A;&#x5BF8;&#x548C; L  L  L  &#x5177;&#x6709;&#x76F8;&#x540C;&#x7684;&#x5F62;&#x72B6;<code>&#x8F93;&#x5165;</code>&#x3002;&#x5982;&#x679C;<code>&#x8FD8;&#x539F;</code>&#x4E0D;&#x662F;<code>&apos;&#x65E0;&apos;</code>&#xFF08;&#x9ED8;&#x8BA4;<code>&apos;&#x5E73;&#x5747;&apos;</code>&#xFF09;&#xFF0C;&#x7136;&#x540E;&#xFF1A;</p>
<p>&#x2113;(x,y)={mean&#x2061;(L),if reduction=&#x2019;mean&#x2019;;sum&#x2061;(L),if reduction=&#x2019;sum&#x2019;.\ell(x, y) =
\begin{cases} \operatorname{mean}(L), &amp; \text{if reduction} = \text{&apos;mean&apos;;}
\\ \operatorname{sum}(L), &amp; \text{if reduction} = \text{&apos;sum&apos;.} \end{cases}
&#x2113;(x,y)={mean(L),sum(L),&#x200B;if reduction=&#x2019;mean&#x2019;;if reduction=&#x2019;sum&#x2019;.&#x200B;</p>
<p>&#x5728;&#x9ED8;&#x8BA4;<code>&#x8FD8;&#x539F;</code>&#x6A21;&#x5F0F;<code>&apos;&#x5E73;&#x5747;&apos;</code>&#x65F6;&#xFF0C;&#x635F;&#x8017;&#x8D85;&#x8FC7;&#x89C2;&#x6D4B;&#x5E73;&#x5747;&#x6BCF;&#x4E2A;minibatch <strong>&#x4EE5;&#x53CA;</strong> &#x5728;&#x5C3A;&#x5BF8;&#x3002; <code>&apos;batchmean&apos;</code>&#x6A21;&#x5F0F;&#x7ED9;&#x51FA;&#x6B63;&#x786E;&#x7684;KL&#x6563;&#x5EA6;&#xFF0C;&#x5176;&#x4E2D;&#x635F;&#x8017;&#x8FDB;&#x884C;&#x5E73;&#x5747;&#x4EC5;&#x6279;&#x6B21;&#x7684;&#x5C3A;&#x5BF8;&#x3002; <code>&apos;&#x5E73;&#x5747;&apos;</code>&#x6A21;&#x5F0F;&#x7684;&#x884C;&#x4E3A;&#x5C06;&#x88AB;&#x66F4;&#x6539;&#x4E3A;&#x4E0E;<code>&apos;batchmean&apos;</code>&#x5728;&#x63A5;&#x4E0B;&#x6765;&#x7684;&#x4E3B;&#x8981;&#x7248;&#x672C;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>size_average</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field <code>size_average</code>is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></p>
</li>
<li><p><strong>reduce</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code>is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></p>
</li>
<li><p><strong>&#x8FD8;&#x539F;</strong> &#xFF08; <em>&#x4E32;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x6307;&#x5B9A;&#x8FD8;&#x539F;&#x5E94;&#x7528;&#x5230;&#x8F93;&#x51FA;&#xFF1A;<code>&apos;&#x65E0;&apos;</code>| <code>&apos;batchmean&apos;</code>| <code>&apos;&#x548C;&apos;</code>| <code>&apos;&#x610F;&#x5473;&apos;</code>&#x3002; <code>&apos;&#x65E0;&apos;</code>&#xFF1A;&#x4E0D;&#x964D;&#x4F4E;&#x5C06;&#x88AB;&#x5E94;&#x7528;&#x3002; <code>&apos;batchmean&apos;</code>&#xFF1A;&#x5C06;&#x8F93;&#x51FA;&#x7684;&#x603B;&#x548C;&#x5C06;&#x901A;&#x8FC7;BATCHSIZE&#x8FDB;&#x884C;&#x5212;&#x5206;&#x3002; <code>&apos;&#x548C;&apos;</code>&#xFF1A;&#x8F93;&#x51FA;&#x5C06;&#x88AB;&#x7D2F;&#x52A0;&#x3002; <code>&#x201C;&#x610F;&#x5473;&#x201D;</code>&#xFF1A;&#x8F93;&#x51FA;&#x5C06;&#x901A;&#x8FC7;&#x5728;&#x8F93;&#x51FA;&#x5143;&#x4EF6;&#x7684;&#x6570;&#x76EE;&#x6765;&#x5212;&#x5206;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&apos;&#x5E73;&#x5747;&apos;</code></p>
</li>
</ul>
<p>Note</p>
<p><code>size_average</code>&#x548C;<code>&#x51CF;&#x5C11;</code>&#x5904;&#x4E8E;&#x88AB;&#x6DD8;&#x6C70;&#xFF0C;&#x5E76;&#x4E14;&#x5728;&#x6B64;&#x540C;&#x65F6;&#xFF0C;&#x6307;&#x5B9A;&#x662F;&#x8FD9;&#x4E24;&#x4E2A;&#x53C2;&#x6570;&#x7684;&#x4E2A;&#x6570;&#x5C06;&#x8986;&#x76D6;<code>&#x8FD8;&#x539F;</code>&#x3002;</p>
<p>Note</p>
<p><code>&#x8FD8;&#x539F;</code>= <code>&apos;&#x5E73;&#x5747;&apos;</code>&#x4E0D;&#x56DE;&#x771F;&#x6B63;&#x7684;KL&#x6563;&#x5EA6;&#x503C;&#xFF0C;&#x8BF7;&#x4F7F;&#x7528;<code>&#x8FD8;&#x539F;</code>= <code>&apos;batchmean&apos;</code>&#x4E0E;KL&#x6570;&#x5B66;&#x5B9A;&#x4E49;&#x5BF9;&#x9F50;&#x3002;&#x5728;&#x63A5;&#x4E0B;&#x6765;&#x7684;&#x4E3B;&#x8981;&#x7248;&#x672C;&#xFF0C;<code>&apos;&#x7684;&#x610F;&#x601D;&#x662F;&apos;</code>&#x5C06;&#x6539;&#x4E3A;&#x540C;<code>&apos;batchmean&apos; [HTG23&#x3002;</code></p>
<p>Shape:</p>
<ul>
<li><p>Input: (N,&#x2217;)(N, <em>)(N,&#x2217;) where &#x2217;</em>&#x2217; means, any number of additional dimensions</p>
</li>
<li><p>Target: (N,&#x2217;)(N, *)(N,&#x2217;) , same shape as the input</p>
</li>
<li><p>&#x8F93;&#x51FA;&#xFF1A;&#x5728;&#x9ED8;&#x8BA4;&#x60C5;&#x51B5;&#x4E0B;&#x6807;&#x3002;&#x5982;&#x679C;&#xFF1A;ATTR&#xFF1A;<code>&#x8FD8;&#x539F;</code>&#x662F;<code>&apos;&#x65E0;&apos;</code>&#xFF0C;&#x7136;&#x540E; &#xFF08; N  &#xFF0C; <em>  &#xFF09; &#xFF08;N&#xFF0C;</em>&#xFF09; &#xFF08; N  &#xFF0C; *  &#xFF09; &#xFF0C;&#x76F8;&#x540C;&#x7684;&#x5F62;&#x72B6;&#x4F5C;&#x4E3A;&#x8F93;&#x5165;</p>
</li>
</ul>
<h3 id="bceloss">BCELoss</h3>
<p><em>class</em><code>torch.nn.``BCELoss</code>( <em>weight=None</em> , <em>size_average=None</em> ,
<em>reduce=None</em> , <em>reduction=&apos;mean&apos;</em>
)<a href="_modules/torch/nn/modules/loss.html#BCELoss">[source]</a></p>
<p>&#x521B;&#x5EFA;&#x4E00;&#x4E2A;&#x6D4B;&#x91CF;&#x76EE;&#x6807;&#x4E0E;&#x8F93;&#x51FA;&#x4E4B;&#x95F4;&#x7684;&#x4E8C;&#x8FDB;&#x5236;&#x4EA4;&#x53C9;&#x71B5;&#x7684;&#x6807;&#x51C6;&#xFF1A;</p>
<p>The unreduced (i.e. with <code>reduction</code>set to <code>&apos;none&apos;</code>) loss can be described
as:</p>
<p>&#x2113;(x,y)=L={l1,&#x2026;,lN}&#x22A4;,ln=&#x2212;wn[yn&#x22C5;log&#x2061;xn+(1&#x2212;yn)&#x22C5;log&#x2061;(1&#x2212;xn)],\ell(x, y) = L =
\{l_1,\dots,l_N\}^\top, \quad l_n = - w_n \left[ y_n \cdot \log x_n + (1 -
y_n) \cdot \log (1 - x_n) \right],
&#x2113;(x,y)=L={l1&#x200B;,&#x2026;,lN&#x200B;}&#x22A4;,ln&#x200B;=&#x2212;wn&#x200B;[yn&#x200B;&#x22C5;logxn&#x200B;+(1&#x2212;yn&#x200B;)&#x22C5;log(1&#x2212;xn&#x200B;)],</p>
<p>where NNN is the batch size. If <code>reduction</code>is not <code>&apos;none&apos;</code>(default
<code>&apos;mean&apos;</code>), then</p>
<p>&#x2113;(x,y)={mean&#x2061;(L),if reduction=&#x2019;mean&#x2019;;sum&#x2061;(L),if reduction=&#x2019;sum&#x2019;.\ell(x, y) =
\begin{cases} \operatorname{mean}(L), &amp; \text{if reduction} =
\text{&apos;mean&apos;;}\\ \operatorname{sum}(L), &amp; \text{if reduction} = \text{&apos;sum&apos;.}
\end{cases} &#x2113;(x,y)={mean(L),sum(L),&#x200B;if reduction=&#x2019;mean&#x2019;;if reduction=&#x2019;sum&#x2019;.&#x200B;</p>
<p>&#x8FD9;&#x662F;&#x7528;&#x4E8E;&#x6D4B;&#x91CF;&#x4E00;&#x4E2A;&#x91CD;&#x5EFA;&#x7684;&#x8BEF;&#x5DEE;&#x5728;&#x4F8B;&#x5982;&#x4E00;&#x4E2A;&#x81EA;&#x52A8;&#x7F16;&#x7801;&#x5668;&#x3002;&#x8BF7;&#x6CE8;&#x610F;&#xFF0C;&#x76EE;&#x6807; Y  Y  Y  &#x5E94;&#x8BE5;&#x662F;0&#x548C;1&#x4E4B;&#x95F4;&#x7684;&#x6570;&#x5B57;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x91CD;&#x91CF;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x7ED9;&#x4E88;&#x6BCF;&#x4E2A;&#x6279;&#x6B21;&#x5143;&#x7D20;&#x7684;&#x635F;&#x5931;&#x7684;&#x624B;&#x52A8;&#x91CD;&#x65B0;&#x7F29;&#x653E;&#x6743;&#x91CD;&#x3002;&#x5982;&#x679C;&#x7ED9;&#x5B9A;&#x7684;&#xFF0C;&#x5FC5;&#x987B;&#x662F;&#x5C3A;&#x5BF8; nbatch &#x7684;&#x5F20;&#x91CF;&#x3002;</p>
</li>
<li><p><strong>size_average</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field <code>size_average</code>is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></p>
</li>
<li><p><strong>reduce</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code>is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></p>
</li>
<li><p><strong>reduction</strong> ( <em>string</em> <em>,</em> <em>optional</em> ) &#x2013; Specifies the reduction to apply to the output: <code>&apos;none&apos;</code>| <code>&apos;mean&apos;</code>| <code>&apos;sum&apos;</code>. <code>&apos;none&apos;</code>: no reduction will be applied, <code>&apos;mean&apos;</code>: the sum of the output will be divided by the number of elements in the output, <code>&apos;sum&apos;</code>: the output will be summed. Note: <code>size_average</code>and <code>reduce</code>are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: <code>&apos;mean&apos;</code></p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>Input: (N,&#x2217;)(N, <em>)(N,&#x2217;) where &#x2217;</em>&#x2217; means, any number of additional dimensions</p>
</li>
<li><p>Target: (N,&#x2217;)(N, *)(N,&#x2217;) , same shape as the input</p>
</li>
<li><p>&#x8F93;&#x51FA;&#xFF1A;&#x6807;&#x91CF;&#x3002;&#x5982;&#x679C;<code>&#x8FD8;&#x539F;</code>&#x662F;<code>&apos;&#x65E0;&apos;</code>&#xFF0C;&#x7136;&#x540E; &#xFF08; N  &#xFF0C; <em>  &#xFF09; &#xFF08;N&#xFF0C;</em>&#xFF09; &#xFF08; N  &#xFF0C; *  &#xFF09; &#xFF0C;&#x76F8;&#x540C;&#x7684;&#x5F62;&#x72B6;&#x4F5C;&#x4E3A;&#x8F93;&#x5165;&#x3002;</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.Sigmoid()
&gt;&gt;&gt; loss = nn.BCELoss()
&gt;&gt;&gt; input = torch.randn(3, requires_grad=True)
&gt;&gt;&gt; target = torch.empty(3).random_(2)
&gt;&gt;&gt; output = loss(m(input), target)
&gt;&gt;&gt; output.backward()
</code></pre><h3 id="bcewithlogitsloss">BCEWithLogitsLoss</h3>
<p><em>class</em><code>torch.nn.``BCEWithLogitsLoss</code>( <em>weight=None</em> , <em>size_average=None</em> ,
<em>reduce=None</em> , <em>reduction=&apos;mean&apos;</em> , <em>pos_weight=None</em>
)<a href="_modules/torch/nn/modules/loss.html#BCEWithLogitsLoss">[source]</a></p>
<p>&#x8FD9;&#x79CD;&#x635F;&#x5931;&#x7ED3;&#x5408;&#x4E86;&#x4E59;&#x72B6;&#x7ED3;&#x80A0;&#x5C42;&#x548C; BCELoss &#x5728;&#x4E00;&#x4E2A;&#x5355;&#x72EC;&#x7684;&#x7C7B;&#x3002;&#x8FD9;&#x4E2A;&#x7248;&#x672C;&#x6BD4;&#x4F7F;&#x7528;&#x7EAF;&#x66F4;&#x6570;&#x503C;&#x7A33;&#x5B9A;&#x4E59;&#x72B6;&#x7ED3;&#x80A0;&#x63A5;&#x7740;&#x5BF9;&#x6570;&#x548C;-EXP&#x7279;&#x6280;&#x8FDB;&#x884C;&#x6570;&#x503C;&#x7684; BCELoss
&#x5982;&#xFF0C;&#x901A;&#x8FC7;&#x64CD;&#x4F5C;&#x7EC4;&#x5408;&#x4E3A;&#x4E00;&#x4E2A;&#x5C42;&#xFF0C;&#x6211;&#x4EEC;&#x5229;&#x7528;&#x7A33;&#x5B9A;&#x6027;&#x3002;</p>
<p>The unreduced (i.e. with <code>reduction</code>set to <code>&apos;none&apos;</code>) loss can be described
as:</p>
<p>&#x2113;(x,y)=L={l1,&#x2026;,lN}&#x22A4;,ln=&#x2212;wn[yn&#x22C5;log&#x2061;&#x3C3;(xn)+(1&#x2212;yn)&#x22C5;log&#x2061;(1&#x2212;&#x3C3;(xn))],\ell(x, y) = L =
\{l_1,\dots,l_N\}^\top, \quad l_n = - w_n \left[ y_n \cdot \log \sigma(x_n)
+ (1 - y_n) \cdot \log (1 - \sigma(x_n)) \right],
&#x2113;(x,y)=L={l1&#x200B;,&#x2026;,lN&#x200B;}&#x22A4;,ln&#x200B;=&#x2212;wn&#x200B;[yn&#x200B;&#x22C5;log&#x3C3;(xn&#x200B;)+(1&#x2212;yn&#x200B;)&#x22C5;log(1&#x2212;&#x3C3;(xn&#x200B;))],</p>
<p>where NNN is the batch size. If <code>reduction</code>is not <code>&apos;none&apos;</code>(default
<code>&apos;mean&apos;</code>), then</p>
<p>&#x2113;(x,y)={mean&#x2061;(L),if reduction=&#x2019;mean&#x2019;;sum&#x2061;(L),if reduction=&#x2019;sum&#x2019;.\ell(x, y) =
\begin{cases} \operatorname{mean}(L), &amp; \text{if reduction} =
\text{&apos;mean&apos;;}\\ \operatorname{sum}(L), &amp; \text{if reduction} = \text{&apos;sum&apos;.}
\end{cases} &#x2113;(x,y)={mean(L),sum(L),&#x200B;if reduction=&#x2019;mean&#x2019;;if reduction=&#x2019;sum&#x2019;.&#x200B;</p>
<p>&#x8FD9;&#x662F;&#x7528;&#x4E8E;&#x6D4B;&#x91CF;&#x4E00;&#x4E2A;&#x91CD;&#x5EFA;&#x7684;&#x8BEF;&#x5DEE;&#x5728;&#x4F8B;&#x5982;&#x4E00;&#x4E2A;&#x81EA;&#x52A8;&#x7F16;&#x7801;&#x5668;&#x3002;&#x6CE8;&#x610F;&#xFF0C;&#x76EE;&#x6807; T [1] &#x5E94;&#x8BE5;&#x662F;0&#x548C;1&#x4E4B;&#x95F4;&#x7684;&#x6570;&#x5B57;&#x3002;</p>
<p>&#x8FD9;&#x53EF;&#x4EE5;&#x901A;&#x8FC7;&#x589E;&#x52A0;&#x914D;&#x91CD;&#xFF0C;&#x4EE5;&#x79EF;&#x6781;&#x7684;&#x4F8B;&#x5B50;&#x6743;&#x8861;&#x53EC;&#x56DE;&#x548C;&#x7CBE;&#x5EA6;&#x3002;&#x5728;&#x591A;&#x6807;&#x7B7E;&#x5206;&#x7C7B;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#x7684;&#x635F;&#x5931;&#x53EF;&#x4EE5;&#x88AB;&#x63CF;&#x8FF0;&#x4E3A;&#xFF1A;</p>
<p>&#x2113;c(x,y)=Lc={l1,c,&#x2026;,lN,c}&#x22A4;,ln,c=&#x2212;wn,c[pcyn,c&#x22C5;log&#x2061;&#x3C3;(xn,c)+(1&#x2212;yn,c)&#x22C5;log&#x2061;(1&#x2212;&#x3C3;(xn,c))],\ell<em>c(x,
y) = L_c = \{l</em>{1,c},\dots,l<em>{N,c}\}^\top, \quad l</em>{n,c} = - w<em>{n,c} \left[
p_c y</em>{n,c} \cdot \log \sigma(x<em>{n,c}) + (1 - y</em>{n,c}) \cdot \log (1 -
\sigma(x_{n,c})) \right],
&#x2113;c&#x200B;(x,y)=Lc&#x200B;={l1,c&#x200B;,&#x2026;,lN,c&#x200B;}&#x22A4;,ln,c&#x200B;=&#x2212;wn,c&#x200B;[pc&#x200B;yn,c&#x200B;&#x22C5;log&#x3C3;(xn,c&#x200B;)+(1&#x2212;yn,c&#x200B;)&#x22C5;log(1&#x2212;&#x3C3;(xn,c&#x200B;))],</p>
<p>&#x5176;&#x4E2D; C  C  C  &#x662F;&#x7C7B;&#x6570;&#xFF08;HTG24]  C  &amp; GT ;  1  C &amp; GT ; 1  C  &amp; GT ;  1
[HTG63&#x7528;&#x4E8E;&#x591A;&#x6807;&#x7B7E;&#x4E8C;&#x5143;&#x5206;&#x7C7B;&#xFF0C; C  =  1  C = 1  C  =  1  &#x4E3A;&#x5355;&#x6807;&#x7B7E;&#x4E8C;&#x5143;&#x5206;&#x7C7B;&#xFF09;&#xFF0C; n&#x7684; n&#x7684; n&#x7684; &#x662F;&#x5728;&#x6279;&#x5904;&#x7406;&#x7684;&#x6837;&#x54C1;&#x7684;&#x6570;&#x76EE;&#x548C; p
C  P_C  p  C  &#x662F;&#x80AF;&#x5B9A;&#x7684;&#x56DE;&#x7B54;&#xFF0C;&#x4E3A;&#x7C7B; [&#x91CD;&#x91CF;HTG183]  C  C  C  &#x3002;</p>
<p>P  C  &amp; GT ;  1  P_C &amp; GT ; 1  p  C  [ - - ] GT ;  1  &#x589E;&#x52A0;&#x53EC;&#x56DE;&#xFF0C;  p  C  &amp; LT ;  1
P_C &amp; LT ; 1  p  C  &amp; LT ;  1  &#x63D0;&#x9AD8;&#x7CBE;&#x5EA6;&#x3002;</p>
<p>&#x4F8B;&#x5982;&#xFF0C;&#x5982;&#x679C;&#x6570;&#x636E;&#x96C6;&#x5305;&#x542B;&#x5355;&#x4E2A;&#x7C7B;&#x7684;100&#x4E2A;&#x7684;&#x6B63;&#x548C;300&#x53CD;&#x9762;&#x7684;&#x4F8B;&#x5B50;&#xFF0C;&#x5219; pos_weight &#x4E3A;&#x7C7B;&#x5E94;&#x7B49;&#x4E8E; 300  100  =  3  \&#x538B;&#x88C2;{300}
{100} = 3  1  0  0  3  0  0  =  3  &#x3002;&#x635F;&#x5931;&#x5C06;&#x5145;&#x5F53;&#x5982;&#x679C;&#x6570;&#x636E;&#x96C6;&#x5305;&#x542B; 3  &#xD7; 100  =  300  3 \&#x500D;100 =
300  3  &#xD7; 1  0  0  =  3  0  0  &#x6B63;&#x4F8B;&#x3002;</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; target = torch.ones([10, 64], dtype=torch.float32)  # 64 classes, batch size = 10
&gt;&gt;&gt; output = torch.full([10, 64], 0.999)  # A prediction (logit)
&gt;&gt;&gt; pos_weight = torch.ones([64])  # All weights are equal to 1
&gt;&gt;&gt; criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)
&gt;&gt;&gt; criterion(output, target)  # -log(sigmoid(0.999))
tensor(0.3135)
</code></pre><p>Parameters</p>
<ul>
<li><p><strong>weight</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; a manual rescaling weight given to the loss of each batch element. If given, has to be a Tensor of size nbatch.</p>
</li>
<li><p><strong>size_average</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field <code>size_average</code>is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></p>
</li>
<li><p><strong>reduce</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code>is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></p>
</li>
<li><p><strong>reduction</strong> ( <em>string</em> <em>,</em> <em>optional</em> ) &#x2013; Specifies the reduction to apply to the output: <code>&apos;none&apos;</code>| <code>&apos;mean&apos;</code>| <code>&apos;sum&apos;</code>. <code>&apos;none&apos;</code>: no reduction will be applied, <code>&apos;mean&apos;</code>: the sum of the output will be divided by the number of elements in the output, <code>&apos;sum&apos;</code>: the output will be summed. Note: <code>size_average</code>and <code>reduce</code>are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: <code>&apos;mean&apos;</code></p>
</li>
<li><p><strong>pos_weight</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x6B63;&#x4F8B;&#x7684;&#x6743;&#x91CD;&#x3002;&#x5FC5;&#x987B;&#x4E0E;&#x957F;&#x5EA6;&#x7B49;&#x4E8E;&#x7C7B;&#x7684;&#x6570;&#x91CF;&#x7684;&#x77E2;&#x91CF;&#x3002;</p>
</li>
</ul>
<p>Shape:</p>
<blockquote>
<ul>
<li>&#x8F93;&#x5165;&#xFF1A; &#xFF08; N  &#xFF0C; <em>  &#xFF09; &#xFF08;N&#xFF0C;</em>&#xFF09; &#xFF08; N  &#xFF0C; <em>  &#xFF09; &#x5176;&#x4E2D; </em>  <em>  </em>  &#x624B;&#x6BB5;&#xFF0C;&#x4EFB;&#x610F;&#x6570;&#x91CF;&#x7684;&#x9644;&#x52A0;&#x7EF4;&#x5EA6;&#x7684;</li>
</ul>
</blockquote>
<p>&gt;</p>
<blockquote>
<ul>
<li>Target: (N,&#x2217;)(N, *)(N,&#x2217;) , same shape as the input</li>
</ul>
</blockquote>
<p>&gt;</p>
<blockquote>
<ul>
<li>Output: scalar. If <code>reduction</code>is <code>&apos;none&apos;</code>, then (N,&#x2217;)(N, *)(N,&#x2217;) , same
shape as input.</li>
</ul>
</blockquote>
<p>&gt;</p>
<p>&gt;</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; loss = nn.BCEWithLogitsLoss()
&gt;&gt;&gt; input = torch.randn(3, requires_grad=True)
&gt;&gt;&gt; target = torch.empty(3).random_(2)
&gt;&gt;&gt; output = loss(input, target)
&gt;&gt;&gt; output.backward()
</code></pre><h3 id="marginrankingloss">MarginRankingLoss</h3>
<p><em>class</em><code>torch.nn.``MarginRankingLoss</code>( <em>margin=0.0</em> , <em>size_average=None</em> ,
<em>reduce=None</em> , <em>reduction=&apos;mean&apos;</em>
)<a href="_modules/torch/nn/modules/loss.html#MarginRankingLoss">[source]</a></p>
<p>&#x521B;&#x5EFA;&#x6D4B;&#x91CF;&#x635F;&#x8017;&#x7ED9;&#x5B9A;&#x7684;&#x8F93;&#x5165; &#xD7; 1  X1 [HTG12&#x4E00;&#x4E2A;&#x6807;&#x51C6;]  &#xD7; 1  &#xFF0C; &#xD7; 2  X2  &#xD7; 2  &#xFF0C;&#x4E24;&#x4E2A;&#x4E00;&#x7EF4;&#x5C0F;&#x6279;&#x91CF;&#x5F20;&#x91CF;&#xFF0C;&#x548C;&#x6807;&#x7B7E;1D&#x5C0F;&#x6279;&#x91CF;&#x5F20;&#x91CF; Y
Y  Y  &#xFF08;&#x542B;&#x6709;1&#x6216;-1&#xFF09;&#x3002;</p>
<p>&#x5982;&#x679C; Y  =  1  Y = 1  Y  =  1  &#x7136;&#x540E;&#x5C06;&#x5176;&#x5047;&#x5B9A;&#x7B2C;&#x4E00;&#x8F93;&#x5165;&#x5E94;&#x8BE5;&#x88AB;&#x6392;&#x540D;&#x66F4;&#x9AD8;&#xFF08;&#x5177;&#x6709;&#x66F4;&#x5927;&#x7684;&#x503C;&#xFF09;&#x5927;&#x4E8E;&#x7B2C;&#x4E8C;&#x8F93;&#x5165;&#xFF0C;&#x53CD;&#x4E4B;&#x4EA6;&#x7136;&#x4E3A; Y  =  -  1
Y = -1  Y  =  -  1  &#x3002;</p>
<p>&#x5728;&#x5C0F;&#x6279;&#x91CF;&#x6BCF;&#x4E2A;&#x6837;&#x54C1;&#x7684;&#x635F;&#x5931;&#x51FD;&#x6570;&#x662F;&#xFF1A;</p>
<p>loss(x,y)=max&#x2061;(0,&#x2212;y&#x2217;(x1&#x2212;x2)+margin)\text{loss}(x, y) = \max(0, -y * (x1 - x2)</p>
<ul>
<li>\text{margin}) loss(x,y)=max(0,&#x2212;y&#x2217;(x1&#x2212;x2)+margin)</li>
</ul>
<p>Parameters</p>
<ul>
<li><p><strong>&#x4F59;&#x91CF;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5177;&#x6709;&#x7684; &#x9ED8;&#x8BA4;&#x503C; 0  0  0  &#x3002;</p>
</li>
<li><p><strong>size_average</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field <code>size_average</code>is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></p>
</li>
<li><p><strong>reduce</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code>is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></p>
</li>
<li><p><strong>reduction</strong> ( <em>string</em> <em>,</em> <em>optional</em> ) &#x2013; Specifies the reduction to apply to the output: <code>&apos;none&apos;</code>| <code>&apos;mean&apos;</code>| <code>&apos;sum&apos;</code>. <code>&apos;none&apos;</code>: no reduction will be applied, <code>&apos;mean&apos;</code>: the sum of the output will be divided by the number of elements in the output, <code>&apos;sum&apos;</code>: the output will be summed. Note: <code>size_average</code>and <code>reduce</code>are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: <code>&apos;mean&apos;</code></p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>&#x8F93;&#x5165;&#xFF1A; &#xFF08; N  &#xFF0C; d  &#xFF09; &#xFF08;N&#xFF0C;d&#xFF09; &#xFF08; N  &#xFF0C; d  &#xFF09; &#x5176;&#x4E2D; N &#x662F;&#x6279;&#x91CF;&#x5927;&#x5C0F;&#x548C; d &#x662F;&#x4E00;&#x4E2A;&#x6837;&#x54C1;&#x7684;&#x5927;&#x5C0F;&#x3002;</p>
</li>
<li><p>&#x76EE;&#x6807;&#xFF1A; &#xFF08; N  &#xFF09; &#xFF08;N&#xFF09;  &#xFF08; N  &#xFF09;</p>
</li>
<li><p>&#x8F93;&#x51FA;&#xFF1A;&#x6807;&#x91CF;&#x3002;&#x5982;&#x679C;<code>&#x8FD8;&#x539F;</code>&#x662F;<code>&apos;&#x65E0;&apos;</code>&#xFF0C;&#x7136;&#x540E; &#xFF08; N  &#xFF09; &#xFF08;N&#xFF09; &#xFF08; N  &#xFF09; &#x3002;</p>
</li>
</ul>
<h3 id="hingeembeddingloss">HingeEmbeddingLoss</h3>
<p><em>class</em><code>torch.nn.``HingeEmbeddingLoss</code>( <em>margin=1.0</em> , <em>size_average=None</em> ,
<em>reduce=None</em> , <em>reduction=&apos;mean&apos;</em>
)<a href="_modules/torch/nn/modules/loss.html#HingeEmbeddingLoss">[source]</a></p>
<p>&#x63AA;&#x65BD;&#x7684;&#x635F;&#x5931;&#x7ED9;&#x5B9A;&#x7684;&#x8F93;&#x5165;&#x5F20;&#x91CF; &#xD7; &#xD7; &#xD7; &#x548C;&#x6807;&#x7B7E;&#x5F20;&#x91CF; Y  Y  Y  &#xFF08;&#x542B;&#x6709;1&#x6216;-1&#xFF09;&#x3002;&#x8FD9;&#x901A;&#x5E38;&#x662F;&#x7528;&#x4E8E;&#x6D4B;&#x91CF;&#x4E24;&#x4E2A;&#x8F93;&#x5165;&#x662F;&#x5426;&#x662F;&#x76F8;&#x4F3C;&#x6216;&#x4E0D;&#x76F8;&#x4F3C;&#xFF0C;&#x4F8B;&#x5982;&#x4F7F;&#x7528;L1&#x6210;&#x5BF9;&#x8DDD;&#x79BB;&#x4E3A; &#xD7;
&#xD7; &#xD7; &#xFF0C;&#x5E76;&#x4E14;&#x901A;&#x5E38;&#x7528;&#x4E8E;&#x5B66;&#x4E60;&#x7684;&#x5D4C;&#x5165;&#x7684;&#x975E;&#x7EBF;&#x6027;&#x6216;&#x534A;&#x76D1;&#x7763;&#x5B66;&#x4E60;&#x3002;</p>
<p>&#x635F;&#x8017;&#x51FD;&#x6570;&#x4E3A; n&#x7684; n&#x7684; n&#x7684; &#x5728;&#x8FF7;&#x4F60;&#x6279;&#x6B21;&#x53F7;&#x7684;&#x62BD;&#x6837;&#x662F;</p>
<p>ln={xn,if yn=1,max&#x2061;{0,&#x394;&#x2212;xn},if yn=&#x2212;1,l_n = \begin{cases} x_n, &amp; \text{if}\;
y_n = 1,\\ \max \{0, \Delta - x_n\}, &amp; \text{if}\; y_n = -1, \end{cases}
ln&#x200B;={xn&#x200B;,max{0,&#x394;&#x2212;xn&#x200B;},&#x200B;ifyn&#x200B;=1,ifyn&#x200B;=&#x2212;1,&#x200B;</p>
<p>&#x548C;&#x603B;&#x635F;&#x8017;&#x51FD;&#x6570;&#x662F;</p>
<p>&#x2113;(x,y)={mean&#x2061;(L),if reduction=&#x2019;mean&#x2019;;sum&#x2061;(L),if reduction=&#x2019;sum&#x2019;.\ell(x, y) =
\begin{cases} \operatorname{mean}(L), &amp; \text{if reduction} =
\text{&apos;mean&apos;;}\\ \operatorname{sum}(L), &amp; \text{if reduction} = \text{&apos;sum&apos;.}
\end{cases} &#x2113;(x,y)={mean(L),sum(L),&#x200B;if reduction=&#x2019;mean&#x2019;;if reduction=&#x2019;sum&#x2019;.&#x200B;</p>
<p>&#x5176;&#x4E2D; L  =  { L  1  &#xFF0C; ...  &#xFF0C; L  N  }  &#x22A4; L = \ {L_1&#xFF0C;\&#x70B9;&#xFF0C;L_N \} ^ \&#x9876; L  =  { L  1
&#xFF0C; ...  &#xFF0C; L  N  }  &#x22A4; &#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x4F59;&#x91CF;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5177;&#x6709;&#x7684; 1 &#x7684;&#x9ED8;&#x8BA4;&#x503C;&#x3002;</p>
</li>
<li><p><strong>size_average</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field <code>size_average</code>is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></p>
</li>
<li><p><strong>reduce</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code>is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></p>
</li>
<li><p><strong>reduction</strong> ( <em>string</em> <em>,</em> <em>optional</em> ) &#x2013; Specifies the reduction to apply to the output: <code>&apos;none&apos;</code>| <code>&apos;mean&apos;</code>| <code>&apos;sum&apos;</code>. <code>&apos;none&apos;</code>: no reduction will be applied, <code>&apos;mean&apos;</code>: the sum of the output will be divided by the number of elements in the output, <code>&apos;sum&apos;</code>: the output will be summed. Note: <code>size_average</code>and <code>reduce</code>are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: <code>&apos;mean&apos;</code></p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>&#x8F93;&#x5165;&#xFF1A; &#xFF08; <em>  &#xFF09; &#xFF08;</em>&#xFF09;  &#xFF08; <em>  &#xFF09; &#x5176;&#x4E2D; </em>  <em>  </em>  &#x624B;&#x6BB5;&#xFF0C;&#x4EFB;&#x610F;&#x7EF4;&#x6570;&#x3002;&#x603B;&#x548C;&#x64CD;&#x4F5C;&#x8FC7;&#x7684;&#x6240;&#x6709;&#x5143;&#x7D20;&#x8FDB;&#x884C;&#x64CD;&#x4F5C;&#x3002;</p>
</li>
<li><p>&#x76EE;&#x6807;&#xFF1A; &#xFF08; <em>  &#xFF09; &#xFF08;</em>&#xFF09;  &#xFF08; *  &#xFF09; &#xFF0C;&#x76F8;&#x540C;&#x5F62;&#x72B6;&#x7684;&#x8F93;&#x5165;</p>
</li>
<li><p>&#x8F93;&#x51FA;&#xFF1A;&#x6807;&#x91CF;&#x3002;&#x5982;&#x679C;<code>&#x8FD8;&#x539F;</code>&#x662F;<code>&apos;&#x65E0;&apos;</code>&#xFF0C;&#x7136;&#x540E;&#x76F8;&#x540C;&#x7684;&#x5F62;&#x72B6;&#x4F5C;&#x4E3A;&#x8F93;&#x5165;</p>
</li>
</ul>
<h3 id="multilabelmarginloss">MultiLabelMarginLoss</h3>
<p><em>class</em><code>torch.nn.``MultiLabelMarginLoss</code>( <em>size_average=None</em> , <em>reduce=None</em>
, <em>reduction=&apos;mean&apos;</em>
)<a href="_modules/torch/nn/modules/loss.html#MultiLabelMarginLoss">[source]</a></p>
<p>&#x521B;&#x5EFA;&#x4F18;&#x5316;&#x8F93;&#x5165; &#xD7; &#x4E4B;&#x95F4;&#x7684;&#x591A;&#x7EA7;&#x591A;&#x5206;&#x7C7B;&#x94F0;&#x94FE;&#x635F;&#x5931;&#xFF08;&#x57FA;&#x4E8E;&#x5BB9;&#x9650;&#x7684;&#x635F;&#x5931;&#xFF09;&#x7684;&#x6807;&#x51C6;&#xD7; &#xD7; &#xFF08;&#x4E8C;&#x7EF4;&#x5C0F;&#x6279;&#x91CF;&#x5F20;&#x91CF;&#xFF09;&#x548C;&#x8F93;&#x51FA; Y  Y  Y  &#xFF08;&#x8FD9;&#x662F;&#x4E00;&#x4E2A;2D
&#x5F20;&#x91CF;&#x76EE;&#x6807;&#x7C7B;&#x6307;&#x6570;&#xFF09;&#x3002;&#x5BF9;&#x4E8E;&#x5728;&#x5C0F;&#x6279;&#x91CF;&#x6BCF;&#x4E2A;&#x6837;&#x54C1;&#xFF1A;</p>
<p>loss(x,y)=&#x2211;ijmax&#x2061;(0,1&#x2212;(x[y[j]]&#x2212;x[i]))x.size(0)\text{loss}(x, y) =
\sum_{ij}\frac{\max(0, 1 - (x[y[j]] - x[i]))}{\text{x.size}(0)}
loss(x,y)=ij&#x2211;&#x200B;x.size(0)max(0,1&#x2212;(x[y[j]]&#x2212;x[i]))&#x200B;</p>
<p>&#x5176;&#x4E2D; &#xD7; &#x2208; { 0  &#xFF0C; &#x22EF; &#xFF0C; x.size  &#xFF08; 0  &#xFF09; -  1  }  &#xD7; \&#x5728;\&#x5DE6;\ {0&#xFF0C;\ [] \ cdots&#xFF0C;\ [] \ {&#x6587;&#x672C;}
x.size&#xFF08;0&#xFF09; - 1 \&#x53F3;\}  &#xD7; &#x2208; { 0  &#xFF0C; &#x22EF; &#xFF0C; x.size  &#xFF08; 0  &#xFF09; -  1  }  &#xFF0C; Y  &#x2208; { 0  &#xFF0C; &#x22EF; &#xFF0C;
y.size  &#xFF08; 0  &#xFF09; -  1  }  &#x5728;\ Y \&#x5DE6;\ {0&#xFF0C;\ [] \ cdots&#xFF0C;\ [] \ {&#x6587;&#x672C;} y.size&#xFF08;0&#xFF09; - 1
\&#x53F3;\}  Y  &#x2208; { 0  &#xFF0C; [H TG186] &#x22EF; &#xFF0C; y.size  &#xFF08; 0  &#xFF09; -  1  }  &#xFF0C; 0  &#x2264; Y  [
[HTG238&#xFF1A;J  &#x2264; x.size  &#xFF08; 0  &#xFF09; -  1  0 \&#x5F53;&#x91CF;Y [j]&#x7684;\&#x5F53;&#x91CF;\&#x6587;&#x672C;{x.size}&#xFF08;0&#xFF09;-1  0  &#x200B;&#x200B;&#x2264; Y  [
[HTG282&#xFF1A;J  &#x2264; x.size  &#xFF08; 0  &#xFF09; -  1  &#x548C; i&#x7684; &#x2260; Y  [ [HTG336&#xFF1A;J  I \ NEQ Y [j]&#x7684; i&#x7684; &#xE020;
=  Y  [ [HTG400&#xFF1A;J  &#x6240;&#x6709; i&#x7684; i&#x7684; i&#x7684; &#x548C; [HTG438&#xFF1A;J  [HTG441&#xFF1A;J  [HTG450&#xFF1A;J  &#x3002;</p>
<p>Y  Y  Y  &#x548C; &#xD7; &#xD7; &#xD7; &#x5FC5;&#x987B;&#x5177;&#x6709;&#x76F8;&#x540C;&#x7684;&#x5927;&#x5C0F;&#x3002;</p>
<p>&#x8BE5;&#x6807;&#x51C6;&#x4EC5;&#x8003;&#x8651;&#x7684;&#x975E;&#x8D1F;&#x76EE;&#x6807;&#x7684;&#x8FDE;&#x7EED;&#x7684;&#x5757;&#x5F00;&#x59CB;&#x4E8E;&#x524D;&#x9762;&#x3002;</p>
<p>&#x8FD9;&#x5141;&#x8BB8;&#x4E0D;&#x540C;&#x7684;&#x6837;&#x54C1;&#xFF0C;&#x4EE5;&#x5177;&#x6709;&#x53EF;&#x53D8;&#x7684;&#x91CF;&#x7684;&#x9776;&#x7C7B;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>size_average</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field <code>size_average</code>is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></p>
</li>
<li><p><strong>reduce</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code>is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></p>
</li>
<li><p><strong>reduction</strong> ( <em>string</em> <em>,</em> <em>optional</em> ) &#x2013; Specifies the reduction to apply to the output: <code>&apos;none&apos;</code>| <code>&apos;mean&apos;</code>| <code>&apos;sum&apos;</code>. <code>&apos;none&apos;</code>: no reduction will be applied, <code>&apos;mean&apos;</code>: the sum of the output will be divided by the number of elements in the output, <code>&apos;sum&apos;</code>: the output will be summed. Note: <code>size_average</code>and <code>reduce</code>are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: <code>&apos;mean&apos;</code></p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>&#x8F93;&#x5165;&#xFF1A; &#xFF08; C  &#xFF09; &#xFF08;C&#xFF09;  &#xFF08; C  &#xFF09; &#x6216; &#xFF08; N  &#xFF0C; C  &#xFF09; &#xFF08;N&#xFF0C;C&#xFF09; &#xFF08;  N  &#xFF0C; C  &#xFF09; &#x5176;&#x4E2D; N &#x662F;&#x6279;&#x91CF;&#x5927;&#x5C0F;&#x548C; C &#x662F;&#x7C7B;&#x7684;&#x6570;&#x91CF;&#x3002;</p>
</li>
<li><p>&#x76EE;&#x6807;&#xFF1A; &#xFF08; C  &#xFF09; &#xFF08;C&#xFF09;  &#xFF08; C  &#xFF09; &#x6216; &#xFF08; N  &#xFF0C; C  &#xFF09; &#xFF08;N&#xFF0C;C&#xFF09; &#xFF08;  N  &#xFF0C; C  &#xFF09; &#x6807;&#x7B7E;&#x7684;&#x76EE;&#x6807;-1&#x4FDD;&#x8BC1;&#x76F8;&#x540C;&#x5F62;&#x72B6;&#x7684;&#x8F93;&#x5165;&#x586B;&#x5145;&#x3002;</p>
</li>
<li><p>Output: scalar. If <code>reduction</code>is <code>&apos;none&apos;</code>, then (N)(N)(N) .</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; loss = nn.MultiLabelMarginLoss()
&gt;&gt;&gt; x = torch.FloatTensor([[0.1, 0.2, 0.4, 0.8]])
&gt;&gt;&gt; # for target y, only consider labels 3 and 0, not after label -1
&gt;&gt;&gt; y = torch.LongTensor([[3, 0, -1, 1]])
&gt;&gt;&gt; loss(x, y)
&gt;&gt;&gt; # 0.25 * ((1-(0.1-0.2)) + (1-(0.1-0.4)) + (1-(0.8-0.2)) + (1-(0.8-0.4)))
tensor(0.8500)
</code></pre><h3 id="smoothl1loss">SmoothL1Loss</h3>
<p><em>class</em><code>torch.nn.``SmoothL1Loss</code>( <em>size_average=None</em> , <em>reduce=None</em> ,
<em>reduction=&apos;mean&apos;</em>
)<a href="_modules/torch/nn/modules/loss.html#SmoothL1Loss">[source]</a></p>
<p>&#x521B;&#x5EFA;&#x4E00;&#x4E2A;&#x4F7F;&#x7528;&#x5E73;&#x65B9;&#x9879;&#x5982;&#x679C;&#x7EDD;&#x5BF9;&#x9010;&#x5143;&#x7D20;&#x8BEF;&#x5DEE;&#x4F4E;&#x4E8E;1&#x548C;L1&#x672F;&#x8BED;&#x5426;&#x5219;&#x7684;&#x6807;&#x51C6;&#x3002;&#x8FD9;&#x662F;&#x5F02;&#x5E38;&#x503C;&#x6BD4; MSELoss &#x5728;&#x67D0;&#x4E9B;&#x60C5;&#x51B5;&#x4E0B;&#x8F83;&#x4E0D;&#x654F;&#x611F;&#x9632;&#x6B62;&#x7206;&#x70B8;&#x68AF;&#x5EA6;&#xFF08;&#x4F8B;&#x5982;&#xFF0C;&#x8BF7;&#x53C2;&#x89C1;&#x7531;Ross
Girshick&#x5FEB;&#x901F;R-CNN &#x7EB8;&#xFF09;&#x3002;&#x53C8;&#x79F0;&#x80E1;&#x8D1D;&#x5C14;&#x635F;&#x5931;&#xFF1A;</p>
<p>loss(x,y)=1n&#x2211;izi\text{loss}(x, y) = \frac{1}{n} \sum<em>{i} z</em>{i}
loss(x,y)=n1&#x200B;i&#x2211;&#x200B;zi&#x200B;</p>
<p>&#x5176;&#x4E2D; Z  i&#x7684; Z_ {I}  Z  i&#x7684; &#x7531;&#x4E0B;&#x5F0F;&#x7ED9;&#x51FA;&#xFF1A;</p>
<p>zi={0.5(xi&#x2212;yi)2,if &#x2223;xi&#x2212;yi&#x2223;&lt;1&#x2223;xi&#x2212;yi&#x2223;&#x2212;0.5,otherwise z_{i} = \begin{cases} 0.5
(x_i - y_i)^2, &amp; \text{if } |x_i - y_i| &lt; 1 \\ |x_i - y_i| - 0.5, &amp;
\text{otherwise } \end{cases} zi&#x200B;={0.5(xi&#x200B;&#x2212;yi&#x200B;)2,&#x2223;xi&#x200B;&#x2212;yi&#x200B;&#x2223;&#x2212;0.5,&#x200B;if
&#x2223;xi&#x200B;&#x2212;yi&#x200B;&#x2223;&lt;1otherwise &#x200B;</p>
<p>&#xD7; &#xD7; &#xD7; &#x548C; Y  Y  Y  &#x7684;&#x4EFB;&#x610F;&#x5F62;&#x72B6;&#xFF0C;&#x603B;&#x7684; n&#x7684; n&#x7684; n&#x7684; &#x6BCF;&#x4E2A;&#x6C42;&#x548C;&#x64CD;&#x4F5C;&#x4ECD;&#x7136;&#x5DE5;&#x4F5C;&#x5728;&#x6240;&#x6709;&#x7684;&#x5143;&#x7D20;&#xFF0C;&#x5E76;&#x4E14;&#x901A;&#x8FC7;&#x5206;&#x5272;&#x5143;&#x7D20; n&#x7684; n&#x7684; n&#x7684; &#x3002;</p>
<p>&#x9664;&#x4EE5; n&#x7684; n&#x7684; n&#x7684; &#x53EF;&#x907F;&#x514D;&#x5982;&#x679C;&#x96C6;<code>&#x8FD8;&#x539F; =  &apos;&#x548C;&apos;</code>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>size_average</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field <code>size_average</code>is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></p>
</li>
<li><p><strong>reduce</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code>is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></p>
</li>
<li><p><strong>reduction</strong> ( <em>string</em> <em>,</em> <em>optional</em> ) &#x2013; Specifies the reduction to apply to the output: <code>&apos;none&apos;</code>| <code>&apos;mean&apos;</code>| <code>&apos;sum&apos;</code>. <code>&apos;none&apos;</code>: no reduction will be applied, <code>&apos;mean&apos;</code>: the sum of the output will be divided by the number of elements in the output, <code>&apos;sum&apos;</code>: the output will be summed. Note: <code>size_average</code>and <code>reduce</code>are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: <code>&apos;mean&apos;</code></p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>Input: (N,&#x2217;)(N, <em>)(N,&#x2217;) where &#x2217;</em>&#x2217; means, any number of additional dimensions</p>
</li>
<li><p>Target: (N,&#x2217;)(N, *)(N,&#x2217;) , same shape as the input</p>
</li>
<li><p>Output: scalar. If <code>reduction</code>is <code>&apos;none&apos;</code>, then (N,&#x2217;)(N, *)(N,&#x2217;) , same shape as the input</p>
</li>
</ul>
<h3 id="softmarginloss">SoftMarginLoss</h3>
<p><em>class</em><code>torch.nn.``SoftMarginLoss</code>( <em>size_average=None</em> , <em>reduce=None</em> ,
<em>reduction=&apos;mean&apos;</em>
)<a href="_modules/torch/nn/modules/loss.html#SoftMarginLoss">[source]</a></p>
<p>&#x521B;&#x5EFA;&#x4F18;&#x5316;&#x4E4B;&#x95F4;&#x7684;&#x4E8C;&#x7C7B;&#x522B;&#x5206;&#x7C7B;&#x7684;&#x7269;&#x6D41;&#x635F;&#x5931;&#x7684;&#x6807;&#x51C6;&#x8F93;&#x5165;&#x5F20;&#x91CF; &#xD7; &#xD7; &#xD7; &#x548C;&#x76EE;&#x6807;&#x5F20;&#x91CF; Y  Y  Y  &#xFF08;&#x542B;&#x6709;1&#x6216;-1&#xFF09;&#x3002;</p>
<p>loss(x,y)=&#x2211;ilog&#x2061;(1+exp&#x2061;(&#x2212;y[i]&#x2217;x[i]))x.nelement()\text{loss}(x, y) = \sum_i
\frac{\log(1 + \exp(-y[i]*x[i]))}{\text{x.nelement}()}
loss(x,y)=i&#x2211;&#x200B;x.nelement()log(1+exp(&#x2212;y[i]&#x2217;x[i]))&#x200B;</p>
<p>Parameters</p>
<ul>
<li><p><strong>size_average</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field <code>size_average</code>is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></p>
</li>
<li><p><strong>reduce</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code>is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></p>
</li>
<li><p><strong>reduction</strong> ( <em>string</em> <em>,</em> <em>optional</em> ) &#x2013; Specifies the reduction to apply to the output: <code>&apos;none&apos;</code>| <code>&apos;mean&apos;</code>| <code>&apos;sum&apos;</code>. <code>&apos;none&apos;</code>: no reduction will be applied, <code>&apos;mean&apos;</code>: the sum of the output will be divided by the number of elements in the output, <code>&apos;sum&apos;</code>: the output will be summed. Note: <code>size_average</code>and <code>reduce</code>are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: <code>&apos;mean&apos;</code></p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>&#x8F93;&#x5165;&#xFF1A; &#xFF08; <em>  &#xFF09; &#xFF08;</em>&#xFF09;  &#xFF08; <em>  &#xFF09; &#x5176;&#x4E2D; </em>  <em>  </em>  &#x624B;&#x6BB5;&#xFF0C;&#x4EFB;&#x610F;&#x6570;&#x91CF;&#x7684;&#x9644;&#x52A0;&#x7EF4;&#x5EA6;&#x7684;</p>
</li>
<li><p>Target: (&#x2217;)(*)(&#x2217;) , same shape as the input</p>
</li>
<li><p>Output: scalar. If <code>reduction</code>is <code>&apos;none&apos;</code>, then same shape as the input</p>
</li>
</ul>
<h3 id="multilabelsoftmarginloss">MultiLabelSoftMarginLoss</h3>
<p><em>class</em><code>torch.nn.``MultiLabelSoftMarginLoss</code>( <em>weight=None</em> ,
<em>size_average=None</em> , <em>reduce=None</em> , <em>reduction=&apos;mean&apos;</em>
)<a href="_modules/torch/nn/modules/loss.html#MultiLabelSoftMarginLoss">[source]</a></p>
<p>&#x521B;&#x5EFA;&#x4F18;&#x5316;&#x7684;&#x591A;&#x6807;&#x7B7E;&#x4E00;&#x4E2A;&#x6297;&#x6240;&#x6709;&#x57FA;&#x4E8E;&#x6700;&#x5927;&#x71B5;&#x635F;&#x5931;&#x7684;&#x6807;&#x51C6;&#xFF0C;&#x4E4B;&#x95F4;&#x8F93;&#x5165; &#xD7; &#xD7; &#xD7; &#x548C;&#x76EE;&#x6807; Y  Y  Y  &#x7684;&#x5927;&#x5C0F; &#xFF08; N  &#xFF0C; C  &#xFF09; &#xFF08;N&#xFF0C;C&#xFF09; &#xFF08; N  &#xFF0C; C
&#xFF09; &#x3002;&#x5BF9;&#x4E8E;&#x5728;minibatch&#x6BCF;&#x4E2A;&#x6837;&#x54C1;&#xFF1A;</p>
<p>loss(x,y)=&#x2212;1C&#x2217;&#x2211;iy[i]&#x2217;log&#x2061;((1+exp&#x2061;(&#x2212;x[i]))&#x2212;1)+(1&#x2212;y[i])&#x2217;log&#x2061;(exp&#x2061;(&#x2212;x[i])(1+exp&#x2061;(&#x2212;x[i])))loss(x,
y) = - \frac{1}{C} <em> \sum_i y[i] </em> \log((1 + \exp(-x[i]))^{-1}) + (1-y[i]) *
\log\left(\frac{\exp(-x[i])}{(1 + \exp(-x[i]))}\right)
loss(x,y)=&#x2212;C1&#x200B;&#x2217;i&#x2211;&#x200B;y[i]&#x2217;log((1+exp(&#x2212;x[i]))&#x2212;1)+(1&#x2212;y[i])&#x2217;log((1+exp(&#x2212;x[i]))exp(&#x2212;x[i])&#x200B;)</p>
<p>&#x5176;&#x4E2D; i&#x7684; &#x2208; { 0  &#xFF0C; &#x22EF; &#xFF0C; x.nElement  &#xFF08; &#xFF09; -  1  }  i&#x7684;\ \&#x5DE6;\ {0 \ [] \ cdots&#xFF0C;\ [] \
{&#x6587;&#x672C;} x.nElement&#xFF08;&#xFF09; - 1 \&#x53F3;\}  i&#x7684; &#x2208; { 0  &#xFF0C; &#x22EF; &#xFF0C; x.nElement  &#xFF08; &#xFF09;&#x200B;&#x200B; -  1  }  &#xFF0C; Y
[ i&#x7684; &#x2208; { 0  &#xFF0C; 1  }  &#x503C;Y [i] \&#x5728;\&#x5DE6;\ {0&#xFF0C;\ [] 1 \&#x53F3;\}  Y  [  i&#x7684; &#x2208; { 0  &#xFF0C; 1  }  &#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>weight</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; a manual rescaling weight given to each class. If given, it has to be a Tensor of size C. Otherwise, it is treated as if having all ones.</p>
</li>
<li><p><strong>size_average</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field <code>size_average</code>is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></p>
</li>
<li><p><strong>reduce</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code>is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></p>
</li>
<li><p><strong>reduction</strong> ( <em>string</em> <em>,</em> <em>optional</em> ) &#x2013; Specifies the reduction to apply to the output: <code>&apos;none&apos;</code>| <code>&apos;mean&apos;</code>| <code>&apos;sum&apos;</code>. <code>&apos;none&apos;</code>: no reduction will be applied, <code>&apos;mean&apos;</code>: the sum of the output will be divided by the number of elements in the output, <code>&apos;sum&apos;</code>: the output will be summed. Note: <code>size_average</code>and <code>reduce</code>are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: <code>&apos;mean&apos;</code></p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>&#x8F93;&#x5165;&#xFF1A; &#xFF08; N  &#xFF0C; C  &#xFF09; &#xFF08;N&#xFF0C;C&#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF09; &#x5176;&#x4E2D; N &#x662F;&#x6279;&#x91CF;&#x5927;&#x5C0F;&#x548C; C &#x662F;&#x7C7B;&#x7684;&#x6570;&#x91CF;&#x3002;</p>
</li>
<li><p>&#x76EE;&#x6807;&#xFF1A; &#xFF08; N  &#xFF0C; C  &#xFF09; &#xFF08;N&#xFF0C;C&#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF09; &#xFF0C;&#x6807;&#x8BB0;&#x76EE;&#x6807;&#x586B;&#x5145;&#x7531;-1&#x786E;&#x4FDD;&#x76F8;&#x540C;&#x7684;&#x5F62;&#x72B6;&#x7684;&#x8F93;&#x5165;&#x3002;</p>
</li>
<li><p>Output: scalar. If <code>reduction</code>is <code>&apos;none&apos;</code>, then (N)(N)(N) .</p>
</li>
</ul>
<h3 id="cosineembeddingloss">CosineEmbeddingLoss</h3>
<p><em>class</em><code>torch.nn.``CosineEmbeddingLoss</code>( <em>margin=0.0</em> , <em>size_average=None</em> ,
<em>reduce=None</em> , <em>reduction=&apos;mean&apos;</em>
)<a href="_modules/torch/nn/modules/loss.html#CosineEmbeddingLoss">[source]</a></p>
<p>&#x521B;&#x5EFA;&#x4E00;&#x79CD;&#x6D4B;&#x91CF;&#x6807;&#x51C6;&#x7684;&#x635F;&#x5931;&#x7ED9;&#x5B9A;&#x7684;&#x8F93;&#x5165;&#x5F20;&#x91CF; &#xD7; 1  X_1  &#xD7; 1  &#xFF0C; &#xD7; 2  X_2  &#xD7; 2  [HT G102]  &#x548C;a &#x5F20;&#x91CF;&#x6807;&#x7B7E; Y  Y  Y
&#x4E0E;&#x503C;1&#x6216;-1&#x3002;&#x6B64;&#x88AB;&#x7528;&#x4E8E;&#x6D4B;&#x91CF;&#x4E24;&#x4E2A;&#x8F93;&#x5165;&#x662F;&#x5426;&#x662F;&#x76F8;&#x4F3C;&#x6216;&#x4E0D;&#x76F8;&#x4F3C;&#xFF0C;&#x4F7F;&#x7528;&#x4F59;&#x5F26;&#x8DDD;&#x79BB;&#xFF0C;&#x5E76;&#x4E14;&#x901A;&#x5E38;&#x7528;&#x4E8E;&#x5B66;&#x4E60;&#x7684;&#x5D4C;&#x5165;&#x7684;&#x975E;&#x7EBF;&#x6027;&#x6216;&#x534A;&#x76D1;&#x7763;&#x5B66;&#x4E60;&#x3002;</p>
<p>&#x6BCF;&#x4E2A;&#x6837;&#x54C1;&#x7684;&#x635F;&#x5931;&#x51FD;&#x6570;&#x662F;&#xFF1A;</p>
<p>loss(x,y)={1&#x2212;cos&#x2061;(x1,x2),if y=1max&#x2061;(0,cos&#x2061;(x1,x2)&#x2212;margin),if
y=&#x2212;1\text{loss}(x, y) = \begin{cases} 1 - \cos(x_1, x_2), &amp; \text{if } y = 1
\\ \max(0, \cos(x_1, x_2) - \text{margin}), &amp; \text{if } y = -1 \end{cases}
loss(x,y)={1&#x2212;cos(x1&#x200B;,x2&#x200B;),max(0,cos(x1&#x200B;,x2&#x200B;)&#x2212;margin),&#x200B;if y=1if y=&#x2212;1&#x200B;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x4F59;&#x91CF;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5E94;&#x8BE5;&#x662F;&#x4ECE; [HTG12&#x7684;&#x6570;]  -  1  -1-  -  1  &#x81F3; 1  1  1  &#xFF0C; 0  0  0  &#x81F3; 0.5  0.5  [H TG102]  0  &#x3002;  5  &#x5EFA;&#x8BAE;&#x3002;&#x5982;&#x679C;<code>&#x4F59;&#x91CF;</code>&#x7F3A;&#x5931;&#xFF0C;&#x5219;&#x9ED8;&#x8BA4;&#x503C;&#x4E3A; 0  0  0  &#x3002;</p>
</li>
<li><p><strong>size_average</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field <code>size_average</code>is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></p>
</li>
<li><p><strong>reduce</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code>is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></p>
</li>
<li><p><strong>reduction</strong> ( <em>string</em> <em>,</em> <em>optional</em> ) &#x2013; Specifies the reduction to apply to the output: <code>&apos;none&apos;</code>| <code>&apos;mean&apos;</code>| <code>&apos;sum&apos;</code>. <code>&apos;none&apos;</code>: no reduction will be applied, <code>&apos;mean&apos;</code>: the sum of the output will be divided by the number of elements in the output, <code>&apos;sum&apos;</code>: the output will be summed. Note: <code>size_average</code>and <code>reduce</code>are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: <code>&apos;mean&apos;</code></p>
</li>
</ul>
<h3 id="multimarginloss">MultiMarginLoss</h3>
<p><em>class</em><code>torch.nn.``MultiMarginLoss</code>( <em>p=1</em> , <em>margin=1.0</em> , <em>weight=None</em> ,
<em>size_average=None</em> , <em>reduce=None</em> , <em>reduction=&apos;mean&apos;</em>
)<a href="_modules/torch/nn/modules/loss.html#MultiMarginLoss">[source]</a></p>
<p>&#x521B;&#x5EFA;&#x4F18;&#x5316;&#x8F93;&#x5165; &#xD7; &#x4E4B;&#x95F4;&#x7684;&#x591A;&#x7C7B;&#x5206;&#x7C7B;&#x94F0;&#x94FE;&#x635F;&#x5931;&#xFF08;&#x57FA;&#x4E8E;&#x5BB9;&#x9650;&#x7684;&#x635F;&#x5931;&#xFF09;[HTG9&#x7684;&#x6807;&#x51C6;]&#xD7;  &#xD7; &#xFF08;&#x4E8C;&#x7EF4;&#x5FAE;&#x578B;&#x6279;&#x6B21;&#x5F20;&#x91CF;&#xFF09;&#x548C;&#x8F93;&#x51FA; Y  Y  Y
&#xFF08;&#x5B83;&#x662F;&#x76EE;&#x6807;&#x7C7B;&#x7D22;&#x5F15;&#x7684;1D&#x5F20;&#x91CF;&#xFF0C; 0  &#x2264; Y  &#x2264; x.size  &#xFF08; 1  &#xFF09; -  1  0 \&#x5F53;&#x91CF;Y \&#x5F53;&#x91CF;\&#x6587;&#x672C;{x.size}&#xFF08;1&#xFF09;-1  0
&#x2264; Y  &#x2264; x.size  &#xFF08; 1  &#xFF09; -  1  &#xFF09;&#xFF1A;</p>
<p>&#x5BF9;&#x4E8E;&#x6BCF;&#x4E2A;&#x5C0F;&#x6279;&#x91CF;&#x6837;&#x54C1;&#xFF0C;&#x5728;&#x4E00;&#x7EF4;&#x8F93;&#x5165;&#x65B9;&#x9762;&#x635F;&#x5931; &#xD7; &#xD7; &#xD7; &#x548C;&#x6807;&#x91CF;&#x8F93;&#x51FA; Y  Y  Y  &#x662F;&#xFF1A;</p>
<p>loss(x,y)=&#x2211;imax&#x2061;(0,margin&#x2212;x[y]+x[i]))px.size(0)\text{loss}(x, y) =
\frac{\sum_i \max(0, \text{margin} - x[y] + x[i]))^p}{\text{x.size}(0)}
loss(x,y)=x.size(0)&#x2211;i&#x200B;max(0,margin&#x2212;x[y]+x[i]))p&#x200B;</p>
<p>&#x5176;&#x4E2D; &#xD7; &#x2208; { 0  &#xFF0C; &#x22EF; &#xFF0C; x.size  &#xFF08; 0  &#xFF09; -  1  }  &#xD7; \&#x5728;\&#x5DE6;\ {0&#xFF0C;\ [] \ cdots&#xFF0C;\ [] \ {&#x6587;&#x672C;}
x.size&#xFF08;0&#xFF09; - 1 \&#x53F3;\}  &#xD7; &#x2208; { 0  &#xFF0C; &#x22EF; &#xFF0C; x.size  &#xFF08; 0  &#xFF09; -  1  }  &#x548C; i&#x7684; &#x2260; Y  I \ NEQ
Y  i&#x7684; &#xE020; =  Y  &#x3002;</p>
<p>&#x53EF;&#x9009;&#x5730;&#xFF0C;&#x53EF;&#x4EE5;&#x901A;&#x8FC7;&#x4F7F;1D <code>&#x91CD;&#x91CF;</code>&#x5F20;&#x91CF;&#x5230;&#x6784;&#x9020;&#x7ED9;&#x51FA;&#x7684;&#x7C7B;&#x4E0D;&#x76F8;&#x7B49;&#x7684;&#x6743;&#x91CD;&#x3002;</p>
<p>&#x90A3;&#x4E48;&#x635F;&#x5931;&#x51FD;&#x6570;&#x53D8;&#x4E3A;&#xFF1A;</p>
<p>loss(x,y)=&#x2211;imax&#x2061;(0,w[y]&#x2217;(margin&#x2212;x[y]+x[i]))p)x.size(0)\text{loss}(x, y) =
\frac{\sum_i \max(0, w[y] * (\text{margin} - x[y] +
x[i]))^p)}{\text{x.size}(0)}
loss(x,y)=x.size(0)&#x2211;i&#x200B;max(0,w[y]&#x2217;(margin&#x2212;x[y]+x[i]))p)&#x200B;</p>
<p>Parameters</p>
<ul>
<li><p><strong>P</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5177;&#x6709;&#x7684; &#x9ED8;&#x8BA4;&#x503C; 1  1  1  &#x3002;  1  1  1  &#x548C; 2  2  2  &#x662F;&#x552F;&#x4E00;&#x652F;&#x6301;&#x7684;&#x503C;&#x3002;</p>
</li>
<li><p><strong>&#x4F59;&#x91CF;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5177;&#x6709;&#x7684; &#x9ED8;&#x8BA4;&#x503C; 1  1  1  &#x3002;</p>
</li>
<li><p><strong>weight</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; a manual rescaling weight given to each class. If given, it has to be a Tensor of size C. Otherwise, it is treated as if having all ones.</p>
</li>
<li><p><strong>size_average</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field <code>size_average</code>is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></p>
</li>
<li><p><strong>reduce</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code>is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></p>
</li>
<li><p><strong>reduction</strong> ( <em>string</em> <em>,</em> <em>optional</em> ) &#x2013; Specifies the reduction to apply to the output: <code>&apos;none&apos;</code>| <code>&apos;mean&apos;</code>| <code>&apos;sum&apos;</code>. <code>&apos;none&apos;</code>: no reduction will be applied, <code>&apos;mean&apos;</code>: the sum of the output will be divided by the number of elements in the output, <code>&apos;sum&apos;</code>: the output will be summed. Note: <code>size_average</code>and <code>reduce</code>are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: <code>&apos;mean&apos;</code></p>
</li>
</ul>
<h3 id="tripletmarginloss">TripletMarginLoss</h3>
<p><em>class</em><code>torch.nn.``TripletMarginLoss</code>( <em>margin=1.0</em> , <em>p=2.0</em> , <em>eps=1e-06</em> ,
<em>swap=False</em> , <em>size_average=None</em> , <em>reduce=None</em> , <em>reduction=&apos;mean&apos;</em>
)<a href="_modules/torch/nn/modules/loss.html#TripletMarginLoss">[source]</a></p>
<p>&#x521B;&#x5EFA;&#x6D4B;&#x91CF;&#x4E09;&#x91CD;&#x635F;&#x5931;&#x7684;&#x6807;&#x51C6;&#x7ED9;&#x5B9A;&#x7684;&#x8F93;&#x5165;&#x5F20;&#x91CF; &#xD7; 1  X1  &#xD7; 1  &#xFF0C; &#xD7; 2  X2  &#xD7; 2  &#xFF0C; &#xD7; 3  &#xD7;3  &#xD7; 3  &#x5E76;&#x4EE5;&#x66F4;&#x5927;&#x7684;&#x503C;&#x7684;&#x88D5;&#x5EA6;&#x6BD4; 0
0  0  &#x3002;&#x6B64;&#x88AB;&#x7528;&#x4E8E;&#x6D4B;&#x91CF;&#x6837;&#x672C;&#x4E4B;&#x95F4;&#x7684;&#x76F8;&#x5BF9;&#x76F8;&#x4F3C;&#x6027;&#x3002;&#x4E09;&#x5143;&#x7EC4;&#x662F;&#x7531;&#x4E00;&#xFF0C; P &#x548C; n&#x7684;&#xFF08;&#x5373;&#x951A;&#xFF0C;&#x6B63;&#x4F8B;&#x548C;[HTG118&#x7EC4;&#x6210;]&#x53CD;&#x4F8B;&#x5206;&#x522B;&#x5730;&#xFF09;&#x3002;&#x6240;&#x6709;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x7684;&#x5F62;&#x72B6;&#x5E94;&#x662F; &#xFF08; N
&#xFF0C; d  &#xFF09; &#xFF08;N&#xFF0C;d&#xFF09; &#xFF08;  N  &#xFF0C; d  &#xFF09; &#x3002;</p>
<p>&#x8DDD;&#x79BB;&#x4EA4;&#x6362;&#x4E2D;&#x8BE6;&#x7EC6;&#x7EB8;&#x5F20;<a href="http://www.bmva.org/bmvc/2016/papers/paper119/index.html" target="_blank">&#x5B66;&#x4E60;&#x4E0E;&#x4E09;&#x91CD;&#x6001;&#x635F;&#x8017;</a>&#x7531;V.
Balntas&#xFF0C;E.&#x91CC;&#x5DF4;&#x7B49;&#x4EBA;&#x6D45;&#x5377;&#x79EF;&#x7279;&#x5F81;&#x63CF;&#x8FF0;&#x7B26;&#x63CF;&#x8FF0;&#x3002;</p>
<p>The loss function for each sample in the mini-batch is:</p>
<p>L(a,p,n)=max&#x2061;{d(ai,pi)&#x2212;d(ai,ni)+margin,0}L(a, p, n) = \max \{d(a_i, p_i) -
d(a_i, n_i) + {\rm margin}, 0\} L(a,p,n)=max{d(ai&#x200B;,pi&#x200B;)&#x2212;d(ai&#x200B;,ni&#x200B;)+margin,0}</p>
<p>&#x54EA;&#x91CC;</p>
<p>d(xi,yi)=&#x2225;xi&#x2212;yi&#x2225;pd(x_i, y_i) = \left\lVert {\bf x}_i - {\bf y}_i
\right\rVert_p d(xi&#x200B;,yi&#x200B;)=&#x2225;xi&#x200B;&#x2212;yi&#x200B;&#x2225;p&#x200B;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x4F59;&#x91CF;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x9ED8;&#x8BA4;&#x503C;&#xFF1A; 1  1  1  &#x3002;</p>
</li>
<li><p><strong>P</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x8303;&#x6570;&#x5EA6;&#x6210;&#x5BF9;&#x8DDD;&#x79BB;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A; 2  2  2  &#x3002;</p>
</li>
<li><p><strong>&#x4EA4;&#x6362;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x7684;&#x8DDD;&#x79BB;&#x4EA4;&#x6362;&#x88AB;&#x8BE6;&#x7EC6;&#x63CF;&#x8FF0;&#x5728;&#x672C;&#x6587;&#x4E2D;&#x63CF;&#x8FF0;HTG10]&#x5B66;&#x4E60;&#x6D45;&#x5377;&#x79EF;&#x7279;&#x5F81;&#x63CF;&#x8FF0;&#x7B26;&#x4E0E;&#x4E09;&#x91CD;&#x6001;&#x635F;&#x8017;&#x7531;V. Balntas&#xFF0C;E.&#x91CC;&#x5DF4;&#x7B49;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x5047; [HTG15&#x3002;</code></p>
</li>
<li><p><strong>size_average</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field <code>size_average</code>is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></p>
</li>
<li><p><strong>reduce</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code>is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></p>
</li>
<li><p><strong>reduction</strong> ( <em>string</em> <em>,</em> <em>optional</em> ) &#x2013; Specifies the reduction to apply to the output: <code>&apos;none&apos;</code>| <code>&apos;mean&apos;</code>| <code>&apos;sum&apos;</code>. <code>&apos;none&apos;</code>: no reduction will be applied, <code>&apos;mean&apos;</code>: the sum of the output will be divided by the number of elements in the output, <code>&apos;sum&apos;</code>: the output will be summed. Note: <code>size_average</code>and <code>reduce</code>are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: <code>&apos;mean&apos;</code></p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>&#x8F93;&#x5165;&#xFF1A; &#xFF08; N  &#xFF0C; d  &#xFF09; &#xFF08;N&#xFF0C;d&#xFF09; &#xFF08; N  &#xFF0C; d  &#xFF09; &#x5176;&#x4E2D; d  d  d  &#x662F;&#x77E2;&#x91CF;&#x7EF4;&#x6570;&#x3002;</p>
</li>
<li><p>Output: scalar. If <code>reduction</code>is <code>&apos;none&apos;</code>, then (N)(N)(N) .</p>
</li>
</ul>
<pre><code>&gt;&gt;&gt; triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)
&gt;&gt;&gt; anchor = torch.randn(100, 128, requires_grad=True)
&gt;&gt;&gt; positive = torch.randn(100, 128, requires_grad=True)
&gt;&gt;&gt; negative = torch.randn(100, 128, requires_grad=True)
&gt;&gt;&gt; output = triplet_loss(anchor, positive, negative)
&gt;&gt;&gt; output.backward()
</code></pre><h2 id="&#x89C6;&#x529B;&#x5C42;">&#x89C6;&#x529B;&#x5C42;</h2>
<h3 id="pixelshuffle">PixelShuffle</h3>
<p><em>class</em><code>torch.nn.``PixelShuffle</code>( <em>upscale_factor</em>
)<a href="_modules/torch/nn/modules/pixelshuffle.html#PixelShuffle">[source]</a></p>
<p>&#x91CD;&#x65B0;&#x6392;&#x5217;&#x7684;&#x5143;&#x4EF6;&#x5728;&#x5F62;&#x72B6; &#xFF08; <em>  &#xFF0C; &#x2103;&#x7684;&#x5F20;&#x91CF; &#xD7; R  2  &#xFF0C; H  &#xFF0C; W  &#xFF09; &#xFF08;</em>&#xFF0C;C \&#x500D;R ^ 2&#xFF0C;H&#xFF0C;W&#xFF09; &#xFF08; <em>  &#xFF0C; C  &#xD7; R  2
&#xFF0C; H  &#xFF0C; W  &#xFF09; &#x5230;&#x7684;&#x5F20;&#x91CF;&#x5B9A;&#x578B; &#xFF08; </em>  &#xFF0C; C  H  &#xD7; R  &#xFF0C; W  &#xD7; R  &#xFF09; &#xFF08;<em>&#xFF0C;C&#xFF0C;H \&#x500D;R&#xFF0C;W \&#x6B21;&#x6570;R&#xFF09; &#xFF08; </em>  &#xFF0C; C
&#xFF0C; H  &#xD7; R  W  &#xD7; R  &#xFF09; &#x3002;</p>
<p>&#x8FD9;&#x662F;&#x7528;&#x4E8E;&#x5B9E;&#x73B0;&#x9AD8;&#x6548;&#x7684;&#x5B50;&#x50CF;&#x7D20;&#x5377;&#x79EF;&#x7528;&#x7684; 1  /  R A&#x6B65;&#x5E45;&#x6709;&#x7528; 1 / R  1  /  R  &#x3002;</p>
<p>&#x89C1;&#x6587;&#x7AE0;&#xFF1A;<a href="https://arxiv.org/abs/1609.05158" target="_blank">&#x5B9E;&#x65F6;&#x5355;&#x5E45;&#x56FE;&#x50CF;&#x548C;&#x89C6;&#x9891;&#x8D85;&#x5206;&#x8FA8;&#x7387;&#x91C7;&#x7528;&#x9AD8;&#x6548;&#x7684;&#x5B50;&#x50CF;&#x7D20;&#x5377;&#x79EF;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;</a>&#x7531;Shi&#x7B49;&#x3002;&#x4EBA;&#xFF0C;&#xFF08;2016&#xFF09;&#x7684;&#x66F4;&#x591A;&#x7EC6;&#x8282;&#x3002;</p>
<p>Parameters</p>
<p><strong>upscale_factor</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em>
</a>&#xFF09;</p>
<ul>
<li>&#x56E0;&#x5B50;&#x4EE5;&#x589E;&#x52A0;&#x7531;&#x7A7A;&#x95F4;&#x5206;&#x8FA8;&#x7387;</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>&#x8F93;&#x5165;&#xFF1A; &#xFF08; N  &#xFF0C; L  &#xFF0C; H  i&#x7684; n&#x7684; &#xFF0C; W  i&#x7684; n&#x7684; &#xFF09; &#xFF08;N&#xFF0C;L&#xFF0C;H<em> {IN} &#xFF0C;W</em> {&#x5728;}&#xFF09; &#xFF08; N  &#xFF0C; L  &#xFF0C; H  i&#x7684; n&#x7684; &#xFF0C; W  i&#x7684; n&#x7684; &#xFF09; &#x5176;&#x4E2D; L  =  C  &#xD7; upscale_factor  2  L = c ^ \&#x500D;\&#x6587;&#x672C;{&#x9AD8;&#x6863;\ _factor} ^ 2  L  =  C  &#xD7; upscale_factor  [HTG19 6]  2 </p>
</li>
<li><p>&#x8F93;&#x51FA;&#xFF1A; &#xFF08; N  &#xFF0C; C  &#xFF0C; H  O  U  T  &#xFF0C; W  O  U  T  &#xFF09; &#xFF08;N&#xFF0C;C&#xFF0C;H<em> {&#x51FA;}&#xFF0C;W</em> {&#x51FA;}&#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF0C; H  O  U  T  &#xFF0C; W  O  U  T  &#xFF09; &#x5176;&#x4E2D; H  O  U  T  =  H  i&#x7684; n&#x7684; &#xD7; upscale<em>factor  H</em> {&#x51FA;} = H<em> {&#x5728;} \&#x500D;\&#x6587;&#x672C;{&#x9AD8;&#x6863;\ _factor}  H  [H TG196]  O  U  T  =  H  i&#x7684; n&#x7684; &#xD7; &#x200B;&#x200B;  upscale_factor  &#x548C; W&#xAF;&#xAF; [HTG29 2]  O  U  T  =  W  i&#x7684; n&#x7684; &#xD7; upscale_factor  W</em> {&#x51FA;} = W_ {&#x5728;} \&#x500D;\&#x6587;&#x672C;{&#x9AD8;&#x6863;\ _factor}  W  O  U  T  =  W  I  n&#x7684; &#xD7; upscale_factor </p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; pixel_shuffle = nn.PixelShuffle(3)
&gt;&gt;&gt; input = torch.randn(1, 9, 4, 4)
&gt;&gt;&gt; output = pixel_shuffle(input)
&gt;&gt;&gt; print(output.size())
torch.Size([1, 1, 12, 12])
</code></pre><h3 id="&#x4E0A;&#x91C7;&#x6837;">&#x4E0A;&#x91C7;&#x6837;</h3>
<p><em>class</em><code>torch.nn.``Upsample</code>( <em>size=None</em> , <em>scale_factor=None</em> ,
<em>mode=&apos;nearest&apos;</em> , <em>align_corners=None</em>
)<a href="_modules/torch/nn/modules/upsampling.html#Upsample">[source]</a></p>
<p>&#x4E0A;&#x91C7;&#x6837;&#x4E00;&#x4E2A;&#x7ED9;&#x5B9A;&#x7684;&#x591A;&#x901A;&#x9053;1D&#xFF08;&#x65F6;&#x95F4;&#xFF09;&#xFF0C;&#x4E8C;&#x7EF4;&#xFF08;&#x7A7A;&#x95F4;&#xFF09;&#x6216;3D&#xFF08;&#x4F53;&#x79EF;&#xFF09;&#x7684;&#x6570;&#x636E;&#x3002;</p>
<p>&#x6240;&#x8FF0;&#x8F93;&#x5165;&#x6570;&#x636E;&#x88AB;&#x5047;&#x5B9A;&#x4E3A;&#x5F62;&#x5F0F; minibatch
X&#x901A;&#x9053;&#xD7;[&#x53EF;&#x9009;&#x6DF1;&#x5EA6;]&#xD7;[&#x53EF;&#x9009;&#x9AD8;&#x5EA6;]&#xD7;&#x5BBD;&#x5EA6;&#x7684;&#x3002;&#x56E0;&#x6B64;&#xFF0C;&#x5BF9;&#x4E8E;&#x7A7A;&#x95F4;&#x7684;&#x6295;&#x5165;&#xFF0C;&#x6211;&#x4EEC;&#x9884;&#x8BA1;&#x56DB;&#x7EF4;&#x5F20;&#x91CF;&#x548C;&#x4F53;&#x79EF;&#x7684;&#x6295;&#x5165;&#xFF0C;&#x6211;&#x4EEC;&#x9884;&#x8BA1;5D&#x5F20;&#x91CF;&#x3002;</p>
<p>&#x53EF;&#x7528;&#x4E8E;&#x4E0A;&#x91C7;&#x6837;&#x7684;&#x7B97;&#x6CD5;&#x5206;&#x522B;&#x662F;3D&#xFF0C;4D&#x548C;5D&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x6700;&#x8FD1;&#x90BB;&#x548C;&#x7EBF;&#x6027;&#xFF0C;&#x53CC;&#x7EBF;&#x6027;&#xFF0C;&#x53CC;&#x4E09;&#x6B21;&#x548C;&#x4E09;&#x7EBF;&#x6027;&#x3002;</p>
<p>&#x4E00;&#x4E2A;&#x53EF;&#x4EE5;&#x5F97;&#x5230;<code>scale_factor</code>&#x6216;&#x76EE;&#x6807;&#x8F93;&#x51FA;<code>&#x5927;&#x5C0F;</code>&#x6765;&#x8BA1;&#x7B97;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x3002; &#xFF08;&#x4F60;&#x4E0D;&#x80FD;&#x7ED9;&#x53CC;&#x65B9;&#xFF0C;&#x56E0;&#x4E3A;&#x5B83;&#x662F;&#x4E0D;&#x660E;&#x786E;&#xFF09;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x5927;&#x5C0F;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#x6216;</em> <em>&#x5143;&#x7EC4;</em> <em>[</em> <a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>]&#x6216;</em> <em>&#x5143;&#x7EC4;</em> <em>[</em> <a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>]&#x6216;</em> <em>&#x5143;&#x7EC4;</em> <em>[</em> <a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>_ </em>&#xFF0C;<em> </em>&#x53EF;&#x9009;&#x7684;_ &#xFF09; - &#x8F93;&#x51FA;&#x7A7A;&#x95F4;&#x5C3A;&#x5BF8;</p>
</li>
<li><p><strong>scale_factor</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a> <em>&#x6216;</em> <em>&#x5143;&#x7EC4;</em> <em>[</em> <a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a> <em>]&#x6216;</em> <em>&#x5143;&#x7EC4;</em> <em>[</em> <a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a> <em>&#xFF0C;</em> <a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a> <em>]&#x6216;</em> <em>&#x5143;&#x7EC4;</em> <em>[</em> <a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a> <em>&#xFF0C;</em> <a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a> <em>&#xFF0C;</em> <a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a> <em>_ </em>&#xFF0C;<em> </em>&#x53EF;&#x9009;&#x7684;_ &#xFF09; - &#x4E58;&#x6CD5;&#x5668;&#xFF0C;&#x7528;&#x4E8E;&#x7A7A;&#x95F4;&#x5C3A;&#x5BF8;&#x3002;&#x6709;&#xFF0C;&#x5982;&#x679C;&#x5B83;&#x662F;&#x4E00;&#x4E2A;&#x5143;&#x7EC4;&#x5339;&#x914D;&#x8F93;&#x5165;&#x7684;&#x5185;&#x5BB9;&#x3002;</p>
</li>
<li><p><strong>&#x6A21;&#x5F0F;</strong> &#xFF08;<a href="https://docs.python.org/3/library/stdtypes.html#str" title="\(in Python v3.7\)" target="_blank"> <em>STR</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x4E0A;&#x91C7;&#x6837;&#x7B97;&#x6CD5;&#xFF1A;&#x7684;<code>&#x4E00;&#x4E2A;&apos;&#x6700;&#x8FD1;&#x201D;</code>&#xFF0C;<code>&apos;&#x7EBF;&#x6027;&apos;</code>&#xFF0C;<code>&apos;&#x53CC;&#x7EBF;&#x6027;&apos;</code>&#xFF0C;<code>&apos;&#x53CC;&#x4E09;&#x6B21;&apos;</code>&#x548C;<code>&apos;&#x4E09;&#x7EBF;&#x6027;&apos;</code>&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&apos;&#x6700;&#x8FD1;&apos;</code></p>
</li>
<li><p><strong>align_corners</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5982;&#x679C;<code>&#x771F;</code>&#xFF0C;&#x8F93;&#x5165;&#x548C;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684;&#x62D0;&#x89D2;&#x50CF;&#x7D20;&#x88AB;&#x5BF9;&#x9F50;&#xFF0C;&#x4ECE;&#x800C;&#x4FDD;&#x6301;&#x5728;&#x90A3;&#x4E9B;&#x50CF;&#x7D20;&#x7684;&#x503C;&#x3002;&#x8FD9;&#x4EC5;&#x5177;&#x6709;&#x6548;&#x529B;&#x65F6;<code>&#x6A21;&#x5F0F;</code>&#x662F;<code>&apos;&#x7EBF;&#x6027;&apos;</code>&#xFF0C;<code>&apos;&#x53CC;&#x7EBF;&#x6027;&apos;</code>&#x6216;<code>&apos;&#x4E09;&#x7EBF;&#x6027;&apos;</code>&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x5047;</code></p>
</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>&#x8F93;&#x5165;&#xFF1A; &#xFF08; N  &#xFF0C; C  &#xFF0C; W  i&#x7684; n&#x7684; &#xFF09; &#xFF08;N&#xFF0C;C&#xFF0C;W<em> {&#x5728;}&#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF0C; W  i&#x7684; n&#x7684; &#xFF09; &#xFF0C; &#xFF08; N  &#xFF0C; C  &#xFF0C; H  i&#x7684; n&#x7684; &#xFF0C; W  i&#x7684; n&#x7684; &#xFF09; &#xFF08;N&#xFF0C;C&#xFF0C;H</em> {IN}&#xFF0C;W<em> {IN} &#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF0C; H  i&#x7684; n&#x7684; &#xFF0C; W  [HTG20 1]  i&#x7684; n&#x7684; &#xFF09; &#x6216; &#xFF08; N  &#xFF0C; C  &#xFF0C; d  i&#x7684; n&#x7684; &#xFF0C; H  i&#x7684; n&#x7684; &#xFF0C; W &#x200B;&#x200B;  i&#x7684; n&#x7684; &#xFF09; &#xFF08;N&#xFF0C;C&#xFF0C;D</em> {IN}&#xFF0C;H<em> {IN}&#xFF0C;W</em> {&#x5728;}&#xFF09; &#xFF08; N  &#xFF0C;[HTG2 95]  C  &#xFF0C; d  i&#x7684; n&#x7684; &#xFF0C; H  i&#x7684; n&#x7684; &#xFF0C; W  i&#x7684; n&#x7684; [HTG39 3]  &#xFF09;</p>
</li>
<li><p>&#x8F93;&#x51FA;&#xFF1A; &#xFF08; N  &#xFF0C; C  &#xFF0C; W  O  U  T  &#xFF09; &#xFF08;N&#xFF0C;C&#xFF0C;W<em> {&#x51FA;}&#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF0C; W  O  U  T  &#xFF09;  &#xFF0C; &#xFF08; N  &#xFF0C; C  &#xFF0C; H  O  U  T  &#xFF0C; W  O  U  T  &#xFF09; &#xFF08;N&#xFF0C;C&#xFF0C;H</em> {&#x51FA;}&#xFF0C;W<em> {&#x51FA;}&#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF0C; H  O  U  T  &#xFF0C; W  O  U  T  &#xFF09; &#x6216; &#xFF08; N  &#xFF0C; C  &#xFF0C; d  O  U  T  &#xFF0C; &#x200B;&#x200B;  H  O  U  T  &#xFF0C; W  O  U  T  &#xFF09;[HTG2 97]  &#xFF08;N&#xFF0C;C&#xFF0C;D</em> {&#x51FA;}&#xFF0C;H<em> {&#x51FA;}&#xFF0C;W</em> {&#x51FA;}&#xFF09; &#xFF08; N  &#xFF0C; C  &#xFF0C; d  O  U  T  &#xFF0C; H  O  U  T  [H TG392]  &#xFF0C; W  O  U  T  &#xFF09; &#xFF0C;&#x5176;&#x4E2D;</p>
</li>
</ul>
<p>Dout=&#x230A;Din&#xD7;scale<em>factor&#x230B;D</em>{out} = \left\lfloor D_{in} \times
\text{scale\_factor} \right\rfloor Dout&#x200B;=&#x230A;Din&#x200B;&#xD7;scale_factor&#x230B;</p>
<p>Hout=&#x230A;Hin&#xD7;scale<em>factor&#x230B;H</em>{out} = \left\lfloor H_{in} \times
\text{scale\_factor} \right\rfloor Hout&#x200B;=&#x230A;Hin&#x200B;&#xD7;scale_factor&#x230B;</p>
<p>Wout=&#x230A;Win&#xD7;scale<em>factor&#x230B;W</em>{out} = \left\lfloor W_{in} \times
\text{scale\_factor} \right\rfloor Wout&#x200B;=&#x230A;Win&#x200B;&#xD7;scale_factor&#x230B;</p>
<p>Warning</p>
<p>&#x4E0E;<code>align_corners  =  &#x771F;</code>&#x65F6;&#xFF0C;&#x7EBF;&#x6027;&#x5730;&#x5185;&#x63D2;&#x6A21;&#x5F0F;&#xFF08;&#x7EBF;&#x6027;&#xFF0C;&#x53CC;&#x7EBF;&#x6027;
&#x53CC;&#x4E09;&#x6B21;&#x548C;&#x4E09;&#x7EBF;&#x6027;&#xFF09;&#x4E0D;&#x6309;&#x6BD4;&#x4F8B;&#x5BF9;&#x9F50;&#x7684;&#x8F93;&#x51FA;&#x548C;&#x8F93;&#x5165;&#x7684;&#x50CF;&#x7D20;&#xFF0C;&#x548C;&#x56E0;&#x6B64;&#x8F93;&#x51FA;&#x503C;&#x53EF;&#x4EE5;&#x4F9D;&#x8D56;&#x4E8E;&#x8F93;&#x5165;&#x7684;&#x5927;&#x5C0F;&#x3002;&#x8FD9;&#x662F;&#x8FD9;&#x4E9B;&#x6A21;&#x5F0F;&#x53EF;&#x652F;&#x6301;&#x9AD8;&#x8FBE;0.3.1&#x7248;&#x672C;&#x7684;&#x9ED8;&#x8BA4;&#x884C;&#x4E3A;&#x3002;&#x6B64;&#x540E;&#xFF0C;&#x7F3A;&#x7701;&#x884C;&#x4E3A;&#x662F;<code>align_corners  =  &#x5047;</code>&#x3002;&#x89C1;&#x4E0B;&#x9762;&#x5173;&#x4E8E;&#x8FD9;&#x5C06;&#x5982;&#x4F55;&#x5F71;&#x54CD;&#x8F93;&#x51FA;&#x7684;&#x5177;&#x4F53;&#x4F8B;&#x5B50;&#x3002;</p>
<p>Note</p>
<p>&#x5982;&#x679C;&#x60A8;&#x60F3;&#x91C7;&#x6837;/&#x4E00;&#x822C;&#x5927;&#x5C0F;&#x8C03;&#x6574;&#xFF0C;&#x4F60;&#x5E94;&#x8BE5;&#x4F7F;&#x7528;<code>&#x63D2;&#x503C;&#xFF08;&#xFF09; [HTG3&#x3002;</code></p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; input = torch.arange(1, 5, dtype=torch.float32).view(1, 1, 2, 2)
&gt;&gt;&gt; input
tensor([[[[ 1.,  2.],
          [ 3.,  4.]]]])

&gt;&gt;&gt; m = nn.Upsample(scale_factor=2, mode=&apos;nearest&apos;)
&gt;&gt;&gt; m(input)
tensor([[[[ 1.,  1.,  2.,  2.],
          [ 1.,  1.,  2.,  2.],
          [ 3.,  3.,  4.,  4.],
          [ 3.,  3.,  4.,  4.]]]])

&gt;&gt;&gt; m = nn.Upsample(scale_factor=2, mode=&apos;bilinear&apos;)  # align_corners=False
&gt;&gt;&gt; m(input)
tensor([[[[ 1.0000,  1.2500,  1.7500,  2.0000],
          [ 1.5000,  1.7500,  2.2500,  2.5000],
          [ 2.5000,  2.7500,  3.2500,  3.5000],
          [ 3.0000,  3.2500,  3.7500,  4.0000]]]])

&gt;&gt;&gt; m = nn.Upsample(scale_factor=2, mode=&apos;bilinear&apos;, align_corners=True)
&gt;&gt;&gt; m(input)
tensor([[[[ 1.0000,  1.3333,  1.6667,  2.0000],
          [ 1.6667,  2.0000,  2.3333,  2.6667],
          [ 2.3333,  2.6667,  3.0000,  3.3333],
          [ 3.0000,  3.3333,  3.6667,  4.0000]]]])

&gt;&gt;&gt; # Try scaling the same data in a larger tensor
&gt;&gt;&gt;
&gt;&gt;&gt; input_3x3 = torch.zeros(3, 3).view(1, 1, 3, 3)
&gt;&gt;&gt; input_3x3[:, :, :2, :2].copy_(input)
tensor([[[[ 1.,  2.],
          [ 3.,  4.]]]])
&gt;&gt;&gt; input_3x3
tensor([[[[ 1.,  2.,  0.],
          [ 3.,  4.,  0.],
          [ 0.,  0.,  0.]]]])

&gt;&gt;&gt; m = nn.Upsample(scale_factor=2, mode=&apos;bilinear&apos;)  # align_corners=False
&gt;&gt;&gt; # Notice that values in top left corner are the same with the small input (except at boundary)
&gt;&gt;&gt; m(input_3x3)
tensor([[[[ 1.0000,  1.2500,  1.7500,  1.5000,  0.5000,  0.0000],
          [ 1.5000,  1.7500,  2.2500,  1.8750,  0.6250,  0.0000],
          [ 2.5000,  2.7500,  3.2500,  2.6250,  0.8750,  0.0000],
          [ 2.2500,  2.4375,  2.8125,  2.2500,  0.7500,  0.0000],
          [ 0.7500,  0.8125,  0.9375,  0.7500,  0.2500,  0.0000],
          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]])

&gt;&gt;&gt; m = nn.Upsample(scale_factor=2, mode=&apos;bilinear&apos;, align_corners=True)
&gt;&gt;&gt; # Notice that values in top left corner are now changed
&gt;&gt;&gt; m(input_3x3)
tensor([[[[ 1.0000,  1.4000,  1.8000,  1.6000,  0.8000,  0.0000],
          [ 1.8000,  2.2000,  2.6000,  2.2400,  1.1200,  0.0000],
          [ 2.6000,  3.0000,  3.4000,  2.8800,  1.4400,  0.0000],
          [ 2.4000,  2.7200,  3.0400,  2.5600,  1.2800,  0.0000],
          [ 1.2000,  1.3600,  1.5200,  1.2800,  0.6400,  0.0000],
          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]])
</code></pre><h3 id="upsamplingnearest2d">UpsamplingNearest2d</h3>
<p><em>class</em><code>torch.nn.``UpsamplingNearest2d</code>( <em>size=None</em> , <em>scale_factor=None</em>
)<a href="_modules/torch/nn/modules/upsampling.html#UpsamplingNearest2d">[source]</a></p>
<p>&#x65BD;&#x52A0;2D&#x6700;&#x8FD1;&#x90BB;&#x4E0A;&#x91C7;&#x6837;&#x5230;&#x51E0;&#x4E2A;&#x8F93;&#x5165;&#x4FE1;&#x9053;&#x7EC4;&#x6210;&#x7684;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x3002;</p>
<p>&#x8981;&#x6307;&#x5B9A;&#x89C4;&#x6A21;&#xFF0C;&#x5B83;&#x9700;&#x8981;&#x8981;&#x4E48;<code>&#x5927;&#x5C0F;</code>&#x6216;<code>scale_factor</code>&#xFF0C;&#x56E0;&#x4E3A;&#x5B83;&#x7684;&#x6784;&#x9020;&#x51FD;&#x6570;&#x7684;&#x53C2;&#x6570;&#x3002;</p>
<p>&#x5F53;<code>&#x5927;&#x5C0F;</code>&#x4E2D;&#x7ED9;&#x51FA;&#xFF0C;&#x5B83;&#x662F;&#x56FE;&#x50CF;&#x7684;&#x8F93;&#x51FA;&#x5C3A;&#x5BF8;&#xFF08;H&#xFF0C;W&#xFF09;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x5927;&#x5C0F;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#x6216;</em> <em>&#x5143;&#x7EC4;</em> <em>[</em> <a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>_ </em>&#xFF0C;<em> </em>&#x53EF;&#x9009;_ &#xFF09; - &#x8F93;&#x51FA;&#x7A7A;&#x95F4;&#x5927;&#x5C0F;</p>
</li>
<li><p><strong>scale_factor</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a> <em>&#x6216;</em> <em>&#x5143;&#x7EC4;</em> <em>[</em> <a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a> <em>&#xFF0C;</em> <a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a> <em>_ </em>&#xFF0C;<em> </em>&#x53EF;&#x9009;_ &#xFF09; - &#x4E58;&#x6CD5;&#x5668;&#xFF0C;&#x7528;&#x4E8E;&#x7A7A;&#x95F4;&#x7684;&#x5927;&#x5C0F;&#x3002;</p>
</li>
</ul>
<p>Warning</p>
<p>&#x8FD9;&#x4E2A;&#x7C7B;&#x662F;&#x6709;&#x5229;&#x4E8E;<code>&#x63D2;&#x503C;&#xFF08;&#xFF09;</code>&#x5F03;&#x7528;&#x3002;</p>
<p>Shape:</p>
<ul>
<li><p>Input: (N,C,Hin,Win)(N, C, H<em>{in}, W</em>{in})(N,C,Hin&#x200B;,Win&#x200B;)</p>
</li>
<li><p>Output: (N,C,Hout,Wout)(N, C, H<em>{out}, W</em>{out})(N,C,Hout&#x200B;,Wout&#x200B;) where</p>
</li>
</ul>
<p>Hout=&#x230A;Hin&#xD7;scale<em>factor&#x230B;H</em>{out} = \left\lfloor H_{in} \times
\text{scale\_factor} \right\rfloor Hout&#x200B;=&#x230A;Hin&#x200B;&#xD7;scale_factor&#x230B;</p>
<p>Wout=&#x230A;Win&#xD7;scale<em>factor&#x230B;W</em>{out} = \left\lfloor W_{in} \times
\text{scale\_factor} \right\rfloor Wout&#x200B;=&#x230A;Win&#x200B;&#xD7;scale_factor&#x230B;</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; input = torch.arange(1, 5, dtype=torch.float32).view(1, 1, 2, 2)
&gt;&gt;&gt; input
tensor([[[[ 1.,  2.],
          [ 3.,  4.]]]])

&gt;&gt;&gt; m = nn.UpsamplingNearest2d(scale_factor=2)
&gt;&gt;&gt; m(input)
tensor([[[[ 1.,  1.,  2.,  2.],
          [ 1.,  1.,  2.,  2.],
          [ 3.,  3.,  4.,  4.],
          [ 3.,  3.,  4.,  4.]]]])
</code></pre><h3 id="upsamplingbilinear2d">UpsamplingBilinear2d</h3>
<p><em>class</em><code>torch.nn.``UpsamplingBilinear2d</code>( <em>size=None</em> , <em>scale_factor=None</em>
)<a href="_modules/torch/nn/modules/upsampling.html#UpsamplingBilinear2d">[source]</a></p>
<p>&#x65BD;&#x52A0;2D&#x53CC;&#x7EBF;&#x6027;&#x4E0A;&#x91C7;&#x6837;&#x5230;&#x51E0;&#x4E2A;&#x8F93;&#x5165;&#x4FE1;&#x9053;&#x7EC4;&#x6210;&#x7684;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x3002;</p>
<p>To specify the scale, it takes either the <code>size</code>or the <code>scale_factor</code>as it&#x2019;s
constructor argument.</p>
<p>When <code>size</code>is given, it is the output size of the image (h, w).</p>
<p>Parameters</p>
<ul>
<li><p><strong>size</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>or</em> <em>Tuple</em> <em>[</em><a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>,</em><a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>]</em> <em>,</em> <em>optional</em> ) &#x2013; output spatial sizes</p>
</li>
<li><p><strong>scale_factor</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>float</em></a> <em>or</em> <em>Tuple</em> <em>[</em><a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>float</em></a> <em>,</em><a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>float</em></a> <em>]</em> <em>,</em> <em>optional</em> ) &#x2013; multiplier for spatial size.</p>
</li>
</ul>
<p>Warning</p>
<p>&#x8FD9;&#x4E2A;&#x7C7B;&#x662F;&#x6709;&#x5229;&#x4E8E;<code>&#x63D2;&#x503C;&#xFF08;&#xFF09;</code>&#x5F03;&#x7528;&#x3002;&#x5B83;&#x7B49;&#x540C;&#x4E8E;<code>nn.functional.interpolate&#xFF08;...&#xFF0C; &#x6A21;&#x5F0F;= &apos;&#x53CC;&#x7EBF;&#x6027;&apos;&#xFF0C;
align_corners =&#x771F;&#xFF09;</code>&#x3002;</p>
<p>Shape:</p>
<ul>
<li><p>Input: (N,C,Hin,Win)(N, C, H<em>{in}, W</em>{in})(N,C,Hin&#x200B;,Win&#x200B;)</p>
</li>
<li><p>Output: (N,C,Hout,Wout)(N, C, H<em>{out}, W</em>{out})(N,C,Hout&#x200B;,Wout&#x200B;) where</p>
</li>
</ul>
<p>Hout=&#x230A;Hin&#xD7;scale<em>factor&#x230B;H</em>{out} = \left\lfloor H_{in} \times
\text{scale\_factor} \right\rfloor Hout&#x200B;=&#x230A;Hin&#x200B;&#xD7;scale_factor&#x230B;</p>
<p>Wout=&#x230A;Win&#xD7;scale<em>factor&#x230B;W</em>{out} = \left\lfloor W_{in} \times
\text{scale\_factor} \right\rfloor Wout&#x200B;=&#x230A;Win&#x200B;&#xD7;scale_factor&#x230B;</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; input = torch.arange(1, 5, dtype=torch.float32).view(1, 1, 2, 2)
&gt;&gt;&gt; input
tensor([[[[ 1.,  2.],
          [ 3.,  4.]]]])

&gt;&gt;&gt; m = nn.UpsamplingBilinear2d(scale_factor=2)
&gt;&gt;&gt; m(input)
tensor([[[[ 1.0000,  1.3333,  1.6667,  2.0000],
          [ 1.6667,  2.0000,  2.3333,  2.6667],
          [ 2.3333,  2.6667,  3.0000,  3.3333],
          [ 3.0000,  3.3333,  3.6667,  4.0000]]]])
</code></pre><h2 id="&#x6570;&#x636E;&#x5E76;&#x884C;&#x5C42;&#xFF08;&#x591A;gpu&#xFF0C;&#x5206;&#x5E03;&#x5F0F;&#xFF09;">&#x6570;&#x636E;&#x5E76;&#x884C;&#x5C42;&#xFF08;&#x591A;GPU&#xFF0C;&#x5206;&#x5E03;&#x5F0F;&#xFF09;</h2>
<h3 id="&#x6570;&#x636E;&#x5E76;&#x884C;">&#x6570;&#x636E;&#x5E76;&#x884C;</h3>
<p><em>class</em><code>torch.nn.``DataParallel</code>( <em>module</em> , <em>device_ids=None</em> ,
<em>output_device=None</em> , <em>dim=0</em>
)<a href="_modules/torch/nn/parallel/data_parallel.html#DataParallel">[source]</a></p>
<p>&#x5B9E;&#x73B0;&#x4E86;&#x5728;&#x6A21;&#x5757;&#x7EA7;&#x6570;&#x636E;&#x5E76;&#x884C;&#x3002;</p>
<p>&#x8FD9;&#x4E2A;&#x5BB9;&#x5668;&#x901A;&#x8FC7;&#x5728;&#x6279;&#x5C3A;&#x5BF8;&#x5206;&#x5757;&#x5206;&#x5272;&#x5728;&#x6574;&#x4E2A;&#x6307;&#x5B9A;&#x7684;&#x8BBE;&#x5907;&#x4E0A;&#x7684;&#x8F93;&#x5165;&#x5E76;&#x884C;&#x5316;&#x7ED9;&#x5B9A;&#x7684;<code>&#x6A21;&#x5757;</code>&#x7684;&#x5E94;&#x7528;&#xFF08;&#x5176;&#x4ED6;&#x76EE;&#x7684;&#x5C06;&#x6BCF;&#x4E00;&#x6B21;&#x8BBE;&#x5907;&#x4E2D;&#x590D;&#x5236;&#xFF09;&#x3002;&#x5728;&#x6B63;&#x5411;&#x901A;&#xFF0C;&#x8BE5;&#x6A21;&#x5757;&#x88AB;&#x590D;&#x5236;&#x5728;&#x6BCF;&#x4E2A;&#x8BBE;&#x5907;&#x4E0A;&#xFF0C;&#x548C;&#x6BCF;&#x4E2A;&#x590D;&#x5236;&#x54C1;&#x5904;&#x7406;&#x8F93;&#x5165;&#x7684;&#x4E00;&#x90E8;&#x5206;&#x3002;&#x5728;&#x5411;&#x540E;&#x4F20;&#x9012;&#xFF0C;&#x4ECE;&#x6BCF;&#x4E2A;&#x526F;&#x672C;&#x68AF;&#x5EA6;&#x6C42;&#x548C;&#x6210;&#x539F;&#x6765;&#x7684;&#x6A21;&#x5757;&#x3002;</p>
<p>&#x5C06;&#x6279;&#x6599;&#x5C3A;&#x5BF8;&#x5E94;&#x6BD4;&#x4F7F;&#x7528;GPU&#x7684;&#x6570;&#x91CF;&#x5927;&#x3002;</p>
<p>&#x53E6;&#x8BF7;&#x53C2;&#x89C1;&#xFF1A;<a href="notes/cuda.html#cuda-nn-dataparallel-instead"> &#x4F7F;&#x7528;nn.DataParallel&#x800C;&#x4E0D;&#x662F;&#x591A;&#x5904;&#x7406; </a></p>
<p>&#x4EFB;&#x610F;&#x4F4D;&#x7F6E;&#x548C;&#x5173;&#x952E;&#x5B57;&#x8F93;&#x5165;&#x88AB;&#x5141;&#x8BB8;&#x88AB;&#x4F20;&#x9012;&#x5230;&#x6570;&#x636E;&#x5E76;&#x884C;&#x4F46;&#x67D0;&#x4E9B;&#x7C7B;&#x578B;&#x662F;&#x7279;&#x6B8A;&#x5904;&#x7406;&#x7684;&#x3002;&#x5F20;&#x91CF;&#x5C06; <strong>&#x6563;</strong>
&#x4E0A;&#x6697;&#x6DE1;&#x6307;&#x5B9A;&#xFF08;&#x9ED8;&#x8BA4;&#x4E3A;0&#xFF09;&#x3002;&#x5143;&#x7EC4;&#xFF0C;&#x5217;&#x8868;&#x548C;&#x5B57;&#x5178;&#x7C7B;&#x578B;&#x5C06;&#x662F;&#x6D45;&#x590D;&#x5236;&#x3002;&#x5176;&#x4ED6;&#x7C7B;&#x578B;&#x5C06;&#x4E0D;&#x540C;&#x7684;&#x7EBF;&#x7A0B;&#x4E4B;&#x95F4;&#x5171;&#x4EAB;&#xFF0C;&#x5E76;&#x4E14;&#x53EF;&#x4EE5;&#x5982;&#x679C;&#x6A21;&#x578B;&#x4E2D;&#x7684;&#x76F4;&#x4F20;&#x5199;&#x5165;&#x88AB;&#x7834;&#x574F;&#x3002;</p>
<p>&#x5E76;&#x884C;&#x5316;<code>&#x6A21;&#x5757;</code>&#x5FC5;&#x987B;&#x5BF9;<code>&#x5B83;&#x7684;&#x53C2;&#x6570;&#x548C;&#x7F13;&#x51B2;&#x5242;device_ids [0]</code>&#x8FD0;&#x884C;&#x524D;&#x6B64; <code>&#x6570;&#x636E;&#x5E76;&#x884C;</code>&#x6A21;&#x5757;&#x3002;</p>
<p>Warning</p>
<p>&#x5728;&#x6BCF;&#x4E00;&#x4E2A;&#x524D;&#x5411;&#xFF0C;<code>&#x6A21;&#x5757;</code>&#x662F; <strong>&#x590D;&#x5236;</strong> &#x5728;&#x6BCF;&#x53F0;&#x8BBE;&#x5907;&#x4E0A;&#xFF0C;&#x56E0;&#x6B64;&#x4EFB;&#x4F55;&#x66F4;&#x65B0;&#x5728;<code>&#x5411;&#x524D;</code>&#x8FD0;&#x884C;&#x6A21;&#x5757;&#x4F1A;&#x8FF7;&#x8DEF;&#x3002;&#x4F8B;&#x5982;&#xFF0C;&#x5982;&#x679C;<code>&#x6A21;&#x5757;</code>&#x5177;&#x6709;&#x5728;&#x6BCF;&#x4E2A;<code>&#x9012;&#x589E;&#x8BA1;&#x6570;&#x5668;&#x5C5E;&#x6027;&#x5411;&#x524D;</code>&#xFF0C;&#x5B83;&#x4F1A;&#x4E00;&#x76F4;&#x505C;&#x7559;&#x5728;&#x521D;&#x59CB;&#x503C;&#xFF0C;&#x56E0;&#x4E3A;&#x66F4;&#x65B0;&#x5176;&#x4E0A;&#x540E;<code>&#x8F6C;&#x53D1;</code>&#x7834;&#x574F;&#x4E86;&#x526F;&#x672C;&#x5B8C;&#x6210;&#x3002;&#x7136;&#x800C;&#xFF0C; <code>&#x6570;&#x636E;&#x5E76;&#x884C;</code>&#x4FDD;&#x8BC1;<code>&#x8BBE;&#x5907;&#x4E0A;&#x7684;&#x526F;&#x672C;[0]</code>&#x5C06;&#x5177;&#x6709;&#x5176;&#x53C2;&#x6570;&#x548C;&#x7F13;&#x51B2;&#x5668;&#x5171;&#x4EAB;&#x5B58;&#x50A8;&#x4E0E;&#x57FA;&#x90E8;&#x5E76;&#x884C;<code>&#x6A21;&#x5757;</code>&#x3002;&#x6240;&#x4EE5; <strong>&#x5C31;&#x5730;</strong> [0] &#x5C06;&#x88AB;&#x8BB0;&#x5F55;<code>&#x8BBE;&#x5907;&#x4E0A;&#x66F4;&#x65B0;&#x7684;&#x53C2;&#x6570;&#x6216;&#x7F13;&#x51B2;&#x5242;&#x3002;&#x4F8B;&#x5982;&#xFF0C;</code>BatchNorm2d
<code>&#x548C;</code>spectral_norm&#xFF08;&#xFF09; <code>&#x4F9D;&#x9760;&#x8FD9;&#x79CD;&#x884C;&#x4E3A;&#x6765;&#x66F4;&#x65B0;&#x7F13;&#x51B2;&#x533A;&#x3002;</code></p>
<p>Warning</p>
<p>&#x4E0A;<code>&#x5411;&#x524D;&#x548C;&#x5411;&#x540E;&#x7684;&#x94A9;&#x5B50;&#x5B9A;&#x4E49;&#x7684;&#x6A21;&#x5757;</code>&#x53CA;&#x5176;&#x5B50;&#x6A21;&#x5757;&#x5C06;&#x88AB;&#x8C03;&#x7528;<code>LEN&#xFF08;device_ids&#xFF09;</code>&#x6B21;&#xFF0C;&#x6BCF;&#x6B21;&#x7528;&#x5B9A;&#x4F4D;&#x4E8E;&#x7279;&#x5B9A;&#x7684;&#x8F93;&#x5165;&#x8BBE;&#x5907;&#x3002;&#x7279;&#x522B;&#x5730;&#xFF0C;&#x94A9;&#x53EA;&#x4FDD;&#x8BC1;&#x4EE5;&#x6B63;&#x786E;&#x7684;&#x987A;&#x5E8F;&#x76F8;&#x5BF9;&#x4E8E;&#x64CD;&#x4F5C;&#x4E0A;&#x7684;&#x76F8;&#x5E94;&#x88C5;&#x7F6E;&#x6765;&#x6267;&#x884C;&#x3002;&#x4F8B;&#x5982;&#xFF0C;&#x5B83;&#x4E0D;&#x80FD;&#x4FDD;&#x8BC1;&#x901A;&#x8FC7; <code>&#x88AB;&#x6240;&#x6709; `
len&#x4E2A;&#x4E4B;&#x524D;&#x6267;&#x884C;register_forward_pre_hook&#xFF08;&#xFF09;&#x8BBE;&#x7F6E;&#x6302;&#x94A9;&#xFF08; device_ids&#xFF09;</code>&#x5411;&#x524D;&#xFF08;&#xFF09; <code>&#x7535;&#x8BDD;&#xFF0C;&#x4F46;&#x6BCF;&#x4E00;&#x4E2A;&#x8FD9;&#x6837;&#x7684;&#x94A9;&#x4E4B;&#x524D;&#x6267;&#x884C;&#x76F8;&#x5E94;&#x7684;</code>&#x8BE5;&#x8BBE;&#x5907;&#x7684;&#x524D;&#x5411;&#xFF08;&#xFF09; `&#x547C;&#x53EB;&#x3002;</p>
<p>Warning</p>
<p>&#x5F53;<code>&#x6A21;&#x5757;</code>&#x5728;<code>&#x5411;&#x524D;&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x6807;&#xFF08;&#x5373;&#xFF0C;0&#x7EF4;&#x5F20;&#x91CF;&#xFF09;&#xFF08;&#xFF09;</code>&#xFF0C;&#x6B64;&#x5305;&#x88C5;&#x5C06;&#x8FD4;&#x56DE;&#x957F;&#x5EA6;&#x7684;&#x77E2;&#x91CF;&#x7B49;&#x4E8E;&#x5728;&#x6570;&#x636E;&#x5E76;&#x884C;&#x4F7F;&#x7528;&#x7684;&#x8BBE;&#x5907;&#xFF0C;&#x5305;&#x542B;&#x6765;&#x81EA;&#x6BCF;&#x4E2A;&#x8BBE;&#x5907;&#x7684;&#x7ED3;&#x679C;&#x7684;&#x6570;&#x91CF;&#x3002;</p>
<p>Note</p>
<p>&#x6709;&#x5728;&#x4F7F;&#x7528;<code>&#x6536;&#x62FE; &#x5E8F;&#x5217;&#x7684;&#x5FAE;&#x5999; - &amp; GT ;  &#x590D;&#x53D1; &#x7F51;&#x7EDC; - &amp; GT ;  &#x89E3;&#x538B; &#x5E8F;&#x5217;</code>&#x5728; <code>&#x6A21;&#x5757;&#x56FE;&#x6848;</code>&#x5305;&#x88F9;&#x5728; <code>&#x6570;&#x636E;&#x5E76;&#x884C;</code>&#x3002;&#x53C2;&#x89C1;<a href="notes/faq.html#pack-rnn-unpack-with-data-%0Aparallelism">
&#x6211;&#x7ECF;&#x5E38;&#x6027;&#x7684;&#x7F51;&#x7EDC;&#x4E0D;&#x4E0E;&#x6570;&#x636E;&#x5E76;&#x884C; </a>&#x90E8;&#x5206;&#x4E2D;&#x5E38;&#x89C1;&#x95EE;&#x9898;&#x7684;&#x7EC6;&#x8282;&#x5DE5;&#x4F5C;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x6A21;&#x5757;</strong> &#xFF08; <em>&#x6A21;&#x5757;</em> &#xFF09; - &#x6A21;&#x5757;&#x53EF;&#x4EE5;&#x5E76;&#x884C;</p>
</li>
<li><p><strong>device_ids</strong> &#xFF08; <em>&#x87D2;&#x7684;&#x5217;&#x8868;&#xFF1A;INT</em> <em>&#x6216;</em> <a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"> <em>torch.device</em> </a>&#xFF09; - CUDA&#x8BBE;&#x5907;&#xFF08;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;&#x6240;&#x6709;&#x8BBE;&#x5907;&#xFF09;</p>
</li>
<li><p><strong>output_device</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#x6216;</em> <a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"> <em>torch.device</em> </a>&#xFF09; - &#x8F93;&#x51FA;&#x7684;&#x8BBE;&#x5907;&#x4F4D;&#x7F6E;&#xFF08;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;device_ids [0]&#xFF09;</p>
</li>
</ul>
<p>Variables</p>
<p><strong>&#x301C;DataParallel.module</strong> &#xFF08; <em>&#x6A21;&#x5757;</em> &#xFF09; - &#x8BE5;&#x6A21;&#x5757;&#x53EF;&#x4EE5;&#x5E76;&#x884C;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; net = torch.nn.DataParallel(model, device_ids=[0, 1, 2])
&gt;&gt;&gt; output = net(input_var)  # input_var can be on any device, including CPU
</code></pre><h3 id="distributeddataparallel">DistributedDataParallel</h3>
<p><em>class</em><code>torch.nn.parallel.``DistributedDataParallel</code>( <em>module</em> ,
<em>device_ids=None</em> , <em>output_device=None</em> , <em>dim=0</em> , <em>broadcast_buffers=True</em>
, <em>process_group=None</em> , <em>bucket_cap_mb=25</em> , <em>find_unused_parameters=False</em> ,
<em>check_reduction=False</em>
)<a href="_modules/torch/nn/parallel/distributed.html#DistributedDataParallel">[source]</a></p>
<p>&#x5206;&#x5E03;&#x5F0F;&#x6570;&#x636E;&#x5E76;&#x884C;&#x5B9E;&#x73B0;&#x4E86;&#x57FA;&#x4E8E;<code>torch.distributed</code>&#x5C01;&#x88C5;&#x5728;&#x6A21;&#x5757;&#x7EA7;&#x3002;</p>
<p>&#x8FD9;&#x4E2A;&#x5BB9;&#x5668;&#x901A;&#x8FC7;&#x5728;&#x6279;&#x5C3A;&#x5BF8;&#x5206;&#x5757;&#x5206;&#x5272;&#x5728;&#x6574;&#x4E2A;&#x6307;&#x5B9A;&#x7684;&#x8BBE;&#x5907;&#x4E0A;&#x7684;&#x8F93;&#x5165;&#x5E76;&#x884C;&#x5316;&#x7ED9;&#x5B9A;&#x7684;&#x6A21;&#x5757;&#x7684;&#x5E94;&#x7528;&#x3002;&#x8BE5;&#x6A21;&#x5757;&#x88AB;&#x590D;&#x5236;&#x6BCF;&#x53F0;&#x673A;&#x5668;&#x548C;&#x6BCF;&#x4E2A;&#x8BBE;&#x5907;&#x4E0A;&#xFF0C;&#x5E76;&#x4E14;&#x6BCF;&#x4E2A;&#x8FD9;&#x6837;&#x7684;&#x526F;&#x672C;&#x5904;&#x7406;&#x8F93;&#x5165;&#x7684;&#x4E00;&#x90E8;&#x5206;&#x3002;&#x5728;&#x5411;&#x540E;&#x4F20;&#x9012;&#xFF0C;&#x4ECE;&#x6BCF;&#x4E2A;&#x8282;&#x70B9;&#x7684;&#x68AF;&#x5EA6;&#x8FDB;&#x884C;&#x5E73;&#x5747;&#x3002;</p>
<p>&#x5C06;&#x6279;&#x6599;&#x5C3A;&#x5BF8;&#x5E94;&#x6BD4;&#x672C;&#x5730;&#x4F7F;&#x7528;GPU&#x7684;&#x6570;&#x91CF;&#x5927;&#x3002;</p>
<p>&#x53C2;&#x89C1;&#xFF1A;<a href="distributed.html#distributed-basics"> &#x57FA;&#x7840; </a>&#x548C;[ &#x4F7F;&#x7528;nn.DataParallel&#x800C;&#x4E0D;&#x662F;&#x591A;&#x5904;&#x7406;
<a href="notes/cuda.html#cuda-nn-dataparallel-instead">HTG7&#x3002;&#x5982;</a> <code>&#x4E0A;&#x8F93;&#x5165;&#x76F8;&#x540C;&#x7684;&#x7EA6;&#x675F;torch.nn.DataParallel</code>&#x9002;&#x7528;&#x3002;</p>
<p>&#x8FD9;&#x4E2A;&#x7C7B;&#x7684;&#x521B;&#x4F5C;&#x8981;&#x6C42;<code>torch.distributed</code>&#x88AB;&#x5DF2;&#x7ECF;&#x521D;&#x59CB;&#x5316;&#xFF0C;&#x901A;&#x8FC7;&#x8C03;&#x7528;<a href="distributed.html#torch.distributed.init_process_group" title="torch.distributed.init_process_group"> <code>torch.distributed.init_process_group&#xFF08;&#xFF09;</code>
</a>&#x3002;</p>
<p><code>DistributedDataParallel</code>&#x53EF;&#x5728;&#x4EE5;&#x4E0B;&#x4E24;&#x79CD;&#x65B9;&#x5F0F;&#x4F7F;&#x7528;&#xFF1A;</p>
<ol>
<li>&#x5355;&#x8FDB;&#x7A0B;&#x591A;GPU</li>
</ol>
<p>&#x5728;&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x4E00;&#x4E2A;&#x5355;&#x4E00;&#x7684;&#x8FC7;&#x7A0B;&#x5C06;&#x6BCF;&#x4E2A;&#x4E3B;&#x673A;/&#x8282;&#x70B9;&#x4E0A;&#x50AC;&#x751F;&#x4E86;&#x6BCF;&#x4E2A;&#x8FDB;&#x7A0B;&#x5C06;&#x5728;&#x5B83;&#x7684;&#x8FD0;&#x884C;&#x8282;&#x70B9;&#x7684;&#x6240;&#x6709;GPU&#x7684;&#x64CD;&#x4F5C;&#x3002;&#x8981;&#x4F7F;&#x7528;<code>DistributedDataParallel
[HTG3&#x4EE5;&#x8FD9;&#x79CD;&#x65B9;&#x5F0F;&#xFF0C;&#x4F60;&#x53EF;&#x4EE5;&#x7B80;&#x5355;&#x7684;&#x6784;&#x5EFA;&#x6A21;&#x578B;&#xFF0C;&#x5982;&#x4E0B;&#xFF1A;</code></p>
<pre><code>&gt;&gt;&gt; torch.distributed.init_process_group(backend=&quot;nccl&quot;)
&gt;&gt;&gt; model = DistributedDataParallel(model) # device_ids will include all GPU devices by default
</code></pre><ol>
<li>&#x591A;&#x8FDB;&#x7A0B;&#x5355;GPU</li>
</ol>
<p>&#x8FD9;&#x662F;&#x4F7F;&#x7528;<code>&#x9AD8;&#x5EA6;&#x63A8;&#x8350;&#x7684;&#x65B9;&#x6CD5;DistributedDataParallel</code>&#xFF0C;&#x5176;&#x4E2D;&#x591A;&#x4E2A;&#x8FDB;&#x7A0B;&#xFF0C;&#x5176;&#x4E2D;&#x7684;&#x6BCF;&#x4E00;&#x4E2A;&#x5728;&#x5355;&#x4E2A;GPU&#x5DE5;&#x4F5C;&#x3002;&#x8FD9;&#x662F;&#x76EE;&#x524D;&#x4F7F;&#x7528;PyTorch&#x505A;&#x6570;&#x636E;&#x5E76;&#x884C;&#x8BAD;&#x7EC3;&#x6700;&#x5FEB;&#x7684;&#x65B9;&#x6CD5;&#x548C;&#x9002;&#x7528;&#x4E8E;&#x5355;&#x8282;&#x70B9;&#xFF08;&#x591A;GPU&#xFF09;&#x548C;&#x591A;&#x8282;&#x70B9;&#x6570;&#x636E;&#x5E76;&#x884C;&#x8BAD;&#x7EC3;&#x3002;&#x5B83;&#x88AB;&#x8BC1;&#x660E;&#x6BD4;
<code>torch.nn.DataParallel</code>&#x4E3A;&#x5355;&#x8282;&#x70B9;&#x591A;GPU&#x6570;&#x636E;&#x5E76;&#x884C;&#x8BAD;&#x7EC3;&#x663E;&#x8457;&#x66F4;&#x5FEB;&#x3002;</p>
<p>&#x8FD9;&#x91CC;&#x662F;&#x5982;&#x4F55;&#x4F7F;&#x7528;&#x5B83;&#xFF1A;&#x7528;N
GPU&#x7684;&#x6BCF;&#x53F0;&#x4E3B;&#x673A;&#x4E0A;&#xFF0C;&#x4F60;&#x5E94;&#x8BE5;&#x4EA7;&#x5375;N&#x4E2A;&#x6D41;&#x7A0B;&#xFF0C;&#x540C;&#x65F6;&#x786E;&#x4FDD;&#x6BCF;&#x9053;&#x5DE5;&#x5E8F;&#x4E0A;&#x7684;&#x5355;&#x4E2A;GPU&#x5355;&#x72EC;&#x5DE5;&#x4F5C;&#x4ECE;0&#x5230;N-1&#x3002;&#x56E0;&#x6B64;&#xFF0C;&#x5B83;&#x662F;&#x4F60;&#x7684;&#x5DE5;&#x4F5C;&#xFF0C;&#x4EE5;&#x786E;&#x4FDD;&#x60A8;&#x7684;&#x57F9;&#x8BAD;&#x811A;&#x672C;&#x901A;&#x8FC7;&#x8C03;&#x7528;&#x4E00;&#x4E2A;&#x7ED9;&#x5B9A;&#x7684;GPU&#x5DE5;&#x4F5C;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; torch.cuda.set_device(i)
</code></pre><p>&#x5176;&#x4E2D;i&#x662F;&#x4ECE;0&#x5230;N-1&#x3002;&#x5728;&#x6BCF;&#x4E00;&#x4E2A;&#x8FC7;&#x7A0B;&#x4E2D;&#xFF0C;&#x4F60;&#x5E94;&#x8BE5;&#x53C2;&#x8003;&#x4EE5;&#x4E0B;&#x6784;&#x9020;&#x6B64;&#x6A21;&#x5757;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; torch.distributed.init_process_group(backend=&apos;nccl&apos;, world_size=4, init_method=&apos;...&apos;)
&gt;&gt;&gt; model = DistributedDataParallel(model, device_ids=[i], output_device=i)
</code></pre><p>&#x4E3A;&#x4E86;&#x4EA7;&#x5375;&#x4E86;&#x6BCF;&#x4E2A;&#x8282;&#x70B9;&#x7684;&#x591A;&#x4E2A;&#x8FDB;&#x7A0B;&#xFF0C;&#x5219;&#x53EF;&#x4EE5;&#x4F7F;&#x7528;<code>torch.distributed.launch</code>&#x6216;<code>torch.multiprocessing.spawn</code></p>
<p>Note</p>
<p><code>NCCL</code>&#x540E;&#x7AEF;&#x662F;&#x76EE;&#x524D;&#x4E0E;&#x591A;&#x8FDB;&#x7A0B;&#x5355;GPU&#x5206;&#x5E03;&#x5F0F;&#x8BAD;&#x7EC3;&#x4E2D;&#x4F7F;&#x7528;&#x6700;&#x5FEB;&#x548C;&#x9AD8;&#x5EA6;&#x63A8;&#x8350;&#x7684;&#x540E;&#x7AEF;&#xFF0C;&#x8FD9;&#x65E2;&#x9002;&#x7528;&#x4E8E;&#x5355;&#x8282;&#x70B9;&#x548C;&#x591A;&#x8282;&#x70B9;&#x5206;&#x5E03;&#x5F0F;&#x8BAD;&#x7EC3;</p>
<p>Note</p>
<p>&#x8BE5;&#x6A21;&#x5757;&#x8FD8;&#x652F;&#x6301;&#x6DF7;&#x5408;&#x7CBE;&#x5EA6;&#x5206;&#x5E03;&#x5F0F;&#x8BAD;&#x7EC3;&#x3002;&#x8FD9;&#x610F;&#x5473;&#x7740;&#xFF0C;&#x4F60;&#x7684;&#x6A21;&#x578B;&#x53EF;&#x4EE5;&#x6709;&#x4E0D;&#x540C;&#x7C7B;&#x578B;&#x7684;&#x53C2;&#x6570;&#xFF0C;&#x5982;&#x6DF7;&#x5408;&#x7C7B;&#x578B;FP16&#x548C;FP32&#x7684;&#xFF0C;&#x5BF9;&#x8FD9;&#x4E9B;&#x6DF7;&#x5408;&#x7C7B;&#x578B;&#x7684;&#x53C2;&#x6570;&#x68AF;&#x5EA6;&#x51CF;&#x5C11;&#x5C06;&#x53EA;&#x662F;&#x6B63;&#x5E38;&#x5DE5;&#x4F5C;&#x3002;&#x8FD8;&#x8981;&#x6CE8;&#x610F;&#x7684;&#x662F;<code>NCCL</code>&#x540E;&#x7AEF;&#x662F;&#x76EE;&#x524D;FP16 / FP32&#x6DF7;&#x5408;&#x7CBE;&#x5EA6;&#x8BAD;&#x7EC3;&#x6700;&#x5FEB;&#xFF0C;&#x5F3A;&#x70C8;&#x63A8;&#x8350;&#x540E;&#x7AEF;&#x3002;</p>
<p>Note</p>
<p>&#x5982;&#x679C;&#x60A8;&#x4F7F;&#x7528;<code>torch.save</code>&#x5728;&#x4E00;&#x4E2A;&#x8FDB;&#x7A0B;&#x8BBE;&#x7F6E;&#x68C0;&#x67E5;&#x70B9;&#x6A21;&#x5757;&#xFF0C;&#x4EE5;&#x53CA;<code>torch.load</code>&#x4E00;&#x4E9B;&#x5176;&#x4ED6;&#x8FDB;&#x7A0B;&#x6765;&#x6062;&#x590D;&#x5B83;&#xFF0C;&#x786E;&#x4FDD;<code>map_location</code>&#x6B63;&#x786E;&#x6BCF;&#x4E2A;&#x8FDB;&#x7A0B;&#x914D;&#x7F6E;&#x3002;&#x65E0;<code>map_location</code>&#xFF0C;<code>torch.load</code>&#x5C06;&#x6062;&#x590D;&#x6A21;&#x5757;&#xFF0C;&#x5176;&#x4E2D;&#x6240;&#x8FF0;&#x6A21;&#x5757;&#x662F;&#x4ECE;&#x4FDD;&#x5B58;&#x5668;&#x4EF6;&#x3002;</p>
<p>Warning</p>
<p>&#x6B64;&#x6A21;&#x5757;&#x53EA;&#x80FD;&#x4E0E;<code>GLOO</code>&#x548C;<code>NCCL</code>&#x540E;&#x7AEF;&#x3002;</p>
<p>Warning</p>
<p>&#x6784;&#x9020;&#xFF0C;&#x5411;&#x524D;&#x65B9;&#x6CD5;&#xFF0C;&#x4EE5;&#x53CA;&#x8F93;&#x51FA;&#xFF08;&#x6216;&#x8BE5;&#x6A21;&#x5757;&#x7684;&#x8F93;&#x51FA;&#x7684;&#x51FD;&#x6570;&#xFF09;&#x7684;&#x5206;&#x5316;&#x662F;&#x4E00;&#x4E2A;&#x5206;&#x5E03;&#x5F0F;&#x540C;&#x6B65;&#x70B9;&#x3002;&#x8003;&#x8651;&#x5230;&#x8FD9;&#x4E00;&#x70B9;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x4E0D;&#x540C;&#x7684;&#x8FC7;&#x7A0B;&#x53EF;&#x80FD;&#x4F1A;&#x6267;&#x884C;&#x4E0D;&#x540C;&#x7684;&#x4EE3;&#x7801;&#x3002;</p>
<p>Warning</p>
<p>&#x8BE5;&#x6A21;&#x5757;&#x5047;&#x5B9A;&#x6240;&#x6709;&#x7684;&#x53C2;&#x6570;&#x90FD;&#x5728;&#x88AB;&#x5B83;&#x521B;&#x5EFA;&#x7684;&#x65F6;&#x5019;&#x6A21;&#x578B;&#x4E0A;&#x6CE8;&#x518C;&#x3002;&#x6CA1;&#x6709;&#x53C2;&#x6570;&#x5E94;&#x589E;&#x52A0;&#xFF0C;&#x4E5F;&#x6CA1;&#x6709;&#x665A;&#x5220;&#x9664;&#x3002;&#x540C;&#x6837;&#x9002;&#x7528;&#x4E8E;&#x7F13;&#x51B2;&#x533A;&#x3002;</p>
<p>Warning</p>
<p>&#x8BE5;&#x6A21;&#x5757;&#x5047;&#x5B9A;&#x5728;&#x6BCF;&#x4E2A;&#x5206;&#x5E03;&#x5F0F;&#x8FC7;&#x7A0B;&#x7684;&#x6A21;&#x578B;&#x88AB;&#x6CE8;&#x518C;&#x7684;&#x6240;&#x6709;&#x53C2;&#x6570;&#x90FD;&#x4EE5;&#x76F8;&#x540C;&#x7684;&#x987A;&#x5E8F;&#x3002;&#x6A21;&#x5757;&#x672C;&#x8EAB;&#x5C06;&#x4EE5;&#x4E0B;&#x6A21;&#x578B;&#x7684;&#x767B;&#x8BB0;&#x53C2;&#x6570;&#x7684;&#x76F8;&#x53CD;&#x987A;&#x5E8F;&#x8FDB;&#x884C;&#x68AF;&#x5EA6;&#x6240;&#x6709;&#x8FD8;&#x539F;&#x3002;&#x6362;&#x53E5;&#x8BDD;&#x8BF4;&#xFF0C;&#x5B83;&#x662F;&#x7528;&#x6237;&#x7684;&#x8D23;&#x4EFB;&#xFF0C;&#x4EE5;&#x786E;&#x4FDD;&#x6BCF;&#x4E2A;&#x5206;&#x5E03;&#x5F0F;&#x8FC7;&#x7A0B;&#x5177;&#x6709;&#x5B8C;&#x5168;&#x76F8;&#x540C;&#x7684;&#x6A21;&#x578B;&#xFF0C;&#x56E0;&#x6B64;&#x5B8C;&#x5168;&#x76F8;&#x540C;&#x7684;&#x53C2;&#x6570;&#x767B;&#x8BB0;&#x987A;&#x5E8F;&#x3002;</p>
<p>Warning</p>
<p>&#x8BE5;&#x6A21;&#x5757;&#x5047;&#x5B9A;&#x6240;&#x6709;&#x7684;&#x7F13;&#x51B2;&#x533A;&#x548C;&#x68AF;&#x5EA6;&#x5BC6;&#x96C6;&#x3002;</p>
<p>Warning</p>
<p>&#x6B64;&#x6A21;&#x5757;&#x4E0D;&#x4E00;&#x8D77;&#x5DE5;&#x4F5C;<a href="autograd.html#torch.autograd.grad" title="torch.autograd.grad"> <code>torch.autograd.grad&#xFF08;&#xFF09;</code></a>&#xFF08;&#x5373;&#xFF0C;&#x5B83;&#x5C06;&#x4EC5;&#x5F53;&#x68AF;&#x5EA6;&#x5728;<code>[HTG7&#x8981;&#x7D2F;&#x79EF;&#x7684;&#x5DE5;&#x4F5C;] .grad</code>&#x7684;&#x53C2;&#x6570;&#x5C5E;&#x6027;&#xFF09;&#x3002;</p>
<p>Warning</p>
<p>&#x5982;&#x679C;&#x6253;&#x7B97;&#x4F7F;&#x7528;&#x8BE5;&#x6A21;&#x5757;&#x5E26;&#x6709;<code>NCCL</code>&#x540E;&#x7AEF;&#x6216;<code>GLOO</code>&#x540E;&#x7AEF;&#xFF08;&#x5373;&#x4F7F;&#x7528;&#x7684;Infiniband&#xFF09;&#xFF0C;&#x5177;&#x6709;&#x7684;DataLoader&#x5728;&#x4E00;&#x8D77;&#x4F7F;&#x7528;&#x591A;&#x4E2A;&#x5DE5;&#x4EBA;&#xFF0C;&#x8BF7;&#x6539;&#x53D8;&#x591A;&#x5904;&#x7406;&#x5F00;&#x59CB;&#x65B9;&#x6CD5;<code>forkserver</code>&#xFF08;Python
3&#x4E2D;&#x53EA;&#xFF09;&#x6216;<code>&#x83CC;&#x79CD;</code>&#x3002;&#x4E0D;&#x5E78;&#x7684;&#x662F;GLOO&#xFF08;&#x4F7F;&#x7528;&#x7684;Infiniband&#xFF09;&#x548C;NCCL2&#x4E0D;&#x662F;&#x53C9;&#x5B89;&#x5168;&#xFF0C;&#x4F60;&#x53EF;&#x80FD;&#x4F1A;&#x7ECF;&#x5386;&#x6B7B;&#x9501;&#xFF0C;&#x5982;&#x679C;&#x4F60;&#x4E0D;&#x66F4;&#x6539;&#x6B64;&#x8BBE;&#x7F6E;&#x3002;</p>
<p>Warning</p>
<p>&#x4E0A;<code>&#x6A21;&#x5757;</code>&#x53CA;&#x5176;&#x5B50;&#x6A21;&#x5757;&#x5C06;&#x4E0D;&#x518D;&#x88AB;&#x8C03;&#x7528;&#xFF0C;&#x9664;&#x975E;&#x94A9;&#x5728;<code>&#x5411;&#x524D;&#x521D;&#x59CB;&#x5316;&#xFF08;&#xFF09; [HTG7&#x5411;&#x524D;&#x548C;&#x5411;&#x540E;&#x7684;&#x94A9;&#x5B50;&#x9650;&#x5B9A;] &#x65B9;&#x6CD5;&#x3002;</code></p>
<p>Warning</p>
<p>&#x4F60;&#x4E0D;&#x5E94;&#x8BE5;&#x5C1D;&#x8BD5;DistributedDataParallel&#x7ED3;&#x675F;&#x4E86;&#x4F60;&#x7684;&#x6A21;&#x578B;&#x540E;&#xFF0C;&#x6539;&#x53D8;&#x4F60;&#x7684;&#x6A21;&#x578B;&#x53C2;&#x6570;&#x3002;&#x6362;&#x53E5;&#x8BDD;&#x8BF4;&#xFF0C;&#x7ED3;&#x675F;&#x4E86;&#x4E0E;DistributedDataParallel&#x4F60;&#x7684;&#x6A21;&#x578B;&#x65F6;&#xFF0C;DistributedDataParallel&#x7684;&#x6784;&#x9020;&#x51FD;&#x6570;&#x5C06;&#x767B;&#x8BB0;&#x4E8E;&#x6A21;&#x578B;&#x672C;&#x8EAB;&#x7684;&#x6240;&#x6709;&#x53C2;&#x6570;&#x5728;&#x65BD;&#x5DE5;&#x65F6;&#x7684;&#x9644;&#x52A0;&#x68AF;&#x5EA6;&#x964D;&#x4F4E;&#x529F;&#x80FD;&#x3002;&#x5982;&#x679C;&#x60A8;&#x5728;DistributedDataParallel&#x65BD;&#x5DE5;&#x540E;&#x6539;&#x53D8;&#x6A21;&#x578B;&#x7684;&#x53C2;&#x6570;&#xFF0C;&#x8FD9;&#x662F;&#x4E0D;&#x652F;&#x6301;&#x7684;&#x548C;&#x610F;&#x60F3;&#x4E0D;&#x5230;&#x7684;&#x884C;&#x4E3A;&#x53EF;&#x80FD;&#x53D1;&#x751F;&#xFF0C;&#x56E0;&#x4E3A;&#x6709;&#x4E9B;&#x53C2;&#x6570;&#x7684;&#x68AF;&#x5EA6;&#x51CF;&#x5C11;&#x529F;&#x80FD;&#x53EF;&#x80FD;&#x4E0D;&#x4F1A;&#x88AB;&#x8C03;&#x7528;&#x3002;</p>
<p>Note</p>
<p>&#x53C2;&#x6570;&#x4ECE;&#x4E0D;&#x8FDB;&#x7A0B;&#x4E4B;&#x95F4;&#x64AD;&#x51FA;&#x3002;&#x8BE5;&#x6A21;&#x5757;&#x5BF9;&#x68AF;&#x5EA6;&#x5168;&#x51CF;&#x5C11;&#x6B65;&#x9AA4;&#xFF0C;&#x5E76;&#x4E14;&#x5047;&#x8BBE;&#x5B83;&#x4EEC;&#x5C06;&#x88AB;&#x4F18;&#x5316;&#x5668;&#x4E2D;&#x4EE5;&#x76F8;&#x540C;&#x7684;&#x65B9;&#x5F0F;&#x7684;&#x6240;&#x6709;&#x8FC7;&#x7A0B;&#x88AB;&#x4FEE;&#x6539;&#x3002;&#x7F13;&#x51B2;&#x6DB2;&#xFF08;&#x4F8B;&#x5982;BatchNorm&#x6570;&#x636E;&#xFF09;&#x662F;&#x4ECE;&#x5217;0&#x7684;&#x8FC7;&#x7A0B;&#x4E2D;&#x7684;&#x6A21;&#x5757;&#x5E7F;&#x64AD;&#xFF0C;&#x7ED9;&#x7CFB;&#x7EDF;&#x4E2D;&#x7684;&#x5728;&#x6BCF;&#x4E00;&#x4E2A;&#x8FED;&#x4EE3;&#x4E2D;&#x7684;&#x6240;&#x6709;&#x5176;&#x5B83;&#x590D;&#x5236;&#x54C1;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>module</strong> ( <em>Module</em>) &#x2013; module to be parallelized</p>
</li>
<li><p><strong>device_ids</strong> &#xFF08; <em>&#x87D2;&#x7684;&#x5217;&#x8868;&#xFF1A;INT</em> <em>&#x6216;</em> <a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"> <em>torch.device</em> </a>&#xFF09; - CUDA&#x8BBE;&#x5907;&#x3002;&#x5F53;&#x8F93;&#x5165;&#x6A21;&#x5757;&#x9A7B;&#x7559;&#x5728;&#x5355;&#x4E2A;CUDA&#x8BBE;&#x5907;&#x4E0A;&#x8FD9;&#x5E94;&#x8BE5;&#x53EA;&#x88AB;&#x63D0;&#x4F9B;&#x3002;&#x5BF9;&#x4E8E;&#x5355;&#x5668;&#x4EF6;&#x6A21;&#x5757;&#x65F6;&#xFF0C;<code>i``th  &#xFF1A;ATTR&#xFF1A;</code>module<code>&#x526F;&#x672C; &#x662F; &#x653E;&#x7F6E; &#x5728; ``device_ids [I]</code>&#x3002;&#x5BF9;&#x4E8E;&#x591A;&#x8BBE;&#x5907;&#x6A21;&#x5757;&#x548C;CPU&#x6A21;&#x5757;&#xFF0C;device_ids&#x5FC5;&#x987B;&#x662F;&#x65E0;&#x6216;&#x4E00;&#x4E2A;&#x7A7A;&#x5217;&#x8868;&#xFF0C;&#x5E76;&#x4E14;&#x8F93;&#x5165;&#x6570;&#x636E;&#x4E3A;&#x76F4;&#x4F20;&#x5FC5;&#x987B;&#x653E;&#x7F6E;&#x5728;&#x6B63;&#x786E;&#x7684;&#x8BBE;&#x5907;&#x4E0A;&#x3002; &#xFF08;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;&#x5355;&#x5668;&#x4EF6;&#x6A21;&#x5757;&#x7684;&#x6240;&#x6709;&#x8BBE;&#x5907;&#xFF09;</p>
</li>
<li><p><strong>output_device</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#x6216;</em> <a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"> <em>torch.device</em> </a>&#xFF09; - &#x7528;&#x4E8E;&#x8F93;&#x51FA;&#x7684;&#x88C5;&#x7F6E;&#x4F4D;&#x7F6E;&#x5355;&#x8BBE;&#x5907;CUDA&#x6A21;&#x5757;&#x3002;&#x5BF9;&#x4E8E;&#x591A;&#x8BBE;&#x5907;&#x6A21;&#x5757;&#x548C;CPU&#x6A21;&#x5757;&#xFF0C;&#x5B83;&#x5FC5;&#x987B;&#x662F;&#x65E0;&#xFF0C;&#x6A21;&#x5757;&#x672C;&#x8EAB;&#x51B3;&#x5B9A;&#x4E86;&#x8F93;&#x51FA;&#x4F4D;&#x7F6E;&#x3002; &#xFF08;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;device_ids [0]&#x4E3A;&#x5355;&#x5668;&#x4EF6;&#x6A21;&#x5757;&#xFF09;</p>
</li>
<li><p><strong>broadcast_buffers</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a>&#xFF09; - &#x6807;&#x5FD7;&#xFF0C;&#x4F7F;&#x5728;&#x524D;&#x8FDB;&#x529F;&#x80FD;&#x7684;&#x5F00;&#x59CB;&#x540C;&#x6B65;&#x7684;&#x6A21;&#x5757;&#x7684;&#xFF08;&#x5E7F;&#x64AD;&#xFF09;&#x7684;&#x7F13;&#x51B2;&#x5668;&#x3002; &#xFF08;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x771F;</code>&#xFF09;</p>
</li>
<li><p><strong>process_group</strong> - &#x7528;&#x4E8E;&#x5206;&#x5E03;&#x5F0F;&#x6570;&#x636E;&#x6240;&#x6709;&#x8FD8;&#x539F;&#x5904;&#x7406;&#x7EC4;&#x3002;&#x5982;&#x679C;<code>&#x65E0;</code>&#xFF0C;&#x9ED8;&#x8BA4;&#x5904;&#x7406;&#x7EC4;&#xFF0C;&#x5B83;&#x662F;&#x7531;<code>&#x5EFA;&#x7ACB;</code>torch.distributed.init_process_group<code>`&#xFF0C;&#x5C06;&#x88AB;&#x4F7F;&#x7528;&#x3002; &#xFF08;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;</code>&#x65E0; `&#xFF09;</p>
</li>
<li><p><strong>bucket_cap_mb</strong> - DistributedDataParallel&#x5C06;&#x6876;&#x7684;&#x53C2;&#x6570;&#x5206;&#x6210;&#x591A;&#x4E2A;&#x5B58;&#x50A8;&#x6876;&#xFF0C;&#x4F7F;&#x5F97;&#x6BCF;&#x4E2A;&#x6876;&#x7684;&#x68AF;&#x5EA6;&#x51CF;&#x5C0F;&#x53EF;&#x4EE5;&#x6F5C;&#x5728;&#x5730;&#x4E0E;&#x5411;&#x540E;&#x8BA1;&#x7B97;&#x91CD;&#x53E0;&#x3002; <code>bucket_cap_mb</code>&#x63A7;&#x5236;&#x4EE5;&#x5146;&#x5B57;&#x8282;&#x4E3A;&#x6876;&#x5927;&#x5C0F;&#xFF08;MB&#xFF09;&#xFF08;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;25&#xFF09;</p>
</li>
<li><p><strong>find_unused_pa&#x200B;&#x200B;rameters</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a>&#xFF09; - &#x904D;&#x5386;&#x5305;&#x542B;&#x5728;&#x8FD4;&#x56DE;&#x503C;&#x7684;&#x6240;&#x6709;&#x5F20;&#x91CF;&#x7684;autograd&#x66F2;&#x7EBF;&#x56FE;&#x4E2D;&#x7684;&#x5305;&#x88C5;&#x7684;&#x6A21;&#x5757;&#x7684;<code>&#x5411;&#x524D;</code>&#x529F;&#x80FD;&#x3002;&#x4E0D;&#x63A5;&#x6536;&#x68AF;&#x5EA6;&#xFF0C;&#x56E0;&#x4E3A;&#x8FD9;&#x56FE;&#x7684;&#x4E00;&#x90E8;&#x5206;&#x53C2;&#x6570;&#x88AB;&#x62A2;&#x5148;&#x6807;&#x8BB0;&#x4E3A;&#x51C6;&#x5907;&#x51CF;&#x5C11;&#x3002;&#x8BF7;&#x6CE8;&#x610F;&#xFF0C;&#x6240;&#x6709;<code>&#x5411;&#x524D;</code>&#x4ECE;&#x6A21;&#x5757;&#x53C2;&#x6570;&#x5BFC;&#x51FA;&#x7684;&#x8F93;&#x51FA;&#x5FC5;&#x987B;&#x53C2;&#x4E0E;&#x8BA1;&#x7B97;&#x635F;&#x5931;&#x548C;&#x66F4;&#x9AD8;&#x7684;&#x68AF;&#x5EA6;&#x8BA1;&#x7B97;&#x3002;&#x5982;&#x679C;&#x4ED6;&#x4EEC;&#x4E0D;&#x8FD9;&#x6837;&#x505A;&#xFF0C;&#x8FD9;&#x4E2A;&#x5305;&#x88C5;&#x5C06;&#x6302;&#x8D77;&#x7B49;&#x5F85;autograd&#x4EA7;&#x751F;&#x8FD9;&#x4E9B;&#x53C2;&#x6570;&#x7684;&#x68AF;&#x5EA6;&#x3002;&#x5176;&#x4ED6;&#x672A;&#x4F7F;&#x7528;&#x7684;&#x4ECE;&#x6A21;&#x5757;&#x7684;&#x53C2;&#x6570;&#x5BFC;&#x51FA;&#x7684;&#x4EFB;&#x4F55;&#x8F93;&#x51FA;&#x53EF;&#x4ECE;autograd&#x56FE;&#x8868;&#x4F7F;&#x7528;<code>torch.Tensor.detach</code>&#x8131;&#x79BB;&#x3002; &#xFF08;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x5047;</code>&#xFF09;</p>
</li>
<li><p><strong>check_reduction</strong> - &#x8BBE;&#x7F6E;&#x4E3A;<code>&#x771F;</code>&#xFF0C;&#x5B83;&#x4F7F;DistributedDataParallel&#x5982;&#x679C;&#x524D;&#x4E00;&#x6B21;&#x8FED;&#x4EE3;&#x843D;&#x540E;&#x7684;&#x51CF;&#x5C11;&#x662F;&#x5728;&#x6BCF;&#x6B21;&#x8FED;&#x4EE3;&#x7684;&#x6B63;&#x5411;&#x529F;&#x80FD;&#x5E74;&#x521D;&#x6210;&#x529F;&#x53D1;&#x884C;&#x81EA;&#x52A8;&#x68C0;&#x67E5;&#x65F6;&#x3002;&#x4F60;&#x901A;&#x5E38;&#x4E0D;&#x9700;&#x8981;&#x542F;&#x7528;&#x8FD9;&#x4E2A;&#x9009;&#x9879;&#xFF0C;&#x9664;&#x975E;&#x4F60;&#x6B63;&#x5728;&#x89C2;&#x5BDF;&#x602A;&#x5F02;&#x884C;&#x4E3A;&#xFF0C;&#x5982;&#x4E0D;&#x540C;&#x7B49;&#x7EA7;&#x8D8A;&#x6765;&#x8D8A;&#x4E0D;&#x540C;&#x68AF;&#x5EA6;&#xFF0C;&#x5982;&#x679C;DistributedDataParallel&#x6B63;&#x786E;&#x4F7F;&#x7528;&#x5E94;&#x8BE5;&#x4E0D;&#x4F1A;&#x53D1;&#x751F;&#x3002; &#xFF08;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x5047;</code>&#xFF09;</p>
</li>
</ul>
<p>Variables</p>
<p><strong>&#x301C;DistributedDataParallel.module</strong> &#xFF08; <em>&#x6A21;&#x5757;</em> &#xFF09; - &#x8BE5;&#x6A21;&#x5757;&#x53EF;&#x4EE5;&#x5E76;&#x884C;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.distributed.init_process_group(backend=&apos;nccl&apos;, world_size=4, init_method=&apos;...&apos;)
&gt;&gt;&gt; net = torch.nn.DistributedDataParallel(model, pg)
</code></pre><p><code>no_sync</code>()<a href="_modules/torch/nn/parallel/distributed.html#DistributedDataParallel.no_sync">[source]</a></p>
<p>&#x4E0A;&#x4E0B;&#x6587;&#x7BA1;&#x7406;&#x5668;&#x8DE8;DDP&#x8FC7;&#x7A0B;&#x7981;&#x7528;&#x68AF;&#x5EA6;&#x540C;&#x6B65;&#x3002;&#x5728;&#x8FD9;&#x65B9;&#x9762;&#xFF0C;&#x5C06;&#x68AF;&#x5EA6;&#x4E0A;&#x6A21;&#x5757;&#x7684;&#x53D8;&#x91CF;&#xFF0C;&#x8FD9;&#x5C06;&#x5728;&#x540E;&#x9762;&#x7684;&#x7B2C;&#x4E00;&#x4E2A;&#x524D;&#x540E;&#x901A;&#x79BB;&#x5F00;&#x4E0A;&#x4E0B;&#x6587;&#x540C;&#x6B65;&#x8FDB;&#x884C;&#x7D2F;&#x79EF;&#x3002;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; ddp = torch.nn.DistributedDataParallel(model, pg)
&gt;&gt;&gt; with ddp.no_sync():
...   for input in inputs:
...     ddp(input).backward()  # no synchronization, accumulate grads
... ddp(another_input).backward()  # synchronize grads
</code></pre><h2 id="&#x516C;&#x7528;&#x4E8B;&#x4E1A;">&#x516C;&#x7528;&#x4E8B;&#x4E1A;</h2>
<h3 id="clipgradnorm">clip<em>grad_norm</em></h3>
<p><code>torch.nn.utils.``clip_grad_norm_</code>( <em>parameters</em> , <em>max_norm</em> , <em>norm_type=2</em>
)<a href="_modules/torch/nn/utils/clip_grad.html#clip_grad_norm_">[source]</a></p>
<p>&#x7684;&#x53C2;&#x6570;&#x53EF;&#x8FED;&#x4EE3;&#x7684;&#x89C6;&#x9891;&#x68AF;&#x5EA6;&#x8303;&#x6570;&#x3002;</p>
<p>&#x8303;&#x6570;&#x4E0A;&#x8BA1;&#x7B97;&#x6240;&#x6709;&#x68AF;&#x5EA6;&#x5728;&#x4E00;&#x8D77;&#xFF0C;&#x5982;&#x540C;&#x5B83;&#x4EEC;&#x8FDE;&#x63A5;&#x6210;&#x4E00;&#x4E2A;&#x5355;&#x4E00;&#x7684;&#x8F7D;&#x4F53;&#x4E2D;&#x3002;&#x68AF;&#x5EA6;&#x5C31;&#x5730;&#x4FEE;&#x6539;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x53C2;&#x6570;</strong> &#xFF08; <em>&#x53EF;&#x8FED;&#x4EE3;</em> <em>[</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a> <em>]&#x6216;</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x5F20;&#x91CF;&#x7684;&#x4E00;&#x4E2A;&#x53EF;&#x8FED;&#x4EE3;&#x6216;&#x5355;&#x4E2A;&#x5F20;&#x91CF;&#xFF0C;&#x5C06;&#x6709;&#x68AF;&#x5EA6;&#x5F52;&#x4E00;&#x5316;</p>
</li>
<li><p><strong>max_norm</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a> <em>&#x6216;</em> <a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x6700;&#x5927;&#x68AF;&#x5EA6;&#x7684;&#x8303;&#x6570;</p>
</li>
<li><p><strong>norm_type</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a> <em>&#x6216;</em> <a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x6240;&#x4F7F;&#x7528;&#x7684;p-&#x8303;&#x6570;&#x7684;&#x7C7B;&#x578B;&#x3002;&#x53EF;&#x4EE5;&#x7528;<code>&apos;INF&apos;</code>&#x4E3A;&#x65E0;&#x7A77;&#x89C4;&#x8303;&#x3002;</p>
</li>
</ul>
<p>Returns</p>
<p>&#x7684;&#x53C2;&#x6570;&#x7684;&#x603B;&#x8303;&#x6570;&#xFF08;&#x89C6;&#x4E3A;&#x5355;&#x4E2A;&#x77E2;&#x91CF;&#xFF09;&#x3002;</p>
<h3 id="clipgradvalue">clip<em>grad_value</em></h3>
<p><code>torch.nn.utils.``clip_grad_value_</code>( <em>parameters</em> , <em>clip_value</em>
)<a href="_modules/torch/nn/utils/clip_grad.html#clip_grad_value_">[source]</a></p>
<p>&#x5728;&#x6307;&#x5B9A;&#x7684;&#x503C;&#x7684;&#x53C2;&#x6570;&#x53EF;&#x8FED;&#x4EE3;&#x7684;&#x89C6;&#x9891;&#x68AF;&#x5EA6;&#x3002;</p>
<p>&#x68AF;&#x5EA6;&#x5C31;&#x5730;&#x4FEE;&#x6539;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>parameters</strong> ( <em>Iterable</em> <em>[</em><a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>] or</em><a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; an iterable of Tensors or a single Tensor that will have gradients normalized</p>
</li>
<li><p><strong>clip_value</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a> <em>&#x6216;</em> <a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x6700;&#x5927;&#x5141;&#x8BB8;&#x68AF;&#x5EA6;&#x7684;&#x503C;&#x3002;&#x7684;&#x68AF;&#x5EA6;&#x7684;&#x8303;&#x56F4;&#x9650;&#x5E45; [ -clip_value  &#xFF0C; clip_value  \&#x5DE6;[\ {&#x6587;&#x672C;-clip \ _value}&#xFF0C;\ {&#x6587;&#x672C;&#x5939;\ _value} \&#x53F3;]  [ -clip_value  &#xFF0C; clip_value </p>
</li>
</ul>
<h3 id="parameterstovector">parameters_to_vector</h3>
<p><code>torch.nn.utils.``parameters_to_vector</code>( <em>parameters</em>
)<a href="_modules/torch/nn/utils/convert_parameters.html#parameters_to_vector">[source]</a></p>
<p>&#x8F6C;&#x6362;&#x53C2;&#x6570;&#xFF0C;&#x4EE5;&#x4E00;&#x4E2A;&#x77E2;&#x91CF;</p>
<p>Parameters</p>
<p><strong>&#x53C2;&#x6570;</strong> &#xFF08; <em>&#x53EF;&#x8FED;&#x4EE3;</em> <em>[</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a> __ &#xFF09; -
&#x5F20;&#x91CF;&#x7684;&#x8FED;&#x4EE3;&#x5668;&#xFF0C;&#x662F;&#x6A21;&#x578B;&#x7684;&#x53C2;&#x6570;&#x3002;</p>
<p>Returns</p>
<p>&#x7531;&#x5355;&#x4E2A;&#x5411;&#x91CF;&#x8868;&#x793A;&#x7684;&#x53C2;&#x6570;</p>
<h3 id="vectortoparameters">vector_to_parameters</h3>
<p><code>torch.nn.utils.``vector_to_parameters</code>( <em>vec</em> , <em>parameters</em>
)<a href="_modules/torch/nn/utils/convert_parameters.html#vector_to_parameters">[source]</a></p>
<p>&#x5C06;&#x4E00;&#x4E2A;&#x5411;&#x91CF;&#x7684;&#x53C2;&#x6570;</p>
<p>Parameters</p>
<ul>
<li><p><strong>VEC</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x4E00;&#x4E2A;&#x5355;&#x4E00;&#x7684;&#x77E2;&#x91CF;&#x8868;&#x793A;&#x7684;&#x6A21;&#x578B;&#x7684;&#x53C2;&#x6570;&#x3002;</p>
</li>
<li><p><strong>parameters</strong> ( <em>Iterable</em> <em>[</em><a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>]</em> ) &#x2013; an iterator of Tensors that are the parameters of a model.</p>
</li>
</ul>
<h3 id="weightnorm">weight_norm</h3>
<p><code>torch.nn.utils.``weight_norm</code>( <em>module</em> , <em>name=&apos;weight&apos;</em> , <em>dim=0</em>
)<a href="_modules/torch/nn/utils/weight_norm.html#weight_norm">[source]</a></p>
<p>&#x9002;&#x7528;&#x91CD;&#x91CF;&#x5F52;&#x4E00;&#x5316;&#x5230;&#x7ED9;&#x5B9A;&#x6A21;&#x5757;&#x4E2D;&#x7684;&#x53C2;&#x6570;&#x3002;</p>
<p>w=gv&#x2225;v&#x2225;\mathbf{w} = g \dfrac{\mathbf{v}}{|\mathbf{v}|} w=g&#x2225;v&#x2225;v&#x200B;</p>
<p>&#x91CD;&#x91CF;&#x5F52;&#x4E00;&#x5316;&#x662F;&#x89E3;&#x8026;&#x5176;&#x65B9;&#x5411;&#x7684;&#x5F20;&#x91CF;&#x91CD;&#x91CF;&#x7684;&#x5927;&#x5C0F;&#x7684;&#x91CD;&#x65B0;&#x53C2;&#x6570;&#x5316;&#x3002;&#x8FD9;&#x53D6;&#x4EE3;&#x7531;<code>name&#x6307;&#x5B9A;&#x7684;&#x53C2;&#x6570;</code>&#xFF08;&#x4F8B;&#x5982;<code>&apos;&#x91CD;&#x91CF;&apos;</code>&#xFF09;&#x4F7F;&#x7528;&#x4E24;&#x4E2A;&#x53C2;&#x6570;&#xFF1A;&#x4E00;&#x4E2A;&#x6307;&#x5B9A;&#x7684;&#x5E45;&#x5EA6;&#xFF08;&#x4F8B;&#x5982;&#xFF0C;<code>&apos;weight_g&apos;</code>&#xFF09;&#x548C;&#x4E00;&#x4E2A;&#x6307;&#x5B9A;&#x65B9;&#x5411;&#xFF08;&#x4F8B;&#x5982;&#xFF0C;<code>&apos;weight_v&apos;</code>&#xFF09;&#x3002;&#x91CD;&#x91CF;&#x5F52;&#x4E00;&#x5316;&#x662F;&#x901A;&#x8FC7;&#x6BCF;&#x4E00;&#x4E2A;<code>&#x5411;&#x524D;&#xFF08;&#xFF09;</code>&#x547C;&#x53EB;&#x4E4B;&#x524D;&#x91CD;&#x65B0;&#x8BA1;&#x7B97;&#x4ECE;&#x6240;&#x8FF0;&#x5E45;&#x5EA6;&#x548C;&#x65B9;&#x5411;&#x7684;&#x91CD;&#x91CF;&#x5F20;&#x91CF;&#x7684;&#x94A9;&#x5B9E;&#x73B0;&#x3002;</p>
<p>&#x9ED8;&#x8BA4;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x4E0E;<code>&#x6697;&#x6DE1;= 0</code>&#xFF0C;&#x89C4;&#x8303;&#x72EC;&#x7ACB;&#x5730;&#xFF0C;&#x6BCF;&#x4E2A;&#x8F93;&#x51FA;&#x901A;&#x9053;/&#x5E73;&#x9762;&#x6765;&#x8BA1;&#x7B97;&#x3002;&#x4E3A;&#x4E86;&#x8BA1;&#x7B97;&#x5728;&#x6574;&#x4E2A;&#x91CD;&#x91CF;&#x5F20;&#x91CF;&#x8303;&#x6570;&#xFF0C;&#x7528;<code>&#x6697;&#x6DE1;=&#x65E0;</code>&#x3002;</p>
<p>&#x53C2;&#x89C1;<a href="https://arxiv.org/abs/1602.07868" target="_blank"> https://arxiv.org/abs/1602.07868 </a></p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x6A21;&#x5757;</strong> &#xFF08; <em>&#x6A21;&#x5757;</em> &#xFF09; - &#x5305;&#x542B;&#x6A21;&#x5757;</p>
</li>
<li><p><strong>&#x540D;&#x79F0;</strong> &#xFF08;<a href="https://docs.python.org/3/library/stdtypes.html#str" title="\(in Python v3.7\)" target="_blank"> <em>STR</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x6743;&#x91CD;&#x53C2;&#x6570;&#x7684;&#x540D;&#x79F0;</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x7EF4;&#x5728;&#x5176;&#x4E0A;&#x8BA1;&#x7B97;&#x6807;&#x51C6;</p>
</li>
</ul>
<p>Returns</p>
<p>&#x539F;&#x59CB;&#x6A21;&#x5757;&#x4E0E;&#x91CD;&#x91CF;&#x89C4;&#x8303;&#x94A9;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; m = weight_norm(nn.Linear(20, 40), name=&apos;weight&apos;)
&gt;&gt;&gt; m
Linear(in_features=20, out_features=40, bias=True)
&gt;&gt;&gt; m.weight_g.size()
torch.Size([40, 1])
&gt;&gt;&gt; m.weight_v.size()
torch.Size([40, 20])
</code></pre><h3 id="removeweightnorm">remove_weight_norm</h3>
<p><code>torch.nn.utils.``remove_weight_norm</code>( <em>module</em> , <em>name=&apos;weight&apos;</em>
)<a href="_modules/torch/nn/utils/weight_norm.html#remove_weight_norm">[source]</a></p>
<p>&#x5220;&#x9664;&#x4ECE;&#x4E00;&#x4E2A;&#x6A21;&#x5757;&#x7684;&#x91CD;&#x91CF;&#x5F52;&#x4E00;&#x5316;&#x91CD;&#x65B0;&#x53C2;&#x6570;&#x5316;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>module</strong> ( <em>Module</em>) &#x2013; containing module</p>
</li>
<li><p><strong>name</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#str" title="\(in Python v3.7\)" target="_blank"> <em>str</em></a> <em>,</em> <em>optional</em> ) &#x2013; name of weight parameter</p>
</li>
</ul>
<p>&#x4F8B;</p>
<pre><code>&gt;&gt;&gt; m = weight_norm(nn.Linear(20, 40))
&gt;&gt;&gt; remove_weight_norm(m)
</code></pre><h3 id="spectralnorm">spectral_norm</h3>
<p><code>torch.nn.utils.``spectral_norm</code>( <em>module</em> , <em>name=&apos;weight&apos;</em> ,
<em>n_power_iterations=1</em> , <em>eps=1e-12</em> , <em>dim=None</em>
)<a href="_modules/torch/nn/utils/spectral_norm.html#spectral_norm">[source]</a></p>
<p>&#x9002;&#x7528;&#x8C31;&#x5F52;&#x4E00;&#x5316;&#x5230;&#x7ED9;&#x5B9A;&#x6A21;&#x5757;&#x4E2D;&#x7684;&#x53C2;&#x6570;&#x3002;</p>
<p>WSN=W&#x3C3;(W),&#x3C3;(W)=max&#x2061;h:h&#x2260;0&#x2225;Wh&#x2225;2&#x2225;h&#x2225;2\mathbf{W}<em>{SN} =
\dfrac{\mathbf{W}}{\sigma(\mathbf{W})}, \sigma(\mathbf{W}) = \max</em>{\mathbf{h}:
\mathbf{h} \ne 0} \dfrac{|\mathbf{W} \mathbf{h}|_2}{|\mathbf{h}|_2}
WSN&#x200B;=&#x3C3;(W)W&#x200B;,&#x3C3;(W)=h:h&#xE020;&#x200B;=0max&#x200B;&#x2225;h&#x2225;2&#x200B;&#x2225;Wh&#x2225;2&#x200B;&#x200B;</p>
<p>&#x8C31;&#x5F52;&#x4E00;&#x5316;&#x901A;&#x8FC7;&#x91CD;&#x65B0;&#x7F29;&#x653E;&#x91CD;&#x91CF;&#x5F20;&#x91CF;&#x4E0E;&#x8C31;&#x8303;&#x6570; &#x3C3;&#x7A33;&#x5B9A;&#x5728;&#x5256;&#x6210;&#x5BF9;&#x6297;&#x6027;&#x7F51;&#x7EDC;&#xFF08;&#x7518;&#x65AF;&#xFF09;&#x9274;&#x522B;&#x5668;&#xFF08;&#x5F71;&#x8BC4;&#xFF09;&#x7684;&#x8BAD;&#x7EC3; \&#x897F;&#x683C;&#x739B; &#x3C3;
&#x4F7F;&#x7528;&#x5E42;&#x8FED;&#x4EE3;&#x65B9;&#x6CD5;&#x8BA1;&#x7B97;&#x7684;&#x6743;&#x91CD;&#x77E9;&#x9635;&#x7684;&#x3002;&#x5982;&#x679C;&#x91CD;&#x91CF;&#x5F20;&#x91CF;&#x7684;&#x7EF4;&#x6570;&#x5927;&#x4E8E;2&#xFF0C;&#x5B83;&#x88AB;&#x6574;&#x5F62;&#x4EE5;&#x5E42;&#x8FED;&#x4EE3;&#x6CD5;&#x5728;&#x4E8C;&#x7EF4;&#x83B7;&#x5F97;&#x8C31;&#x8303;&#x6570;&#x3002;&#x8FD9;&#x662F;&#x901A;&#x8FC7;&#x8BA1;&#x7B97;&#x5149;&#x8C31;&#x8303;&#x6570;&#x548C;&#x518D;&#x7F29;&#x653E;&#x91CD;&#x91CF;&#x4E4B;&#x524D;&#x6BCF;<code>&#x5411;&#x524D;</code>&#xFF08;&#xFF09;&#x8C03;&#x7528;&#x7684;&#x94A9;&#x5B9E;&#x73B0;&#x3002;</p>
<p>&#x89C1;&#x5256;&#x6210;&#x5BF9;&#x6297;&#x6027;&#x7F51;&#x7EDC; <a href="https://arxiv.org/abs/1802.05957" target="_blank">&#x8C31;&#x5F52;&#x3002;</a></p>
<p>Parameters</p>
<ul>
<li><p><strong>module</strong> ( <em>nn.Module</em>) &#x2013; containing module</p>
</li>
<li><p><strong>name</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#str" title="\(in Python v3.7\)" target="_blank"> <em>str</em></a> <em>,</em> <em>optional</em> ) &#x2013; name of weight parameter</p>
</li>
<li><p><strong>n_power_iterations</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x529F;&#x7387;&#x7684;&#x8FED;&#x4EE3;&#x6B21;&#x6570;&#x6765;&#x8BA1;&#x7B97;&#x8C31;&#x8303;</p>
</li>
<li><p><strong>EPS</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5C0F;&#x91CF;&#x5728;&#x8BA1;&#x7B97;&#x51C6;&#x5219;&#x7684;&#x6570;&#x503C;&#x7A33;&#x5B9A;&#x6027;</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x7EF4;&#x5BF9;&#x5E94;&#x7684;&#x8F93;&#x51FA;&#x6570;&#xFF0C;&#x9ED8;&#x8BA4;&#x662F;<code>0</code>&#xFF0C;&#x9664;&#x4E86;&#x6A21;&#x5757;&#xFF0C;&#x5176;ConvTranspose {1,2,3} d&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x5F53;&#x5B83;&#x662F;<code>1</code></p>
</li>
</ul>
<p>Returns</p>
<p>&#x539F;&#x59CB;&#x6A21;&#x5757;&#x4E0E;&#x6240;&#x8FF0;&#x8C31;&#x8303;&#x6570;&#x94A9;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; m = spectral_norm(nn.Linear(20, 40))
&gt;&gt;&gt; m
Linear(in_features=20, out_features=40, bias=True)
&gt;&gt;&gt; m.weight_u.size()
torch.Size([40])
</code></pre><h3 id="removespectralnorm">remove_spectral_norm</h3>
<p><code>torch.nn.utils.``remove_spectral_norm</code>( <em>module</em> , <em>name=&apos;weight&apos;</em>
)<a href="_modules/torch/nn/utils/spectral_norm.html#remove_spectral_norm">[source]</a></p>
<p>&#x5220;&#x9664;&#x4ECE;&#x4E00;&#x4E2A;&#x6A21;&#x5757;&#x7684;&#x5149;&#x200B;&#x200B;&#x8C31;&#x5F52;&#x4E00;&#x5316;&#x91CD;&#x65B0;&#x53C2;&#x6570;&#x5316;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>module</strong> ( <em>Module</em>) &#x2013; containing module</p>
</li>
<li><p><strong>name</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#str" title="\(in Python v3.7\)" target="_blank"> <em>str</em></a> <em>,</em> <em>optional</em> ) &#x2013; name of weight parameter</p>
</li>
</ul>
<p>Example</p>
<pre><code>&gt;&gt;&gt; m = spectral_norm(nn.Linear(40, 10))
&gt;&gt;&gt; remove_spectral_norm(m)
</code></pre><h3 id="packedsequence">PackedSequence</h3>
<p><code>torch.nn.utils.rnn.``PackedSequence</code>( <em>data</em> , <em>batch_sizes=None</em> ,
<em>sorted_indices=None</em> , <em>unsorted_indices=None</em>
)<a href="_modules/torch/nn/utils/rnn.html#PackedSequence">[source]</a></p>
<p>&#x4FDD;&#x6301;&#x6570;&#x636E;&#x548C;&#x586B;&#x5145;&#x5E8F;&#x5217;&#x7684;batch_sizes &#x7684;<code>&#x5217;&#x8868;&#x3002;</code></p>
<p>&#x6240;&#x6709;RNN&#x6A21;&#x5757;&#x63A5;&#x53D7;&#x6253;&#x5305;&#x5E8F;&#x5217;&#x4F5C;&#x4E3A;&#x8F93;&#x5165;&#x3002;</p>
<p>Note</p>
<p>&#x8FD9;&#x4E2A;&#x7C7B;&#x7684;&#x5B9E;&#x4F8B;&#x7EDD;&#x4E0D;&#x5E94;&#x624B;&#x52A8;&#x521B;&#x5EFA;&#x3002;&#x8FD9;&#x610F;&#x5473;&#x7740;&#x5B83;&#x4EEC;&#x662F;&#x7528;&#x76F8;&#x540C;&#x7684;&#x529F;&#x80FD;&#x88AB;&#x5B9E;&#x4F8B;&#x5316;<code>pack_padded_sequence&#xFF08;&#xFF09;</code>&#x3002;</p>
<p>&#x6279;&#x91CF;&#x5927;&#x5C0F;&#x8868;&#x793A;&#x5728;&#x6279;&#x6B21;&#x4E2D;&#x7684;&#x6BCF;&#x4E2A;&#x5E8F;&#x5217;&#x6B65;&#x9AA4;&#x7684;&#x6570;&#x91CF;&#x7684;&#x5143;&#x4EF6;&#xFF0C;&#x800C;&#x4E0D;&#x662F;&#x4E0D;&#x540C;&#x7684;&#x5E8F;&#x5217;&#x957F;&#x5EA6;&#x4F20;&#x9012;&#x5230; <code>pack_padded_sequence&#xFF08;&#xFF09;</code>&#x3002;&#x4F8B;&#x5982;&#xFF0C;&#x7ED9;&#x5B9A;&#x7684;&#x6570;&#x636E;<code>ABC</code>&#x548C;<code>&#xD7;</code>&#x4E2D;&#x7684; <code>PackedSequence</code>&#x5C06;&#x5305;&#x542B;&#x4E0E;<code>batch_sizes = [&#x6570;&#x636E;</code>axbc<code>2,1,1]</code>&#x3002;</p>
<p>Variables</p>
<ul>
<li><p><strong>&#x301C;PackedSequence.data</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x5F20;&#x91CF;&#x542B;&#x6709;&#x5305;&#x88C5;&#x5E8F;&#x5217;</p>
</li>
<li><p><strong>&#x301C;PackedSequence.batch_sizes</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x5728;&#x6BCF;&#x4E2A;&#x5E8F;&#x5217;&#x6B65;&#x9AA4;&#x4FDD;&#x6301;&#x7EA6;&#x6279;&#x91CF;&#x5927;&#x5C0F;&#x4FE1;&#x606F;&#x7684;&#x6574;&#x6570;&#x7684;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>&#x301C;PackedSequence.sorted_indices</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x6574;&#x6570;&#x5F20;&#x91CF;&#x4FDD;&#x6301;&#x5982;&#x4F55; <code>PackedSequence</code>&#x662F;&#x4ECE;&#x5E8F;&#x5217;&#x6784;&#x5EFA;&#x3002;</p>
</li>
<li><p><strong>&#x301C;PackedSequence.unsorted_indices</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x6574;&#x6570;&#x5F20;&#x91CF;&#x4FDD;&#x6301;&#x5982;&#x4F55;&#x4EE5;&#x6062;&#x590D;&#x539F;&#x59CB;&#x4E0E;&#x6B63;&#x786E;&#x7684;&#x987A;&#x5E8F;&#x7684;&#x5E8F;&#x5217;&#x3002;</p>
</li>
</ul>
<p>Note</p>
<p><code>&#x6570;&#x636E;</code>&#x53EF;&#x4EE5;&#x5728;&#x4EFB;&#x610F;&#x8BBE;&#x5907;&#x548C;&#x4EFB;&#x610F;&#x7684;D&#x578B;&#x3002; <code>sorted_indices</code>&#x548C;<code>unsorted_indices</code>&#x5FC5;&#x987B;<code>torch.int64</code>&#x76F8;&#x540C;&#x88C5;&#x7F6E;&#x4E0A;&#x5F20;&#x91CF;<code>&#x6570;&#x636E;</code>&#x3002;</p>
<p>&#x7136;&#x800C;&#xFF0C;<code>batch_sizes</code>&#x5E94;&#x5F53;&#x603B;&#x662F;&#x4E00;&#x4E2A;CPU <code>torch.int64</code>&#x5F20;&#x91CF;&#x3002;</p>
<p>&#x8FD9;&#x4E2A;&#x4E0D;&#x53D8;&#x7684;&#x4FDD;&#x6301;&#x5728;&#x6574;&#x4E2A; <code>PackedSequence</code>&#x7C7B;&#xFF0C;&#x5E76;&#x4E14;&#x8BE5;&#x6784;&#x5EFA;&#x4F53;&#x7684;&#x6240;&#x6709;&#x529F;&#x80FD;&#x7684;&#xFF1A;&#x7C7B;&#xFF1A;PackedSequence
&#x5728;PyTorch&#xFF08;&#x5373;&#xFF0C;&#x5B83;&#x4EEC;&#x53EA;&#x901A;&#x8FC7;&#x5728;&#x5F20;&#x91CF;&#x7B26;&#x5408;&#x6B64;&#x7EA6;&#x675F;&#xFF09;&#x3002;</p>
<h3 id="packpaddedsequence">pack_padded_sequence</h3>
<p><code>torch.nn.utils.rnn.``pack_padded_sequence</code>( <em>input</em> , <em>lengths</em> ,
<em>batch_first=False</em> , <em>enforce_sorted=True</em>
)<a href="_modules/torch/nn/utils/rnn.html#pack_padded_sequence">[source]</a></p>
<p>&#x5305;&#x542B;&#x53EF;&#x53D8;&#x957F;&#x5EA6;&#x7684;&#x586B;&#x5145;&#x5E8F;&#x5217;&#x7684;&#x5F20;&#x91CF;&#x3002;</p>
<p><code>&#x8F93;&#x5165;</code>&#x53EF;&#x4EE5;&#x662F;&#x5927;&#x5C0F;<code>T  &#xD7; B  &#xD7; *</code>&#x5176;&#x4E2D; T &#x662F;&#x6700;&#x957F;&#x7684;&#x5E8F;&#x5217;&#x7684;&#x957F;&#x5EA6;&#xFF08;&#x7B49;&#x4E8E;<code>&#x957F;&#x5EA6;[0]</code>&#xFF09;&#xFF0C;<code>B</code>&#x662F;&#x6279;&#x91CF;&#x5927;&#x5C0F;&#xFF0C;&#x548C;<code>*</code>&#x662F;&#x4EFB;&#x4F55;&#x6570;&#x76EE;&#x7684;&#x7EF4;&#x5EA6;&#xFF08;&#x5305;&#x62EC;0&#xFF09;&#x7684;&#x3002;&#x5982;&#x679C;<code>batch_first</code>&#x662F;<code>&#x771F;</code>&#xFF0C;<code>B  &#xD7; T ...  &#xD7; *``&#x8F93;&#x5165;</code>&#x9884;&#x8BA1;&#x3002;</p>
<p>&#x5BF9;&#x4E8E;&#x672A;&#x6392;&#x5E8F;&#x7684;&#x5E8F;&#x5217;&#xFF0C;&#x7528; enforce_sorted =&#x5047;&#x3002;&#x5982;&#x679C;<code>enforce_sorted</code>&#x662F;<code>&#x771F;</code>&#xFF0C;&#x6240;&#x8FF0;&#x5E8F;&#x5217;&#x5E94;&#x901A;&#x8FC7;&#x957F;&#x5EA6;&#x4EE5;&#x9012;&#x51CF;&#x7684;&#x987A;&#x5E8F;&#x6392;&#x5E8F;&#x7684;&#xFF0C;&#x5373;<code>&#x8F93;&#x5165;[ &#xFF1A;0]</code>&#x5E94;&#x8BE5;&#x662F;&#x6700;&#x957F;&#x7684;&#x5E8F;&#x5217;&#xFF0C;&#x548C;<code>&#x8F93;&#x5165;[&#xFF1A;&#xFF0C;B-1]</code>&#x6700;&#x77ED;&#x7684;&#x4E00;&#x4E2A;&#x3002;  enforce_sorted =&#x771F;&#x4EC5;&#x7528;&#x4E8E;ONNX&#x51FA;&#x53E3;&#x5FC5;&#x8981;&#x7684;&#x3002;</p>
<p>Note</p>
<p>&#x8BE5;&#x51FD;&#x6570;&#x63A5;&#x53D7;&#x5177;&#x6709;&#x81F3;&#x5C11;&#x4E24;&#x4E2A;&#x7EF4;&#x5EA6;&#x7684;&#x4EFB;&#x4F55;&#x8F93;&#x5165;&#x3002;&#x4F60;&#x53EF;&#x4EE5;&#x8FD0;&#x7528;&#x5B83;&#x6765;&#x5305;&#x88C5;&#x6807;&#x7B7E;&#xFF0C;&#x5E76;&#x4F7F;&#x7528;RNN&#x7684;&#x8F93;&#x51FA;&#x4E0E;&#x4ED6;&#x4EEC;&#x76F4;&#x63A5;&#x8BA1;&#x7B97;&#x7684;&#x635F;&#x5931;&#x3002;&#x5F20;&#x91CF;&#x53EF;&#x4EE5;&#x4ECE; <code>PackedSequence</code>
&#x5BF9;&#x8C61;&#x901A;&#x8FC7;&#x8BBF;&#x95EE;&#x5176;<code>&#x3002;&#x6570;&#x636E;</code>&#x5C5E;&#x6027;&#x8FDB;&#x884C;&#x68C0;&#x7D22;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x586B;&#x5145;&#x6599;&#x7684;&#x53EF;&#x53D8;&#x957F;&#x5EA6;&#x7684;&#x5E8F;&#x5217;&#x3002;</p>
</li>
<li><p><strong>&#x957F;&#x5EA6;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x6BCF;&#x6279;&#x5143;&#x4EF6;&#x7684;&#x5E8F;&#x5217;&#x957F;&#x5EA6;&#x7684;&#x5217;&#x8868;&#x3002;</p>
</li>
<li><p><strong>batch_first</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5982;&#x679C;<code>&#x771F;</code>&#xFF0C;&#x8F93;&#x5165;&#x9884;&#x8BA1;<code>B  &#xD7; T  &#xD7; *</code>&#x683C;&#x5F0F;&#x3002;</p>
</li>
<li><p><strong>enforce_sorted</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5982;&#x679C;<code>&#x771F;</code>&#xFF0C;&#x8F93;&#x5165;&#x88AB;&#x9884;&#x671F;&#x542B;&#x6709;&#x7531;&#x957F;&#x5EA6;&#x4EE5;&#x9012;&#x51CF;&#x7684;&#x987A;&#x5E8F;&#x6392;&#x5E8F;&#x7684;&#x5E8F;&#x5217;&#x3002;&#x5982;&#x679C;<code>&#x5047;</code>&#xFF0C;&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0D;&#x68C0;&#x67E5;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x771F; [HTG21&#x3002;</code></p>
</li>
</ul>
<p>Returns</p>
<p>&#x4E00;&#x4E2A; <code>PackedSequence</code>&#x5BF9;&#x8C61;</p>
<h3 id="padpackedsequence">pad_packed_sequence</h3>
<p><code>torch.nn.utils.rnn.``pad_packed_sequence</code>( <em>sequence</em> , <em>batch_first=False</em> ,
<em>padding_value=0.0</em> , <em>total_length=None</em>
)<a href="_modules/torch/nn/utils/rnn.html#pad_packed_sequence">[source]</a></p>
<p>&#x57AB;&#x7684;&#x586B;&#x5145;&#x6599;&#x7684;&#x53EF;&#x53D8;&#x957F;&#x5EA6;&#x7684;&#x5E8F;&#x5217;&#x3002;</p>
<p>&#x8FD9;&#x662F;&#x4E00;&#x4E2A;&#x9006;&#x8FD0;&#x7B97;&#x4E3A; <code>pack_padded_sequence&#xFF08;&#xFF09;</code>&#x3002;</p>
<p>&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x7684;&#x6570;&#x636E;&#x5C06;&#x662F;&#x5927;&#x5C0F;<code>T  &#xD7; B  &#xD7; *</code>&#x7684;&#xFF0C;&#x5176;&#x4E2D; T &#x662F;&#x6700;&#x957F;&#x7684;&#x5E8F;&#x5217;&#x7684;&#x957F;&#x5EA6;&#x548C; B &#x662F;&#x6279;&#x91CF;&#x5927;&#x5C0F;&#x3002;&#x5982;&#x679C;<code>batch_first</code>&#x662F;True&#xFF0C;&#x5219;&#x6570;&#x636E;&#x5C06;&#x88AB;&#x79FB;&#x4F4D;&#x5230;<code>B  &#xD7; T  X  *</code>&#x683C;&#x5F0F;&#x3002;</p>
<p>&#x6279;&#x5904;&#x7406;&#x5143;&#x7D20;&#x5C06;&#x901A;&#x8FC7;&#x9010;&#x6E10;&#x964D;&#x4F4E;&#x5B83;&#x4EEC;&#x7684;&#x957F;&#x5EA6;&#x8FDB;&#x884C;&#x6392;&#x5E8F;&#x3002;</p>
<p>Note</p>
<p><code>total_length</code>&#x662F;&#x6709;&#x7528;&#x7684;&#x5B9E;&#x65BD;<code>&#x6536;&#x62FE; &#x5E8F;&#x5217; - &amp; GT ;  &#x590D;&#x53D1; &#x7F51;&#x7EDC; - &amp; GT ;  &#x89E3;&#x538B; &#x5E8F;&#x5217;</code>&#x5728;&#x56FE;&#x6848;&#x4E00;&#x4E2A; <code>&#x6A21;&#x5757;</code>
&#x5305;&#x88F9;&#x5728; <code>&#x6570;&#x636E;&#x5E76;&#x884C;</code>&#x3002;&#x53C2;&#x89C1;<a href="notes/faq.html#pack-rnn-unpack-with-data-%0Aparallelism"> &#x8FD9;&#x4E2A;&#x5E38;&#x89C1;&#x95EE;&#x9898;&#x89E3;&#x7B54;&#x90E8;&#x5206; </a>&#x4E86;&#x89E3;&#x8BE6;&#x60C5;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x5E8F;&#x5217;</strong> &#xFF08; <em>PackedSequence</em> &#xFF09; - &#x6279;&#x6B21;&#x5230;&#x57AB;</p>
</li>
<li><p><strong>batch_first</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5982;&#x679C;<code>&#x771F;</code>&#xFF0C;&#x8F93;&#x51FA;&#x5C06;&#x5728;<code>B  &#xD7; T  &#xD7; *</code>&#x683C;&#x5F0F;&#x3002;</p>
</li>
<li><p><strong>padding_value</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x7528;&#x4E8E;&#x586B;&#x5145;&#x5143;&#x7D20;&#x7684;&#x503C;&#x3002;</p>
</li>
<li><p><strong>total_length</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5982;&#x679C;&#x4E0D;&#x662F;<code>&#x65E0;</code>&#xFF0C;&#x8F93;&#x51FA;&#x5C06;&#x88AB;&#x586B;&#x5145;&#x5230;&#x5177;&#x6709;&#x957F;&#x5EA6;<code>total_length</code>&#x3002;&#x6B64;&#x65B9;&#x6CD5;&#x5C06;&#x629B;&#x51FA;<a href="https://docs.python.org/3/library/exceptions.html#ValueError" title="\(in Python v3.7\)" target="_blank"> <code>ValueError&#x5F02;&#x5E38;</code></a>&#x5982;&#x679C;<code>total_length</code>&#x5C0F;&#x4E8E;<code>&#x6700;&#x5927;&#x5E8F;&#x5217;&#x957F;&#x5EA6;&#x5E8F;&#x5217;</code>&#x3002;</p>
</li>
</ul>
<p>Returns</p>
<p>&#x5F20;&#x91CF;&#x7684;&#x542B;&#x5143;&#x7EC4;&#x4E2D;&#x7684;&#x586B;&#x5145;&#x5E8F;&#x5217;&#xFF0C;&#x548C;&#x5305;&#x542B;&#x5728;&#x6240;&#x8FF0;&#x6279;&#x6B21;&#x4E2D;&#x7684;&#x6BCF;&#x4E2A;&#x5E8F;&#x5217;&#x7684;&#x957F;&#x5EA6;&#x7684;&#x5217;&#x8868;&#x4E2D;&#x7684;&#x5F20;&#x91CF;&#x3002;</p>
<h3 id="padsequence">pad_sequence</h3>
<p><code>torch.nn.utils.rnn.``pad_sequence</code>( <em>sequences</em> , <em>batch_first=False</em> ,
<em>padding_value=0</em> )<a href="_modules/torch/nn/utils/rnn.html#pad_sequence">[source]</a></p>
<p>&#x57AB;&#x53EF;&#x53D8;&#x957F;&#x5EA6;&#x5F20;&#x91CF;&#x4E0E;<code>padding_value&#x5217;&#x8868;</code></p>
<p><code>pad_sequence</code>&#x5806;&#x53E0;&#x5F20;&#x91CF;&#x7684;&#x6CBF;&#x4E00;&#x4E2A;&#x65B0;&#x7684;&#x7EF4;&#x5EA6;&#x7684;&#x5217;&#x8868;&#xFF0C;&#x5E76;&#x628A;&#x5B83;&#x4EEC;&#x57AB;&#x76F8;&#x7B49;&#x7684;&#x957F;&#x5EA6;&#x3002;&#x4F8B;&#x5982;&#xFF0C;&#x5982;&#x679C;&#x8F93;&#x5165;&#x662F;&#x5E8F;&#x5217;&#x7684;&#x5927;&#x5C0F;&#x4E3A;&#x5217;&#x8868;<code>L  &#xD7; *</code>&#x5E76;&#x4E14;&#x5982;&#x679C;batch_first&#x662F;False&#xFF0C;&#x5E76;&#x4E14;<code>T  &#xD7; B  &#xD7; *</code>&#x5426;&#x5219;&#x3002;</p>
<p>B &#x662F;&#x6279;&#x91CF;&#x5927;&#x5C0F;&#x3002;&#x5B83;&#x7B49;&#x4E8E;&#x5728;<code>&#x5E8F;&#x5217;</code>&#x7684;&#x5143;&#x7D20;&#x6570;&#x3002;  T &#x662F;&#x6700;&#x957F;&#x7684;&#x5E8F;&#x5217;&#x7684;&#x957F;&#x5EA6;&#x3002;  L &#x662F;&#x5E8F;&#x5217;&#x7684;&#x957F;&#x5EA6;&#x3002;  * &#x662F;&#x4EFB;&#x610F;&#x6570;&#x91CF;&#x7684;&#x5C3E;&#x968F;&#x5C3A;&#x5BF8;&#xFF0C;&#x5305;&#x62EC;&#x6CA1;&#x6709;&#x7684;&#x3002;</p>
<p>Example</p>
<pre><code>&gt;&gt;&gt; from torch.nn.utils.rnn import pad_sequence
&gt;&gt;&gt; a = torch.ones(25, 300)
&gt;&gt;&gt; b = torch.ones(22, 300)
&gt;&gt;&gt; c = torch.ones(15, 300)
&gt;&gt;&gt; pad_sequence([a, b, c]).size()
torch.Size([25, 3, 300])
</code></pre><p>Note</p>
<p>&#x6B64;&#x51FD;&#x6570;&#x8FD4;&#x56DE;&#x7684;&#x5927;&#x5C0F;<code>T  &#xD7; B  &#xD7; *</code>&#x6216;&#x5F20;&#x91CF;<code>B  &#xD7; T  &#xD7; *</code>&#x5176;&#x4E2D; T
&#x662F;&#x6700;&#x957F;&#x7684;&#x5E8F;&#x5217;&#x7684;&#x957F;&#x5EA6;&#x3002;&#x6B64;&#x51FD;&#x6570;&#x5047;&#x5B9A;&#x5C3E;&#x968F;&#x5C3A;&#x5BF8;&#x548C;&#x5E8F;&#x5217;&#x7684;&#x6240;&#x6709;&#x7684;&#x5F20;&#x91CF;&#x7684;&#x7C7B;&#x578B;&#x76F8;&#x540C;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x5E8F;&#x5217;</strong> &#xFF08;<a href="https://docs.python.org/3/library/stdtypes.html#list" title="\(in Python v3.7\)" target="_blank"> <em>&#x5217;&#x8868;</em> </a> <em>[</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a> __ &#xFF09; - &#x53EF;&#x53D8;&#x957F;&#x5EA6;&#x7684;&#x5E8F;&#x5217;&#x7684;&#x5217;&#x8868;&#x3002;</p>
</li>
<li><p><strong>batch_first</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x8F93;&#x51FA;&#x5C06;&#x5904;&#x4E8E;<code>B  &#xD7; T  &#xD7; *</code>&#x5982;&#x679C;&#x4E3A;True&#xFF0C;&#x6216;&#x5728;<code>T  X  B  &#xD7; *</code>&#x5426;&#x5219;</p>
</li>
<li><p><strong>padding_value</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x7528;&#x4E8E;&#x586B;&#x5145;&#x5143;&#x4EF6;&#x503C;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;0&#x3002;</p>
</li>
</ul>
<p>Returns</p>
<p>&#x7684;&#x5927;&#x5C0F;<code>T  &#xD7; B  &#xD7; *</code>&#x5982;&#x679C;<code>&#x5F20;&#x91CF; batch_first</code>&#x662F;<code>&#x5047;</code>&#x3002;&#x7684;&#x5F20;&#x91CF;&#x5927;&#x5C0F;<code>B  &#xD7; T  &#xD7; *</code>&#x5426;&#x5219;</p>
<h3 id="packsequence">pack_sequence</h3>
<p><code>torch.nn.utils.rnn.``pack_sequence</code>( <em>sequences</em> , <em>enforce_sorted=True</em>
)<a href="_modules/torch/nn/utils/rnn.html#pack_sequence">[source]</a></p>
<p>&#x5305;&#x957F;&#x5EA6;&#x53EF;&#x53D8;&#x5F20;&#x91CF;&#x7684;&#x5217;&#x8868;</p>
<p><code>&#x5E8F;&#x5217;</code>&#x5E94;&#x8BE5;&#x662F;&#x5927;&#x5C0F;<code>L  &#xD7; *</code>&#xFF0C;&#x5176;&#x4E2D;&#x7684;&#x5F20;&#x91CF;&#x7684;&#x5217;&#x8868; L &#x662F;&#x4E00;&#x4E2A;&#x5E8F;&#x5217;&#x7684;&#x957F;&#x5EA6;&#x548C; * &#x662F;&#x4EFB;&#x610F;&#x6570;&#x91CF;&#x7684;&#x5C3E;&#x968F;&#x5C3A;&#x5BF8;&#xFF0C;&#x5305;&#x62EC;&#x96F6;&#x7684;&#x3002;</p>
<p>&#x5BF9;&#x4E8E;&#x672A;&#x6392;&#x5E8F;&#x7684;&#x5E8F;&#x5217;&#xFF0C;&#x7528; enforce_sorted =&#x5047;&#x3002;&#x5982;&#x679C;<code>enforce_sorted</code>&#x662F;<code>&#x771F;</code>&#xFF0C;&#x6240;&#x8FF0;&#x5E8F;&#x5217;&#x5E94;&#x5728;&#x964D;&#x4F4E;&#x957F;&#x5EA6;&#x7684;&#x987A;&#x5E8F;&#x8FDB;&#x884C;&#x6392;&#x5E8F;&#x3002; <code>enforce_sorted  =  &#x771F;</code>&#x4EC5;&#x7528;&#x4E8E;ONNX&#x51FA;&#x53E3;&#x5FC5;&#x8981;&#x7684;&#x3002;</p>
<p>Example</p>
<pre><code>&gt;&gt;&gt; from torch.nn.utils.rnn import pack_sequence
&gt;&gt;&gt; a = torch.tensor([1,2,3])
&gt;&gt;&gt; b = torch.tensor([4,5])
&gt;&gt;&gt; c = torch.tensor([6])
&gt;&gt;&gt; pack_sequence([a, b, c])
PackedSequence(data=tensor([ 1,  4,  6,  2,  5,  3]), batch_sizes=tensor([ 3,  2,  1]))
</code></pre><p>Parameters</p>
<ul>
<li><p><strong>&#x5E8F;&#x5217;</strong> &#xFF08;<a href="https://docs.python.org/3/library/stdtypes.html#list" title="\(in Python v3.7\)" target="_blank"> <em>&#x5217;&#x8868;</em> </a> <em>[</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a> __ &#xFF09; - &#x51CF;&#x5C0F;&#x957F;&#x5EA6;&#x7684;&#x5E8F;&#x5217;&#x7684;&#x5217;&#x8868;&#x3002;</p>
</li>
<li><p><strong>enforce_sorted</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5982;&#x679C;<code>&#x771F;</code>&#xFF0C;&#x68C0;&#x67E5;&#x8BE5;&#x8F93;&#x5165;&#x5305;&#x542B;&#x7531;&#x957F;&#x5EA6;&#x4EE5;&#x9012;&#x51CF;&#x7684;&#x987A;&#x5E8F;&#x6392;&#x5E8F;&#x7684;&#x5E8F;&#x5217;&#x3002;&#x5982;&#x679C;<code>&#x5047;</code>&#xFF0C;&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0D;&#x68C0;&#x67E5;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x771F; [HTG21&#x3002;</code></p>
</li>
</ul>
<p>Returns</p>
<p>a <code>PackedSequence</code>object</p>
<h3 id="&#x62FC;&#x5408;">&#x62FC;&#x5408;</h3>
<p><a href="nn.functional.html" title="torch.nn.functional">Next <img src="_static/images/chevron-right-orange.svg" alt=""></a> <a href="storage.html" title="torch.Storage"><img src="_static/images/chevron-right-orange.svg" alt="">
Previous</a></p>
<hr>
<p>&#xA9;&#x7248;&#x6743;&#x6240;&#x6709;2019&#x5E74;&#xFF0C;Torch &#x8D21;&#x732E;&#x8005;&#x3002;</p>
<p><hr></p>
<div align="center">
    <p><a href="http://www.apachecn.org" target="_blank"><font face="KaiTi" size="6" color="red">&#x6211;&#x4EEC;&#x4E00;&#x76F4;&#x5728;&#x52AA;&#x529B;</font></a></p>
    <p><a href="https://github.com/apachecn/pytorch-doc-zh/" target="_blank">apachecn/pytorch-doc-zh</a></p>
    <p><iframe align="middle" src="https://ghbtns.com/github-btn.html?user=apachecn&amp;repo=pytorch-doc-zh&amp;type=watch&amp;count=true&amp;v=2" frameborder="0" scrolling="0" width="100px" height="25px"></iframe>
    <iframe align="middle" src="https://ghbtns.com/github-btn.html?user=apachecn&amp;repo=pytorch-doc-zh&amp;type=star&amp;count=true" frameborder="0" scrolling="0" width="100px" height="25px"></iframe>
    <iframe align="middle" src="https://ghbtns.com/github-btn.html?user=apachecn&amp;repo=pytorch-doc-zh&amp;type=fork&amp;count=true" frameborder="0" scrolling="0" width="100px" height="25px"></iframe>
    <a target="_blank" href="shang.qq.com/wpa/qunwpa"><img border="0" src="http://data.apachecn.org/img/logo/ApacheCN-group.png" alt="ML | ApacheCN" title="ML | ApacheCN"></a></p>
</div>
 <div style="text-align:center;margin:0 0 10.5px;">
     <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
     <ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-3565452474788507" data-ad-slot="2543897000">
     </ins>
     <script>(adsbygoogle = window.adsbygoogle || []).push({});</script>

    <script>
      var _hmt = _hmt || [];
      (function() {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91";
        var s = document.getElementsByTagName("script")[0]; 
        s.parentNode.insertBefore(hm, s);
      })();
    </script>

    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-102475051-10"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-102475051-10');
    </script>
</div>

<p><meta name="google-site-verification" content="pyo9N70ZWyh8JB43bIu633mhxesJ1IcwWCZlM3jUfFo"></p>
<p><iframe src="https://www.bilibili.com/read/cv2710377" style="display:none"></iframe>
<img src="http://t.cn/AiCoDHwb" hidden="hidden"></p>
<div>
    <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
    <script src="https://unpkg.com/gitalk@latest/dist/gitalk.min.js"></script>
    <script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>
    <div id="gitalk-container"></div>
    <script type="text/javascript">
        const gitalk = new Gitalk({
        clientID: '2e62dee5b9896e2eede6',
        clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53',
        repo: 'pytorch-doc-zh',
        owner: 'apachecn',
        admin: ['jiangzhonglian', 'wizardforcel'],
        id: md5(location.pathname),
        distractionFreeMode: false
        })
        gitalk.render('gitalk-container')
    </script>
</div>

<footer class="page-footer"><span class="copyright">Copyright &#xA9; ibooker.org.cn 2019 all right reserved&#xFF0C;&#x7531; ApacheCN &#x56E2;&#x961F;&#x63D0;&#x4F9B;&#x652F;&#x6301;</span><span class="footer-modification">&#x8BE5;&#x6587;&#x4EF6;&#x4FEE;&#x8BA2;&#x65F6;&#x95F4;&#xFF1A; 
2019-09-23 17:49:19
</span></footer>
<script>console.log("plugin-popup....");document.onclick = function(e){ e.target.tagName === "IMG" && window.open(e.target.src,e.target.src)}</script><style>img{cursor:pointer}</style>
                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="storage.html" class="navigation navigation-prev " aria-label="Previous page: torch.Storage">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="nn.functional.html" class="navigation navigation-next " aria-label="Next page: torch.nn.functional">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"torch.nn","level":"1.3.3.8","depth":3,"next":{"title":"torch.nn.functional","level":"1.3.3.9","depth":3,"path":"nn.functional.md","ref":"nn.functional.md","articles":[]},"previous":{"title":"torch.Storage","level":"1.3.3.7","depth":3,"path":"storage.md","ref":"storage.md","articles":[]},"dir":"ltr"},"config":{"plugins":["github","github-buttons","-sharing","insert-logo","sharing-plus","back-to-top-button","code","copy-code-button","mathjax","pageview-count","edit-link","emphasize","alerts","auto-scroll-table","popup","hide-element","page-toc-button","tbfed-pagefooter","sitemap","advanced-emoji","expandable-chapters","splitter","search-pro"],"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"pluginsConfig":{"tbfed-pagefooter":{"copyright":"Copyright &copy ibooker.org.cn 2019","modify_label":"该文件修订时间： ","modify_format":"YYYY-MM-DD HH:mm:ss"},"emphasize":{},"github":{"url":"https://github.com/apachecn/pytorch-doc-zh"},"splitter":{},"search-pro":{},"search":{},"sharing-plus":{"qq":false,"all":["facebook","google","twitter","instapaper","linkedin","pocket","stumbleupon"],"douban":false,"facebook":true,"weibo":false,"instapaper":false,"whatsapp":false,"hatenaBookmark":false,"twitter":true,"messenger":false,"line":false,"vk":false,"pocket":true,"google":false,"viber":false,"stumbleupon":false,"qzone":false,"linkedin":false},"auto-scroll-table":{},"popup":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"code":{"copyButtons":true},"hide-element":{"elements":[".gitbook-link"]},"fontsettings":{"theme":"white","family":"sans","size":2},"highlight":{},"sitemap":{"hostname":"http://pytorch.apachecn.org"},"page-toc-button":{"maxTocDepth":4,"minTocSize":4},"back-to-top-button":{},"pageview-count":{},"alerts":{},"github-buttons":{"buttons":[{"user":"apachecn","repo":"pytorch-doc-zh","type":"star","count":true,"size":"small"}]},"mathjax":{"forceSVG":false,"version":"2.6-latest"},"copy-code-button":{},"advanced-emoji":{"embedEmojis":false},"sharing":{"qq":false,"all":["qq","douban","facebook","google","linkedin","twitter","weibo","whatsapp"],"douban":false,"facebook":false,"weibo":true,"whatsapp":false,"twitter":false,"line":false,"google":false,"qzone":true},"edit-link":{"label":"编辑本页","base":"https://github.com/apachecn/pytorch-doc-zh/blob/master/docs/1.2"},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false},"insert-logo":{"style":"background: none; max-height: 150px; min-height: 150px","url":"http://data.apachecn.org/img/logo.jpg"},"expandable-chapters":{}},"my_links":{"sidebar":{"Home":"https://www.baidu.com"}},"theme":"default","author":"ApacheCN","my_plugins":["donate","todo","-lunr","-search","expandable-chapters-small","chapter-fold","expandable-chapters","expandable-chapters-small","back-to-top-button","ga","baidu","sitemap","tbfed-pagefooter","advanced-emoji","sectionx","page-treeview","simple-page-toc","ancre-navigation","theme-apachecn@git+https://github.com/apachecn/theme-apachecn#HEAD","pagefooter-apachecn@git+https://github.com/apachecn/gitbook-plugin-pagefooter-apachecn#HEAD"],"my_pluginsConfig":{"page-treeview":{"copyright":"Copyright &#169; aleen42","minHeaderCount":"2","minHeaderDeep":"2"},"ignores":["node_modules"],"simple-page-toc":{"maxDepth":3,"skipFirstH1":true},"page-copyright":{"wisdom":"Designer, Frontend Developer & overall web enthusiast","noPowered":false,"copyright":"Copyright &#169; 你的名字","style":"normal","timeColor":"#666","utcOffset":"8","format":"YYYY-MM-dd hh:mm:ss","signature":"你的签名","copyrightColor":"#666","description":"modified at"},"donate":{"wechat":"微信收款的二维码URL","alipay":"支付宝收款的二维码URL","title":"","button":"赏","alipayText":"支付宝打赏","wechatText":"微信打赏"},"page-toc-button":{"maxTocDepth":2,"minTocSize":2},"github-buttons":{"buttons":[{"user":"apachecn","repo":"pytorch-doc-zh","type":"star","count":true,"size":"small"},{"user":"apachecn","width":"160","type":"follow","count":true,"size":"small"}]},"ga":{"token":"UA-102475051-10"},"baidu":{"token":"75439e2cbd22bdd813226000e9dcc12f"},"pagefooter-apachecn":{"copyright":"Copyright &copy ibooker.org.cn 2019","modify_label":"该文件修订时间： ","modify_format":"YYYY-MM-DD HH:mm:ss"}},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"variables":{},"title":"Pytorch 中文文档","language":"zh-hans","gitbook":"*","description":"Pytorch 中文文档: 教程和文档"},"file":{"path":"nn.md","mtime":"2019-09-23T17:49:19.999Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2019-09-23T17:53:13.712Z"},"basePath":".","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="gitbook/gitbook.js"></script>
    <script src="gitbook/theme.js"></script>
    
        
        <script src="gitbook/gitbook-plugin-github/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-github-buttons/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-insert-logo/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-sharing-plus/buttons.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-back-to-top-button/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-code/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-copy-code-button/toggle.js"></script>
        
    
        
        <script src="https://cdn.mathjax.org/mathjax/2.6-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-mathjax/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-pageview-count/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-edit-link/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-alerts/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-auto-scroll-table/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-hide-element/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-page-toc-button/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-expandable-chapters/expandable-chapters.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-splitter/splitter.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search-pro/jquery.mark.min.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search-pro/search.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

