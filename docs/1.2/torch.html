
<!DOCTYPE HTML>
<html lang="zh-hans" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>torch · Pytorch 中文文档</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        <meta name="author" content="ApacheCN">
        
        
    
    <link rel="stylesheet" href="gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-insert-logo/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-back-to-top-button/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-code/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-pageview-count/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-emphasize/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-alerts/style.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-auto-scroll-table/website.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-page-toc-button/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-tbfed-pagefooter/footer.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-advanced-emoji/emoji-website.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-expandable-chapters/expandable-chapters.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-splitter/splitter.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-search-pro/search.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="tensors.html" />
    
    

    <style>
    @media only screen and (max-width: 640px) {
        .book-header .hidden-mobile {
            display: none;
        }
    }
    </style>
    <script>
        window["gitbook-plugin-github-buttons"] = {"buttons":[{"user":"apachecn","repo":"pytorch-doc-zh","type":"star","count":true,"size":"small"}]};
    </script>

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="输入并搜索" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="./">
            
                <a href="./">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" >
            
                <span>
            
                    
                    中文教程
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.1" >
            
                <span>
            
                    
                    入门
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.1.1" data-path="beginner/deep_learning_60min_blitz.html">
            
                <a href="beginner/deep_learning_60min_blitz.html">
            
                    
                    PyTorch 深度学习: 60 分钟极速入门
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.2" data-path="beginner/data_loading_tutorial.html">
            
                <a href="beginner/data_loading_tutorial.html">
            
                    
                    数据加载和处理教程
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.3" data-path="beginner/pytorch_with_examples.html">
            
                <a href="beginner/pytorch_with_examples.html">
            
                    
                    用例子学习 PyTorch
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.4" data-path="beginner/transfer_learning_tutorial.html">
            
                <a href="beginner/transfer_learning_tutorial.html">
            
                    
                    迁移学习教程
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.5" data-path="beginner/deploy_seq2seq_hybrid_frontend_tutorial.html">
            
                <a href="beginner/deploy_seq2seq_hybrid_frontend_tutorial.html">
            
                    
                    部署与TorchScript一个Seq2Seq模型
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.6" data-path="intermediate/tensorboard_tutorial.html">
            
                <a href="intermediate/tensorboard_tutorial.html">
            
                    
                    可视化模型，数据，和与训练TensorBoard
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.7" data-path="beginner/saving_loading_models.html">
            
                <a href="beginner/saving_loading_models.html">
            
                    
                    保存和加载模型
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.8" data-path="beginner/nn_tutorial.html">
            
                <a href="beginner/nn_tutorial.html">
            
                    
                    torch.nn 到底是什么？
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.2" >
            
                <span>
            
                    
                    图片
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.2.1" data-path="intermediate/torchvision_tutorial.html">
            
                <a href="intermediate/torchvision_tutorial.html">
            
                    
                    TorchVision对象检测教程细化和微调
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2.2" data-path="beginner/finetuning_torchvision_models_tutorial.html">
            
                <a href="beginner/finetuning_torchvision_models_tutorial.html">
            
                    
                    微调Torchvision模型
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2.3" data-path="intermediate/spatial_transformer_tutorial.html">
            
                <a href="intermediate/spatial_transformer_tutorial.html">
            
                    
                    空间变压器网络教程
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2.4" data-path="advanced/neural_style_tutorial.html">
            
                <a href="advanced/neural_style_tutorial.html">
            
                    
                    使用PyTorch进行神经网络传递
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2.5" data-path="beginner/fgsm_tutorial.html">
            
                <a href="beginner/fgsm_tutorial.html">
            
                    
                    对抗性示例生成
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2.6" data-path="beginner/dcgan_faces_tutorial.html">
            
                <a href="beginner/dcgan_faces_tutorial.html">
            
                    
                    DCGAN教程
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.3" >
            
                <span>
            
                    
                    音频
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.3.1" data-path="beginner/audio_preprocessing_tutorial.html">
            
                <a href="beginner/audio_preprocessing_tutorial.html">
            
                    
                    torchaudio教程
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.4" >
            
                <span>
            
                    
                    文本
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.4.1" data-path="intermediate/char_rnn_classification_tutorial.html">
            
                <a href="intermediate/char_rnn_classification_tutorial.html">
            
                    
                    NLP从头：判断名称与字符级RNN
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.4.2" data-path="intermediate/char_rnn_generation_tutorial.html">
            
                <a href="intermediate/char_rnn_generation_tutorial.html">
            
                    
                    NLP从头：生成名称与字符级RNN
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.4.3" data-path="intermediate/seq2seq_translation_tutorial.html">
            
                <a href="intermediate/seq2seq_translation_tutorial.html">
            
                    
                    NLP从无到有：用序列到序列网络和翻译注意
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.4.4" data-path="beginner/text_sentiment_ngrams_tutorial.html">
            
                <a href="beginner/text_sentiment_ngrams_tutorial.html">
            
                    
                    文本分类与TorchText 
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.4.5" data-path="beginner/torchtext_translation_tutorial.html">
            
                <a href="beginner/torchtext_translation_tutorial.html">
            
                    
                    语言翻译与TorchText 
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.4.6" data-path="beginner/transformer_tutorial.html">
            
                <a href="beginner/transformer_tutorial.html">
            
                    
                    序列到序列与nn.Transformer和TorchText建模
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.5" >
            
                <span>
            
                    
                    在生产部署PyTorch模型
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.5.1" data-path="intermediate/flask_rest_api_tutorial.html">
            
                <a href="intermediate/flask_rest_api_tutorial.html">
            
                    
                    1.部署PyTorch在Python经由REST API从Flask
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5.2" data-path="beginner/Intro_to_TorchScript_tutorial.html">
            
                <a href="beginner/Intro_to_TorchScript_tutorial.html">
            
                    
                    2.介绍TorchScript
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5.3" data-path="advanced/cpp_export.html">
            
                <a href="advanced/cpp_export.html">
            
                    
                    3.装载++一个TorchScript模型在C 
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5.4" data-path="advanced/super_resolution_with_onnxruntime.html">
            
                <a href="advanced/super_resolution_with_onnxruntime.html">
            
                    
                    4.（可选）从导出到PyTorch一个ONNX模型并使用ONNX运行时运行它
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.6" >
            
                <span>
            
                    
                    并行和分布式训练
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.6.1" data-path="intermediate/model_parallel_tutorial.html">
            
                <a href="intermediate/model_parallel_tutorial.html">
            
                    
                    1.型号并行最佳实践
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6.2" data-path="intermediate/ddp_tutorial.html">
            
                <a href="intermediate/ddp_tutorial.html">
            
                    
                    2.入门分布式数据并行
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6.3" data-path="intermediate/dist_tuto.html">
            
                <a href="intermediate/dist_tuto.html">
            
                    
                    3. PyTorch编写分布式应用
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6.4" data-path="beginner/aws_distributed_training_tutorial.html">
            
                <a href="beginner/aws_distributed_training_tutorial.html">
            
                    
                    4.（高级）PyTorch 1.0分布式训练与Amazon AWS
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.7" >
            
                <span>
            
                    
                    扩展PyTorch
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.7.1" data-path="advanced/torch_script_custom_ops.html">
            
                <a href="advanced/torch_script_custom_ops.html">
            
                    
                    使用自定义 C++ 扩展算TorchScript 
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.2" data-path="advanced/numpy_extensions_tutorial.html">
            
                <a href="advanced/numpy_extensions_tutorial.html">
            
                    
                    创建扩展使用numpy的和SciPy的
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.3" data-path="advanced/cpp_extension.html">
            
                <a href="advanced/cpp_extension.html">
            
                    
                    自定义 C++ 和CUDA扩展
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.8" >
            
                <span>
            
                    
                    PyTorch在其他语言
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.8.1" data-path="advanced/cpp_frontend.html">
            
                <a href="advanced/cpp_frontend.html">
            
                    
                    使用PyTorch C++ 前端
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3" >
            
                <span>
            
                    
                    中文文档
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.1" >
            
                <span>
            
                    
                    注解
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.1.1" data-path="notes/autograd.html">
            
                <a href="notes/autograd.html">
            
                    
                    自动求导机制
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.2" data-path="notes/broadcasting.html">
            
                <a href="notes/broadcasting.html">
            
                    
                    广播语义
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.3" data-path="notes/cpu_threading_torchscript_inference.html">
            
                <a href="notes/cpu_threading_torchscript_inference.html">
            
                    
                    CPU线程和TorchScript推理
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.4" data-path="notes/cuda.html">
            
                <a href="notes/cuda.html">
            
                    
                    CUDA语义
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.5" data-path="notes/extending.html">
            
                <a href="notes/extending.html">
            
                    
                    扩展PyTorch
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.6" data-path="notes/faq.html">
            
                <a href="notes/faq.html">
            
                    
                    常见问题
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.7" data-path="notes/large_scale_deployments.html">
            
                <a href="notes/large_scale_deployments.html">
            
                    
                    对于大规模部署的特点
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.8" data-path="notes/multiprocessing.html">
            
                <a href="notes/multiprocessing.html">
            
                    
                    多处理最佳实践
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.9" data-path="notes/randomness.html">
            
                <a href="notes/randomness.html">
            
                    
                    重复性
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.10" data-path="notes/serialization.html">
            
                <a href="notes/serialization.html">
            
                    
                    序列化语义
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.11" data-path="notes/windows.html">
            
                <a href="notes/windows.html">
            
                    
                    Windows 常见问题
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3.2" >
            
                <span>
            
                    
                    社区
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.2.1" data-path="community/contribution_guide.html">
            
                <a href="community/contribution_guide.html">
            
                    
                    PyTorch贡献说明书
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.2" data-path="community/governance.html">
            
                <a href="community/governance.html">
            
                    
                    PyTorch治理
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.3" data-path="community/persons_of_interest.html">
            
                <a href="community/persons_of_interest.html">
            
                    
                    PyTorch治|兴趣的人
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3.3" >
            
                <span>
            
                    
                    封装参考文献
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter active" data-level="1.3.3.1" data-path="torch.html">
            
                <a href="torch.html">
            
                    
                    torch
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.2" data-path="tensors.html">
            
                <a href="tensors.html">
            
                    
                    torch.Tensor
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.3" data-path="tensor_attributes.html">
            
                <a href="tensor_attributes.html">
            
                    
                    Tensor Attributes
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.4" data-path="type_info.html">
            
                <a href="type_info.html">
            
                    
                    Type Info
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.5" data-path="sparse.html">
            
                <a href="sparse.html">
            
                    
                    torch.sparse
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.6" data-path="cuda.html">
            
                <a href="cuda.html">
            
                    
                    torch.cuda
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.7" data-path="storage.html">
            
                <a href="storage.html">
            
                    
                    torch.Storage
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.8" data-path="nn.html">
            
                <a href="nn.html">
            
                    
                    torch.nn
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.9" data-path="nn.functional.html">
            
                <a href="nn.functional.html">
            
                    
                    torch.nn.functional
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.10" data-path="nn.init.html">
            
                <a href="nn.init.html">
            
                    
                    torch.nn.init
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.11" data-path="optim.html">
            
                <a href="optim.html">
            
                    
                    torch.optim
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.12" data-path="autograd.html">
            
                <a href="autograd.html">
            
                    
                    torch.autograd
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.13" data-path="distributed.html">
            
                <a href="distributed.html">
            
                    
                    torch.distributed
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.14" data-path="distributions.html">
            
                <a href="distributions.html">
            
                    
                    torch.distributions
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.15" data-path="hub.html">
            
                <a href="hub.html">
            
                    
                    torch.hub
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.16" data-path="jit.html">
            
                <a href="jit.html">
            
                    
                    torch.jit
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.17" data-path="multiprocessing.html">
            
                <a href="multiprocessing.html">
            
                    
                    torch.multiprocessing
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.18" data-path="random.html">
            
                <a href="random.html">
            
                    
                    torch.random
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.19" data-path="bottleneck.html">
            
                <a href="bottleneck.html">
            
                    
                    torch.utils.bottleneck
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.20" data-path="checkpoint.html">
            
                <a href="checkpoint.html">
            
                    
                    torch.utils.checkpoint
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.21" data-path="cpp_extension.html">
            
                <a href="cpp_extension.html">
            
                    
                    torch.utils.cpp_extension
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.22" data-path="data.html">
            
                <a href="data.html">
            
                    
                    torch.utils.data
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.23" data-path="dlpack.html">
            
                <a href="dlpack.html">
            
                    
                    torch.utils.dlpack
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.24" data-path="model_zoo.html">
            
                <a href="model_zoo.html">
            
                    
                    torch.utils.model_zoo
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.25" data-path="tensorboard.html">
            
                <a href="tensorboard.html">
            
                    
                    torch.utils.tensorboard
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.26" data-path="onnx.html">
            
                <a href="onnx.html">
            
                    
                    torch.onnx
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.27" data-path="__config__.md">
            
                <span>
            
                    
                    torch. config
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3.4" >
            
                <span>
            
                    
                    torchvision 参考文献
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.4.1" data-path="torchvision/">
            
                <a href="torchvision/">
            
                    
                    torchvision
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3.5" >
            
                <span>
            
                    
                    torchaudio Reference
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.5.1" >
            
                <a target="_blank" href="https://pytorch.org/audio">
            
                    
                    torchaudio
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3.6" >
            
                <span>
            
                    
                    torchtext Reference
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.6.1" >
            
                <a target="_blank" href="https://pytorch.org/text">
            
                    
                    torchtext
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

            </ul>
            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            本书使用 GitBook 发布
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href="." >torch</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h1 id="torch">torch</h1>
<p>Torch &#x5305;&#x4E2D;&#x5305;&#x542B;&#x7528;&#x4E8E;&#x5BF9;&#x8FD9;&#x4E9B;&#x591A;&#x7EF4;&#x5F20;&#x91CF;&#x548C;&#x6570;&#x5B66;&#x8FD0;&#x7B97;&#x88AB;&#x5B9A;&#x4E49;&#x7684;&#x6570;&#x636E;&#x7ED3;&#x6784;&#x3002;&#x6B64;&#x5916;&#xFF0C;&#x5B83;&#x63D0;&#x4F9B;&#x4E86;&#x5F20;&#x91CF;&#x7684;&#x9AD8;&#x6548;&#x4E32;&#x884C;&#x5316;&#x548C;&#x4EFB;&#x610F;&#x7C7B;&#x578B;&#xFF0C;&#x4EE5;&#x53CA;&#x5176;&#x4ED6;&#x6709;&#x7528;&#x7684;&#x5DE5;&#x5177;&#x7684;&#x8BB8;&#x591A;&#x5DE5;&#x5177;&#x3002;</p>
<p>&#x5B83;&#x5177;&#x6709;CUDA&#x5BF9;&#x5E94;&#xFF0C;&#x4F7F;&#x60A8;&#x53EF;&#x4EE5;&#x5728;&#x4E00;&#x4E2A;NVIDIA GPU&#x8BA1;&#x7B97;&#x80FD;&#x529B;&amp; GT&#x8FD0;&#x884C;&#x8BA1;&#x7B97;&#x5F20;[] = 3.0&#x3002;</p>
<h2 id="&#x5F20;&#x91CF;">&#x5F20;&#x91CF;</h2>
<p><code>torch.``is_tensor</code>( <em>obj</em> )<a href="_modules/torch.html#is_tensor">[source]</a></p>
<p>&#x5982;&#x679C; OBJ &#x662F;&#x4E00;&#x4E2A;PyTorch&#x5F20;&#x8FD4;&#x56DE;True&#x3002;</p>
<p>Parameters</p>
<p><strong>OBJ</strong> &#xFF08; <em>&#x5BF9;&#x8C61;</em> &#xFF09; - &#x5BF9;&#x8C61;&#x6D4B;&#x8BD5;</p>
<p><code>torch.``is_storage</code>( <em>obj</em> )<a href="_modules/torch.html#is_storage">[source]</a></p>
<p>&#x5982;&#x679C; OBJ &#x662F;&#x4E00;&#x4E2A;PyTorch&#x5B58;&#x50A8;&#x5BF9;&#x8C61;&#x8FD4;&#x56DE;&#x771F;&#x3002;</p>
<p>Parameters</p>
<p><strong>obj</strong> ( <em>Object</em> ) &#x2013; Object to test</p>
<p><code>torch.``is_floating_point</code>( <em>input) - &gt; (bool</em>)</p>
<p>&#x8FD4;&#x56DE;true&#x5982;&#x679C;&#x8F93;&#x5165;&#x7684;<code>&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;</code>&#x662F;&#x4E00;&#x4E2A;&#x6D6E;&#x70B9;&#x6570;&#x636E;&#x7C7B;&#x578B;&#xFF0C;&#x5373;&#xFF0C;torch.float64 &#x7684;<code>&#x4E4B;&#x4E00;&#xFF0C;</code>torch.float32<code>&#x548C;</code>
torch.float16<code>&#x3002;</code></p>
<p>Parameters</p>
<p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x7684;PyTorch&#x5F20;&#x91CF;&#xFF0C;&#x4EE5;&#x6D4B;&#x8BD5;</p>
<p><code>torch.``set_default_dtype</code>( <em>d</em>
)<a href="_modules/torch.html#set_default_dtype">[source]</a></p>
<p>&#x8BBE;&#x7F6E;&#x9ED8;&#x8BA4;&#x6D6E;&#x70B9;D&#x578B;&#x4E3A;<code>d</code>&#x3002;&#x8FD9;&#x79CD;&#x7C7B;&#x578B;&#x7684;&#x5C06;&#x88AB;&#x7528;&#x4F5C;&#x9ED8;&#x8BA4;&#x6D6E;&#x70B9;&#x7C7B;&#x578B;&#x4E3A;&#x7C7B;&#x578B;&#x63A8;&#x65AD;&#x5728; <code>torch.tensor&#xFF08;&#xFF09;</code>&#x3002;</p>
<p>&#x9ED8;&#x8BA4;&#x6D6E;&#x70B9;D&#x578B;&#x7EC6;&#x80DE;&#x662F;&#x6700;&#x521D;<code>torch.float32</code>&#x3002;</p>
<p>Parameters</p>
<p><strong>d</strong> &#xFF08;<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"> <code>torch.dtype</code></a>&#xFF09; - &#x6D6E;&#x70B9;D&#x578B;&#x7EC6;&#x80DE;&#xFF0C;&#x4F7F;&#x9ED8;&#x8BA4;</p>
<p>&#x4F8B;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; torch.tensor([1.2, 3]).dtype           # initial default for floating point is torch.float32
torch.float32
&gt;&gt;&gt; torch.set_default_dtype(torch.float64)
&gt;&gt;&gt; torch.tensor([1.2, 3]).dtype           # a new floating point tensor
torch.float64
</code></pre><p><code>torch.``get_default_dtype</code>() &#x2192; torch.dtype</p>
<p>&#x83B7;&#x5F97;&#x5F53;&#x524D;&#x9ED8;&#x8BA4;&#x7684;&#x6D6E;&#x70B9;[ <code>torch.dtype</code><a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype">HTG5&#x3002;</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.get_default_dtype()  # initial default for floating point is torch.float32
torch.float32
&gt;&gt;&gt; torch.set_default_dtype(torch.float64)
&gt;&gt;&gt; torch.get_default_dtype()  # default is now changed to torch.float64
torch.float64
&gt;&gt;&gt; torch.set_default_tensor_type(torch.FloatTensor)  # setting tensor type also affects this
&gt;&gt;&gt; torch.get_default_dtype()  # changed to torch.float32, the dtype for torch.FloatTensor
torch.float32
</code></pre><p><code>torch.``set_default_tensor_type</code>( <em>t</em>
)<a href="_modules/torch.html#set_default_tensor_type">[source]</a></p>
<p>&#x8BBE;&#x7F6E;&#x9ED8;&#x8BA4;<code>torch.Tensor</code>&#x7C7B;&#x578B;&#x5230;&#x6D6E;&#x70B9;&#x578B;&#x5F20;&#x91CF; <code>T</code>&#x3002;&#x8FD9;&#x79CD;&#x7C7B;&#x578B;&#x4E5F;&#x5C06;&#x88AB;&#x7528;&#x4F5C;&#x9ED8;&#x8BA4;&#x6D6E;&#x70B9;&#x7C7B;&#x578B;&#x4E3A;&#x7C7B;&#x578B;&#x63A8;&#x65AD;&#x5728; <code>torch.tensor&#xFF08;&#xFF09;</code>
&#x3002;</p>
<p>&#x9ED8;&#x8BA4;&#x6D6E;&#x70B9;&#x5F20;&#x91CF;&#x7C7B;&#x578B;&#x6700;&#x521D;&#x662F;<code>torch.FloatTensor</code>&#x3002;</p>
<p>Parameters</p>
<p><strong>T</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#type" title="\(in
Python v3.7\)" target="_blank"> <em>&#x8F93;&#x5165;</em> </a> <em>&#x6216;</em> <em>&#x4E32;</em> &#xFF09; - &#x6D6E;&#x70B9;&#x5F20;&#x91CF;&#x7C7B;&#x578B;&#x6216;&#x540D;&#x79F0;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.tensor([1.2, 3]).dtype    # initial default for floating point is torch.float32
torch.float32
&gt;&gt;&gt; torch.set_default_tensor_type(torch.DoubleTensor)
&gt;&gt;&gt; torch.tensor([1.2, 3]).dtype    # a new floating point tensor
torch.float64
</code></pre><p><code>torch.``numel</code>( <em>input</em> ) &#x2192; int</p>
<p>&#x8FD4;&#x56DE;&#x5143;&#x4EF6;&#x5728;<code>&#x8F93;&#x5165;</code>&#x5F20;&#x91CF;&#x7684;&#x603B;&#x6570;&#x3002;</p>
<p>Parameters</p>
<p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x8F93;&#x5165;&#x5F20;&#x91CF;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(1, 2, 3, 4, 5)
&gt;&gt;&gt; torch.numel(a)
120
&gt;&gt;&gt; a = torch.zeros(4,4)
&gt;&gt;&gt; torch.numel(a)
16
</code></pre><p><code>torch.``set_printoptions</code>( <em>precision=None</em> , <em>threshold=None</em> ,
<em>edgeitems=None</em> , <em>linewidth=None</em> , <em>profile=None</em> , <em>sci_mode=None</em>
)<a href="_modules/torch/_tensor_str.html#set_printoptions">[source]</a></p>
<p>&#x6253;&#x5370;&#x8BBE;&#x7F6E;&#x9009;&#x9879;&#x3002;&#x9879;&#x76EE;&#x65E0;&#x803B;&#x5730;&#x4ECE;NumPy&#x7684;&#x62CD;&#x6444;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x7CBE;&#x5EA6;</strong> - &#x7684;&#x7528;&#x4E8E;&#x6D6E;&#x70B9;&#x8F93;&#x51FA;&#xFF08;&#x7F3A;&#x7701;&#x503C;= 4&#xFF09;&#x7684;&#x7CBE;&#x5EA6;&#x4F4D;&#x6570;&#x3002;</p>
</li>
<li><p><strong>&#x9608;</strong> - &#x5176;&#x4E2D;&#x89E6;&#x53D1;&#x603B;&#x7ED3;&#x800C;&#x4E0D;&#x662F;&#x5B8C;&#x6574;&#x518D;&#x7248;&#xFF08;&#x9ED8;&#x8BA4;= 1000&#xFF09;&#x7684;&#x6570;&#x7EC4;&#x5143;&#x7D20;&#x7684;&#x603B;&#x6570;&#x3002;</p>
</li>
<li><p><strong>edgeitems</strong> - &#x4E2D;&#x7684;&#x6BCF;&#x4E2A;&#x7EF4;&#x5EA6;&#xFF08;&#x7F3A;&#x7701;= 3&#xFF09;&#x7684;&#x5F00;&#x59CB;&#x548C;&#x7ED3;&#x675F;&#x65F6;&#x5728;&#x6458;&#x8981;&#x6570;&#x7EC4;&#x9879;&#x6570;&#x3002;</p>
</li>
<li><p><strong>&#x7EBF;&#x5BBD;</strong> - &#x6BCF;&#x884C;&#x7684;&#x5B57;&#x7B26;&#x7528;&#x4E8E;&#x63D2;&#x5165;&#x6362;&#x884C;&#xFF08;&#x7F3A;&#x7701;&#x503C;= 80&#xFF09;&#x7684;&#x76EE;&#x7684;&#x7684;&#x6570;&#x91CF;&#x3002;&#x9608;&#x503C;&#x7684;&#x77E9;&#x9635;&#x5C06;&#x5FFD;&#x7565;&#x6B64;&#x53C2;&#x6570;&#x3002;</p>
</li>
<li><p><strong>&#x4E2A;&#x4EBA;&#x8D44;&#x6599;</strong> - &#x4E3A;&#x6F02;&#x4EAE;&#x7684;&#x5370;&#x5237;&#x7406;&#x667A;&#x7684;&#x9ED8;&#x8BA4;&#x503C;&#x3002;&#x53EF;&#x4E0E;&#x4E0A;&#x8FF0;&#x4EFB;&#x4F55;&#x9009;&#x9879;&#x8986;&#x76D6;&#x3002; &#xFF08;&#x7684;&#x4E2D;&#x4EFB;&#x4E00;&#x9879;&#x9ED8;&#x8BA4;&#xFF0C;&#x77ED;&#xFF0C;&#x5145;&#x6EE1;&#xFF09;</p>
</li>
<li><p><strong>sci_mode</strong> - &#x542F;&#x7528;&#xFF08;True&#xFF09;&#x6216;&#x7981;&#x7528;&#xFF08;&#x5047;&#xFF09;&#x79D1;&#x5B66;&#x8BB0;&#x6570;&#x6CD5;&#x3002;&#x5982;&#x679C;&#x6307;&#x5B9A;&#x65E0;&#xFF08;&#x9ED8;&#x8BA4;&#x503C;&#xFF09;&#xFF0C;&#x8BE5;&#x503C;&#x662F;&#x7531; _Formatter&#x5B9A;&#x4E49;</p>
</li>
</ul>
<p><code>torch.``set_flush_denormal</code>( <em>mode</em> ) &#x2192; bool</p>
<p>&#x7981;&#x6B62;&#x975E;&#x6B63;&#x89C4;&#x6D6E;&#x4E8E;CPU&#x7F16;&#x53F7;&#x3002;</p>
<p>&#x8FD4;&#x56DE;<code>&#x771F;</code>&#x5982;&#x679C;&#x4F60;&#x7684;&#x7CFB;&#x7EDF;&#x652F;&#x6301;&#x975E;&#x6807;&#x51C6;&#x51B2;&#x6D17;&#x6570;&#x5B57;&#x548C;&#x5B83;&#x6210;&#x529F;&#x914D;&#x7F6E;&#x5237;&#x65B0;&#x975E;&#x6807;&#x51C6;&#x6A21;&#x5F0F;&#x3002;<code>set_flush_denormal&#xFF08;&#xFF09;</code>
&#x4EC5;&#x652F;&#x6301;x86&#x67B6;&#x6784;&#xFF0C;&#x652F;&#x6301;SSE3&#x3002;</p>
<p>Parameters</p>
<p><strong>&#x6A21;&#x5F0F;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in
Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a>&#xFF09; - &#x63A7;&#x5236;&#x662F;&#x5426;&#x542F;&#x7528;&#x51B2;&#x6D17;&#x53CD;&#x89C4;&#x8303;&#x6A21;&#x5F0F;&#x6216;&#x4E0D;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.set_flush_denormal(True)
True
&gt;&gt;&gt; torch.tensor([1e-323], dtype=torch.float64)
tensor([ 0.], dtype=torch.float64)
&gt;&gt;&gt; torch.set_flush_denormal(False)
True
&gt;&gt;&gt; torch.tensor([1e-323], dtype=torch.float64)
tensor(9.88131e-324 *
       [ 1.0000], dtype=torch.float64)
</code></pre><h3 id="&#x521B;&#x5EFA;&#x884C;&#x52A8;">&#x521B;&#x5EFA;&#x884C;&#x52A8;</h3>
<p>&#x6CE8;&#x610F;</p>
<p>&#x968F;&#x673A;&#x91C7;&#x6837;&#x751F;&#x6210;OPS&#x88AB;&#x4E0B;&#x5217;&#x51FA;&#x968F;&#x673A;&#x91C7;&#x6837; &#x548C;&#x5305;&#x62EC;&#xFF1A; <code>torch.rand&#xFF08;&#xFF09; ``torch.rand_like&#xFF08;&#xFF09; ``torch.randn&#xFF08;&#xFF09;
``torch.randn_like&#xFF08;&#xFF09; ``torch.randint&#xFF08;&#xFF09; ``torch&#x3002; randint_like&#xFF08;&#xFF09; ``
torch.randperm&#xFF08;&#xFF09;</code>&#x60A8;&#x4E5F;&#x53EF;&#x4EE5;&#x4F7F;&#x7528; <code>Torch &#x3002;&#x7A7A;&#xFF08;&#xFF09;</code>&#x4E0E; &#x5C31;&#x5730;&#x968F;&#x673A;&#x62BD;&#x6837; &#x7684;&#x65B9;&#x6CD5;&#x6765;&#x521B;&#x5EFA;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <code>torch.Tensor</code>
</a> s&#x7684;&#x503C;&#x4ECE;&#x66F4;&#x5E7F;&#x9614;&#x7684;&#x8303;&#x56F4;&#x5185;&#x5206;&#x5E03;&#x7684;&#x91C7;&#x6837;&#x3002;</p>
<p><code>torch.``tensor</code>( <em>data</em> , <em>dtype=None</em> , <em>device=None</em> ,
<em>requires_grad=False</em> , <em>pin_memory=False</em> ) &#x2192; Tensor</p>
<p>&#x6784;&#x9020;&#x5177;&#x6709;<code>&#x6570;&#x636E;</code>&#x7684;&#x5F20;&#x91CF;&#x3002;</p>
<p>&#x8B66;&#x544A;</p>
<p><code>torch.tensor&#xFF08;&#xFF09;</code>&#x603B;&#x662F;&#x526F;&#x672C;<code>&#x6570;&#x636E;</code>&#x3002;&#x5982;&#x679C;&#x4F60;&#x6709;&#x4E00;&#x4E2A;&#x5F20;&#x91CF;<code>&#x6570;&#x636E;</code>&#xFF0C;&#x5E76;&#x5E0C;&#x671B;&#x907F;&#x514D;&#x62F7;&#x8D1D;&#xFF0C;&#x4F7F;&#x7528;<a href="tensors.html#torch.Tensor.requires_grad_" title="torch.Tensor.requires_grad_"> <code>torch.Tensor.requires_grad_&#xFF08;&#xFF09;</code></a>&#x6216;<a href="tensors.html#torch.Tensor.detach" title="torch.Tensor.detach"> <code>torch.Tensor.detach&#xFF08;&#xFF09;</code>
</a>&#x3002;&#x5982;&#x679C;&#x4F60;&#x6709;&#x4E00;&#x4E2A;&#x4E0E;NumPy <code>ndarray</code>&#xFF0C;&#x5E76;&#x5E0C;&#x671B;&#x907F;&#x514D;&#x62F7;&#x8D1D;&#xFF0C;&#x4F7F;&#x7528; <code>torch.as_tensor&#xFF08;&#xFF09;</code>[HTG35&#x3002;</p>
<p>Warning</p>
<p>&#x5F53;&#x6570;&#x636E;&#x662F;&#x5F20;&#x91CF;&#xD7;&#xFF0C; <code>torch.tensor&#xFF08;&#xFF09;</code>&#x8BFB;&#x51FA;&#x4ECE;&#x4E0D;&#x7BA1;&#x5B83;&#x662F;&#x901A;&#x8FC7; &apos;&#x6570;&#x636E;&apos;&#xFF0C;&#x548C;&#x6784;&#x9020;&#x53F6;&#x53D8;&#x91CF;&#x3002;&#x56E0;&#x6B64;<code>torch.tensor&#xFF08;X&#xFF09;</code>&#x7B49;&#x4E8E;<code>x.clone&#xFF08;&#xFF09;&#x3002;&#x5206;&#x79BB;&#xFF08;&#xFF09;</code>&#x548C;<code>torch.tensor&#xFF08;X&#xFF0C; requires_grad =&#x771F;&#xFF09;</code>&#x7B49;&#x4E8E;<code>x.clone&#xFF08;&#xFF09;&#x3002;&#x5206;&#x79BB;&#xFF08;&#xFF09;&#x3002;requires_grad_&#xFF08;&#x771F;&#xFF09;</code>&#x3002;&#x4F7F;&#x7528;<code>&#x514B;&#x9686;&#xFF08;&#xFF09;</code>&#x548C;<code>&#x5206;&#x79BB;&#xFF08;&#xFF09;&#x7684;&#x5F53;&#x91CF;&#x7684;&#x5EFA;&#x8BAE;</code>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x6570;&#x636E;</strong> &#xFF08; <em>array_like</em> &#xFF09; - &#x4E3A;&#x5BF9;&#x5F20;&#x91CF;&#x521D;&#x59CB;&#x6570;&#x636E;&#x3002;&#x53EF;&#x8FDB;&#x884C;&#x5217;&#x8868;&#xFF0C;&#x5143;&#x7EC4;&#xFF0C;NumPy&#x7684;<code>ndarray</code>&#xFF0C;&#x6807;&#x91CF;&#xFF0C;&#x548C;&#x5176;&#x4ED6;&#x7C7B;&#x578B;&#x3002;</p>
</li>
<li><p><strong>DTYPE</strong> &#xFF08;<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"> <code>torch.dtype</code></a>&#xFF0C;&#x53EF;&#x9009;&#xFF09; - &#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x7684;&#x6240;&#x5E0C;&#x671B;&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;&#x5982;&#x679C;<code>&#x65E0;</code>&#xFF0C;&#x4ECE;<code>&#x6570;&#x636E;</code>&#x63A8;&#x65AD;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x3002;</p>
</li>
<li><p><strong>&#x88C5;&#x7F6E;</strong> &#xFF08;<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"> <code>torch.device</code></a>&#xFF0C;&#x53EF;&#x9009;&#xFF09; - &#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x7684;&#x6240;&#x9700;&#x7684;&#x8BBE;&#x5907;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;&#x5982;&#x679C;<code>&#x65E0;</code>&#xFF0C;&#x4F7F;&#x7528;&#x5F53;&#x524D;&#x8BBE;&#x5907;&#x7684;&#x9ED8;&#x8BA4;&#x5F20;&#x91CF;&#x7C7B;&#x578B;&#xFF08;&#x89C1; <code>torch.set_default_tensor_type&#xFF08;&#xFF09;</code>&#xFF09;&#x3002; <code>&#x88C5;&#x7F6E;</code>&#x5C06;&#x6210;&#x4E3A;CPU&#x5F20;&#x91CF;&#x7C7B;&#x578B;&#x7684;CPU&#x548C;&#x7528;&#x4E8E;CUDA&#x5F20;&#x91CF;&#x7C7B;&#x578B;&#x5F53;&#x524D;CUDA&#x8BBE;&#x5907;&#x3002;</p>
</li>
<li><p><strong>requires_grad</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5982;&#x679C;autograd&#x5E94;&#x8FD4;&#x56DE;&#x7684;&#x8BB0;&#x5F55;&#x5F20;&#x64CD;&#x4F5C;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x5047; [HTG13&#x3002;</code></p>
</li>
<li><p><strong>pin_memory</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5982;&#x679C;&#x8BBE;&#x7F6E;&#xFF0C;&#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x5C06;&#x5728;&#x56FA;&#x5B9A;&#x7684;&#x5185;&#x5B58;&#x5206;&#x914D;&#x3002;&#x53EA;&#x4E3A;CPU&#x5F20;&#x91CF;&#x5DE5;&#x4F5C;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x5047; [HTG13&#x3002;</code></p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]])
tensor([[ 0.1000,  1.2000],
        [ 2.2000,  3.1000],
        [ 4.9000,  5.2000]])

&gt;&gt;&gt; torch.tensor([0, 1])  # Type inference on data
tensor([ 0,  1])

&gt;&gt;&gt; torch.tensor([[0.11111, 0.222222, 0.3333333]],
                 dtype=torch.float64,
                 device=torch.device(&apos;cuda:0&apos;))  # creates a torch.cuda.DoubleTensor
tensor([[ 0.1111,  0.2222,  0.3333]], dtype=torch.float64, device=&apos;cuda:0&apos;)

&gt;&gt;&gt; torch.tensor(3.14159)  # Create a scalar (zero-dimensional tensor)
tensor(3.1416)

&gt;&gt;&gt; torch.tensor([])  # Create an empty tensor (of size (0,))
tensor([])
</code></pre><p><code>torch.``sparse_coo_tensor</code>( <em>indices</em> , <em>values</em> , <em>size=None</em> , <em>dtype=None</em>
, <em>device=None</em> , <em>requires_grad=False</em> ) &#x2192; Tensor</p>
<p>&#x6784;&#x9020;&#x5728;COO&#xFF08;rdinate&#xFF09;&#x683C;&#x5F0F;&#x7684;&#x7A00;&#x758F;&#x5F20;&#x91CF;&#x4E0E;&#x5728;&#x7ED9;&#x5B9A;&#x7684;<code>&#x6307;&#x6570;</code>&#x4E0E;&#x7ED9;&#x5B9A;&#x7684;<code>&#x503C;</code>&#x7684;&#x975E;&#x96F6;&#x5143;&#x7D20;&#x3002;&#x7A00;&#x758F;&#x5F20;&#x91CF;&#x53EF;&#x4EE5;&#x672A;&#x805A;&#x7ED3;&#x7684;&#xFF0C;&#x5728;&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x6709;&#x5728;&#x7D22;&#x5F15;&#x91CD;&#x590D;&#x5750;&#x6807;&#xFF0C;&#x8BE5;&#x7D22;&#x5F15;&#x7684;&#x503C;&#x662F;&#x6240;&#x6709;&#x91CD;&#x590D;&#x503C;&#x9879;&#x4E4B;&#x548C;&#xFF1A;<a href="https://pytorch.org/docs/stable/sparse.html" target="_blank"> torch.sparse
</a>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p>&#x7528;&#x4E8E;&#x5F20;&#x91CF;&#x521D;&#x59CB;&#x6570;&#x636E; - <strong>&#x6307;&#x6570;</strong> &#xFF08; <em>array_like</em> &#xFF09;&#x3002;&#x53EF;&#x8FDB;&#x884C;&#x5217;&#x8868;&#xFF0C;&#x5143;&#x7EC4;&#xFF0C;NumPy&#x7684;<code>ndarray</code>&#xFF0C;&#x6807;&#x91CF;&#xFF0C;&#x548C;&#x5176;&#x4ED6;&#x7C7B;&#x578B;&#x3002;&#x5C06;&#x88AB;&#x8F6C;&#x6362;&#x4E3A;<code>torch.LongTensor</code>&#x5185;&#x90E8;&#x3002;&#x8BE5;&#x6307;&#x6570;&#x662F;&#x5728;&#x77E9;&#x9635;&#x4E2D;&#x7684;&#x975E;&#x96F6;&#x503C;&#x7684;&#x5750;&#x6807;&#xFF0C;&#x4E14;&#x56E0;&#x6B64;&#x5E94;&#x662F;&#x4E8C;&#x7EF4;&#xFF0C;&#x5176;&#x4E2D;&#x7B2C;&#x4E00;&#x7EF4;&#x662F;&#x5F20;&#x91CF;&#x7684;&#x7EF4;&#x6570;&#xFF0C;&#x7B2C;&#x4E8C;&#x7EF4;&#x662F;&#x975E;&#x96F6;&#x503C;&#x7684;&#x6570;&#x76EE;&#x3002;</p>
</li>
<li><p>&#x7528;&#x4E8E;&#x5F20;&#x91CF;&#x7684;&#x521D;&#x59CB;&#x503C; - <strong>&#x503C;</strong> &#xFF08; <em>array_like</em> &#xFF09;&#x3002;&#x53EF;&#x8FDB;&#x884C;&#x5217;&#x8868;&#xFF0C;&#x5143;&#x7EC4;&#xFF0C;NumPy&#x7684;<code>ndarray</code>&#xFF0C;&#x6807;&#x91CF;&#xFF0C;&#x548C;&#x5176;&#x4ED6;&#x7C7B;&#x578B;&#x3002;</p>
</li>
<li><p><strong>&#x5927;&#x5C0F;</strong> &#xFF08;&#x5217;&#x8868;&#xFF0C;&#x5143;&#x7EC4;&#xFF0C;&#x6216;<code>torch.Size</code>&#xFF0C;&#x53EF;&#x9009;&#xFF09; - &#x7A00;&#x758F;&#x5F20;&#x91CF;&#x7684;&#x5927;&#x5C0F;&#x3002;&#x5982;&#x679C;&#x6CA1;&#x6709;&#x63D0;&#x4F9B;&#x89C4;&#x6A21;&#x5C06;&#x88AB;&#x63A8;&#x65AD;&#x4E3A;&#x6700;&#x5C0F;&#x5C3A;&#x5BF8;&#x5927;&#x5230;&#x8DB3;&#x4EE5;&#x5BB9;&#x7EB3;&#x6240;&#x6709;&#x975E;&#x96F6;&#x5143;&#x7D20;&#x3002;</p>
</li>
<li><p><strong>DTYPE</strong> &#xFF08;<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"> <code>torch.dtype</code></a>&#xFF0C;&#x53EF;&#x9009;&#xFF09; - &#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x7684;&#x6240;&#x5E0C;&#x671B;&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;&#x5982;&#x679C;&#x65E0;&#xFF0C;&#x4ECE;<code>&#x503C;</code>&#x63A8;&#x65AD;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x3002;</p>
</li>
<li><p><strong>&#x88C5;&#x7F6E;</strong> &#xFF08;<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"> <code>torch.device</code></a>&#xFF0C;&#x53EF;&#x9009;&#xFF09; - &#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x7684;&#x6240;&#x9700;&#x7684;&#x8BBE;&#x5907;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;&#x5982;&#x679C;&#x6CA1;&#x6709;&#xFF0C;&#x5219;&#x4F7F;&#x7528;&#x5F53;&#x524D;&#x8BBE;&#x5907;&#x7684;&#x9ED8;&#x8BA4;&#x5F20;&#x91CF;&#x7C7B;&#x578B;&#xFF08;&#x89C1; <code>torch.set_default_tensor_type&#xFF08;&#xFF09;</code>&#xFF09;&#x3002; <code>&#x88C5;&#x7F6E;</code>&#x5C06;&#x6210;&#x4E3A;CPU&#x5F20;&#x91CF;&#x7C7B;&#x578B;&#x7684;CPU&#x548C;&#x7528;&#x4E8E;CUDA&#x5F20;&#x91CF;&#x7C7B;&#x578B;&#x5F53;&#x524D;CUDA&#x8BBE;&#x5907;&#x3002;</p>
</li>
<li><p><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; i = torch.tensor([[0, 1, 1],
                      [2, 0, 2]])
&gt;&gt;&gt; v = torch.tensor([3, 4, 5], dtype=torch.float32)
&gt;&gt;&gt; torch.sparse_coo_tensor(i, v, [2, 4])
tensor(indices=tensor([[0, 1, 1],
                       [2, 0, 2]]),
       values=tensor([3., 4., 5.]),
       size=(2, 4), nnz=3, layout=torch.sparse_coo)

&gt;&gt;&gt; torch.sparse_coo_tensor(i, v)  # Shape inference
tensor(indices=tensor([[0, 1, 1],
                       [2, 0, 2]]),
       values=tensor([3., 4., 5.]),
       size=(2, 3), nnz=3, layout=torch.sparse_coo)

&gt;&gt;&gt; torch.sparse_coo_tensor(i, v, [2, 4],
                            dtype=torch.float64,
                            device=torch.device(&apos;cuda:0&apos;))
tensor(indices=tensor([[0, 1, 1],
                       [2, 0, 2]]),
       values=tensor([3., 4., 5.]),
       device=&apos;cuda:0&apos;, size=(2, 4), nnz=3, dtype=torch.float64,
       layout=torch.sparse_coo)

# Create an empty sparse tensor with the following invariants:
#   1. sparse_dim + dense_dim = len(SparseTensor.shape)
#   2. SparseTensor._indices().shape = (sparse_dim, nnz)
#   3. SparseTensor._values().shape = (nnz, SparseTensor.shape[sparse_dim:])
#
# For instance, to create an empty sparse tensor with nnz = 0, dense_dim = 0 and
# sparse_dim = 1 (hence indices is a 2D tensor of shape = (1, 0))
&gt;&gt;&gt; S = torch.sparse_coo_tensor(torch.empty([1, 0]), [], [1])
tensor(indices=tensor([], size=(1, 0)),
       values=tensor([], size=(0,)),
       size=(1,), nnz=0, layout=torch.sparse_coo)

# and to create an empty sparse tensor with nnz = 0, dense_dim = 1 and
# sparse_dim = 1
&gt;&gt;&gt; S = torch.sparse_coo_tensor(torch.empty([1, 0]), torch.empty([0, 2]), [1, 2])
tensor(indices=tensor([], size=(1, 0)),
       values=tensor([], size=(0, 2)),
       size=(1, 2), nnz=0, layout=torch.sparse_coo)
</code></pre><p><code>torch.``as_tensor</code>( <em>data</em> , <em>dtype=None</em> , <em>device=None</em> ) &#x2192; Tensor</p>
<p>&#x5C06;&#x6570;&#x636E;&#x8F6C;&#x6362;&#x6210; torch.Tensor &#x3002;&#x5982;&#x679C;&#x6570;&#x636E;&#x5DF2;&#x7ECF;&#x662F;&#x4E00;&#x4E2A;&#x5F20;&#x91CF;&#x7528;&#x76F8;&#x540C;&#x7684; DTYPE
&#x548C;&#x88C5;&#x7F6E;&#xFF0C;&#x6CA1;&#x6709;&#x526F;&#x672C;&#x5C06;&#x88AB;&#x6267;&#x884C;&#xFF0C;&#x5426;&#x5219;&#x4E00;&#x4E2A;&#x65B0;&#x7684;&#x5F20;&#x91CF;&#x5C06;&#x662F;&#x4E0E;&#x8BA1;&#x7B97;&#x56FE;&#x5F62;&#x8FD4;&#x56DE;&#x4FDD;&#x7559;&#x5982;&#x679C;&#x6570;&#x636E;&#x5F20;&#x91CF;&#x5177;&#x6709;<code>requires_grad =&#x771F;</code>&#x3002;&#x7C7B;&#x4F3C;&#x5730;&#xFF0C;&#x5982;&#x679C;&#x6570;&#x636E;&#x662F;&#x4E00;&#x4E2A;<code>ndarray</code>&#x7684;&#x76F8;&#x5E94;&#x7684; DTYPE &#x548C;&#x88C5;&#x7F6E;&#x662F;CPU&#xFF0C;&#x6CA1;&#x6709;&#x590D;&#x5236;&#x5C06;&#x88AB;&#x6267;&#x884C;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>data</strong> ( <em>array_like</em> ) &#x2013; Initial data for the tensor. Can be a list, tuple, NumPy <code>ndarray</code>, scalar, and other types.</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned tensor. Default: if <code>None</code>, infers data type from <code>data</code>.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <code>torch.set_default_tensor_type()</code>). <code>device</code>will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = numpy.array([1, 2, 3])
&gt;&gt;&gt; t = torch.as_tensor(a)
&gt;&gt;&gt; t
tensor([ 1,  2,  3])
&gt;&gt;&gt; t[0] = -1
&gt;&gt;&gt; a
array([-1,  2,  3])

&gt;&gt;&gt; a = numpy.array([1, 2, 3])
&gt;&gt;&gt; t = torch.as_tensor(a, device=torch.device(&apos;cuda&apos;))
&gt;&gt;&gt; t
tensor([ 1,  2,  3])
&gt;&gt;&gt; t[0] = -1
&gt;&gt;&gt; a
array([1,  2,  3])
</code></pre><p><code>torch.``as_strided</code>( <em>input</em> , <em>size</em> , <em>stride</em> , <em>storage_offset=0</em> ) &#x2192;
Tensor</p>
<p>&#x4E0E;&#x6307;&#x5B9A;&#x7684;<code>&#x5927;&#x5C0F;&#x521B;&#x5EFA;&#x73B0;&#x6709; torch.Tensor</code>&#x8F93;&#x5165; <code>&#x7684;&#x89C6;&#x56FE;</code>&#xFF0C;<code>&#x8DE8;&#x6B65;</code>&#x548C;<code>storage_offset</code>&#x3002;</p>
<p>Warning</p>
<p>&#x521B;&#x5EFA;&#x7684;&#x5F20;&#x91CF;&#x7684;&#x591A;&#x4E8E;&#x4E00;&#x4E2A;&#x7684;&#x5143;&#x4EF6;&#x53EF;&#x6307;&#x4EE3;&#x5355;&#x4E2A;&#x5B58;&#x50A8;&#x5668;&#x4F4D;&#x7F6E;&#x3002;&#x5176;&#x7ED3;&#x679C;&#x662F;&#xFF0C;&#x5C31;&#x5730;&#x64CD;&#x4F5C;&#xFF08;&#x7279;&#x522B;&#x662F;&#x90A3;&#x4E9B;&#x6709;&#x91CF;&#x5316;&#x7684;&#xFF09;&#x53EF;&#x80FD;&#x4F1A;&#x5BFC;&#x81F4;&#x4E0D;&#x6B63;&#x786E;&#x7684;&#x884C;&#x4E3A;&#x3002;&#x5982;&#x679C;&#x4F60;&#x9700;&#x8981;&#x5199;&#x5F20;&#x91CF;&#xFF0C;&#x8BF7;&#x5148;&#x514B;&#x9686;&#x5B83;&#x4EEC;&#x3002;</p>
<p>&#x8BB8;&#x591A;PyTorch&#x529F;&#x80FD;&#xFF0C;&#x5B83;&#x4F1A;&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x5F20;&#x91CF;&#x7684;&#x89C2;&#x70B9;&#xFF0C;&#x5728;&#x5185;&#x90E8;&#x4F7F;&#x7528;&#x6B64;&#x529F;&#x80FD;&#x6765;&#x5B9E;&#x73B0;&#x3002;&#x8FD9;&#x4E9B;&#x529F;&#x80FD;&#xFF0C;&#x5982;<a href="tensors.html#torch.Tensor.expand" title="torch.Tensor.expand"> <code>torch.Tensor.expand&#xFF08;&#xFF09;</code>
</a>&#xFF0C;&#x66F4;&#x5BB9;&#x6613;&#x9605;&#x8BFB;&#xFF0C;&#x56E0;&#x6B64;&#x66F4;&#x53EF;&#x53D6;&#x7684;&#x4F7F;&#x7528;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>&#x5927;&#x5C0F;</strong> &#xFF08;<a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>&#x5143;&#x7EC4;</em> </a> <em>&#x6216;</em> <em>&#x6574;&#x6570;</em> &#xFF09; - &#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684;&#x5F62;&#x72B6;</p>
</li>
<li><p><strong>&#x6B65;&#x5E45;</strong> &#xFF08;<a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>&#x5143;&#x7EC4;</em> </a> <em>&#x6216;</em> <em>&#x6574;&#x6570;</em> &#xFF09; - &#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684;&#x6B65;&#x5E45;</p>
</li>
<li><p><strong>storage_offset</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5728;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684;&#x5E95;&#x5C42;&#x5B58;&#x50A8;&#x7684;&#x504F;&#x79FB;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(3, 3)
&gt;&gt;&gt; x
tensor([[ 0.9039,  0.6291,  1.0795],
        [ 0.1586,  2.1939, -0.4900],
        [-0.1909, -0.7503,  1.9355]])
&gt;&gt;&gt; t = torch.as_strided(x, (2, 2), (1, 2))
&gt;&gt;&gt; t
tensor([[0.9039, 1.0795],
        [0.6291, 0.1586]])
&gt;&gt;&gt; t = torch.as_strided(x, (2, 2), (1, 2), 1)
tensor([[0.6291, 0.1586],
        [1.0795, 2.1939]])
</code></pre><p><code>torch.``from_numpy</code>( <em>ndarray</em> ) &#x2192; Tensor</p>
<p>&#x521B;&#x5EFA;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <code>&#x5F20;&#x91CF;</code></a>&#x4ECE;<a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="\(in NumPy v1.17\)" target="_blank"> <code>numpy.ndarray</code>
</a>&#x3002;</p>
<p>&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x548C;<code>ndarray</code>&#x5171;&#x4EAB;&#x76F8;&#x540C;&#x7684;&#x5B58;&#x50A8;&#x5668;&#x3002;&#x4FEE;&#x6539;&#x5BF9;&#x5F20;&#x91CF;&#x5C06;&#x53CD;&#x6620;&#x5728;<code>ndarray</code>&#xFF0C;&#x53CD;&#x4E4B;&#x4EA6;&#x7136;&#x3002;&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x662F;&#x4E0D;&#x53EF;&#x8C03;&#x6574;&#x5927;&#x5C0F;&#x3002;</p>
<p>&#x5B83;&#x76EE;&#x524D;&#x63A5;&#x53D7;<code>ndarray</code>&#x4E0E;<code>dtypes numpy.float64</code>&#xFF0C;<code>numpy.float32``numpy.float16</code>&#xFF0C;<code>numpy.int64</code>&#xFF0C;<code>numpy.int32</code>&#xFF0C;<code>numpy.int16</code>&#xFF0C;<code>numpy.int8</code>&#xFF0C;<code>numpy.uint8</code>&#x548C;<code>numpy.bool</code>&#x3002;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = numpy.array([1, 2, 3])
&gt;&gt;&gt; t = torch.from_numpy(a)
&gt;&gt;&gt; t
tensor([ 1,  2,  3])
&gt;&gt;&gt; t[0] = -1
&gt;&gt;&gt; a
array([-1,  2,  3])
</code></pre><p><code>torch.``zeros</code>( <em>*size</em> , <em>out=None</em> , <em>dtype=None</em> , <em>layout=torch.strided</em>
, <em>device=None</em> , <em>requires_grad=False</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x586B;&#x5145;&#x6709;&#x6807;&#x91CF;&#x503C; 0 &#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x7531;&#x53EF;&#x53D8;&#x53C2;&#x6570;<code>&#x5927;&#x5C0F;</code>&#x4E2D;&#x5B9A;&#x4E49;&#x7684;&#x5F62;&#x72B6;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x5927;&#x5C0F;</strong> &#xFF08; <em>INT ...</em> &#xFF09; - &#x5B9A;&#x4E49;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684;&#x5F62;&#x72B6;&#x7684;&#x6574;&#x6570;&#x5E8F;&#x5217;&#x3002;&#x53EF;&#x4EE5;&#x7684;&#x53C2;&#x6570;&#x4E2A;&#x6570;&#x53EF;&#x53D8;&#x6216;&#x7C7B;&#x4F3C;&#x7684;&#x5217;&#x8868;&#x6216;&#x5143;&#x7EC4;&#x7684;&#x96C6;&#x5408;&#x3002;</p>
</li>
<li><p><strong>OUT</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x8F93;&#x51FA;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>DTYPE</strong> &#xFF08;<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"> <code>torch.dtype</code></a>&#xFF0C;&#x53EF;&#x9009;&#xFF09; - &#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x7684;&#x6240;&#x5E0C;&#x671B;&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;&#x5982;&#x679C;<code>&#x65E0;</code>&#xFF0C;&#x4F7F;&#x7528;&#x5168;&#x5C40;&#x9ED8;&#x8BA4;&#x8BBE;&#x7F6E;&#xFF08;&#x89C1; <code>torch.set_default_tensor_type&#xFF08;&#xFF09;</code>&#xFF09;&#x3002;</p>
</li>
<li><p><strong>&#x5E03;&#x5C40;</strong> &#xFF08;<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"> <code>torch.layout</code></a>&#xFF0C;&#x53EF;&#x9009;&#xFF09; - &#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x7684;&#x6240;&#x9700;&#x5E03;&#x5C40;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;torch.strided ``&#x3002;</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <code>torch.set_default_tensor_type()</code>). <code>device</code>will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li><p><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.zeros(2, 3)
tensor([[ 0.,  0.,  0.],
        [ 0.,  0.,  0.]])

&gt;&gt;&gt; torch.zeros(5)
tensor([ 0.,  0.,  0.,  0.,  0.])
</code></pre><p><code>torch.``zeros_like</code>( <em>input</em> , <em>dtype=None</em> , <em>layout=None</em> , <em>device=None</em> ,
<em>requires_grad=False</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x586B;&#x5145;&#x6709;&#x6807;&#x91CF;&#x503C; 0 &#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x4EE5;&#x76F8;&#x540C;&#x7684;&#x5927;&#x5C0F;&#x4E3A;<code>&#x8F93;&#x5165;</code>&#x3002; <code>torch.zeros_like&#xFF08;&#x8F93;&#x5165;&#xFF09;</code>&#x7B49;&#x4E8E;<code>torch.zeros&#xFF08;input.size&#xFF08;&#xFF09;&#xFF0C; D&#x578B;&#x7EC6;&#x80DE;= input.dtype&#xFF0C; &#x5E03;&#x5C40;= input.layout&#xFF0C; &#x8BBE;&#x5907;=
input.device&#xFF09;</code>&#x3002;</p>
<p>Warning</p>
<p>&#x5982;&#x4E3A;0.4&#xFF0C;&#x8BE5;&#x529F;&#x80FD;&#x4E0D;&#x652F;&#x6301;<code>OUT</code>&#x5173;&#x952E;&#x5B57;&#x3002;&#x4F5C;&#x4E3A;&#x66FF;&#x4EE3;&#x65B9;&#x6848;&#xFF0C;&#x65E7;<code>torch.zeros_like&#xFF08;&#x8F93;&#x5165;&#xFF0C; OUT =&#x8F93;&#x51FA;&#xFF09;</code>&#x7B49;&#x4E8E;<code>torch.zeros&#xFF08;input.size &#xFF08;&#xFF09;&#xFF0C; OUT =&#x8F93;&#x51FA;&#xFF09;</code>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - <code>&#x8F93;&#x5165;&#x7684;&#x5927;&#x5C0F;</code>&#x5C06;&#x786E;&#x5B9A;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684;&#x5927;&#x5C0F;</p>
</li>
<li><p><strong>DTYPE</strong> &#xFF08;<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"> <code>torch.dtype</code></a>&#xFF0C;&#x53EF;&#x9009;&#xFF09; - &#x6240;&#x9700;&#x7684;&#x6570;&#x636E;&#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x7684;&#x7C7B;&#x578B;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;&#x5982;&#x679C;<code>&#x65E0;</code>&#xFF0C;&#x9ED8;&#x8BA4;&#x4E3A;<code>&#x8F93;&#x5165;</code>&#x7684;D&#x578B;&#x3002;</p>
</li>
<li><p><strong>&#x5E03;&#x5C40;</strong> &#xFF08;<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"> <code>torch.layout</code></a>&#xFF0C;&#x53EF;&#x9009;&#xFF09; - &#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x7684;&#x6240;&#x9700;&#x5E03;&#x5C40;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;&#x5982;&#x679C;<code>&#x65E0;</code>&#xFF0C;&#x9ED8;&#x8BA4;&#x4E3A;<code>&#x8F93;&#x5165;</code>&#x5E03;&#x5C40;&#x3002;</p>
</li>
<li><p><strong>&#x88C5;&#x7F6E;</strong> &#xFF08;<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"> <code>torch.device</code></a>&#xFF0C;&#x53EF;&#x9009;&#xFF09; - &#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x7684;&#x6240;&#x9700;&#x7684;&#x8BBE;&#x5907;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;&#x5982;&#x679C;<code>&#x65E0;</code>&#xFF0C;&#x9ED8;&#x8BA4;&#x4E3A;<code>&#x8F93;&#x5165;</code>&#x8BE5;&#x8BBE;&#x5907;&#x3002;</p>
</li>
<li><p><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; input = torch.empty(2, 3)
&gt;&gt;&gt; torch.zeros_like(input)
tensor([[ 0.,  0.,  0.],
        [ 0.,  0.,  0.]])
</code></pre><p><code>torch.``ones</code>( <em>*size</em> , <em>out=None</em> , <em>dtype=None</em> , <em>layout=torch.strided</em> ,
<em>device=None</em> , <em>requires_grad=False</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x586B;&#x5145;&#x6709;&#x6807;&#x91CF;&#x503C; 1 &#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x7531;&#x53EF;&#x53D8;&#x53C2;&#x6570;<code>&#x5927;&#x5C0F;</code>&#x4E2D;&#x5B9A;&#x4E49;&#x7684;&#x5F62;&#x72B6;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>size</strong> ( <em>int...</em> ) &#x2013; a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <code>torch.set_default_tensor_type()</code>).</p>
</li>
<li><p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <code>torch.set_default_tensor_type()</code>). <code>device</code>will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li><p><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.ones(2, 3)
tensor([[ 1.,  1.,  1.],
        [ 1.,  1.,  1.]])

&gt;&gt;&gt; torch.ones(5)
tensor([ 1.,  1.,  1.,  1.,  1.])
</code></pre><p><code>torch.``ones_like</code>( <em>input</em> , <em>dtype=None</em> , <em>layout=None</em> , <em>device=None</em> ,
<em>requires_grad=False</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x586B;&#x5145;&#x6709;&#x6807;&#x91CF;&#x503C; 1 &#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x4EE5;&#x76F8;&#x540C;&#x7684;&#x5927;&#x5C0F;&#x4E3A;<code>&#x8F93;&#x5165;</code>&#x3002; <code>torch.ones_like&#xFF08;&#x8F93;&#x5165;&#xFF09;</code>&#x7B49;&#x4E8E;<code>torch.ones&#xFF08;input.size&#xFF08;&#xFF09;&#xFF0C; D&#x578B;&#x7EC6;&#x80DE;= input.dtype&#xFF0C; &#x5E03;&#x5C40;= input.layout&#xFF0C; &#x8BBE;&#x5907;=
input.device&#xFF09;</code>&#x3002;</p>
<p>Warning</p>
<p>&#x5982;&#x4E3A;0.4&#xFF0C;&#x8BE5;&#x529F;&#x80FD;&#x4E0D;&#x652F;&#x6301;<code>OUT</code>&#x5173;&#x952E;&#x5B57;&#x3002;&#x4F5C;&#x4E3A;&#x66FF;&#x4EE3;&#x65B9;&#x6848;&#xFF0C;&#x65E7;<code>torch.ones_like&#xFF08;&#x8F93;&#x5165;&#xFF0C; OUT =&#x8F93;&#x51FA;&#xFF09;</code>&#x7B49;&#x4E8E;<code>torch.ones&#xFF08;input.size &#xFF08;&#xFF09;&#xFF0C; OUT =&#x8F93;&#x51FA;&#xFF09;</code>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the size of <code>input</code>will determine size of the output tensor</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned Tensor. Default: if <code>None</code>, defaults to the dtype of <code>input</code>.</p>
</li>
<li><p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned tensor. Default: if <code>None</code>, defaults to the layout of <code>input</code>.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, defaults to the device of <code>input</code>.</p>
</li>
<li><p><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; input = torch.empty(2, 3)
&gt;&gt;&gt; torch.ones_like(input)
tensor([[ 1.,  1.,  1.],
        [ 1.,  1.,  1.]])
</code></pre><p><code>torch.``arange</code>( <em>start=0</em> , <em>end</em> , <em>step=1</em> , <em>out=None</em> , <em>dtype=None</em> ,
<em>layout=torch.strided</em> , <em>device=None</em> , <em>requires_grad=False</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x7684;&#x4E00;&#x4E2A;1-d&#x5F20;&#x91CF;&#x5927;&#x5C0F; &#x2308; &#x7ED3;&#x675F; -  &#x5F00;&#x59CB; &#x6B65;&#x9AA4; &#x2309; \&#x5DE6;\ lceil \&#x538B;&#x88C2;{\&#x6587;&#x672C;{&#x7AEF;} - \&#x6587;&#x672C;{&#x5F00;&#x59CB;}} {\&#x6587;&#x672C;{&#x6B65;&#x9AA4;}} \&#x53F3;\
rceil  &#x2308;  &#x6B65;&#x9AA4; &#x7AEF; -  &#x5F00;&#x59CB; &#x2309; &#x4E0E;&#x6765;&#x81EA;&#x95F4;&#x9694;&#x503C;<code>[&#x5F00;&#x59CB;&#xFF0C; &#x7AEF;&#xFF09;</code>&#x4E0E;&#x666E;&#x901A;&#x5DEE;&#x5206;&#x91C7;&#x53D6;<code>&#x6B65;&#x9AA4;</code>&#x4ECE;&#x5F00;&#x59CB;&#x542F;&#x52A8;&#x3002;</p>
<p>;&#x6CE8;&#x610F;&#xFF0C;&#x975E;&#x6574;&#x6570;<code>&#x6B65;&#x9AA4;</code>&#x9488;&#x5BF9;<code>&#x7ED3;&#x675F;</code>&#x6BD4;&#x8F83;&#x65F6;&#x53D7;&#x5230;&#x6D6E;&#x70B9;&#x820D;&#x5165;&#x8BEF;&#x5DEE;&#xFF0C;&#x4EE5;&#x907F;&#x514D;&#x4E0D;&#x4E00;&#x81F4;&#xFF0C;&#x6211;&#x4EEC;&#x5EFA;&#x8BAE;&#x589E;&#x52A0;&#x4E00;&#x4E2A;&#x5C0F;&#x7684;&#x3B5;-&#x4E3A;<code>&#x5728;&#x8FD9;&#x6837;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#x7ED3;&#x675F;</code>&#x3002;</p>
<p>outi+1=outi+step\text{out}<em>NaN = \text{out}</em>{i} + \text{step}
outi+1&#x200B;=outi&#x200B;+step</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x5F00;&#x59CB;</strong> &#xFF08; <em>&#x53F7;&#x7801;</em> &#xFF09; - &#x4E3A;&#x5BF9;&#x8BBE;&#x5B9A;&#x70B9;&#x7684;&#x521D;&#x59CB;&#x503C;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>0</code>&#x3002;</p>
</li>
<li><p><strong>&#x7ED3;&#x675F;</strong> &#xFF08; <em>&#x53F7;&#x7801;</em> &#xFF09; - &#x4E3A;&#x5BF9;&#x8BBE;&#x5B9A;&#x70B9;&#x7684;&#x7ED3;&#x675F;&#x503C;</p>
</li>
<li><p><strong>&#x6B65;&#x9AA4;</strong> &#xFF08; <em>&#x53F7;&#x7801;</em> &#xFF09; - &#x6BCF;&#x4E00;&#x5BF9;&#x76F8;&#x90BB;&#x7684;&#x70B9;&#x4E4B;&#x95F4;&#x7684;&#x95F4;&#x9699;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>1</code>&#x3002;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
<li><p><strong>DTYPE</strong> &#xFF08;<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"> <code>torch.dtype</code></a>&#xFF0C;&#x53EF;&#x9009;&#xFF09; - &#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x7684;&#x6240;&#x5E0C;&#x671B;&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;&#x5982;&#x679C;<code>&#x65E0;</code>&#xFF0C;&#x4F7F;&#x7528;&#x5168;&#x5C40;&#x9ED8;&#x8BA4;&#x8BBE;&#x7F6E;&#xFF08;&#x89C1; <code>torch.set_default_tensor_type&#xFF08;&#xFF09;</code>&#xFF09;&#x3002;&#x5982;&#x679C; D&#x578B;&#x7EC6;&#x80DE;&#x6CA1;&#x6709;&#x7ED9;&#x51FA;&#xFF0C;&#x63A8;&#x65AD;&#x4ECE;&#x5176;&#x4ED6;&#x8F93;&#x5165;&#x53C2;&#x6570;&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x3002;&#x5982;&#x679C;&#x6709;&#x4EFB;&#x4F55;&#x7684;&#x5F00;&#x59CB;&#xFF0C;&#x7ED3;&#x675F;&#x6216;&#x505C;&#x6B62;&#x662F;&#x6D6E;&#x70B9;&#x65F6;&#xFF0C; DTYPE &#x88AB;&#x63A8;&#x65AD;&#x4E3A;&#x9ED8;&#x8BA4;D&#x578B;&#xFF0C;&#x8BF7;&#x53C2;&#x9605; <code>get_default_dtype&#xFF08;&#xFF09;</code>&#x3002;&#x5426;&#x5219;&#xFF0C; DTYPE &#x88AB;&#x63A8;&#x65AD;&#x4E3A; torch.int64 &#x3002;</p>
</li>
<li><p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <code>torch.set_default_tensor_type()</code>). <code>device</code>will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li><p><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.arange(5)
tensor([ 0,  1,  2,  3,  4])
&gt;&gt;&gt; torch.arange(1, 4)
tensor([ 1,  2,  3])
&gt;&gt;&gt; torch.arange(1, 2.5, 0.5)
tensor([ 1.0000,  1.5000,  2.0000])
</code></pre><p><code>torch.``range</code>( <em>start=0</em> , <em>end</em> , <em>step=1</em> , <em>out=None</em> , <em>dtype=None</em> ,
<em>layout=torch.strided</em> , <em>device=None</em> , <em>requires_grad=False</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x7684;&#x4E00;&#x4E2A;1-d&#x5F20;&#x91CF;&#x5927;&#x5C0F; &#x230A; &#x7ED3;&#x675F; -  &#x5F00;&#x59CB; &#x6B65;&#x9AA4; &#x230B; +  1  \&#x5DE6;\ lfloor \&#x538B;&#x88C2;{\&#x6587;&#x672C;{&#x7AEF;} - \&#x6587;&#x672C;{&#x5F00;&#x59CB;}}
{\&#x6587;&#x672C;{&#x6B65;&#x9AA4;}} \&#x53F3;\ rfloor + 1  &#x230A; &#x6B65;&#x9AA4; &#x7ED3;&#x675F; -  &#x5F00;&#x59CB; [HTG9 5]  &#x230B; +  1  &#x4ECE;<code>&#x503C;&#x5F00;&#x59CB;</code>&#x81F3;<code>&#x7ED3;&#x675F;</code>&#x4E0E;&#x6B65;&#x9AA4;<code>&#x6B65;&#x9AA4;</code>&#x3002;&#x6B65;&#x9AA4;&#x662F;&#x5728;&#x5F20;&#x91CF;&#x7684;&#x4E24;&#x4E2A;&#x503C;&#x4E4B;&#x95F4;&#x7684;&#x5DEE;&#x8DDD;&#x3002;</p>
<p>outi+1=outi+step.\text{out}_{i+1} = \text{out}_i + \text{step}.
outi+1&#x200B;=outi&#x200B;+step.</p>
<p>Warning</p>
<p>&#x6B64;&#x529F;&#x80FD;&#x6709;&#x5229;&#x4E8E;&#x5F03;&#x7528;<code>torch.arange&#xFF08;&#xFF09;</code>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x5F00;&#x59CB;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a>&#xFF09; - &#x4E3A;&#x5BF9;&#x8BBE;&#x5B9A;&#x70B9;&#x7684;&#x521D;&#x59CB;&#x503C;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>0</code>&#x3002;</p>
</li>
<li><p>&#x5BF9;&#x4E8E;&#x8BE5;&#x7EC4;&#x70B9;&#x7684;&#x7ED3;&#x675F;&#x503C; - <strong>&#x7AEF;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a>&#xFF09;</p>
</li>
<li><p><strong>&#x6B65;&#x9AA4;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a>&#xFF09; - &#x6BCF;&#x4E00;&#x5BF9;&#x76F8;&#x90BB;&#x7684;&#x70B9;&#x4E4B;&#x95F4;&#x7684;&#x95F4;&#x9699;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>1</code>&#x3002;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <code>torch.set_default_tensor_type()</code>). If dtype is not given, infer the data type from the other input arguments. If any of start, end, or stop are floating-point, the dtype is inferred to be the default dtype, see <code>get_default_dtype()</code>. Otherwise, the dtype is inferred to be torch.int64.</p>
</li>
<li><p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <code>torch.set_default_tensor_type()</code>). <code>device</code>will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li><p><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.range(1, 4)
tensor([ 1.,  2.,  3.,  4.])
&gt;&gt;&gt; torch.range(1, 4, 0.5)
tensor([ 1.0000,  1.5000,  2.0000,  2.5000,  3.0000,  3.5000,  4.0000])
</code></pre><p><code>torch.``linspace</code>( <em>start</em> , <em>end</em> , <em>steps=100</em> , <em>out=None</em> , <em>dtype=None</em>
, <em>layout=torch.strided</em> , <em>device=None</em> , <em>requires_grad=False</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x6B65;&#x9AA4;<code>&#x4E00;&#x7EF4;&#x5F20;&#x91CF;</code>&#x4E4B;&#x95F4;<code>&#x7B49;&#x8DDD;&#x70B9;&#x5F00;&#x59CB;</code>&#x548C;<code>&#x7ED3;&#x675F;</code>&#x3002;</p>
<p>&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x662F;&#x5927;&#x5C0F;<code>&#x6B65;&#x9AA4;</code>1-d&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p>&#x5BF9;&#x4E8E;&#x8BE5;&#x7EC4;&#x70B9;&#x7684;&#x521D;&#x59CB;&#x503C; - <strong>&#x5F00;&#x59CB;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a>&#xFF09;</p>
</li>
<li><p><strong>end</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>float</em></a>) &#x2013; the ending value for the set of points</p>
</li>
<li><p><strong>&#x6B65;&#x9AA4;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x70B9;&#x6570;&#x4E3A;<code>&#x5F00;&#x59CB;</code>&#x548C;<code>&#x7AEF;&#x4E4B;&#x95F4;&#x91C7;&#x6837;</code>&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>100</code>&#x3002;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <code>torch.set_default_tensor_type()</code>).</p>
</li>
<li><p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <code>torch.set_default_tensor_type()</code>). <code>device</code>will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li><p><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.linspace(3, 10, steps=5)
tensor([  3.0000,   4.7500,   6.5000,   8.2500,  10.0000])
&gt;&gt;&gt; torch.linspace(-10, 10, steps=5)
tensor([-10.,  -5.,   0.,   5.,  10.])
&gt;&gt;&gt; torch.linspace(start=-10, end=10, steps=5)
tensor([-10.,  -5.,   0.,   5.,  10.])
&gt;&gt;&gt; torch.linspace(start=-10, end=10, steps=1)
tensor([-10.])
</code></pre><p><code>torch.``logspace</code>( <em>start</em> , <em>end</em> , <em>steps=100</em> , <em>base=10.0</em> , <em>out=None</em> ,
<em>dtype=None</em> , <em>layout=torch.strided</em> , <em>device=None</em> , <em>requires_grad=False</em>
) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E0E;&#x78B1;<code>&#x78B1;&#x5BF9;&#x6570;&#x95F4;&#x9694;&#x7684;</code>&#x6B65;&#x9AA4; <code>&#x70B9;&#x7684;&#x4E00;&#x7EF4;&#x5F20;&#x91CF;</code>&#x4E4B;&#x95F4; &#x57FA; &#x5F00;&#x59CB; {\&#x6587;&#x672C;{&#x78B1;}} ^ {\&#x6587;&#x672C;{&#x5F00;&#x59CB;}}  &#x57FA; &#x5F00;&#x59CB; &#x548C; &#x57FA; &#x7ED3;&#x675F; {\&#x6587;&#x672C;{&#x78B1;}} ^
{\&#x6587;&#x672C;{&#x7AEF;}}  &#x57FA; &#x7ED3;&#x675F; &#x3002;</p>
<p>The output tensor is 1-D of size <code>steps</code>.</p>
<p>Parameters</p>
<ul>
<li><p><strong>start</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>float</em></a>) &#x2013; the starting value for the set of points</p>
</li>
<li><p><strong>end</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>float</em></a>) &#x2013; the ending value for the set of points</p>
</li>
<li><p><strong>steps</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a>) &#x2013; number of points to sample between <code>start</code>and <code>end</code>. Default: <code>100</code>.</p>
</li>
<li><p><strong>&#x57FA;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a>&#xFF09; - &#x7684;&#x5BF9;&#x6570;&#x51FD;&#x6570;&#x7684;&#x57FA;&#x7840;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>10.0</code>&#x3002;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <code>torch.set_default_tensor_type()</code>).</p>
</li>
<li><p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <code>torch.set_default_tensor_type()</code>). <code>device</code>will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li><p><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.logspace(start=-10, end=10, steps=5)
tensor([ 1.0000e-10,  1.0000e-05,  1.0000e+00,  1.0000e+05,  1.0000e+10])
&gt;&gt;&gt; torch.logspace(start=0.1, end=1.0, steps=5)
tensor([  1.2589,   2.1135,   3.5481,   5.9566,  10.0000])
&gt;&gt;&gt; torch.logspace(start=0.1, end=1.0, steps=1)
tensor([1.2589])
&gt;&gt;&gt; torch.logspace(start=2, end=2, steps=1, base=2)
tensor([4.0])
</code></pre><p><code>torch.``eye</code>( <em>n</em> , <em>m=None</em> , <em>out=None</em> , <em>dtype=None</em> ,
<em>layout=torch.strided</em> , <em>device=None</em> , <em>requires_grad=False</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x5BF9;&#x89D2;&#x7EBF;&#x4E0A;&#xFF0C;&#x4E00;&#x548C;&#x96F6;&#x5176;&#x4ED6;&#x5730;&#x65B9;2 d&#x5F20;&#x91CF;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>n&#x7684;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x7684;&#x884C;&#x6570;</p>
</li>
<li><p><strong>M</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5217;&#x7684;&#x9ED8;&#x8BA4;&#x6570;&#x76EE;&#x4E3A;<code>N</code></p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <code>torch.set_default_tensor_type()</code>).</p>
</li>
<li><p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <code>torch.set_default_tensor_type()</code>). <code>device</code>will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li><p><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Returns</p>
<p>A 2-d&#x5F20;&#x91CF;&#x7684;&#x5BF9;&#x89D2;&#x548C;&#x5176;&#x4ED6;&#x5730;&#x65B9;&#x7684;&#x96F6;&#x90A3;&#x4E9B;</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">&#x5F20;&#x91CF;</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.eye(3)
tensor([[ 1.,  0.,  0.],
        [ 0.,  1.,  0.],
        [ 0.,  0.,  1.]])
</code></pre><p><code>torch.``empty</code>( <em>*size</em> , <em>out=None</em> , <em>dtype=None</em> , <em>layout=torch.strided</em>
, <em>device=None</em> , <em>requires_grad=False</em> , <em>pin_memory=False</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x586B;&#x5145;&#x672A;&#x521D;&#x59CB;&#x5316;&#x7684;&#x6570;&#x636E;&#x7684;&#x5F20;&#x91CF;&#x3002;&#x5F20;&#x91CF;&#x7684;&#x5F62;&#x72B6;&#x7531;&#x53EF;&#x53D8;&#x53C2;&#x6570;<code>&#x5927;&#x5C0F;</code>&#x4E2D;&#x5B9A;&#x4E49;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>size</strong> ( <em>int...</em> ) &#x2013; a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <code>torch.set_default_tensor_type()</code>).</p>
</li>
<li><p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <code>torch.set_default_tensor_type()</code>). <code>device</code>will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li><p><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
<li><p><strong>pin_memory</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; If set, returned tensor would be allocated in the pinned memory. Works only for CPU tensors. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.empty(2, 3)
tensor(1.00000e-08 *
       [[ 6.3984,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000]])
</code></pre><p><code>torch.``empty_like</code>( <em>input</em> , <em>dtype=None</em> , <em>layout=None</em> , <em>device=None</em> ,
<em>requires_grad=False</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E0E;&#x76F8;&#x540C;&#x5C3A;&#x5BF8;<code>&#x8F93;&#x5165;</code>&#x4E00;&#x4E2A;&#x672A;&#x521D;&#x59CB;&#x5316;&#x7684;&#x5F20;&#x91CF;&#x3002; <code>torch.empty_like&#xFF08;&#x8F93;&#x5165;&#xFF09;</code>&#x7B49;&#x4E8E;<code>torch.empty&#xFF08;input.size&#xFF08;&#xFF09;&#xFF0C;
D&#x578B;&#x7EC6;&#x80DE;= input.dtype&#xFF0C; &#x5E03;&#x5C40;= input.layout&#xFF0C; &#x8BBE;&#x5907;= input.device&#xFF09;</code>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the size of <code>input</code>will determine size of the output tensor</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned Tensor. Default: if <code>None</code>, defaults to the dtype of <code>input</code>.</p>
</li>
<li><p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned tensor. Default: if <code>None</code>, defaults to the layout of <code>input</code>.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, defaults to the device of <code>input</code>.</p>
</li>
<li><p><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.empty((2,3), dtype=torch.int64)
tensor([[ 9.4064e+13,  2.8000e+01,  9.3493e+13],
        [ 7.5751e+18,  7.1428e+18,  7.5955e+18]])
</code></pre><p><code>torch.``empty_strided</code>( <em>size</em> , <em>stride</em> , <em>dtype=None</em> , <em>layout=None</em> ,
<em>device=None</em> , <em>requires_grad=False</em> , <em>pin_memory=False</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x586B;&#x5145;&#x672A;&#x521D;&#x59CB;&#x5316;&#x7684;&#x6570;&#x636E;&#x7684;&#x5F20;&#x91CF;&#x3002;&#x7684;&#x5F62;&#x72B6;&#x548C;&#x5F20;&#x91CF;&#x7684;&#x8FDB;&#x5C55;&#x662F;&#x7531;&#x53EF;&#x53D8;&#x53C2;&#x6570;<code>&#x5927;&#x5C0F;</code>&#x548C;<code>&#x6B65;&#x5E45;</code>&#x5206;&#x522B;&#x5B9A;&#x4E49;&#x3002; <code>torch.empty_strided&#xFF08;&#x5927;&#x5C0F;&#xFF0C; &#x6B65;&#x5E45;&#xFF09;</code>&#x7B49;&#x4EF7;&#x4E8E;.as_strided <code>torch.empty&#xFF08;&#x5927;&#x5C0F;&#xFF09;&#xFF08;&#x5927;&#x5C0F;&#xFF0C; &#x6B65;&#x5E45;&#xFF09;</code>&#x3002;</p>
<p>Warning</p>
<p>&#x6240;&#x521B;&#x5EFA;&#x7684;&#x5F20;&#x91CF;&#x7684;&#x591A;&#x4E8E;&#x4E00;&#x4E2A;&#x7684;&#x5143;&#x4EF6;&#x53EF;&#x6307;&#x4EE3;&#x5355;&#x4E2A;&#x5B58;&#x50A8;&#x5668;&#x4F4D;&#x7F6E;&#x3002;&#x5176;&#x7ED3;&#x679C;&#x662F;&#xFF0C;&#x5C31;&#x5730;&#x64CD;&#x4F5C;&#xFF08;&#x7279;&#x522B;&#x662F;&#x90A3;&#x4E9B;&#x6709;&#x91CF;&#x5316;&#x7684;&#xFF09;&#x53EF;&#x80FD;&#x4F1A;&#x5BFC;&#x81F4;&#x4E0D;&#x6B63;&#x786E;&#x7684;&#x884C;&#x4E3A;&#x3002;&#x5982;&#x679C;&#x4F60;&#x9700;&#x8981;&#x5199;&#x5F20;&#x91CF;&#xFF0C;&#x8BF7;&#x5148;&#x514B;&#x9686;&#x5B83;&#x4EEC;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x5927;&#x5C0F;</strong> &#xFF08; <em>&#x87D2;&#x7684;&#x5143;&#x7EC4;&#xFF1A;&#x6574;&#x6570;</em> &#xFF09; - &#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684;&#x5F62;&#x72B6;</p>
</li>
<li><p><strong>&#x6B65;&#x5E45;</strong> &#xFF08; <em>&#x87D2;&#x7684;&#x5143;&#x7EC4;&#xFF1A;&#x6574;&#x6570;</em> &#xFF09; - &#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684;&#x6B65;&#x5E45;</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <code>torch.set_default_tensor_type()</code>).</p>
</li>
<li><p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <code>torch.set_default_tensor_type()</code>). <code>device</code>will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li><p><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
<li><p><strong>pin_memory</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; If set, returned tensor would be allocated in the pinned memory. Works only for CPU tensors. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.empty_strided((2, 3), (1, 2))
&gt;&gt;&gt; a
tensor([[8.9683e-44, 4.4842e-44, 5.1239e+07],
        [0.0000e+00, 0.0000e+00, 3.0705e-41]])
&gt;&gt;&gt; a.stride()
(1, 2)
&gt;&gt;&gt; a.size()
torch.Size([2, 3])
</code></pre><p><code>torch.``full</code>( <em>size</em> , <em>fill_value</em> , <em>out=None</em> , <em>dtype=None</em> ,
<em>layout=torch.strided</em> , <em>device=None</em> , <em>requires_grad=False</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x5145;&#x6EE1;<code>fill_value</code>&#x5927;&#x5C0F;<code>&#x5927;&#x5C0F;</code>&#x7684;&#x5F20;&#x91CF;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x5927;&#x5C0F;</strong> &#xFF08; <em>INT ...</em> &#xFF09; - &#x5217;&#x8868;&#xFF0C;&#x5143;&#x7EC4;&#xFF0C;&#x6216;<code>torch.Size</code>&#x5B9A;&#x4E49;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684;&#x5F62;&#x72B6;&#x7684;&#x6574;&#x6570;&#x3002;</p>
</li>
<li><p><strong>fill_value</strong> - &#x6570;&#x4EE5;&#x586B;&#x5145;&#x4E0E;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x3002;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <code>torch.set_default_tensor_type()</code>).</p>
</li>
<li><p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <code>torch.set_default_tensor_type()</code>). <code>device</code>will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li><p><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.full((2, 3), 3.141592)
tensor([[ 3.1416,  3.1416,  3.1416],
        [ 3.1416,  3.1416,  3.1416]])
</code></pre><p><code>torch.``full_like</code>( <em>input</em> , <em>fill_value</em> , <em>out=None</em> , <em>dtype=None</em> ,
<em>layout=torch.strided</em> , <em>device=None</em> , <em>requires_grad=False</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E0E;<code>&#x8F93;&#x5165;&#x586B;&#x5145;&#x6709;</code>fill_value<code>&#x5982;</code>&#x76F8;&#x540C;&#x5C3A;&#x5BF8;&#x7684;&#x5F20;&#x91CF;&#x3002; <code>torch.full_like&#xFF08;&#x8F93;&#x5165;&#xFF0C; fill_value&#xFF09;</code>&#x7B49;&#x4E8E;<code>torch.full&#xFF08;input.size&#xFF08;&#xFF09;&#xFF0C; fill_value&#xFF0C; D&#x578B;&#x7EC6;&#x80DE;= input.dtype&#xFF0C; &#x5E03;&#x5C40;= input.layout&#xFF0C; &#x8BBE;&#x5907;=
input.device&#xFF09;</code>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the size of <code>input</code>will determine size of the output tensor</p>
</li>
<li><p><strong>fill_value</strong> &#x2013; the number to fill the output tensor with.</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned Tensor. Default: if <code>None</code>, defaults to the dtype of <code>input</code>.</p>
</li>
<li><p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned tensor. Default: if <code>None</code>, defaults to the layout of <code>input</code>.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, defaults to the device of <code>input</code>.</p>
</li>
<li><p><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<h3 id="&#x7D22;&#x5F15;&#xFF0C;&#x5207;&#x7247;&#xFF0C;&#x52A0;&#x5165;&#xFF0C;&#x53D8;&#x5F02;&#x884C;&#x52A8;">&#x7D22;&#x5F15;&#xFF0C;&#x5207;&#x7247;&#xFF0C;&#x52A0;&#x5165;&#xFF0C;&#x53D8;&#x5F02;&#x884C;&#x52A8;</h3>
<p><code>torch.``cat</code>( <em>tensors</em> , <em>dim=0</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x4E32;&#x63A5;SEQ &#x5728;&#x7ED9;&#x5B9A;&#x7684;&#x5C3A;&#x5BF8;&#x5F20;&#x91CF;<code>&#x7ED9;&#x5B9A;&#x7684;&#x5E8F;&#x5217;&#x3002;&#x6240;&#x6709;&#x5F20;&#x91CF;&#x5FC5;&#x987B;&#x5177;&#x6709;&#x76F8;&#x540C;&#x7684;&#x5F62;&#x72B6;&#xFF08;&#x9664;&#x4E86;&#x5728;&#x4E32;&#x63A5;&#x7684;&#x5C3A;&#x5BF8;&#xFF09;&#x6216;&#x4E3A;&#x7A7A;&#x3002;</code></p>
<p><code>torch.cat&#xFF08;&#xFF09;</code>&#x53EF;&#x4EE5;&#x88AB;&#x770B;&#x4F5C;&#x662F;&#x5BF9; <code>torch.split&#xFF09;&#x7684;&#x9006;&#x64CD;&#x4F5C;&#xFF08;</code>&#x548C; <code>torch.chunk&#xFF08;&#xFF09;</code>&#x3002;</p>
<p><code>torch.cat&#xFF08;&#xFF09;</code>&#x53EF;&#x4EE5;&#x901A;&#x8FC7;&#x6700;&#x597D;&#x7684;&#x5B9E;&#x65BD;&#x4F8B;&#x7684;&#x7406;&#x89E3;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x5F20;&#x91CF;</strong> &#xFF08; <em>&#x5F20;&#x91CF;</em> &#x7684;&#x5E8F;&#x5217;&#xFF09; - &#x76F8;&#x540C;&#x7C7B;&#x578B;&#x7684;&#x5F20;&#x91CF;&#x7684;&#x4EFB;&#x4F55;&#x87D2;&#x5E8F;&#x5217;&#x3002;&#x63D0;&#x4F9B;&#x5FC5;&#x987B;&#x5177;&#x6709;&#x76F8;&#x540C;&#x7684;&#x5F62;&#x72B6;&#xFF0C;&#x9664;&#x4E86;&#x5728;&#x732B;&#x5C3A;&#x5BF8;&#x975E;&#x7A7A;&#x5F20;&#x91CF;&#x3002;</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5728;&#x5176;&#x4E0A;&#x5F20;&#x91CF;&#x88AB;&#x7EA7;&#x8054;&#x7684;&#x5C3A;&#x5BF8;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(2, 3)
&gt;&gt;&gt; x
tensor([[ 0.6580, -1.0969, -0.4614],
        [-0.1034, -0.5790,  0.1497]])
&gt;&gt;&gt; torch.cat((x, x, x), 0)
tensor([[ 0.6580, -1.0969, -0.4614],
        [-0.1034, -0.5790,  0.1497],
        [ 0.6580, -1.0969, -0.4614],
        [-0.1034, -0.5790,  0.1497],
        [ 0.6580, -1.0969, -0.4614],
        [-0.1034, -0.5790,  0.1497]])
&gt;&gt;&gt; torch.cat((x, x, x), 1)
tensor([[ 0.6580, -1.0969, -0.4614,  0.6580, -1.0969, -0.4614,  0.6580,
         -1.0969, -0.4614],
        [-0.1034, -0.5790,  0.1497, -0.1034, -0.5790,  0.1497, -0.1034,
         -0.5790,  0.1497]])
</code></pre><p><code>torch.``chunk</code>( <em>input</em> , <em>chunks</em> , <em>dim=0</em> ) &#x2192; List of Tensors</p>
<p>&#x62C6;&#x5206;&#x4E00;&#x4E2A;&#x5F20;&#x6210;&#x5927;&#x5757;&#x7684;&#x5177;&#x4F53;&#x6570;&#x91CF;&#x3002;</p>
<p>&#x6700;&#x540E;&#x4E00;&#x5757;&#x4F1A;&#x6BD4;&#x8F83;&#x5C0F;&#xFF0C;&#x5982;&#x679C;&#x6CBF;&#x7ED9;&#x5B9A;&#x5C3A;&#x5BF8;&#x7684;&#x5927;&#x5C0F;&#x5F20;<code>&#x6697;&#x6DE1;</code>&#x662F;&#x4E0D;&#x662F;<code>&#x5757;</code>&#x6574;&#x9664;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x5F20;&#x91CF;&#x6765;&#x5206;&#x5272;</p>
</li>
<li><p><strong>&#x5757;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x7EC4;&#x5757;&#x7684;&#x6570;&#x76EE;&#x8FD4;&#x56DE;</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x7EF4;&#x6CBF;&#x7740;&#x5206;&#x88C2;&#x5F20;&#x91CF;</p>
</li>
</ul>
<p><code>torch.``gather</code>( <em>input</em> , <em>dim</em> , <em>index</em> , <em>out=None</em> , <em>sparse_grad=False</em>
) &#x2192; Tensor</p>
<p>&#x96C6;&#x503C;&#x4E00;&#x8D77;&#x7531;&#x6697;&#x6DE1;&#x4E2D;&#x6307;&#x5B9A;&#x7684;&#x8F74;&#x3002;</p>
<p>&#x5BF9;&#x4E8E;3-d&#x5F20;&#x91CF;&#x7531;&#x6307;&#x5B9A;&#x7684;&#x8F93;&#x51FA;&#xFF1A;</p>
<pre><code>out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0
out[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1
out[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2
</code></pre><p>&#x5982;&#x679C;<code>&#x8F93;&#x5165;</code>&#x662F;&#x5177;&#x6709;n&#x7EF4;&#x5F20;&#x91CF;&#x5927;&#x5C0F; &#xFF08; &#xD7; 0  &#xFF0C; &#xD7; 1  &#x3002;[HTG27 &#x3002;]  &#xFF0C; &#xD7; i&#x7684; -  1  &#xFF0C; &#xD7; i&#x7684; &#xFF0C; &#xD7; i&#x7684; +  1
&#xFF0C; &#x3002; &#x3002; &#xFF0C; &#xD7; n&#x7684; -  1  &#xFF09; &#xFF08;X<em>0&#xFF0C;X_1 ...&#xFF0C;X</em> {I-1}&#xFF0C;X<em>I&#xFF0C;X</em> {I + 1}&#xFF0C;... &#x3002;&#xFF0C;X<em> {N-1}&#xFF09; &#xFF08; &#xD7;
0  &#xFF0C; &#xD7; 1  &#x3002;  &#x3002;  &#x3002;  &#xFF0C; &#xD7; i&#x7684; -  1  &#xFF0C; &#xD7; i&#x7684; &#xFF0C; &#xD7; i&#x7684; +  1  &#x200B;&#x200B;  &#xFF0C; &#x3002;  &#x3002;  &#x3002;  &#xFF0C; &#xD7; n&#x7684; -
1  &#xFF09; &#x548C;<code>&#x6697;&#x6DE1; =  i&#x7684;</code>&#xFF0C;&#x7136;&#x540E;<code>&#x7D22;&#x5F15;</code>&#x5FC5;&#x987B;&#x4E3A; n&#x7684; n&#x7684; n&#x7684; &#x7EF4;&#x5F20;&#x91CF;&#x4E0E;&#x5C3A;&#x5BF8; &#xFF08; &#xD7; [HTG37 8]  0  &#xFF0C; &#xD7; 1  &#xFF0C; &#x3002;  &#x3002;
&#x3002;  &#xFF0C; &#xD7; i&#x7684; -  1  &#xFF0C; Y  &#xFF0C; &#xD7; i&#x7684; +  1  &#xFF0C; &#x3002;  &#x3002;  &#x3002;  &#xFF0C; &#xD7; n&#x7684; -  1  &#xFF09; &#xFF08;X_0&#xFF0C;X_1&#xFF0C;...&#xFF0C;X</em>
{I-1}&#xFF0C;Y&#xFF0C;X<em> {I + 1}&#xFF0C;...&#xFF0C;X</em> {N-1}&#xFF09; &#xFF08; &#xD7; 0  &#xFF0C; &#xD7; 1  &#xFF0C; &#x3002;  &#x3002;  &#x3002;  &#xFF0C; &#xD7; i&#x7684; -  1  &#xFF0C; Y  &#xFF0C;
&#xD7; i&#x7684; +  1  &#xFF0C; &#x3002;  &#x3002;  &#x3002;  &#xFF0C; &#xD7; n&#x7684; -  1  &#xFF09; &#x5176;&#x4E2D; Y  &#x2265; 1  Y \ GEQ 1  Y  &#x2265; 1  &#x548C;<code>OUT</code>&#x5C06;&#x5177;&#x6709;&#x76F8;&#x540C;&#x7684;&#x5927;&#x5C0F;&#x4E3A;<code>&#x7D22;&#x5F15; [ HTG719&#x3002;</code></p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x6E90;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x7684;&#x8F74;&#xFF0C;&#x6CBF;&#x7740;&#x8BE5;&#x7D22;&#x5F15;</p>
</li>
<li><p><strong>&#x7D22;&#x5F15;</strong> &#xFF08; <em>LongTensor</em> &#xFF09; - &#x5143;&#x7D20;&#x7684;&#x7D22;&#x5F15;&#x6765;&#x6536;&#x96C6;</p>
</li>
<li><p><strong>OUT</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x76EE;&#x7684;&#x5730;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>sparse_grad</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5982;&#x679C;<code>&#x771F;</code>&#xFF0C;&#x68AF;&#x5EA6;WRT <code>&#x8F93;&#x5165;</code>&#x5C06;&#x662F;&#x4E00;&#x4E2A;&#x7A00;&#x758F;&#x5F20;&#x91CF;&#x3002;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; t = torch.tensor([[1,2],[3,4]])
&gt;&gt;&gt; torch.gather(t, 1, torch.tensor([[0,0],[1,0]]))
tensor([[ 1,  1],
        [ 4,  3]])
</code></pre><p><code>torch.``index_select</code>( <em>input</em> , <em>dim</em> , <em>index</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x4F7F;&#x7528;&#x4E2D;&#x7684;&#x6761;&#x76EE;<code>&#x7D22;&#x5F15;&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x65B0;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x7D22;&#x5F15;</code>&#x8F93;&#x5165; <code>&#x6CBF;&#x7740;&#x7EF4;&#x5EA6;&#x5F20;&#x91CF;</code>&#x6697;&#x6DE1; ``&#x5176;&#x662F; LongTensor &#x3002;</p>
<p>&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x5177;&#x6709;&#x76F8;&#x540C;&#x7684;&#x7EF4;&#x6570;&#x4E0E;&#x539F;&#x59CB;&#x5F20;&#x91CF;&#xFF08;<code>&#x8F93;&#x5165;</code>&#xFF09;&#x7684;&#x3002;&#x7684;<code>&#x6697;&#x6DE1;</code>&#x6B21;&#x5C3A;&#x5BF8;&#x6709;&#x5927;&#x5C0F;&#x4E3A;<code>&#x7684;&#x957F;&#x5EA6;&#x76F8;&#x540C;&#x7684;&#x7D22;&#x5F15;</code>;&#x5176;&#x5B83;&#x5C3A;&#x5BF8;&#x5177;&#x6709;&#x76F8;&#x540C;&#x7684;&#x5927;&#x5C0F;&#x4E0E;&#x5728;&#x539F;&#x59CB;&#x5F20;&#x91CF;&#x3002;</p>
<p>Note</p>
<p>&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x786E;&#x5B9E; <strong>&#x4E0D;&#x662F;</strong> &#x4F7F;&#x7528;&#x76F8;&#x540C;&#x7684;&#x5B58;&#x50A8;&#x4E0E;&#x539F;&#x5F20;&#x91CF;&#x3002;&#x5982;&#x679C;<code>&#x51FA;</code>&#x5177;&#x6709;&#x4E0E;&#x9884;&#x671F;&#x4E0D;&#x540C;&#x7684;&#x5F62;&#x72B6;&#xFF0C;&#x6211;&#x4EEC;&#x9ED8;&#x9ED8;&#x5730;&#x5C06;&#x5176;&#x66F4;&#x6539;&#x4E3A;&#x6B63;&#x786E;&#x7684;&#x5F62;&#x72B6;&#xFF0C;&#x5FC5;&#x8981;&#x65F6;&#x91CD;&#x65B0;&#x5206;&#x914D;&#x57FA;&#x7840;&#x5B58;&#x50A8;&#x7A7A;&#x95F4;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x7684;&#x5C3A;&#x5BF8;&#xFF0C;&#x5176;&#x4E2D;&#x6211;&#x4EEC;&#x7D22;&#x5F15;</p>
</li>
<li><p><strong>&#x7D22;&#x5F15;</strong> &#xFF08; <em>LongTensor</em> &#xFF09; - &#x5305;&#x542B;&#x7D22;&#x5F15;&#x5230;&#x7D22;&#x5F15;1-d&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(3, 4)
&gt;&gt;&gt; x
tensor([[ 0.1427,  0.0231, -0.5414, -1.0009],
        [-0.4664,  0.2647, -0.1228, -1.1068],
        [-1.1734, -0.6571,  0.7230, -0.6004]])
&gt;&gt;&gt; indices = torch.tensor([0, 2])
&gt;&gt;&gt; torch.index_select(x, 0, indices)
tensor([[ 0.1427,  0.0231, -0.5414, -1.0009],
        [-1.1734, -0.6571,  0.7230, -0.6004]])
&gt;&gt;&gt; torch.index_select(x, 1, indices)
tensor([[ 0.1427, -0.5414],
        [-0.4664, -0.1228],
        [-1.1734,  0.7230]])
</code></pre><p><code>torch.``masked_select</code>( <em>input</em> , <em>mask</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x65B0;&#x7684;1-d&#x5F20;&#x91CF;&#x7684;<code>&#x8F93;&#x5165;</code>&#x6839;&#x636E;&#x5E03;&#x5C14;&#x63A9;&#x6A21;&#x5F20;&#x91CF;<code>&#x63A9;&#x6A21;&#x54EA;&#x4E9B;&#x7D22;&#x5F15;</code>&#x5176;&#x662F; BoolTensor [ HTG9&#x3002;</p>
<p>&#x5728;<code>&#x9762;&#x5177;</code>&#x5F20;&#x91CF;&#x7684;&#x5F62;&#x72B6;&#x548C;<code>&#x8F93;&#x5165;</code>&#x5F20;&#x91CF;&#x4E0D;&#x9700;&#x8981;&#x5339;&#x914D;&#xFF0C;&#x4F46;&#x662F;&#x4ED6;&#x4EEC;&#x5FC5;&#x987B;<a href="notes/broadcasting.html#broadcasting-semantics"> broadcastable
</a>&#x3002;</p>
<p>Note</p>
<p>&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x5E76; <strong>&#x4E0D;</strong> &#x4F7F;&#x7528;&#x76F8;&#x540C;&#x7684;&#x5B58;&#x50A8;&#x4F5C;&#x4E3A;&#x539F;&#x59CB;&#x5F20;&#x91CF;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x8F93;&#x5165;&#x6570;&#x636E;</p>
</li>
<li><p><strong>&#x63A9;&#x6A21;</strong> &#xFF08;<a href="tensors.html#torch.BoolTensor" title="torch.BoolTensor"> <em>BoolTensor</em> </a>&#xFF09; - &#x5305;&#x542B;&#x5E03;&#x5C14;&#x63A9;&#x7801;&#x7D22;&#x5F15;&#x4E0E;&#x5F20;&#x529B;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(3, 4)
&gt;&gt;&gt; x
tensor([[ 0.3552, -2.3825, -0.8297,  0.3477],
        [-1.2035,  1.2252,  0.5002,  0.6248],
        [ 0.1307, -2.0608,  0.1244,  2.0139]])
&gt;&gt;&gt; mask = x.ge(0.5)
&gt;&gt;&gt; mask
tensor([[False, False, False, False],
        [False, True, True, True],
        [False, False, False, True]])
&gt;&gt;&gt; torch.masked_select(x, mask)
tensor([ 1.2252,  0.5002,  0.6248,  2.0139])
</code></pre><p><code>torch.``narrow</code>( <em>input</em> , <em>dim</em> , <em>start</em> , <em>length</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x65B0;&#x7684;&#x5F20;&#x91CF;&#x662F;<code>&#x8F93;&#x5165;</code>&#x5F20;&#x91CF;&#x7F29;&#x5C0F;&#x7248;&#x672C;&#x3002;&#x5C3A;&#x5BF8;<code>&#x6697;&#x6DE1;</code>&#x662F;&#x4ECE;<code>&#x8F93;&#x5165;&#x5F00;&#x59CB;</code>&#x81F3;<code>&#x5F00;&#x59CB; +  &#x957F;&#x5EA6;</code>&#x3002;&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x548C;<code>&#x8F93;&#x5165;</code>&#x5F20;&#x91CF;&#x5171;&#x4EAB;&#x540C;&#x4E00;&#x57FA;&#x7840;&#x5B58;&#x50A8;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x5F20;&#x91CF;&#x6765;&#x7F29;&#x5C0F;</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x6CBF;&#x5176;&#x5C3A;&#x5BF8;&#x7F29;&#x5C0F;</p>
</li>
<li><p><strong>&#x5F00;&#x59CB;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x7684;&#x8D77;&#x59CB;&#x5C3A;&#x5BF8;</p>
</li>
<li><p><strong>&#x957F;&#x5EA6;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x7684;&#x8DDD;&#x79BB;&#x5230;&#x7ED3;&#x675F;&#x5C3A;&#x5BF8;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
&gt;&gt;&gt; torch.narrow(x, 0, 0, 2)
tensor([[ 1,  2,  3],
        [ 4,  5,  6]])
&gt;&gt;&gt; torch.narrow(x, 1, 1, 2)
tensor([[ 2,  3],
        [ 5,  6],
        [ 8,  9]])
</code></pre><p><code>torch.``nonzero</code>( <em>input</em> , <em>*</em> , <em>out=None</em> , <em>as_tuple=False</em> ) &#x2192;
LongTensor or tuple of LongTensors</p>
<p>[HTG0&#x5F53; <code>as_tuple</code><strong>&#x662F;&#x5047;&#x6216;&#x4E0D;&#x7279;&#x5B9A;&#xFF1A;</strong></p>
<p>&#x8FD4;&#x56DE;&#x5305;&#x542B;&#x8F93;&#x5165;&#x7684;<code>&#x6240;&#x6709;&#x975E;&#x96F6;&#x5143;&#x7D20;</code>&#x7684;&#x7D22;&#x5F15;&#x7684;&#x5F20;&#x91CF;&#x3002;&#x7ED3;&#x679C;&#x4E2D;&#x7684;&#x6BCF;&#x884C;&#x5305;&#x542B;&#x4E00;&#x4E2A;&#x975E;&#x96F6;&#x5143;&#x7D20;&#x7684;&#x7D22;&#x5F15;&#x5728;<code>&#x8F93;&#x5165;</code>&#x3002;&#x5176;&#x7ED3;&#x679C;&#x662F;&#x5B57;&#x5178;&#x987A;&#x5E8F;&#x6392;&#x5E8F;&#xFF0C;&#x6700;&#x540E;&#x6307;&#x6570;&#x53D8;&#x5316;&#x6700;&#x5FEB;&#x7684;&#xFF08;C&#x98CE;&#x683C;&#xFF09;&#x3002;</p>
<p>&#x5982;&#x679C;<code>&#x8F93;&#x5165;</code>&#x5177;&#x6709; n&#x7684;&#x7684;&#x5C3A;&#x5BF8;&#xFF0C;&#x7136;&#x540E;&#x5C06;&#x6240;&#x5F97;&#x7684;&#x6307;&#x6570;&#x5F20;&#x91CF;<code>OUT</code>&#x662F;&#x5927;&#x5C0F;&#x4E3A; &#xFF08; Z  &#xD7; n&#x7684; &#xFF09; &#xFF08;Z \ n&#x6B21;&#xFF09; &#xFF08; Z  &#xD7; n&#x7684; &#xFF09; &#xFF0C;&#x5176;&#x4E2D; Z
Z  Z  &#x662F;&#x5728;<code>&#x8F93;&#x5165;&#x7684;&#x975E;&#x96F6;&#x5143;&#x7D20;&#x7684;&#x603B;&#x6570;</code>&#x5F20;&#x91CF;&#x3002;</p>
<p>[HTG0&#x5F53; <code>as_tuple</code><strong>&#x662F;&#x5426;&#x6210;&#x7ACB;&#xFF1A;</strong></p>
<p>&#x8FD4;&#x56DE;&#x7684;1-d&#x5F20;&#x91CF;&#x5728;<code>&#x8F93;&#x5165;&#x6BCF;&#x4E2A;&#x7EF4;&#x5EA6;&#x4E00;&#x4E2A;&#x5143;&#x7EC4;&#xFF0C;&#x4E00;&#x4E2A;</code>&#xFF0C;&#x5404;&#x542B;&#x6709;&#x7684;<code>&#x6240;&#x6709;&#x975E;&#x96F6;&#x5143;&#x7D20;&#x7684;&#x7D22;&#x5F15;&#xFF08;&#x8BE5;&#x7EF4;&#x5EA6;&#xFF09;&#x8F93;&#x5165;</code>&#x3002;</p>
<p>&#x5982;&#x679C;<code>&#x8F93;&#x5165;</code>&#x5177;&#x6709; n&#x7684;&#x7684;&#x5C3A;&#x5BF8;&#xFF0C;&#x7136;&#x540E;&#x5C06;&#x6240;&#x5F97;&#x7684;&#x5143;&#x7EC4;&#x5305;&#x542B; n&#x7684;&#x5927;&#x5C0F; Z &#xFF0C;&#x5176;&#x4E2D;[HTG10&#x7684;&#x5F20;&#x91CF;] Z &#x662F;&#x5728;<code>&#x8F93;&#x5165;</code>&#x5F20;&#x91CF;&#x7684;&#x975E;&#x96F6;&#x5143;&#x7D20;&#x7684;&#x603B;&#x6570;&#x3002;</p>
<p>&#x4F5C;&#x4E3A;&#x4E00;&#x79CD;&#x7279;&#x6B8A;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x5F53;<code>&#x8F93;&#x5165;</code>&#x5177;&#x6709;&#x96F6;&#x79CD;&#x5C3A;&#x5BF8;&#x548C;&#x975E;&#x96F6;&#x6807;&#x91CF;&#x503C;&#xFF0C;&#x5B83;&#x88AB;&#x89C6;&#x4E3A;&#x4E00;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x4E00;&#x7EF4;&#x5F20;&#x91CF;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>OUT</strong> &#xFF08; <em>LongTensor</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5305;&#x542B;&#x7D22;&#x5F15;&#x7684;&#x8F93;&#x51FA;&#x5F20;&#x91CF;</p>
</li>
</ul>
<p>Returns</p>
<p>&#x5982;&#x679C;<code>as_tuple</code>&#x662F;&#x5047;&#x7684;&#xFF0C;&#x542B;&#x6709;&#x7D22;&#x5F15;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x3002;&#x5982;&#x679C;<code>as_tuple</code>&#x4E3A;&#x771F;&#xFF0C;&#x4E00;&#x4E2A;1-d&#x5F20;&#x91CF;&#x9488;&#x5BF9;&#x6BCF;&#x4E2A;&#x7EF4;&#x5EA6;&#xFF0C;&#x6CBF;&#x5305;&#x542B;&#x8BE5;&#x7EF4;&#x5EA6;&#x7684;&#x6BCF;&#x4E2A;&#x975E;&#x96F6;&#x5143;&#x7D20;&#x7684;&#x7D22;&#x5F15;&#x3002;</p>
<p>Return type</p>
<p>LongTensor&#x6216;LongTensor&#x7684;&#x5143;&#x7EC4;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.nonzero(torch.tensor([1, 1, 1, 0, 1]))
tensor([[ 0],
        [ 1],
        [ 2],
        [ 4]])
&gt;&gt;&gt; torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0],
                                [0.0, 0.4, 0.0, 0.0],
                                [0.0, 0.0, 1.2, 0.0],
                                [0.0, 0.0, 0.0,-0.4]]))
tensor([[ 0,  0],
        [ 1,  1],
        [ 2,  2],
        [ 3,  3]])
&gt;&gt;&gt; torch.nonzero(torch.tensor([1, 1, 1, 0, 1]), as_tuple=True)
(tensor([0, 1, 2, 4]),)
&gt;&gt;&gt; torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0],
                                [0.0, 0.4, 0.0, 0.0],
                                [0.0, 0.0, 1.2, 0.0],
                                [0.0, 0.0, 0.0,-0.4]]), as_tuple=True)
(tensor([0, 1, 2, 3]), tensor([0, 1, 2, 3]))
&gt;&gt;&gt; torch.nonzero(torch.tensor(5), as_tuple=True)
(tensor([0]),)
</code></pre><p><code>torch.``reshape</code>( <em>input</em> , <em>shape</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E0E;&#x5143;&#x4EF6;&#x4E3A;<code>&#x8F93;&#x5165;</code>&#x76F8;&#x540C;&#x7684;&#x6570;&#x636E;&#x548C;&#x6570;&#x5B57;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x4F46;&#x968F;&#x7740;&#x7279;&#x5B9A;&#x7684;&#x5F62;&#x72B6;&#x3002;&#x5982;&#x679C;&#x53EF;&#x80FD;&#x7684;&#x8BDD;&#xFF0C;&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x5C06;&#x662F;<code>&#x8F93;&#x5165;</code>&#x7684;&#x56FE;&#x3002;&#x5426;&#x5219;&#xFF0C;&#x8FD9;&#x5C06;&#x662F;&#x4E00;&#x4E2A;&#x526F;&#x672C;&#x3002;&#x8FDE;&#x7EED;&#x7684;&#x6295;&#x5165;&#x548C;&#x517C;&#x5BB9;&#x7684;&#x8FDB;&#x6B65;&#x6295;&#x5165;&#x800C;&#x4E0D;&#x7528;&#x62F7;&#x8D1D;&#x8FDB;&#x884C;&#x5851;&#x5F62;&#xFF0C;&#x4F46;&#x4F60;&#x4E0D;&#x5E94;&#x8BE5;&#x4F9D;&#x8D56;&#x4E8E;&#x590D;&#x5236;&#x4E0E;&#x6536;&#x89C6;&#x884C;&#x4E3A;&#x3002;</p>
<p>&#x53C2;&#x89C1;<a href="tensors.html#torch.Tensor.view" title="torch.Tensor.view"> <code>torch.Tensor.view&#xFF08;&#xFF09;</code></a>&#x4E0A;&#x65F6;&#xFF0C;&#x5B83;&#x53EF;&#x4EE5;&#x8FD4;&#x56DE;&#x7684;&#x56FE;&#x3002;</p>
<p>&#x5355;&#x4E2A;&#x7EF4;&#x5EA6;&#x53EF;&#x4EE5;&#x662F;-1&#xFF0C;&#x5728;&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x5B83;&#x4ECE;&#x5269;&#x4F59;&#x7684;&#x5C3A;&#x5BF8;&#x548C;&#x5143;&#x4EF6;&#x7684;&#x5728;<code>&#x8F93;&#x5165;</code>&#x7684;&#x6570;&#x91CF;&#x63A8;&#x65AD;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x8981;&#x91CD;&#x5851;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>&#x5F62;&#x72B6;</strong> &#xFF08; <em>&#x87D2;&#x7684;&#x5143;&#x7EC4;&#xFF1A;&#x6574;&#x6570;</em> &#xFF09; - &#x65B0;&#x7684;&#x5F62;&#x72B6;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.arange(4.)
&gt;&gt;&gt; torch.reshape(a, (2, 2))
tensor([[ 0.,  1.],
        [ 2.,  3.]])
&gt;&gt;&gt; b = torch.tensor([[0, 1], [2, 3]])
&gt;&gt;&gt; torch.reshape(b, (-1,))
tensor([ 0,  1,  2,  3])
</code></pre><p><code>torch.``split</code>( <em>tensor</em> , <em>split_size_or_sections</em> , <em>dim=0</em>
)<a href="_modules/torch/functional.html#split">[source]</a></p>
<p>&#x62C6;&#x5206;&#x5F20;&#x6210;&#x5757;&#x3002;</p>
<p>&#x5982;&#x679C;<code>split_size_or_sections</code>&#x662F;&#x6574;&#x6570;&#x7C7B;&#x578B;&#xFF0C;&#x5219; <code>&#x5F20;&#x91CF;</code>&#x5C06;&#x88AB;&#x5206;&#x6210;&#x76F8;&#x7B49;&#x5927;&#x5C0F;&#x7684;&#x5757;&#xFF08;&#x5982;&#x679C;&#x53EF;&#x80FD;&#xFF09;
&#x3002;&#x6700;&#x540E;&#x4E00;&#x5757;&#x4F1A;&#x6BD4;&#x8F83;&#x5C0F;&#xFF0C;&#x5982;&#x679C;&#x6CBF;&#x7ED9;&#x5B9A;&#x5C3A;&#x5BF8;&#x7684;&#x5927;&#x5C0F;&#x5F20;<code>&#x6697;&#x6DE1;</code>&#x662F;&#x4E0D;&#x662F;<code>split_size</code>&#x6574;&#x9664;&#x3002;</p>
<p>&#x5982;&#x679C;<code>split_size_or_sections</code>&#x662F;&#x4E00;&#x4E2A;&#x5217;&#x8868;&#xFF0C;&#x7136;&#x540E; <code>&#x5F20;&#x91CF;</code>&#x5C06;&#x88AB;&#x5206;&#x5272;&#x4E3A;<code>LEN&#xFF08;&#x6839;&#x636E;</code>
split_size_or_sections<code>split_size_or_sections&#xFF09;</code>&#x4E0E;<code>&#x5927;&#x5C0F;&#x7684;&#x5757;&#x6697;&#x6DE1;</code>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x5F20;&#x91CF;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x5F20;&#x91CF;&#x6765;&#x5206;&#x5272;&#x3002;</p>
</li>
<li><p><strong>split_size_or_sections</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF09;&#x6216;</em> <em>&#xFF08;</em> <a href="https://docs.python.org/3/library/stdtypes.html#list" title="\(in Python v3.7\)" target="_blank"> <em>&#x5217;&#x8868;</em> </a> <em>&#xFF08;</em> <a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF09;</em> &#xFF09; - &#x5BF9;&#x4E8E;&#x6BCF;&#x4E2A;&#x5927;&#x5757;&#x5355;&#x4E2A;&#x5757;&#x6216;&#x5217;&#x8868;&#x5C3A;&#x5BF8;&#x7684;&#x5927;&#x5C0F;</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x7EF4;&#x6CBF;&#x7740;&#x5206;&#x88C2;&#x5F20;&#x91CF;&#x3002;</p>
</li>
</ul>
<p><code>torch.``squeeze</code>( <em>input</em> , <em>dim=None</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E0E;&#x9664;&#x53BB; &#x5927;&#x5C0F;&#x7684; 1&#x8F93;&#x5165;&#x7684;<code>&#x6240;&#x6709;&#x5C3A;&#x5BF8;&#x7684;&#x5F20;&#x91CF;&#x3002;</code></p>
<p>&#x4F8B;&#x5982;&#xFF0C;&#x5982;&#x679C;&#x8F93;&#x5165;&#x662F;&#x5F62;&#x72B6;&#x7684;&#xFF1A; &#xFF08; A  &#xD7; 1  &#xD7; B  &#xD7; C  &#xD7; 1  &#xD7; d  &#xFF09; &#xFF08;A \&#x500D;1 \&#x500D;&#x4E59;\ C&#x65F6;&#x4EE3;\&#x500D;1 \&#x500D;d&#xFF09; &#xFF08; A  &#xD7; 1
&#xD7; B  &#xD7; C  &#xD7; 1  &#xD7; d  &#xFF09; &#xFF0C;&#x5219; OUT &#x5F20;&#x91CF;&#x5C06;&#x662F;&#x5F62;&#x72B6;&#x7684;&#xFF1A; &#xFF08; A  &#xD7; B  &#xD7; C  &#xD7; d  &#xFF09; &#xFF08;A \&#x500D;&#x4E59;\ C&#x65F6;&#x4EE3;\&#x500D;d&#xFF09; &#xFF08;
A  &#xD7; B  &#xD7; C  &#xD7; d  &#xFF09; &#x3002;</p>
<p>&#x5F53;<code>&#x6697;&#x6DE1;</code>&#x7ED9;&#x51FA;&#xFF0C;&#x6324;&#x538B;&#x64CD;&#x4F5C;&#x4EC5;&#x5728;&#x7ED9;&#x5B9A;&#x7684;&#x5C3A;&#x5BF8;&#x5B8C;&#x6210;&#x3002;&#x5982;&#x679C;&#x8F93;&#x5165;&#x662F;&#x5F62;&#x72B6;&#x7684;&#xFF1A; &#xFF08; A  &#xD7; 1  &#xD7; B  &#xFF09; &#xFF08;A \&#x500D;1 \&#x500D;B&#xFF09; &#xFF08; A  &#xD7; 1  &#xD7; B
&#xFF09; &#xFF0C;<code>&#x6324;&#x538B;&#xFF08;&#x8F93;&#x5165;&#xFF0C; 0&#xFF09;</code>&#x79BB;&#x5F00;&#x5F20;&#x91CF;&#x4E0D;&#x53D8;&#xFF0C;&#x4F46;&#x662F;<code>&#x6324;&#x538B;&#xFF08;&#x8F93;&#x5165;&#xFF0C; 1&#xFF09;</code>&#x4F1A;&#x6324;&#x538B;&#x5F20;&#x91CF;&#x7684;&#x5F62;&#x72B6; &#xFF08; A  &#xD7; B  [H TG96]&#xFF09;  &#xFF08;A \&#x500D;B&#xFF09; &#xFF08;
A  &#xD7; B  &#xFF09; &#x3002;</p>
<p>Note</p>
<p>&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x80A1;&#x4E0E;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x7684;&#x5B58;&#x50A8;&#xFF0C;&#x6240;&#x4EE5;&#x6539;&#x53D8;&#x4E00;&#x4E2A;&#x7684;&#x5185;&#x5BB9;&#x4F1A;&#x6539;&#x53D8;&#x5176;&#x4ED6;&#x7684;&#x5185;&#x5BB9;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5982;&#x679C;&#x7ED9;&#x5B9A;&#x7684;&#xFF0C;&#x8BE5;&#x8F93;&#x5165;&#x5C06;&#x88AB;&#x53EA;&#x5728;&#x8FD9;&#x4E2A;&#x5C3A;&#x5BF8;&#x6324;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.zeros(2, 1, 2, 1, 2)
&gt;&gt;&gt; x.size()
torch.Size([2, 1, 2, 1, 2])
&gt;&gt;&gt; y = torch.squeeze(x)
&gt;&gt;&gt; y.size()
torch.Size([2, 2, 2])
&gt;&gt;&gt; y = torch.squeeze(x, 0)
&gt;&gt;&gt; y.size()
torch.Size([2, 1, 2, 1, 2])
&gt;&gt;&gt; y = torch.squeeze(x, 1)
&gt;&gt;&gt; y.size()
torch.Size([2, 2, 1, 2])
</code></pre><p><code>torch.``stack</code>( <em>tensors</em> , <em>dim=0</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x4E32;&#x63A5;&#x6CBF;&#x7740;&#x4E00;&#x4E2A;&#x65B0;&#x7684;&#x5C42;&#x9762;&#x5F20;&#x91CF;&#x7684;&#x5E8F;&#x5217;&#x3002;</p>
<p>&#x6240;&#x6709;&#x7684;&#x5F20;&#x91CF;&#x9700;&#x8981;&#x662F;&#x76F8;&#x540C;&#x5927;&#x5C0F;&#x7684;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x5F20;&#x91CF;</strong> &#xFF08; <em>&#x5F20;&#x91CF;&#x7684;&#x5E8F;&#x5217;</em> &#xFF09; - &#x5F20;&#x91CF;&#x7684;&#x5E8F;&#x5217;&#x6765;&#x8FDE;&#x63A5;</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x8981;&#x63D2;&#x5165;&#x7684;&#x5C3A;&#x5BF8;&#x3002;&#x5FC5;&#x987B;&#x662F;0&#x548C;&#x7EA7;&#x8054;&#x5F20;&#x91CF;&#x7684;&#x7EF4;&#x6570;&#xFF08;&#x542B;&#xFF09;&#x4E4B;&#x95F4;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p><code>torch.``t</code>( <em>input</em> ) &#x2192; Tensor</p>
<p>&#x9884;&#x8BA1;<code>&#x8F93;&#x5165;</code>&#x4E3A;&amp; LT ; = 2-d&#x5F20;&#x91CF;&#x548C;&#x8C03;&#x6362;&#x5C3A;&#x5BF8;0&#x548C;1&#x3002;</p>
<p>0-d&#x548C;1-d&#x5F20;&#x91CF;&#x8FD4;&#x56DE;&#xFF0C;&#x56E0;&#x4E3A;&#x5B83;&#x662F;&#x548C;2- d&#x5F20;&#x91CF;&#x53EF;&#x4EE5;&#x88AB;&#x770B;&#x4F5C;&#x662F;&#x4E00;&#x4E2A;&#x77ED;&#x624B;&#x529F;&#x80FD;&#x4E3A;<code>&#x8F6C;&#x7F6E;&#xFF08;&#x8F93;&#x5165;&#xFF0C; 0&#xFF0C; 1&#xFF09;</code>&#x3002;</p>
<p>Parameters</p>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input
tensor</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(())
&gt;&gt;&gt; x
tensor(0.1995)
&gt;&gt;&gt; torch.t(x)
tensor(0.1995)
&gt;&gt;&gt; x = torch.randn(3)
&gt;&gt;&gt; x
tensor([ 2.4320, -0.4608,  0.7702])
&gt;&gt;&gt; torch.t(x)
tensor([.2.4320,.-0.4608,..0.7702])
&gt;&gt;&gt; x = torch.randn(2, 3)
&gt;&gt;&gt; x
tensor([[ 0.4875,  0.9158, -0.5872],
        [ 0.3938, -0.6929,  0.6932]])
&gt;&gt;&gt; torch.t(x)
tensor([[ 0.4875,  0.3938],
        [ 0.9158, -0.6929],
        [-0.5872,  0.6932]])
</code></pre><p><code>torch.``take</code>( <em>input</em> , <em>index</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E0E;&#x8F93;&#x5165; &#x7684;<code>&#x5728;&#x7ED9;&#x5B9A;&#x7684;&#x7D22;&#x5F15;&#x7684;&#x5143;&#x7D20;&#x7684;&#x65B0;&#x7684;&#x5F20;&#x91CF;&#x3002;&#x5982;&#x540C;&#x5176;&#x88AB;&#x770B;&#x4F5C;&#x662F;&#x4E00;&#x4E2A;1-d&#x5F20;&#x91CF;&#x7684;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x5F97;&#x5230;&#x6CBB;&#x7597;&#x3002;&#x7ED3;&#x679C;&#x53D6;&#x76F8;&#x540C;&#x7684;&#x5F62;&#x72B6;&#x6307;&#x6570;&#x3002;</code></p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>&#x7D22;&#x5F15;</strong> &#xFF08; <em>LongTensor</em> &#xFF09; - &#x7D22;&#x5F15;&#x4E3A;&#x5F20;&#x91CF;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; src = torch.tensor([[4, 3, 5],
                        [6, 7, 8]])
&gt;&gt;&gt; torch.take(src, torch.tensor([0, 2, 5]))
tensor([ 4,  5,  8])
</code></pre><p><code>torch.``transpose</code>( <em>input</em> , <em>dim0</em> , <em>dim1</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x5F20;&#x91CF;&#x662F;<code>&#x8F93;&#x5165;</code>&#x8F6C;&#x7F6E;&#x7248;&#x672C;&#x3002;&#x7ED9;&#x5B9A;&#x5C3A;&#x5BF8;&#x7684;<code>DIM0</code>&#x548C;<code>DIM1</code>&#x88AB;&#x4EA4;&#x6362;&#x3002;</p>
<p>&#x5C06;&#x5F97;&#x5230;&#x7684;<code>OUT</code>&#x5F20;&#x91CF;&#x80A1;&#x5B83;&#x5E95;&#x5C42;&#x5B58;&#x50A8;&#x4E0E;<code>&#x8F93;&#x5165;</code>&#x5F20;&#x91CF;&#xFF0C;&#x6240;&#x4EE5;&#x6539;&#x53D8;&#x4E00;&#x4E2A;&#x7684;&#x5185;&#x5BB9;&#x4F1A;&#x6539;&#x53D8;&#x5176;&#x4ED6;&#x7684;&#x5185;&#x5BB9;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>DIM0</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x8981;&#x8C03;&#x6362;&#x7B2C;&#x4E00;&#x7EF4;</p>
</li>
<li><p><strong>DIM1</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x8981;&#x8C03;&#x6362;&#x7684;&#x7B2C;&#x4E8C;&#x7EF4;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(2, 3)
&gt;&gt;&gt; x
tensor([[ 1.0028, -0.9893,  0.5809],
        [-0.1669,  0.7299,  0.4942]])
&gt;&gt;&gt; torch.transpose(x, 0, 1)
tensor([[ 1.0028, -0.1669],
        [-0.9893,  0.7299],
        [ 0.5809,  0.4942]])
</code></pre><p><code>torch.``unbind</code>( <em>input</em> , <em>dim=0</em> ) &#x2192; seq</p>
<p>&#x5220;&#x9664;&#x4E00;&#x4E2A;&#x5F20;&#x91CF;&#x5C3A;&#x5BF8;&#x3002;</p>
<p>&#x8FD4;&#x56DE;&#x6240;&#x6709;&#x5207;&#x7247;&#x7684;&#x5143;&#x7EC4;&#x6CBF;&#x7740;&#x4E00;&#x4E2A;&#x7ED9;&#x5B9A;&#x7684;&#x5C3A;&#x5BF8;&#xFF0C;&#x5DF2;&#x7ECF;&#x79BB;&#x4E0D;&#x5F00;&#x5B83;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x5F20;&#x91CF;&#x89E3;&#x9664;&#x7ED1;&#x5B9A;</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x5C3A;&#x5BF8;&#xFF0C;&#x4EE5;&#x9664;&#x53BB;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.unbind(torch.tensor([[1, 2, 3],
&gt;&gt;&gt;                            [4, 5, 6],
&gt;&gt;&gt;                            [7, 8, 9]]))
(tensor([1, 2, 3]), tensor([4, 5, 6]), tensor([7, 8, 9]))
</code></pre><p><code>torch.``unsqueeze</code>( <em>input</em> , <em>dim</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E0E;&#x5728;&#x6307;&#x5B9A;&#x7684;&#x4F4D;&#x7F6E;&#x63D2;&#x5165;&#x4E00;&#x4E2A;&#x5927;&#x5C0F;&#x7684;&#x5C3A;&#x5BF8;&#x65B0;&#x7684;&#x5F20;&#x91CF;&#x3002;</p>
<p>&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x80A1;&#x4E0E;&#x6B64;&#x5F20;&#x91CF;&#x76F8;&#x540C;&#x7684;&#x57FA;&#x7840;&#x6570;&#x636E;&#x3002;</p>
<p>1&#xFF0C; [ - A <code>`[-input.dim&#xFF08;&#x8303;&#x56F4;&#xFF09; &#x5185;&#x6697;&#x6DE1;</code>&#x503C;HTG11] input.dim&#xFF08;&#xFF09; +  1&#xFF09; <code>&#x53EF;&#x88AB;&#x4F7F;&#x7528;&#x3002;&#x8D1F;</code>&#x6697;&#x6DE1; <code>&#x65BD;&#x52A0;&#x5C06;&#x5BF9;&#x5E94;&#x4E8E;</code>unsqueeze&#xFF08;&#xFF09;&#x5728;<code>&#x6697;&#x6DE1;</code>= <code>&#x6697;&#x6DE1; \+  input.dim&#xFF08;&#xFF09; \+  1</code>&#x3002;`</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x7D22;&#x5F15;&#x5904;&#x63D2;&#x5165;&#x5355;&#x7EF4;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.tensor([1, 2, 3, 4])
&gt;&gt;&gt; torch.unsqueeze(x, 0)
tensor([[ 1,  2,  3,  4]])
&gt;&gt;&gt; torch.unsqueeze(x, 1)
tensor([[ 1],
        [ 2],
        [ 3],
        [ 4]])
</code></pre><p><code>torch.``where</code>()</p>
<p><code>torch.``where</code>( <em>condition</em> , <em>input</em> , <em>other</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x5143;&#x4EF6;&#x7684;&#x4ECE;&#x4EFB;&#x4E00;<code>&#x8F93;&#x5165;</code>&#x6216;<code>&#x5176;&#x4ED6;</code>&#xFF0C;&#x53D6;&#x51B3;&#x4E8E;<code>&#x6761;&#x4EF6; [HTG11&#x9009;&#x62E9;&#x7684;&#x5F20;&#x91CF;]&#x3002;</code></p>
<p>&#x8BE5;&#x64CD;&#x4F5C;&#x88AB;&#x5B9A;&#x4E49;&#x4E3A;&#xFF1A;</p>
<p>outi={inputiif conditioniotheriotherwise\text{out}_i = \begin{cases}
\text{input}_i &amp; \text{if } \text{condition}_i \\ \text{other}_i &amp;
\text{otherwise} \\ \end{cases} outi&#x200B;={inputi&#x200B;otheri&#x200B;&#x200B;if
conditioni&#x200B;otherwise&#x200B;</p>
<p>Note</p>
<p>&#x5F20;&#x91CF;<code>&#x6761;&#x4EF6;</code>&#xFF0C;<code>&#x8F93;&#x5165;</code>&#xFF0C;<code>&#x5176;&#x4ED6;</code>&#x5FC5;&#x987B;<a href="notes/broadcasting.html#broadcasting-semantics"> broadcastable
</a>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x6761;&#x4EF6;</strong> &#xFF08;<a href="tensors.html#torch.BoolTensor" title="torch.BoolTensor"> <em>BoolTensor</em> </a>&#xFF09; - &#x5F53;&#x771F;&#xFF08;&#x975E;&#x96F6;&#xFF09;&#xFF0C;&#x4EA7;&#x7387;&#x7684;x&#xFF0C;&#x5426;&#x5219;&#x6210;&#x54C1;&#x7387;Y</p>
</li>
<li><p><strong>&#xD7;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x5728;&#x7D22;&#x5F15;&#x9009;&#x62E9;&#x7684;&#x503C;&#xFF0C;&#x5176;&#x4E2D;<code>&#x6761;&#x4EF6;</code>&#x662F;<code>&#x771F;</code></p>
</li>
<li><p><strong>Y</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x5728;&#x7D22;&#x5F15;&#x9009;&#x62E9;&#x7684;&#x503C;&#xFF0C;&#x5176;&#x4E2D;<code>&#x6761;&#x4EF6;</code>&#x662F;<code>&#x5047;</code></p>
</li>
</ul>
<p>Returns</p>
<p>&#x5F62;&#x72B6;&#x7684;&#x5F20;&#x91CF;&#x7B49;&#x4E8E;<code>&#x6761;&#x4EF6;</code>&#x6240;&#x5E7F;&#x64AD;&#x7684;&#x5F62;&#x72B6;&#xFF0C;<code>&#x8F93;&#x5165;</code>&#xFF0C;<code>&#x5176;&#x4ED6;</code></p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(3, 2)
&gt;&gt;&gt; y = torch.ones(3, 2)
&gt;&gt;&gt; x
tensor([[-0.4620,  0.3139],
        [ 0.3898, -0.7197],
        [ 0.0478, -0.1657]])
&gt;&gt;&gt; torch.where(x &gt; 0, x, y)
tensor([[ 1.0000,  0.3139],
        [ 0.3898,  1.0000],
        [ 0.0478,  1.0000]])
</code></pre><p><code>torch.``where</code>( <em>condition</em> ) &#x2192; tuple of LongTensor</p>
<p><code>torch.where&#xFF08;&#x6761;&#x4EF6;&#xFF09;</code>&#x662F;&#x76F8;&#x540C;&#x7684;<code>torch.nonzero&#xFF08;&#x6761;&#x4EF6;&#xFF0C; as_tuple =&#x771F;&#xFF09;</code>&#x3002;</p>
<p>Note</p>
<p>&#x53E6;&#x8BF7;&#x53C2;&#x89C1; <code>torch.nonzero&#xFF08;&#xFF09;</code>&#x3002;</p>
<h2 id="&#x53D1;&#x7535;&#x673A;">&#x53D1;&#x7535;&#x673A;</h2>
<p><em>class</em><code>torch._C.``Generator</code>( <em>device=&apos;cpu&apos;</em> ) &#x2192; Generator</p>
<p>&#x521B;&#x5EFA;&#x5E76;&#x8FD4;&#x56DE;&#x5176;&#x7BA1;&#x7406;&#x4EA7;&#x751F;&#x7684;&#x4F2A;&#x968F;&#x673A;&#x6570;&#x7684;&#x7B97;&#x6CD5;&#x7684;&#x72B6;&#x6001;&#x7684;&#x751F;&#x6210;&#x5668;&#x5BF9;&#x8C61;&#x3002;&#x7528;&#x4F5C;&#x8BB8;&#x591A; &#x5C31;&#x5730;&#x968F;&#x673A;&#x62BD;&#x6837; &#x529F;&#x80FD;&#x7684;&#x5173;&#x952E;&#x5B57;&#x53C2;&#x6570;&#x3002;</p>
<p>Parameters</p>
<p><strong>&#x88C5;&#x7F6E;</strong> &#xFF08;<code>torch.device</code>&#xFF0C;&#x53EF;&#x9009;&#xFF09; - &#x7528;&#x4E8E;&#x53D1;&#x7535;&#x673A;&#x6240;&#x9700;&#x7684;&#x8BBE;&#x5907;&#x3002;</p>
<p>Returns</p>
<p>&#x4E00;&#x4E2A;torch.Generator&#x5BF9;&#x8C61;&#x3002;</p>
<p>Return type</p>
<p>&#x53D1;&#x751F;&#x5668;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; g_cpu = torch.Generator()
&gt;&gt;&gt; g_cuda = torch.Generator(device=&apos;cuda&apos;)
</code></pre><p><code>device</code></p>
<p>Generator.device - &amp; GT ;&#x88C5;&#x7F6E;</p>
<p>&#x83B7;&#x53D6;&#x53D1;&#x7535;&#x673A;&#x7684;&#x7535;&#x6D41;&#x8BBE;&#x5907;&#x3002;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; g_cpu = torch.Generator()
&gt;&gt;&gt; g_cpu.device
device(type=&apos;cpu&apos;)
</code></pre><p><code>get_state</code>() &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x53D1;&#x7535;&#x673A;&#x72B6;&#x6001;&#x4F5C;&#x4E3A;<code>torch.ByteTensor</code>&#x3002;</p>
<p>Returns</p>
<p>A <code>torch.ByteTensor &#xFF0C;&#x5176;&#x5305;&#x542B;&#x6240;&#x6709;&#x5FC5;&#x8981;&#x7684;&#x6BD4;&#x7279;&#x5230;&#x53D1;&#x7535;&#x673A;&#x8FD8;&#x539F;&#x5230;&#x5728;&#x7279;&#x5B9A;&#x65F6;&#x95F4;&#x70B9;</code>&#x3002;</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; g_cpu = torch.Generator()
&gt;&gt;&gt; g_cpu.get_state()
</code></pre><p><code>initial_seed</code>() &#x2192; int</p>
<p>&#x8FD4;&#x56DE;&#x7528;&#x4E8E;&#x4EA7;&#x751F;&#x968F;&#x673A;&#x6570;&#x7684;&#x521D;&#x59CB;&#x79CD;&#x5B50;&#x3002;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; g_cpu = torch.Generator()
&gt;&gt;&gt; g_cpu.initial_seed()
2147483647
</code></pre><p><code>manual_seed</code>( <em>seed</em> ) &#x2192; Generator</p>
<p>&#x8BBE;&#x7F6E;&#x751F;&#x6210;&#x968F;&#x673A;&#x6570;&#x79CD;&#x5B50;&#x3002;&#x8FD4;&#x56DE; torch.Generator &#x5BF9;&#x8C61;&#x3002;&#x5B83;&#x5EFA;&#x8BAE;&#x8BBE;&#x7F6E;&#x4E00;&#x4E2A;&#x5927;&#x7684;&#x79CD;&#x5B50;&#xFF0C;&#x5373;&#x4E00;&#x4E2A;&#x6570;&#x5B57;&#xFF0C;&#x5177;&#x6709;0&#x548C;1&#x4F4D;&#x7684;&#x826F;&#x597D;&#x5E73;&#x8861;&#x3002;&#x907F;&#x514D;&#x5728;&#x79CD;&#x5B50;&#x5177;&#x6709;&#x8BB8;&#x591A;0&#x6BD4;&#x7279;&#x3002;</p>
<p>Parameters</p>
<p><strong>&#x79CD;&#x5B50;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in
Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x6240;&#x9700;&#x7684;&#x79CD;&#x5B50;&#x3002;</p>
<p>Returns</p>
<p>An torch.Generator object.</p>
<p>Return type</p>
<p>Generator</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; g_cpu = torch.Generator()
&gt;&gt;&gt; g_cpu.manual_seed(2147483647)
</code></pre><p><code>seed</code>() &#x2192; int</p>
<p>&#x4ECE;&#x83B7;&#x53D6;&#x7684;std :: random_device&#x6216;&#x5F53;&#x524D;&#x65F6;&#x95F4;&#x7684;&#x975E;&#x786E;&#x5B9A;&#x6027;&#x7684;&#x968F;&#x673A;&#x6570;&#xFF0C;&#x5E76;&#x4F7F;&#x7528;&#x8BE5;&#x79CD;&#x5B50;&#x7684;&#x53D1;&#x7535;&#x673A;&#x3002;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; g_cpu = torch.Generator()
&gt;&gt;&gt; g_cpu.seed()
1516516984916
</code></pre><p><code>set_state</code>( <em>new_state</em> ) &#x2192; void</p>
<p>&#x8BBE;&#x7F6E;&#x53D1;&#x7535;&#x673A;&#x7684;&#x72B6;&#x6001;&#x3002;</p>
<p>Parameters</p>
<p><strong>NEW_STATE</strong> &#xFF08; <em>torch.ByteTensor</em> &#xFF09; - &#x6240;&#x9700;&#x7684;&#x72B6;&#x6001;&#x3002;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; g_cpu = torch.Generator()
&gt;&gt;&gt; g_cpu_other = torch.Generator()
&gt;&gt;&gt; g_cpu.set_state(g_cpu_other.get_state())
</code></pre><h2 id="&#x968F;&#x673A;&#x53D6;&#x6837;">&#x968F;&#x673A;&#x53D6;&#x6837;</h2>
<p><code>torch.``seed</code>()<a href="_modules/torch/random.html#seed">[source]</a></p>
<p>&#x8BBE;&#x7F6E;&#x7528;&#x4E8E;&#x4EA7;&#x751F;&#x968F;&#x673A;&#x6570;&#xFF0C;&#x4EE5;&#x975E;&#x786E;&#x5B9A;&#x6027;&#x7684;&#x968F;&#x673A;&#x6570;&#x79CD;&#x5B50;&#x3002;&#x8FD4;&#x56DE;&#x7528;&#x4E8E;&#x64AD;&#x79CD;RNG&#x4E00;&#x4E2A;64&#x4F4D;&#x7684;&#x6570;&#x3002;</p>
<p><code>torch.``manual_seed</code>( <em>seed</em>
)<a href="_modules/torch/random.html#manual_seed">[source]</a></p>
<p>&#x8BBE;&#x7F6E;&#x751F;&#x6210;&#x968F;&#x673A;&#x6570;&#x79CD;&#x5B50;&#x3002;&#x8FD4;&#x56DE; torch.Generator &#x5BF9;&#x8C61;&#x3002;</p>
<p>Parameters</p>
<p><strong>seed</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in
Python v3.7\)" target="_blank"> <em>int</em></a>) &#x2013; The desired seed.</p>
<p><code>torch.``initial_seed</code>()<a href="_modules/torch/random.html#initial_seed">[source]</a></p>
<p>&#x8FD4;&#x56DE;&#x521D;&#x59CB;&#x79CD;&#x5B50;&#x7528;&#x4E8E;&#x4EA7;&#x751F;&#x968F;&#x673A;&#x6570;&#x4F5C;&#x4E3A;&#x4E00;&#x4E2A;Python &#x957F;&#x3002;</p>
<p><code>torch.``get_rng_state</code>()<a href="_modules/torch/random.html#get_rng_state">[source]</a></p>
<p>&#x8FD4;&#x56DE;&#x968F;&#x673A;&#x6570;&#x53D1;&#x751F;&#x5668;&#x72B6;&#x6001;&#x4F5C;&#x4E3A; torch.ByteTensor &#x3002;</p>
<p><code>torch.``set_rng_state</code>( <em>new_state</em>
)<a href="_modules/torch/random.html#set_rng_state">[source]</a></p>
<p>&#x8BBE;&#x7F6E;&#x968F;&#x673A;&#x6570;&#x751F;&#x6210;&#x5668;&#x7684;&#x72B6;&#x6001;&#x3002;</p>
<p>Parameters</p>
<p><strong>NEW_STATE</strong> &#xFF08; <em>torch.ByteTensor</em> &#xFF09; - &#x671F;&#x671B;&#x72B6;&#x6001;</p>
<p><code>torch.``default_generator</code><em>Returns the default CPU torch.Generator</em></p>
<p><code>torch.``bernoulli</code>( <em>input</em> , <em>*</em> , <em>generator=None</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x4ECE;&#x4F2F;&#x52AA;&#x5229;&#x5206;&#x5E03;&#x7ED8;&#x5236;&#x4E8C;&#x8FDB;&#x5236;&#x968F;&#x673A;&#x6570;&#xFF08;0&#x6216;1&#xFF09;&#x3002;</p>
<p>&#x7684;<code>&#x8F93;&#x5165;</code>&#x5F20;&#x91CF;&#x5E94;&#x8BE5;&#x662F;&#x5305;&#x542B;&#x6982;&#x7387;&#x88AB;&#x7528;&#x4E8E;&#x7ED8;&#x5236;&#x4E8C;&#x8FDB;&#x5236;&#x968F;&#x673A;&#x6570;&#x7684;&#x5F20;&#x91CF;&#x3002;&#x56E0;&#x6B64;&#xFF0C;&#x5728;<code>&#x8F93;&#x5165;&#x7684;&#x6240;&#x6709;&#x503C;</code>&#x5FC5;&#x987B;&#x5728;&#x8303;&#x56F4;&#xFF1A; 0  &#x2264; &#x8F93;&#x5165; i&#x7684; &#x2264; 1  0
\&#x5F53;&#x91CF;\&#x6587;&#x672C;{&#x8F93;&#x5165;} _i \&#x5F53;&#x91CF;1  0  &#x2264;  &#x8F93;&#x5165; i&#x7684; &#x2264; 1  &#x3002;</p>
<p>&#x7684; i&#x7684; T  H  \&#x6587;&#x672C;{I} ^ {&#x7B2C;}  I  T  H  &#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684;&#x5143;&#x4EF6;&#x5C06;&#x4EE5;&#x6B64;&#x4E3A;&#x503C; 1  1  1  &#x6839;&#x636E; i&#x7684; T  H  \&#x6587;&#x672C;{I} ^
{}&#x7B2C; i&#x7684; T  H  &#x7684;&#x6982;&#x7387;&#x503C;&#x5728;<code>&#x8F93;&#x5165;&#x7ED9;&#x5B9A;&#x7684;</code>&#x3002;</p>
<p>outi&#x223C;Bernoulli(p=inputi)\text{out}<em>{i} \sim \mathrm{Bernoulli}(p =
\text{input}</em>{i}) outi&#x200B;&#x223C;Bernoulli(p=inputi&#x200B;)</p>
<p>&#x8FD4;&#x56DE;&#x7684;<code>OUT</code>&#x5F20;&#x91CF;&#x4EC5;&#x5177;&#x6709;&#x503C;0&#x6216;1&#xFF0C;&#x662F;&#x76F8;&#x540C;&#x7684;&#x5F62;&#x72B6;&#x7684;&#x4F5C;&#x4E3A;<code>&#x8F93;&#x5165;</code>&#x3002;</p>
<p><code>OUT</code>&#x53EF;&#x4EE5;&#x5177;&#x6709;&#x79EF;&#x5206;<code>DTYPE</code>&#xFF0C;&#x4F46;<code>&#x8F93;&#x5165;</code>&#x5FC5;&#x987B;&#x6D6E;&#x70B9;<code>DTYPE</code>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x6982;&#x7387;&#x503C;&#x7684;&#x4F2F;&#x52AA;&#x5229;&#x5206;&#x5E03;&#x7684;&#x8F93;&#x5165;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.empty(3, 3).uniform_(0, 1)  # generate a uniform random matrix with range [0, 1]
&gt;&gt;&gt; a
tensor([[ 0.1737,  0.0950,  0.3609],
        [ 0.7148,  0.0289,  0.2676],
        [ 0.9456,  0.8937,  0.7202]])
&gt;&gt;&gt; torch.bernoulli(a)
tensor([[ 1.,  0.,  0.],
        [ 0.,  0.,  0.],
        [ 1.,  1.,  1.]])

&gt;&gt;&gt; a = torch.ones(3, 3) # probability of drawing &quot;1&quot; is 1
&gt;&gt;&gt; torch.bernoulli(a)
tensor([[ 1.,  1.,  1.],
        [ 1.,  1.,  1.],
        [ 1.,  1.,  1.]])
&gt;&gt;&gt; a = torch.zeros(3, 3) # probability of drawing &quot;1&quot; is 0
&gt;&gt;&gt; torch.bernoulli(a)
tensor([[ 0.,  0.,  0.],
        [ 0.,  0.,  0.],
        [ 0.,  0.,  0.]])
</code></pre><p><code>torch.``multinomial</code>( <em>input</em> , <em>num_samples</em> , <em>replacement=False</em> ,
<em>out=None</em> ) &#x2192; LongTensor</p>
<p>&#x8FD4;&#x56DE;&#x5176;&#x4E2D;&#x6BCF;&#x884C;&#x90FD;&#x5305;&#x542B;<code>&#x5F20;&#x91CF;num_samples</code>&#x6307;&#x6570;&#x4ECE;&#x4F4D;&#x4E8E;&#x5F20;&#x91CF;&#x8F93;&#x5165; &#x7684;<code>&#x7684;&#x76F8;&#x5E94;&#x884C;&#x4E2D;&#x7684;&#x591A;&#x9879;&#x5F0F;&#x6982;&#x7387;&#x5206;&#x5E03;&#x8FDB;&#x884C;&#x91C7;&#x6837;&#x3002;</code></p>
<p>Note</p>
<p>&#x7684;&#x884C;<code>&#x8F93;&#x5165;</code>&#x4E0D;&#x9700;&#x8981;&#x603B;&#x548C;&#x4E3A;1&#xFF08;&#x5728;&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x6211;&#x4EEC;&#x4F7F;&#x7528;&#x7684;&#x503C;&#x4F5C;&#x4E3A;&#x6743;&#x91CD;&#xFF09;&#xFF0C;&#x4F46;&#x5FC5;&#x987B;&#x662F;&#x975E;&#x8D1F;&#x7684;&#xFF0C;&#x6709;&#x9650;&#x7684;&#xFF0C;&#x5E76;&#x4E14;&#x6709;&#x4E00;&#x4E2A;&#x975E;&#x96F6;&#x548C;&#x3002;</p>
<p>&#x6307;&#x6570;&#x4ECE;&#x5DE6;&#x5230;&#x53F3;&#xFF0C;&#x6839;&#x636E;&#x5F53;&#x6BCF;&#x4E2A;&#x53D6;&#x6837;&#xFF08;&#x7B2C;&#x4E00;&#x6837;&#x54C1;&#x653E;&#x7F6E;&#x5728;&#x7B2C;&#x4E00;&#x5217;&#xFF09;&#x6765;&#x6392;&#x5E8F;&#x3002;</p>
<p>&#x5982;&#x679C;<code>&#x8F93;&#x5165;</code>&#x662F;&#x77E2;&#x91CF;&#xFF0C;<code>OUT</code>&#x662F;&#x5927;&#x5C0F;num_samples &#x7684;<code>&#x7684;&#x8F7D;&#x4F53;&#x3002;</code></p>
<p>&#x5982;&#x679C;<code>&#x8F93;&#x5165;</code>&#x662F;&#x4E0E;&#x7684;&#x77E9;&#x9635;M &#x884C;&#xFF0C;<code>OUT</code>&#x662F;&#x5F62;&#x72B6; [HTG11&#x7684;&#x77E9;&#x9635;]  &#xFF08; M  &#xD7; num_samples  &#xFF09; &#xFF08;M \&#x500D;\&#x6587;&#x672C;{NUM \
_samples}&#xFF09; &#xFF08; M  &#xD7; num_samples  &#xFF09; &#x3002;</p>
<p>&#x5982;&#x679C;&#x66FF;&#x6362;&#x4E3A;<code>&#x771F;</code>&#xFF0C;&#x6837;&#x54C1;&#x7ED8;&#x5236;&#x66F4;&#x6362;&#x3002;</p>
<p>&#x5982;&#x679C;&#x4E0D;&#x662F;&#xFF0C;&#x4ED6;&#x4EEC;&#x7ED8;&#x5236;&#x65E0;&#x9700;&#x66F4;&#x6362;&#xFF0C;&#x8FD9;&#x610F;&#x5473;&#x7740;&#x5F53;&#x6307;&#x6570;&#x6837;&#x672C;&#x7ED8;&#x5236;&#x4E3A;&#x884C;&#xFF0C;&#x4E0D;&#x80FD;&#x518D;&#x4E3A;&#x8BE5;&#x884C;&#x753B;&#x51FA;&#x3002;</p>
<p>Note</p>
<p>&#x5F53;&#x4E0D;&#x9700;&#x66F4;&#x6362;&#x7ED8;&#x5236;&#xFF0C;<code>num_samples</code>&#x5FC5;&#x987B;&#x5927;&#x4E8E;&#xFF08;&#x5728;<code>&#x8F93;&#x5165;</code>&#x975E;&#x96F6;&#x5143;&#x7D20;&#x7684;&#x6570;&#x76EE;&#x6216;&#x6700;&#x5C0F;&#x6570;&#x76EE;&#x7684;&#x975E;&#x4E0B;&#x8F93;&#x5165; &#x5982;&#x679C;&#x5B83;&#x662F;&#x4E00;&#x4E2A;&#x77E9;&#x9635;&#xFF09;&#x7684;<code>&#x6BCF;&#x884C;&#x4E2D;&#x975E;&#x96F6;&#x5143;&#x7D20;&#x3002;</code></p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x5305;&#x542B;&#x6982;&#x7387;&#x8F93;&#x5165;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>num_samples</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x6837;&#x672C;&#x7684;&#x6570;&#x76EE;&#x6765;&#x7ED8;&#x5236;</p>
</li>
<li><p><strong>&#x66FF;&#x6362;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x662F;&#x5426;&#x4E0E;&#x66F4;&#x6362;&#x6216;&#x4E0D;&#x753B;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; weights = torch.tensor([0, 10, 3, 0], dtype=torch.float) # create a tensor of weights
&gt;&gt;&gt; torch.multinomial(weights, 2)
tensor([1, 2])
&gt;&gt;&gt; torch.multinomial(weights, 4) # ERROR!
RuntimeError: invalid argument 2: invalid multinomial distribution (with replacement=False,
not enough non-negative category to sample) at ../aten/src/TH/generic/THTensorRandom.cpp:320
&gt;&gt;&gt; torch.multinomial(weights, 4, replacement=True)
tensor([ 2,  1,  1,  1])
</code></pre><p><code>torch.``normal</code>()</p>
<p><code>torch.``normal</code>( <em>mean</em> , <em>std</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4ECE;&#x72EC;&#x7ACB;&#x7684;&#x6B63;&#x6001;&#x5206;&#x5E03;&#xFF0C;&#x5176;&#x5E73;&#x5747;&#x503C;&#x548C;&#x6807;&#x51C6;&#x504F;&#x5DEE;&#x7ED9;&#x51FA;&#x7ED8;&#x5236;&#x968F;&#x673A;&#x6570;&#x7684;&#x5F20;&#x91CF;&#x3002;</p>
<p>&#x7684; <code>&#x610F;&#x5473;&#x7740;</code>&#x662F;&#x4E0E;&#x6BCF;&#x4E2A;&#x8F93;&#x51FA;&#x5143;&#x4EF6;&#x7684;&#x6B63;&#x5E38;&#x5206;&#x5E03;&#x7684;&#x5E73;&#x5747;&#x503C;&#x7684;&#x5F20;&#x91CF;</p>
<p>&#x7684; <code>STD</code>&#x662F;&#x4E0E;&#x6BCF;&#x4E2A;&#x8F93;&#x51FA;&#x5143;&#x4EF6;&#x7684;&#x6B63;&#x6001;&#x5206;&#x5E03;&#x7684;&#x6807;&#x51C6;&#x504F;&#x5DEE;&#x7684;&#x5F20;&#x91CF;</p>
<p>&#x7684;&#x5F62;&#x72B6; <code>[&#x5E73;&#x5747;HTG3]</code>&#x548C; <code>&#x6027;&#x75C5;</code>&#x4E0D;&#x9700;&#x8981;&#x5339;&#x914D;&#xFF0C;&#x4F46;&#x662F;&#x5728;&#x5404;&#x5F20;&#x91CF;&#x5143;&#x7D20;&#x7684;&#x603B;&#x6570;&#x91CF;&#x9700;&#x8981;&#x662F;&#x76F8;&#x540C;&#x7684;&#x3002;</p>
<p>Note</p>
<p>&#x5F53;&#x5F62;&#x72B6;&#x4E0D;&#x5339;&#x914D;&#xFF0C;&#x7684; <code>&#x5F62;&#x72B6;&#x610F;&#x5473;&#x7740;</code>&#x88AB;&#x7528;&#x4F5C;&#x5F62;&#x72B6;&#x4E3A;&#x8FD4;&#x56DE;&#x7684;&#x8F93;&#x51FA;&#x5F20;&#x91CF;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x610F;&#x5473;&#x7740;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x88C5;&#x7F6E;&#x7684;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>STD</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x6807;&#x51C6;&#x504F;&#x5DEE;&#x7684;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.normal(mean=torch.arange(1., 11.), std=torch.arange(1, 0, -0.1))
tensor([  1.0425,   3.5672,   2.7969,   4.2925,   4.7229,   6.2134,
          8.0505,   8.1408,   9.0563,  10.0566])
</code></pre><p><code>torch.``normal</code>( <em>mean=0.0</em> , <em>std</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x4E0A;&#x8FF0;&#x529F;&#x80FD;&#xFF0C;&#x4F46;&#x662F;&#x7C7B;&#x4F3C;&#x7684;&#x88C5;&#x7F6E;&#x90FD;&#x88AB;&#x7ED8;&#x5236;&#x7684;&#x5143;&#x7D20;&#x4E4B;&#x95F4;&#x5171;&#x4EAB;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x610F;&#x5473;&#x7740;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5E73;&#x5747;&#x4E3A;&#x6240;&#x6709;&#x7684;&#x53D1;&#x884C;</p>
</li>
<li><p><strong>std</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the tensor of per-element standard deviations</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.normal(mean=0.5, std=torch.arange(1., 6.))
tensor([-1.2793, -1.0732, -2.0687,  5.1177, -1.2303])
</code></pre><p><code>torch.``normal</code>( <em>mean</em> , <em>std=1.0</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x4E0E;&#x4E0A;&#x8FF0;&#x7C7B;&#x4F3C;&#x7684;&#x529F;&#x80FD;&#xFF0C;&#x4F46;&#x6807;&#x51C6;&#x504F;&#x5DEE;&#x90FD;&#x7ED8;&#x5236;&#x7684;&#x5143;&#x7D20;&#x4E4B;&#x95F4;&#x5171;&#x4EAB;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>mean</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the tensor of per-element means</p>
</li>
<li><p><strong>STD</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x6240;&#x6709;&#x5206;&#x5E03;&#x7684;&#x6807;&#x51C6;&#x504F;&#x5DEE;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.normal(mean=torch.arange(1., 6.))
tensor([ 1.1552,  2.6148,  2.6535,  5.8318,  4.2361])
</code></pre><p><code>torch.``normal</code>( <em>mean</em> , <em>std</em> , <em>size</em> , <em>*</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x7C7B;&#x4F3C;&#x4E8E;&#x4E0A;&#x9762;&#x7684;&#x529F;&#x80FD;&#xFF0C;&#x4F46;&#x662F;&#x5E73;&#x5747;&#x503C;&#x548C;&#x6807;&#x51C6;&#x504F;&#x5DEE;&#x90FD;&#x7ED8;&#x5236;&#x7684;&#x5143;&#x7D20;&#x4E4B;&#x95F4;&#x5171;&#x4EAB;&#x3002;&#x5C06;&#x5F97;&#x5230;&#x7684;&#x5F20;&#x91CF;&#x5177;&#x6709;&#x7531;<code>&#x5927;&#x5C0F;</code>&#x7ED9;&#x5B9A;&#x7684;&#x5927;&#x5C0F;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x610F;&#x5473;&#x7740;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a>&#xFF09; - &#x5E73;&#x5747;&#x4E3A;&#x6240;&#x6709;&#x7684;&#x53D1;&#x884C;</p>
</li>
<li><p><strong>STD</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;</em> </a>&#xFF09; - &#x6807;&#x51C6;&#x5BF9;&#x4E8E;&#x6240;&#x6709;&#x5206;&#x5E03;&#x504F;&#x5DEE;</p>
</li>
<li><p><strong>&#x5927;&#x5C0F;</strong> &#xFF08; <em>INT ...</em> &#xFF09; - &#x5B9A;&#x4E49;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684;&#x5F62;&#x72B6;&#x7684;&#x6574;&#x6570;&#x5E8F;&#x5217;&#x3002;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.normal(2, 3, size=(1, 4))
tensor([[-1.3987, -1.9544,  3.6048,  0.7909]])
</code></pre><p><code>torch.``rand</code>( <em>*size</em> , <em>out=None</em> , <em>dtype=None</em> , <em>layout=torch.strided</em> ,
<em>device=None</em> , <em>requires_grad=False</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x5728;&#x533A;&#x95F4; [ 0  &#x586B;&#x5145;&#x6709;&#x968F;&#x673A;&#x6570;&#x4ECE;&#x5747;&#x5300;&#x5206;&#x5E03;&#x7684;&#x5F20;&#x91CF;&#xFF0C;  1  &#xFF09; [0&#xFF0C;1&#xFF09; [ 0  &#xFF0C; 1  &#xFF09;</p>
<p>&#x5F20;&#x91CF;&#x7684;&#x5F62;&#x72B6;&#x7531;&#x53EF;&#x53D8;&#x53C2;&#x6570;<code>&#x5927;&#x5C0F;</code>&#x4E2D;&#x5B9A;&#x4E49;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>size</strong> ( <em>int...</em> ) &#x2013; a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <code>torch.set_default_tensor_type()</code>).</p>
</li>
<li><p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <code>torch.set_default_tensor_type()</code>). <code>device</code>will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li><p><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.rand(4)
tensor([ 0.5204,  0.2503,  0.3525,  0.5673])
&gt;&gt;&gt; torch.rand(2, 3)
tensor([[ 0.8237,  0.5781,  0.6879],
        [ 0.3816,  0.7249,  0.0998]])
</code></pre><p><code>torch.``rand_like</code>( <em>input</em> , <em>dtype=None</em> , <em>layout=None</em> , <em>device=None</em> ,
<em>requires_grad=False</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x5177;&#x6709;&#x76F8;&#x540C;&#x5C3A;&#x5BF8;&#x7684;&#x5F20;&#x91CF;&#x4E3A;<code>&#x8F93;&#x5165;</code>&#x586B;&#x5145;&#x6709;&#x968F;&#x673A;&#x6570;&#x4ECE;&#x5747;&#x5300;&#x5206;&#x5E03;&#x5728;&#x533A;&#x95F4; [ 0  &#xFF0C; 1  &#xFF09; [0&#xFF0C;1&#xFF09; [ 0  &#xFF0C; 1  &#xFF09; &#x3002; <code>torch.rand_like&#xFF08;&#x8F93;&#x5165;&#xFF09;</code>&#x7B49;&#x4E8E;<code>torch.rand&#xFF08;input.size&#xFF08;&#xFF09;&#xFF0C; D&#x578B;&#x7EC6;&#x80DE;= input.dtype&#xFF0C; &#x5E03;&#x5C40;=
input.layout&#xFF0C; &#x8BBE;&#x5907;= input.device&#xFF09;</code>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the size of <code>input</code>will determine size of the output tensor</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned Tensor. Default: if <code>None</code>, defaults to the dtype of <code>input</code>.</p>
</li>
<li><p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned tensor. Default: if <code>None</code>, defaults to the layout of <code>input</code>.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, defaults to the device of <code>input</code>.</p>
</li>
<li><p><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p><code>torch.``randint</code>( <em>low=0</em> , <em>high</em> , <em>size</em> , <em>out=None</em> , <em>dtype=None</em> ,
<em>layout=torch.strided</em> , <em>device=None</em> , <em>requires_grad=False</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x586B;&#x5145;&#x6709;&#x968F;&#x673A;&#x6574;&#x6570;&#x7684;&#x5F20;&#x91CF;&#x4EA7;&#x751F;&#x5747;&#x5300;&#x4E4B;&#x95F4;<code>&#x4F4E;</code>&#xFF08;&#x542B;&#xFF09;&#x548C;<code>&#x9AD8;</code>&#xFF08;&#x5F02;&#xFF09;&#x3002;</p>
<p>The shape of the tensor is defined by the variable argument <code>size</code>.</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x4F4E;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x4EE5;&#x4ECE;&#x5206;&#x5E03;&#x4E2D;&#x62BD;&#x53D6;&#x6700;&#x4F4E;&#x6574;&#x6570;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;0&#x3002;</p>
</li>
<li><p><strong>&#x9AD8;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x4E00;&#x4E2A;&#x4EE5;&#x4E0A;&#x7684;&#x6700;&#x5927;&#x6574;&#x6570;&#x4E3A;&#x4ECE;&#x5206;&#x5E03;&#x4E2D;&#x62BD;&#x53D6;&#x3002;</p>
</li>
<li><p><strong>&#x5927;&#x5C0F;</strong> &#xFF08;<a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>&#x5143;&#x7EC4;</em> </a>&#xFF09; - &#x5143;&#x7EC4;&#x9650;&#x5B9A;&#x6240;&#x8FF0;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684;&#x5F62;&#x72B6;&#x3002;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <code>torch.set_default_tensor_type()</code>).</p>
</li>
<li><p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <code>torch.set_default_tensor_type()</code>). <code>device</code>will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li><p><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.randint(3, 5, (3,))
tensor([4, 3, 4])


&gt;&gt;&gt; torch.randint(10, (2, 2))
tensor([[0, 2],
        [5, 5]])


&gt;&gt;&gt; torch.randint(3, 10, (2, 2))
tensor([[4, 5],
        [6, 7]])
</code></pre><p><code>torch.``randint_like</code>( <em>input</em> , <em>low=0</em> , <em>high</em> , <em>dtype=None</em> ,
<em>layout=torch.strided</em> , <em>device=None</em> , <em>requires_grad=False</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E0E;&#x76F8;&#x540C;&#x5F62;&#x72B6;&#x7684;&#x5F20;&#x91CF;&#x4F5C;&#x4E3A;&#x586B;&#x5145;&#x6709;&#x968F;&#x673A;&#x6574;&#x6570;&#x5F20;&#x91CF;<code>&#x8F93;&#x5165;</code>&#x4EA7;&#x751F;&#x5747;&#x5300;&#x4E4B;&#x95F4;<code>&#x4F4E;</code>&#xFF08;&#x542B;&#xFF09;&#x548C;<code>&#x9AD8;</code>&#xFF08;&#x5F02;&#xFF09;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the size of <code>input</code>will determine size of the output tensor</p>
</li>
<li><p><strong>low</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>,</em> <em>optional</em> ) &#x2013; Lowest integer to be drawn from the distribution. Default: 0.</p>
</li>
<li><p><strong>high</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a>) &#x2013; One above the highest integer to be drawn from the distribution.</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned Tensor. Default: if <code>None</code>, defaults to the dtype of <code>input</code>.</p>
</li>
<li><p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned tensor. Default: if <code>None</code>, defaults to the layout of <code>input</code>.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, defaults to the device of <code>input</code>.</p>
</li>
<li><p><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p><code>torch.``randn</code>( <em>*size</em> , <em>out=None</em> , <em>dtype=None</em> , <em>layout=torch.strided</em>
, <em>device=None</em> , <em>requires_grad=False</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x586B;&#x5145;&#x7684;&#x968F;&#x673A;&#x6570;&#x4ECE;&#x6B63;&#x6001;&#x5206;&#x5E03;&#x4E0E;&#x5E73;&#x5747;&#x503C;&#x7684;&#x5F20;&#x91CF; 0 &#x548C;&#x65B9;&#x5DEE;HTG2] 1 &#xFF08;&#x4E5F;&#x79F0;&#x4E3A;&#x6807;&#x51C6;&#x6B63;&#x6001;&#x5206;&#x5E03;&#xFF09;&#x3002;</p>
<p>outi&#x223C;N(0,1)\text{out}_{i} \sim \mathcal{N}(0, 1) outi&#x200B;&#x223C;N(0,1)</p>
<p>The shape of the tensor is defined by the variable argument <code>size</code>.</p>
<p>Parameters</p>
<ul>
<li><p><strong>size</strong> ( <em>int...</em> ) &#x2013; a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <code>torch.set_default_tensor_type()</code>).</p>
</li>
<li><p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <code>torch.set_default_tensor_type()</code>). <code>device</code>will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li><p><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.randn(4)
tensor([-2.1436,  0.9966,  2.3426, -0.6366])
&gt;&gt;&gt; torch.randn(2, 3)
tensor([[ 1.5954,  2.8929, -1.0923],
        [ 1.1719, -0.4709, -0.1996]])
</code></pre><p><code>torch.``randn_like</code>( <em>input</em> , <em>dtype=None</em> , <em>layout=None</em> , <em>device=None</em> ,
<em>requires_grad=False</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x5F20;&#x91CF;&#x7684;&#x5927;&#x5C0F;&#x76F8;&#x540C;<code>&#x8F93;&#x5165;</code>&#x586B;&#x5145;&#x6709;&#x968F;&#x673A;&#x6570;&#x4ECE;&#x6B63;&#x6001;&#x5206;&#x5E03;&#x5747;&#x503C;&#x4E3A;0&#xFF0C;&#x65B9;&#x5DEE;&#x4E3A;1&#x3002;<code>torch.randn_like&#xFF08;&#x8F93;&#x5165;&#xFF09;</code>&#x7B49;&#x4E8E;<code>torch.randn&#xFF08;input.size&#xFF08;&#xFF09;&#xFF0C; D&#x578B;&#x7EC6;&#x80DE;= input.dtype&#xFF0C; &#x5E03;&#x5C40;= input.layout&#xFF0C; &#x8BBE;&#x5907;=
input.device&#xFF09;</code>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the size of <code>input</code>will determine size of the output tensor</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned Tensor. Default: if <code>None</code>, defaults to the dtype of <code>input</code>.</p>
</li>
<li><p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned tensor. Default: if <code>None</code>, defaults to the layout of <code>input</code>.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, defaults to the device of <code>input</code>.</p>
</li>
<li><p><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p><code>torch.``randperm</code>( <em>n</em> , <em>out=None</em> , <em>dtype=torch.int64</em> ,
<em>layout=torch.strided</em> , <em>device=None</em> , <em>requires_grad=False</em> ) &#x2192; LongTensor</p>
<p>&#x8FD4;&#x56DE;&#x6574;&#x6570;&#x7684;&#x968F;&#x673A;&#x7F6E;&#x6362;&#x4ECE;<code>0</code>&#x81F3;<code>n&#x7684; -  1</code>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>n&#x7684;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x4E0A;&#x9650;&#xFF08;&#x4E0D;&#x5305;&#x62EC;&#xFF09;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
<li><p><strong>DTYPE</strong> &#xFF08;<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"> <code>torch.dtype</code></a>&#xFF0C;&#x53EF;&#x9009;&#xFF09; - &#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x7684;&#x6240;&#x5E0C;&#x671B;&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>torch.int64</code>&#x3002;</p>
</li>
<li><p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <code>torch.set_default_tensor_type()</code>). <code>device</code>will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li><p><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.randperm(4)
tensor([2, 1, 0, 3])
</code></pre><h3 id="&#x5C31;&#x5730;&#x968F;&#x673A;&#x62BD;&#x6837;">&#x5C31;&#x5730;&#x968F;&#x673A;&#x62BD;&#x6837;</h3>
<p>&#x6709;&#x5173;&#x4E8E;&#x5F20;&#x91CF;&#x7684;&#x5B9A;&#x4E49;&#x4EE5;&#x53CA;&#x51E0;&#x4E2A;&#x5C31;&#x5730;&#x968F;&#x673A;&#x62BD;&#x6837;&#x529F;&#x80FD;&#x3002;&#x70B9;&#x51FB;&#x8FDB;&#x5165;&#x662F;&#x6307;&#x4ED6;&#x4EEC;&#x7684;&#x6587;&#x6863;&#xFF1A;</p>
<ul>
<li><p><a href="tensors.html#torch.Tensor.bernoulli_" title="torch.Tensor.bernoulli_"> <code>torch.Tensor.bernoulli_&#xFF08;&#xFF09;</code></a> - &#x5C31;&#x5730;&#x7248;&#x672C; <code>torch.bernoulli&#x7684;&#xFF08;&#xFF09;</code></p>
</li>
<li><p><a href="tensors.html#torch.Tensor.cauchy_" title="torch.Tensor.cauchy_"> <code>torch.Tensor.cauchy_&#xFF08;&#xFF09;</code></a> - &#x4ECE;&#x67EF;&#x897F;&#x5206;&#x5E03;&#x62BD;&#x53D6;&#x7684;&#x6570;&#x5B57;</p>
</li>
<li><p><a href="tensors.html#torch.Tensor.exponential_" title="torch.Tensor.exponential_"> <code>torch.Tensor.exponential_&#xFF08;&#xFF09;</code></a> - &#x4ECE;&#x6240;&#x8FF0;&#x6307;&#x6570;&#x5206;&#x5E03;&#x62BD;&#x53D6;&#x7684;&#x6570;&#x5B57;</p>
</li>
<li><p><a href="tensors.html#torch.Tensor.geometric_" title="torch.Tensor.geometric_"> <code>torch.Tensor.geometric_&#xFF08;&#xFF09;</code></a> - &#x4ECE;&#x6240;&#x8FF0;&#x51E0;&#x4F55;&#x5206;&#x5E03;&#x7ED8;&#x5236;&#x7684;&#x5143;&#x7D20;</p>
</li>
<li><p><a href="tensors.html#torch.Tensor.log_normal_" title="torch.Tensor.log_normal_"> <code>torch.Tensor.log_normal_&#xFF08;&#xFF09;</code></a> - &#x4ECE;&#x6240;&#x8FF0;&#x5BF9;&#x6570;&#x6B63;&#x6001;&#x5206;&#x5E03;&#x7684;&#x6837;&#x54C1;</p>
</li>
<li><p><a href="tensors.html#torch.Tensor.normal_" title="torch.Tensor.normal_"> <code>torch.Tensor.normal_&#xFF08;&#xFF09;</code></a> - &#x5C31;&#x5730;&#x7248;&#x672C; <code>torch.normal&#x7684;&#xFF08;&#xFF09;</code></p>
</li>
<li><p><a href="tensors.html#torch.Tensor.random_" title="torch.Tensor.random_"> <code>torch.Tensor.random_&#xFF08;&#xFF09;</code></a> - &#x4ECE;&#x6240;&#x8FF0;&#x79BB;&#x6563;&#x5747;&#x5300;&#x5206;&#x5E03;&#x91C7;&#x6837;&#x7684;&#x6570;&#x5B57;</p>
</li>
<li><p><a href="tensors.html#torch.Tensor.uniform_" title="torch.Tensor.uniform_"> <code>torch.Tensor.uniform_&#xFF08;&#xFF09;</code></a> - &#x4ECE;&#x6240;&#x8FF0;&#x8FDE;&#x7EED;&#x5747;&#x5300;&#x5206;&#x5E03;&#x91C7;&#x6837;&#x7684;&#x6570;&#x5B57;</p>
</li>
</ul>
<h3 id="&#x51C6;&#x968F;&#x673A;&#x91C7;&#x6837;">&#x51C6;&#x968F;&#x673A;&#x91C7;&#x6837;</h3>
<p><em>class</em><code>torch.quasirandom.``SobolEngine</code>( <em>dimension</em> , <em>scramble=False</em> ,
<em>seed=None</em> )<a href="_modules/torch/quasirandom.html#SobolEngine">[source]</a></p>
<p>&#x7684; <code>torch.quasirandom.SobolEngine</code>&#x662F;&#x7528;&#x4E8E;&#x751F;&#x6210;&#xFF08;&#x52A0;&#x6270;&#xFF09;&#x7684;&#x53D1;&#x52A8;&#x673A;Sobol&#x5E8F;&#x5217;&#x3002;
Sobol&#x5E8F;&#x5217;&#x662F;&#x4F4E;&#x5DEE;&#x5F02;&#x51C6;&#x968F;&#x673A;&#x5E8F;&#x5217;&#x7684;&#x4E00;&#x4E2A;&#x4F8B;&#x5B50;&#x3002;</p>
<p>&#x7528;&#x4E8E;Sobol&#x5E8F;&#x5217;&#x7684;&#x53D1;&#x52A8;&#x673A;&#x7684;&#x8FD9;&#x79CD;&#x5B9E;&#x73B0;&#x80FD;&#x591F;&#x91C7;&#x6837;&#x5E8F;&#x5217;&#x7684;&#x9AD8;&#x8FBE;1111&#x5B83;&#x4F7F;&#x7528;&#x65B9;&#x5411;&#x7F16;&#x53F7;&#xFF0C;&#x4EE5;&#x4EA7;&#x751F;&#x8FD9;&#x4E9B;&#x5E8F;&#x5217;&#x7684;&#x6700;&#x5927;&#x5C3A;&#x5BF8;&#xFF0C;&#x5E76;&#x4E14;&#x8FD9;&#x4E9B;&#x6570;&#x5B57;&#x5DF2;&#x7ECF;&#x9002;&#x5E94;&#x4ECE;<a href="http://web.maths.unsw.edu.au/~fkuo/sobol/joe-
kuo-old.1111" target="_blank">&#x8FD9;&#x91CC;</a>&#x3002;</p>
<p>&#x53C2;&#x8003;</p>
<ul>
<li><p>&#x827A;&#x672F;B.&#x6B27;&#x6587;&#x3002;&#x6270;Sobol&#x548C;&#x7684;Niederreiter&#x661F;&#x70B9;&#x3002;&#x8F74;&#x9888;&#x590D;&#x6742;&#x6027;&#xFF0C;14&#xFF08;4&#xFF09;&#xFF1A;466-489&#xFF0C;1998&#x5E74;12&#x6708;&#x3002;</p>
</li>
<li><p>I. M. Sobol&#x3002;&#x70B9;&#x7684;&#x7ACB;&#x65B9;&#x4F53;&#x7684;&#x5206;&#x5E03;&#x548C;&#x79EF;&#x5206;&#x7684;&#x51C6;&#x786E;&#x8BC4;&#x4F30;&#x3002;&#x6DF1;&#x822A;&#x3002; Vychisl&#x3002;&#x57AB;&#x3002;&#x6211;&#x5728;&#x3002;&#x7269;&#x7406;&#x5B66;&#xFF0C;7&#xFF1A;784-802&#xFF0C;1967&#x3002;</p>
</li>
</ul>
<p>Parameters</p>
<ul>
<li><p><strong>&#x7EF4;</strong> &#xFF08; <em>INT</em> &#xFF09; - &#x7684;&#x5E8F;&#x5217;&#x7684;&#x7EF4;&#x5EA6;&#x8981;&#x7ED8;&#x5236;</p>
</li>
<li><p><strong>&#x52A0;&#x6270;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x6B64;&#x8BBE;&#x5B9A;&#x4E3A;<code>&#x771F;</code>&#x5C06;&#x4EA7;&#x751F;&#x52A0;&#x6270;Sobol&#x5E8F;&#x5217;&#x3002;&#x6270;&#x662F;&#x80FD;&#x591F;&#x4EA7;&#x751F;&#x66F4;&#x597D;Sobol&#x5E8F;&#x5217;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x5047; [HTG17&#x3002;</code></p>
</li>
<li><p><strong>&#x79CD;&#x5B50;</strong> &#xFF08; <em>INT</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x8FD9;&#x662F;&#x5BF9;&#x52A0;&#x6270;&#x79CD;&#x5B50;&#x3002;&#x968F;&#x673A;&#x6570;&#x53D1;&#x751F;&#x5668;&#x7684;&#x79CD;&#x5B50;&#x88AB;&#x8BBE;&#x5B9A;&#x4E3A;&#x8FD9;&#x4E00;&#x70B9;&#xFF0C;&#x5982;&#x679C;&#x6307;&#x5B9A;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x65E0;</code></p>
</li>
</ul>
<p>&#x4F8B;&#x5B50;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; soboleng = torch.quasirandom.SobolEngine(dimension=5)
&gt;&gt;&gt; soboleng.draw(3)
tensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
        [0.7500, 0.2500, 0.7500, 0.2500, 0.7500],
        [0.2500, 0.7500, 0.2500, 0.7500, 0.2500]])
</code></pre><p><code>draw</code>( <em>n=1</em> , <em>out=None</em> , <em>dtype=torch.float32</em>
)<a href="_modules/torch/quasirandom.html#SobolEngine.draw">[source]</a></p>
<p>&#x51FD;&#x6570;&#x4ECE;Sobol&#x5E8F;&#x5217;&#x7ED8;&#x5236;&#x7684;<code>n&#x7684;</code>&#x70B9;&#x7684;&#x5E8F;&#x5217;&#x3002;&#x9700;&#x8981;&#x6CE8;&#x610F;&#x7684;&#x662F;&#x6837;&#x54C1;&#x662F;&#x4F9D;&#x8D56;&#x4E8E;&#x5148;&#x524D;&#x7684;&#x6837;&#x672C;&#x3002;&#x7ED3;&#x679C;&#x7684;&#x5927;&#x5C0F;&#x662F; &#xFF08; n&#x7684; &#xFF0C; d  i&#x7684; M  E  n&#x7684; S  i&#x7684; O
n&#x7684; &#xFF09; &#xFF08;N&#xFF0C;&#x5C3A;&#x5BF8;&#xFF09; &#xFF08; n&#x7684; &#xFF0C; d  i&#x7684; M  E  n&#x7684; S  i&#x7684; O  n&#x7684; &#xFF09; &#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>n&#x7684;</strong> &#xFF08; <em>INT</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x70B9;&#x7684;&#x5E8F;&#x5217;&#x7684;&#x957F;&#x5EA6;&#x6765;&#x7ED8;&#x5236;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;1</p>
</li>
<li><p><strong>OUT</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x8F93;&#x51FA;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>DTYPE</strong> &#xFF08;<code>torch.dtype</code>&#xFF0C;&#x53EF;&#x9009;&#xFF09; - &#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x7684;&#x6240;&#x5E0C;&#x671B;&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>torch.float32</code></p>
</li>
</ul>
<p><code>fast_forward</code>( <em>n</em>
)<a href="_modules/torch/quasirandom.html#SobolEngine.fast_forward">[source]</a></p>
<p>&#x901A;&#x8FC7;<code>n&#x7684;</code>&#x6B65;&#x529F;&#x80FD;&#x5FEB;&#x8FDB;&#x7684;<code>SobolEngine</code>&#x7684;&#x72B6;&#x6001;&#x3002;&#x8FD9;&#x7B49;&#x540C;&#x4E8E;&#x5728;&#x4E0D;&#x4F7F;&#x7528;&#x6837;&#x54C1;&#x7ED8;&#x56FE;<code>n&#x7684;</code>&#x6837;&#x54C1;&#x3002;</p>
<p>Parameters</p>
<p><strong>n&#x7684;</strong> &#xFF08; <em>INT</em> &#xFF09; - &#x7684;&#x6B65;&#x6570;&#x7531;&#x5FEB;&#x8FDB;&#x3002;</p>
<p><code>reset</code>()<a href="_modules/torch/quasirandom.html#SobolEngine.reset">[source]</a></p>
<p>&#x529F;&#x80FD;&#x91CD;&#x7F6E;<code>SobolEngine</code>&#x5230;&#x57FA;&#x672C;&#x72B6;&#x6001;&#x3002;</p>
<h2 id="&#x5E8F;&#x5217;">&#x5E8F;&#x5217;</h2>
<p><code>torch.``save</code>( <em>obj</em> , <em>f</em> , <em>pickle_module= <module 'pickle'="" from="" '="" opt="" conda="" lib="" python3.6="" pickle.py'=""></module></em>, <em>pickle_protocol=2</em>
)<a href="_modules/torch/serialization.html#save">[source]</a></p>
<p>&#x4FDD;&#x5B58;&#x5BF9;&#x8C61;&#x5230;&#x78C1;&#x76D8;&#x6587;&#x4EF6;&#x3002;</p>
<p>&#x53C2;&#x89C1;&#xFF1A;<a href="notes/serialization.html#recommend-saving-models"> &#x7528;&#x4E8E;&#x4FDD;&#x5B58;&#x6A21;&#x578B;&#x63A8;&#x8350;&#x7684;&#x65B9;&#x6CD5; </a></p>
<p>Parameters</p>
<ul>
<li><p><strong>OBJ</strong> - &#x4FDD;&#x5B58;&#x5BF9;&#x8C61;</p>
</li>
<li><p><strong>F</strong> - &#x4E00;&#x4E2A;&#x7C7B;&#x6587;&#x4EF6;&#x5BF9;&#x8C61;&#xFF08;&#x5FC5;&#x987B;&#x5B9E;&#x73B0;&#x5199;&#x548C;flush&#xFF09;&#x6216;&#x5305;&#x542B;&#x6587;&#x4EF6;&#x540D;&#x7684;&#x5B57;&#x7B26;&#x4E32;</p>
</li>
<li><p><strong>pickle_module</strong> - &#x6A21;&#x5757;&#x7528;&#x4E8E;&#x9178;&#x6D17;&#x7684;&#x5143;&#x6570;&#x636E;&#x548C;&#x5BF9;&#x8C61;</p>
</li>
<li><p><strong>pickle_protocol</strong> - &#x53EF;&#x4EE5;&#x6307;&#x5B9A;&#x8986;&#x76D6;&#x9ED8;&#x8BA4;&#x534F;&#x8BAE;</p>
</li>
</ul>
<p>Warning</p>
<p>&#x5982;&#x679C;&#x60A8;&#x5728;&#x4F7F;&#x7528;Python 2&#xFF0C; <code>torch.save&#xFF08;&#xFF09;</code>&#x4E0D;&#x652F;&#x6301;<code>StringIO.StringIO</code>&#x4F5C;&#x4E3A;&#x6709;&#x6548;&#x7684;&#x7C7B;&#x6587;&#x4EF6;&#x5BF9;&#x8C61;&#x3002;&#x8FD9;&#x662F;&#x56E0;&#x4E3A;&#x5199;&#x65B9;&#x6CD5;&#x5E94;&#x8BE5;&#x8FD4;&#x56DE;&#x5199;&#x5165;&#x7684;&#x5B57;&#x8282;&#x6570;; <code>StringIO.write&#xFF08;&#xFF09;</code>&#x4E0D;&#x6267;&#x884C;&#x6B64;&#x64CD;&#x4F5C;&#x3002;</p>
<p>&#x8BF7;&#x4F7F;&#x7528;&#x7C7B;&#x4F3C;<a href="https://docs.python.org/3/library/io.html#io.BytesIO" title="\(in Python v3.7\)" target="_blank"> <code>io.BytesIO</code></a>&#x4EE3;&#x66FF;&#x3002;</p>
<p>&#x4F8B;</p>
<pre><code>&gt;&gt;&gt; # Save to file
&gt;&gt;&gt; x = torch.tensor([0, 1, 2, 3, 4])
&gt;&gt;&gt; torch.save(x, &apos;tensor.pt&apos;)
&gt;&gt;&gt; # Save to io.BytesIO buffer
&gt;&gt;&gt; buffer = io.BytesIO()
&gt;&gt;&gt; torch.save(x, buffer)
</code></pre><p><code>torch.``load</code>( <em>f</em> , <em>map_location=None</em> , <em>pickle_module= <module 'pickle'="" from="" '="" opt="" conda="" lib="" python3.6="" pickle.py'=""></module></em>, <em>**pickle_load_args</em>
)<a href="_modules/torch/serialization.html#load">[source]</a></p>
<p>&#x52A0;&#x8F7D;&#x4E00;&#x4E2A;&#x5BF9;&#x8C61;&#x4FDD;&#x5B58; <code>&#x4ECE;&#x6587;&#x4EF6;torch.save&#xFF08;&#xFF09;</code>&#x3002;</p>
<p><code>torch.load&#xFF08;&#xFF09;</code>
&#x4F7F;&#x7528;Python&#x7684;&#x5728;unpickle&#x8BBE;&#x65BD;&#xFF0C;&#x4F46;&#x5BF9;&#x5F85;&#x4ED3;&#x5E93;&#xFF0C;&#x5176;&#x80CC;&#x540E;&#x5F20;&#x91CF;&#xFF0C;&#x7279;&#x522B;&#x662F;&#x3002;&#x4ED6;&#x4EEC;&#x9996;&#x5148;&#x53CD;&#x5E8F;&#x5217;&#x5316;&#x5728;CPU&#x4E0A;&#xFF0C;&#x7136;&#x540E;&#x88AB;&#x8F6C;&#x79FB;&#x5230;&#x4ED6;&#x4EEC;&#x4ECE;&#x5DF2;&#x4FDD;&#x5B58;&#x7684;&#x8BBE;&#x5907;&#x3002;&#x5982;&#x679C;&#x5931;&#x8D25;&#xFF08;&#x4F8B;&#x5982;&#xFF0C;&#x7531;&#x4E8E;&#x8FD0;&#x884C;&#x65F6;&#x7CFB;&#x7EDF;&#x4E0D;&#x5177;&#x6709;&#x4E00;&#x5B9A;&#x7684;&#x8BBE;&#x5907;&#xFF09;&#xFF0C;&#x5C06;&#x5F15;&#x53D1;&#x4E00;&#x4E2A;&#x4F8B;&#x5916;&#x3002;&#x7136;&#x800C;&#xFF0C;&#x5B58;&#x50A8;&#x5668;&#x53EF;&#x4EE5;&#x88AB;&#x52A8;&#x6001;&#x5730;&#x91CD;&#x65B0;&#x6620;&#x5C04;&#x5230;&#x4E00;&#x7EC4;&#x66FF;&#x4EE3;&#x4F7F;&#x7528;<code>map_location</code>&#x53C2;&#x6570;&#x7684;&#x8BBE;&#x5907;&#x3002;</p>
<p>&#x5982;&#x679C;<code>map_location</code>&#x662F;&#x4E00;&#x4E2A;&#x53EF;&#x8C03;&#x7528;&#xFF0C;&#x5B83;&#x5C06;&#x88AB;&#x4E00;&#x6B21;&#x4E3A;&#x6BCF;&#x4E2A;&#x4E32;&#x884C;&#x5316;&#x5B58;&#x50A8;&#x7528;&#x4E24;&#x4E2A;&#x53C2;&#x6570;&#x79F0;&#x4E3A;&#xFF1A;&#x5B58;&#x50A8;&#x548C;&#x4F4D;&#x7F6E;&#x3002;&#x5B58;&#x50A8;&#x53C2;&#x6570;&#x5C06;&#x662F;&#x5B58;&#x50A8;&#x7684;&#x521D;&#x59CB;&#x53CD;&#x5E8F;&#x5217;&#x5316;&#xFF0C;&#x9A7B;&#x7559;&#x5728;&#x6240;&#x8FF0;CPU&#x4E0A;&#x3002;&#x6BCF;&#x4E2A;&#x4E32;&#x884C;&#x5316;&#x5B58;&#x50A8;&#x5177;&#x6709;&#x4E0E;&#x5176;&#x76F8;&#x5173;&#x8054;&#x7684;&#x4F4D;&#x7F6E;&#x6807;&#x7B7E;&#x8BC6;&#x522B;&#x5B83;&#x662F;&#x4ECE;&#x4FDD;&#x5B58;&#x8BE5;&#x8BBE;&#x5907;&#xFF0C;&#x8BE5;&#x6807;&#x7B7E;&#x662F;&#x4F20;&#x9012;&#x7ED9;<code>map_location</code>&#x7B2C;&#x4E8C;&#x4E2A;&#x53C2;&#x6570;&#x3002;&#x5185;&#x7F6E;&#x7684;&#x4F4D;&#x7F6E;&#x4EE3;&#x7801;&#x662F;<code>&apos;CPU&apos; [HTG11&#x7528;&#x4E8E;CPU&#x5F20;&#x91CF;&#x548C;</code>&apos;CUDA&#xFF1A;DEVICE_ID&apos; <code>&#xFF08;&#x4F8B;&#x5982;</code>&apos;
CUDA&#xFF1A;2&apos;<code>&#xFF09;&#x4E3A;CUDA&#x5F20;&#x91CF;&#x3002;</code>map_location<code>&#x8FD4;&#x56DE;&#x503C;&#x5E94;&#x5F53;&#x662F;</code>&#x65E0; <code>&#x6216;&#x5B58;&#x50A8;&#x3002;&#x5982;&#x679C;</code>map_location
<code>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x5B58;&#x50A8;&#xFF0C;&#x5B83;&#x5C06;&#x88AB;&#x7528;&#x4F5C;&#x6700;&#x7EC8;&#x53CD;&#x5E8F;&#x5217;&#x5316;&#x5BF9;&#x8C61;&#xFF0C;&#x5DF2;&#x88AB;&#x79FB;&#x52A8;&#x5230;&#x6B63;&#x786E;&#x7684;&#x8BBE;&#x5907;&#x3002;&#x5426;&#x5219;&#xFF0C;</code>torch.load&#xFF08;&#xFF09; <code>&#x5C06;&#x56DE;&#x843D;&#x5230;&#x9ED8;&#x8BA4;&#x7684;&#x884C;&#x4E3A;&#xFF0C;&#x4EFF;&#x4F5B;</code>
map_location<code>WASN&#x201D; &#x164;&#x89C4;&#x5B9A;&#x3002;</code></p>
<p>&#x5982;&#x679C;<code>map_location</code>&#x662F;<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"> <code>torch.device</code>
</a>&#x5BF9;&#x8C61;&#x6216;&#x5B57;&#x7B26;&#x4E32;contraining&#x8BBE;&#x5907;&#x6807;&#x8BB0;&#xFF0C;&#x5176;&#x6307;&#x793A;&#x7684;&#x4F4D;&#x7F6E;&#x6240;&#x6709;&#x5F20;&#x91CF;&#x5E94;&#x8BE5;&#x88AB;&#x52A0;&#x8F7D;&#x3002;</p>
<p>&#x5426;&#x5219;&#xFF0C;&#x5982;&#x679C;<code>map_location</code>&#x662F;&#x4E00;&#x4E2A;&#x5B57;&#x5178;&#xFF0C;&#x5B83;&#x5C06;&#x88AB;&#x7528;&#x4E8E;&#x91CD;&#x65B0;&#x6620;&#x5C04;&#x51FA;&#x73B0;&#x7684;&#x6587;&#x4EF6;&#xFF08;&#x5BC6;&#x94A5;&#xFF09;&#x7684;&#x4F4D;&#x7F6E;&#x6807;&#x8BB0;&#xFF0C;&#x4EE5;&#x90A3;&#x4E9B;&#x6307;&#x5B9A;&#x5728;&#x54EA;&#x91CC;&#x653E;&#x7F6E;&#x5B58;&#x50A8;&#x5668;&#xFF08;&#x503C;&#xFF09;&#x3002;</p>
<p>&#x7528;&#x6237;&#x6269;&#x5C55;&#x53EF;&#x4F7F;&#x7528;<code>torch.serialization.register_package&#xFF08;&#xFF09;</code>&#x6CE8;&#x518C;&#x81EA;&#x5DF1;&#x7684;&#x4F4D;&#x7F6E;&#x7684;&#x6807;&#x7B7E;&#x548C;&#x6807;&#x8BB0;&#x548C;&#x53CD;&#x4E32;&#x884C;&#x5316;&#x7684;&#x65B9;&#x6CD5;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>F</strong> - &#x4E00;&#x4E2A;&#x7C7B;&#x6587;&#x4EF6;&#x5BF9;&#x8C61;&#xFF08;&#x5FC5;&#x987B;&#x5B9E;&#x73B0;<code>&#x8BFB;&#xFF08;&#xFF09;</code>&#xFF1A;meth<code>readline</code>&#xFF0C;&#xFF1A;meth<code>tell</code>&#xFF0C;&#x5E76;&#x4E14;&#xFF1A;meth<code>seek</code>&#xFF09;&#xFF0C;&#x6216;&#x5305;&#x542B;&#x6587;&#x4EF6;&#x540D;&#x7684;&#x5B57;&#x7B26;&#x4E32;</p>
</li>
<li><p><strong>map_location</strong> - &#x7684;&#x51FD;&#x6570;&#xFF0C;<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"> <code>torch.device</code></a>&#xFF0C;&#x5B57;&#x7B26;&#x4E32;&#x6216;&#x4E00;&#x4E2A;&#x5B57;&#x5178;&#x6307;&#x5B9A;&#x5982;&#x4F55;&#x91CD;&#x65B0;&#x6620;&#x5C04;&#x7684;&#x5B58;&#x50A8;&#x4F4D;&#x7F6E;</p>
</li>
<li><p><strong>pickle_module</strong> - &#x7528;&#x4E8E;&#x53D6;&#x50A8;&#x5B58;&#x5143;&#x6570;&#x636E;&#x548C;&#x5BF9;&#x8C61;&#x6A21;&#x5757;&#xFF08;&#x5177;&#x6709;&#x76F8;&#x5339;&#x914D;&#x7684;<code>pickle_module</code>&#x7528;&#x4E8E;&#x5E8F;&#x5217;&#x5316;&#x6587;&#x4EF6;&#xFF09;</p>
</li>
<li><p><strong>pickle_load_args</strong> - &#x53EF;&#x9009;&#x7684;&#x5173;&#x952E;&#x5B57;&#x53C2;&#x6570;&#x4F20;&#x9012;&#x5230;<code>pickle_module.load&#xFF08;&#xFF09;</code>&#x548C;<code>pickle_module.Unpickler&#xFF08;&#xFF09;</code>&#x4F8B;&#x5982;&#xFF0C;<code>&#x7F16;&#x7801;= ...</code>&#x3002;</p>
</li>
</ul>
<p>Note</p>
<p>&#x5F53;&#x4F60;&#x8C03;&#x7528; <code>torch.load&#xFF08;&#xFF09;</code>&#x5728;&#x5305;&#x542B;GPU&#x5F20;&#x91CF;&#x7684;&#x6587;&#x4EF6;&#x65F6;&#xFF0C;&#x8FD9;&#x4E9B;&#x5F20;&#x91CF;&#x5C06;&#x88AB;&#x9ED8;&#x8BA4;&#x52A0;&#x8F7D;&#x5230;GPU&#x3002;&#x53EF;&#x4EE5;&#x8C03;&#x7528;<code>torch.load&#xFF08;..&#xFF0C;
map_location = &apos;CPU&apos;&#xFF09;</code>&#x2192;<code>load_state_dict&#xFF08;&#xFF09;</code>&#x4EE5;&#x907F;&#x514D;GPU RAM&#x6D6A;&#x6D8C;&#x52A0;&#x8F7D;&#x6A21;&#x578B;&#x7684;&#x68C0;&#x67E5;&#x70B9;&#x65F6;&#x3002;</p>
<p>Note</p>
<p>&#x5728;Python 3&#xFF0C;&#x52A0;&#x8F7D;&#x7531;Python 2&#x4E2D;&#x4FDD;&#x5B58;&#x7684;&#x6587;&#x4EF6;&#x65F6;&#xFF0C;&#x53EF;&#x80FD;&#x4F1A;&#x9047;&#x5230;<code>&#x7684;UnicodeDecodeError&#xFF1A; &apos;ASCII&apos; &#x7F16;&#x89E3;&#x7801;&#x5668; &#x4E0D;&#x80FD; &#x89E3;&#x7801; &#x5B57;&#x8282;
0X ......  [HTG15&#x3002;&#x8FD9;&#x662F;&#x7531;&#x4F60;&#x53EF;&#x4EE5;&#x4F7F;&#x7528;&#x989D;&#x5916;&#x7684;</code>&#x7F16;&#x7801; <code>&#x5173;&#x952E;&#x5B57;&#x53C2;&#x6570;&#x6765;&#x6307;&#x5B9A;&#x8FD9;&#x4E9B;&#x5BF9;&#x8C61;&#x5E94;&#x8BE5;&#x5982;&#x4F55;&#x88AB;&#x52A0;&#x8F7D;&#x540E;&#xFF0C;&#x5728;Python2&#x548C;Python
3&#x5B57;&#x8282;&#x4E32;&#x5904;&#x7406;&#x5DEE;&#x5F02;&#x9020;&#x6210;&#x7684;&#x5982;</code>&#x7F16;&#x7801;= &apos;latin1&#x7684;&apos; <code>&#x4F7F;&#x7528;</code>LATIN1<code>&#x7F16;&#x7801;&#x5BF9;&#x5B83;&#x4EEC;&#x8FDB;&#x884C;&#x89E3;&#x7801;&#x4E3A;&#x5B57;&#x7B26;&#x4E32;&#xFF0C;&#x5E76;</code>&#x7F16;&#x7801;= &apos;&#x5B57;&#x8282;&apos; <code>&#x4F7F;&#x4ED6;&#x4EEC;&#x4F5C;&#x4E3A;&#x53EF;&#x7A0D;&#x540E;</code>
byte_array.decode&#xFF08;......&#xFF09; <code>&#x89E3;&#x7801;&#x5B57;&#x8282;&#x9635;&#x5217;&#x3002;</code></p>
<p>Example</p>
<pre><code>&gt;&gt;&gt; torch.load(&apos;tensors.pt&apos;)
# Load all tensors onto the CPU
&gt;&gt;&gt; torch.load(&apos;tensors.pt&apos;, map_location=torch.device(&apos;cpu&apos;))
# Load all tensors onto the CPU, using a function
&gt;&gt;&gt; torch.load(&apos;tensors.pt&apos;, map_location=lambda storage, loc: storage)
# Load all tensors onto GPU 1
&gt;&gt;&gt; torch.load(&apos;tensors.pt&apos;, map_location=lambda storage, loc: storage.cuda(1))
# Map tensors from GPU 1 to GPU 0
&gt;&gt;&gt; torch.load(&apos;tensors.pt&apos;, map_location={&apos;cuda:1&apos;:&apos;cuda:0&apos;})
# Load tensor from io.BytesIO object
&gt;&gt;&gt; with open(&apos;tensor.pt&apos;, &apos;rb&apos;) as f:
        buffer = io.BytesIO(f.read())
&gt;&gt;&gt; torch.load(buffer)
</code></pre><h2 id="&#x5E76;&#x884C;">&#x5E76;&#x884C;</h2>
<p><code>torch.``get_num_threads</code>() &#x2192; int</p>
<p>&#x8FD4;&#x56DE;&#x7528;&#x4E8E;CPU&#x5E76;&#x884C;&#x64CD;&#x4F5C;&#x7684;&#x7EBF;&#x7A0B;&#x6570;</p>
<p><code>torch.``set_num_threads</code>( <em>int</em> )</p>
<p>&#x8BBE;&#x7F6E;&#x7528;&#x4E8E;CPU&#x5E76;&#x884C;&#x64CD;&#x4F5C;&#x7684;&#x7EBF;&#x7A0B;&#x6570;&#x3002;&#x8B66;&#x544A;&#xFF1A;&#x4E3A;&#x786E;&#x4FDD;&#x4F7F;&#x7528;&#x7684;&#x7EBF;&#x7A0B;&#x6570;&#x76EE;&#x6B63;&#x786E;&#xFF0C;set_num_threads&#x5FC5;&#x987B;&#x5728;&#x8FD0;&#x884C;&#x5FC3;&#x5207;&#xFF0C;JIT&#x6216;autograd&#x4EE3;&#x7801;&#x4E4B;&#x524D;&#x8C03;&#x7528;&#x3002;</p>
<p><code>torch.``get_num_interop_threads</code>() &#x2192; int</p>
<p>&#x8FD4;&#x56DE;&#x7528;&#x4E8E;-OP&#x95F4;&#x5E76;&#x884C;CPU&#x4E0A;&#x7684;&#x7EBF;&#x7A0B;&#x7684;&#x6570;&#x76EE;&#xFF08;&#x4F8B;&#x5982;&#xFF0C;&#x5728;JIT&#x89E3;&#x91CA;&#x5668;&#xFF09;</p>
<p><code>torch.``set_num_interop_threads</code>( <em>int</em> )</p>
<p>&#x8BBE;&#x7F6E;&#x7528;&#x4E8E;&#x4E92;&#x64CD;&#x4F5C;&#x5E76;&#x884C;&#xFF08;&#x4F8B;&#x5982;&#xFF0C;&#x5728;JIT&#x89E3;&#x91CA;&#x5668;&#xFF09;&#x4E0A;&#x7684;CPU&#x7EBF;&#x7A0B;&#x7684;&#x6570;&#x76EE;&#x3002;&#x8B66;&#x544A;&#xFF1A;&#x53EF;&#x4EE5;&#x5728;&#x4EFB;&#x4F55;&#x8DE8;&#x8FD0;&#x5E76;&#x884C;&#x5DE5;&#x4F5C;&#x5F00;&#x59CB;&#xFF08;&#x5982;JIT&#x6267;&#x884C;&#xFF09;&#x4E4B;&#x524D;&#x53EA;&#x80FD;&#x88AB;&#x8C03;&#x7528;&#x4E00;&#x6B21;&#x548C;&#x3002;</p>
<h2 id="&#x672C;&#x5730;&#x7981;&#x7528;&#x68AF;&#x5EA6;&#x8BA1;&#x7B97;">&#x672C;&#x5730;&#x7981;&#x7528;&#x68AF;&#x5EA6;&#x8BA1;&#x7B97;</h2>
<p>&#xFF0C;torch.set_grad_enabled&#x7684;&#x4E0A;&#x4E0B;&#x6587;&#x7BA1;&#x7406;&#x5668;<code>torch.no_grad&#xFF08;&#xFF09;&#xFF0C;</code>torch.enable_grad&#xFF08;&#xFF09; <code>&#x548C;</code>&#xFF08; &#xFF09;
<code>&#x662F;&#x7528;&#x4E8E;&#x5C40;&#x90E8;&#x7981;&#x7528;&#x548C;&#x542F;&#x7528;&#x68AF;&#x5EA6;&#x8BA1;&#x7B97;&#x6709;&#x5E2E;&#x52A9;&#x3002;&#x53C2;&#x89C1;[ &#x672C;&#x5730;&#x7981;&#x7528;&#x68AF;&#x5EA6;&#x8BA1;&#x7B97; ](autograd.html#locally-disable-
grad)&#x5173;&#x4E8E;&#x5176;&#x4F7F;&#x7528;&#x7684;&#x66F4;&#x591A;&#x7EC6;&#x8282;&#x3002;&#x8FD9;&#x4E9B;&#x60C5;&#x5883;&#x7ECF;&#x7406;&#x7EBF;&#x7A0B;&#x5C40;&#x90E8;&#x7684;&#xFF0C;&#x6240;&#x4EE5;&#x5982;&#x679C;&#x4F60;&#x4F7F;&#x7528; &#x53D1;&#x9001;&#x5DE5;&#x4F5C;&#x5230;&#x53E6;&#x4E00;&#x4E2A;&#x7EBF;&#x7A0B;&#xFF0C;&#x4ED6;&#x4EEC;&#x5C06;&#x65E0;&#x6CD5;&#x6B63;&#x5E38;&#x5DE5;&#x4F5C;&#xFF1A;&#x6A21;&#x5757;&#xFF1A;</code>threading<code>&#x6A21;&#x5757;&#x7B49;&#x3002;</code></p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; x = torch.zeros(1, requires_grad=True)
&gt;&gt;&gt; with torch.no_grad():
...     y = x * 2
&gt;&gt;&gt; y.requires_grad
False

&gt;&gt;&gt; is_train = False
&gt;&gt;&gt; with torch.set_grad_enabled(is_train):
...     y = x * 2
&gt;&gt;&gt; y.requires_grad
False

&gt;&gt;&gt; torch.set_grad_enabled(True)  # this can also be used as a function
&gt;&gt;&gt; y = x * 2
&gt;&gt;&gt; y.requires_grad
True

&gt;&gt;&gt; torch.set_grad_enabled(False)
&gt;&gt;&gt; y = x * 2
&gt;&gt;&gt; y.requires_grad
False
</code></pre><h2 id="&#x6570;&#x5B66;&#x8FD0;&#x7B97;">&#x6570;&#x5B66;&#x8FD0;&#x7B97;</h2>
<h3 id="&#x9010;&#x70B9;&#x884C;&#x52A8;">&#x9010;&#x70B9;&#x884C;&#x52A8;</h3>
<p><code>torch.``abs</code>( <em>input</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8BA1;&#x7B97;&#x7ED9;&#x5B9A;&#x7684;<code>&#x8F93;&#x5165;</code>&#x5F20;&#x91CF;&#x7684;&#x9010;&#x5143;&#x7D20;&#x7684;&#x7EDD;&#x5BF9;&#x503C;&#x3002;</p>
<p>outi=&#x2223;inputi&#x2223;\text{out}<em>{i} = |\text{input}</em>{i}| outi&#x200B;=&#x2223;inputi&#x200B;&#x2223;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.abs(torch.tensor([-1, -2, 3]))
tensor([ 1,  2,  3])
</code></pre><p><code>torch.``acos</code>( <em>input</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E0E;&#x8F93;&#x5165;&#x7684;<code>&#x7684;&#x5143;&#x7D20;</code>&#x7684;&#x53CD;&#x4F59;&#x5F26;&#x4E00;&#x4E2A;&#x65B0;&#x7684;&#x5F20;&#x91CF;&#x3002;</p>
<p>outi=cos&#x2061;&#x2212;1(inputi)\text{out}<em>{i} = \cos^{-1}(\text{input}</em>{i})
outi&#x200B;=cos&#x2212;1(inputi&#x200B;)</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([ 0.3348, -0.5889,  0.2005, -0.1584])
&gt;&gt;&gt; torch.acos(a)
tensor([ 1.2294,  2.2004,  1.3690,  1.7298])
</code></pre><p><code>torch.``add</code>()</p>
<p><code>torch.``add</code>( <em>input</em> , <em>other</em> , <em>out=None</em> )</p>
<p>&#x589E;&#x52A0;&#x4E86;&#x6807;&#x91CF;<code>&#x5176;&#x4ED6;</code>&#x5230;&#x8F93;&#x5165;<code>&#x8F93;&#x5165;</code>&#x4E2D;&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#xFF0C;&#x5E76;&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x65B0;&#x7684;&#x7531;&#x6B64;&#x800C;&#x6765;&#x5F20;&#x91CF;&#x3002;</p>
<p>out=input+other\text{out} = \text{input} + \text{other} out=input+other</p>
<p>&#x5982;&#x679C;<code>&#x8F93;&#x5165;</code>&#x662F;&#x7C7B;&#x578B;FloatTensor&#x6216;DoubleTensor&#x7684;&#xFF0C;<code>&#x5176;&#x4ED6;</code>&#x5FC5;&#x987B;&#x662F;&#x4E00;&#x4E2A;&#x5B9E;&#x6570;&#xFF0C;&#x5426;&#x5219;&#x5B83;&#x5E94;&#x8BE5;&#x662F;&#x6574;&#x6570;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>&#x5176;&#x4ED6;</strong> &#xFF08; <em>&#x53F7;&#x7801;</em> &#xFF09; - &#x7684;&#x6570;&#x91CF;&#x88AB;&#x6DFB;&#x52A0;&#x5230;<code>&#x8F93;&#x5165;</code>&#x7684;&#x5404;&#x8981;&#x7D20;</p>
</li>
</ul>
<p>Keyword Arguments</p>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em>
) &#x2013; the output tensor</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([ 0.0202,  1.0985,  1.3506, -0.6056])
&gt;&gt;&gt; torch.add(a, 20)
tensor([ 20.0202,  21.0985,  21.3506,  19.3944])
</code></pre><p><code>torch.``add</code>( <em>input</em> , <em>alpha=1</em> , <em>other</em> , <em>out=None</em> )</p>
<p>&#x5F20;&#x91CF;&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;<code>&#x5176;&#x4ED6;</code>&#x7531;&#x6807;&#x91CF;<code>&#x963F;&#x5C14;&#x6CD5;</code>&#x76F8;&#x4E58;&#xFF0C;&#x5E76;&#x52A0;&#x5165;&#x5230;&#x8BE5;&#x5F20;&#x91CF;&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;<code>&#x8F93;&#x5165;</code>&#x3002;&#x5F97;&#x5230;&#x7684;&#x5F20;&#x91CF;&#x8FD4;&#x56DE;&#x3002;</p>
<p>&#x8F93;&#x5165;&#x7684;<code>&#x5F62;&#x72B6;</code>&#x548C;<code>&#x5176;&#x4ED6;</code>&#x5FC5;&#x987B;<a href="notes/broadcasting.html#broadcasting-%0Asemantics"> broadcastable  </a>&#x3002;</p>
<p>out=input+alpha&#xD7;other\text{out} = \text{input} + \text{alpha} \times
\text{other} out=input+alpha&#xD7;other</p>
<p>&#x5982;&#x679C;<code>&#x5176;&#x4ED6;</code>&#x662F;&#x7C7B;&#x578B;FloatTensor&#x6216;DoubleTensor&#x7684;&#xFF0C;<code>&#x963F;&#x5C14;&#x6CD5;</code>&#x5FC5;&#x987B;&#x662F;&#x4E00;&#x4E2A;&#x5B9E;&#x6570;&#xFF0C;&#x5426;&#x5219;&#x5B83;&#x5E94;&#x8BE5;&#x662F;&#x6574;&#x6570;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x7B2C;&#x4E00;&#x8F93;&#x5165;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>&#x963F;&#x5C14;&#x6CD5;</strong> &#xFF08; <em>&#x53F7;&#x7801;</em> &#xFF09; - &#x4E3A;<code>&#x6807;&#x91CF;&#x4E58;&#x6570;&#x5176;&#x4ED6;</code></p>
</li>
<li><p><strong>&#x5176;&#x4ED6;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x7B2C;&#x4E8C;&#x8F93;&#x5165;&#x5F20;&#x91CF;</p>
</li>
</ul>
<p>Keyword Arguments</p>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em>
) &#x2013; the output tensor</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([-0.9732, -0.3497,  0.6245,  0.4022])
&gt;&gt;&gt; b = torch.randn(4, 1)
&gt;&gt;&gt; b
tensor([[ 0.3743],
        [-1.7724],
        [-0.5811],
        [-0.8017]])
&gt;&gt;&gt; torch.add(a, 10, b)
tensor([[  2.7695,   3.3930,   4.3672,   4.1450],
        [-18.6971, -18.0736, -17.0994, -17.3216],
        [ -6.7845,  -6.1610,  -5.1868,  -5.4090],
        [ -8.9902,  -8.3667,  -7.3925,  -7.6147]])
</code></pre><p><code>torch.``addcdiv</code>( <em>input</em> , <em>value=1</em> , <em>tensor1</em> , <em>tensor2</em> , <em>out=None</em> )
&#x2192; Tensor</p>
<p>&#x901A;&#x8FC7;<code>&#x6267;&#x884C;tensor1</code>&#x7684;<code>&#x9010;&#x5143;&#x7D20;&#x9664;&#x6CD5;tensor2</code>&#xFF0C;&#x7531;&#x6807;&#x91CF;<code>&#x503C;[HTG10&#x76F8;&#x4E58;&#x7684;&#x7ED3;&#x679C;]</code>&#xFF0C;&#x5E76;&#x5C06;&#x5176;&#x6DFB;&#x52A0;&#x5230;<code>&#x8F93;&#x5165;</code>&#x3002;</p>
<p>outi=inputi+value&#xD7;tensor1itensor2i\text{out}_i = \text{input}_i + \text{value}
\times \frac{\text{tensor1}_i}{\text{tensor2}_i}
outi&#x200B;=inputi&#x200B;+value&#xD7;tensor2i&#x200B;tensor1i&#x200B;&#x200B;</p>
<p>&#x7684;&#x5F62;&#x72B6;<code>&#x8F93;&#x5165;</code>&#xFF0C;<code>tensor1</code>&#x548C;<code>tensor2</code>&#x5FC5;&#x987B;<a href="notes/broadcasting.html#broadcasting-semantics"> broadcastable
</a>&#x3002;</p>
<p>&#x5BF9;&#x4E8E;&#x7C7B;&#x578B;&#x7684;&#x8F93;&#x5165; FloatTensor &#x6216; DoubleTensor &#xFF0C;<code>&#x503C;</code>&#x5FC5;&#x987B;&#x662F;&#x4E00;&#x4E2A;&#x5B9E;&#x6570;&#xFF0C;&#x5426;&#x5219;&#x7684;&#x6574;&#x6570;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x8981;&#x6DFB;&#x52A0;&#x7684;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>&#x503C;</strong> &#xFF08; <em>&#x53F7;&#x7801;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x4E58;&#x6570; tensor1  /  tensor2  \&#x6587;&#x672C;{tensor1} / \&#x6587;&#x672C;{tensor2}  tensor1  /  tensor2 </p>
</li>
<li><p><strong>tensor1</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x5206;&#x5B50;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>tensor2</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x5206;&#x6BCD;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; t = torch.randn(1, 3)
&gt;&gt;&gt; t1 = torch.randn(3, 1)
&gt;&gt;&gt; t2 = torch.randn(1, 3)
&gt;&gt;&gt; torch.addcdiv(t, 0.1, t1, t2)
tensor([[-0.2312, -3.6496,  0.1312],
        [-1.0428,  3.4292, -0.1030],
        [-0.5369, -0.9829,  0.0430]])
</code></pre><p><code>torch.``addcmul</code>( <em>input</em> , <em>value=1</em> , <em>tensor1</em> , <em>tensor2</em> , <em>out=None</em> )
&#x2192; Tensor</p>
<p>&#x6267;&#x884C;&#x7531;<code>tensor2 tensor1</code>&#x7684;<code>&#x9010;&#x5143;&#x7D20;&#x4E58;&#x6CD5;</code>&#xFF0C;&#x7531;&#x6807;&#x91CF;<code>&#x503C;[HTG10&#x76F8;&#x4E58;&#x7684;&#x7ED3;&#x679C;]</code>&#xFF0C;&#x5E76;&#x5C06;&#x5176;&#x6DFB;&#x52A0;&#x5230;<code>&#x8F93;&#x5165;</code>&#x3002;</p>
<p>outi=inputi+value&#xD7;tensor1i&#xD7;tensor2i\text{out}_i = \text{input}_i +
\text{value} \times \text{tensor1}_i \times \text{tensor2}_i
outi&#x200B;=inputi&#x200B;+value&#xD7;tensor1i&#x200B;&#xD7;tensor2i&#x200B;</p>
<p>The shapes of <code>input</code>, <code>tensor1</code>, and <code>tensor2</code>must be
<a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a>.</p>
<p>For inputs of type FloatTensor or DoubleTensor, <code>value</code>must be a real number,
otherwise an integer.</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the tensor to be added</p>
</li>
<li><p><strong>&#x503C;</strong> &#xFF08; <em>&#x53F7;&#x7801;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x4E58;&#x6570; T  E  n&#x7684; S  O  R  1.  <em>  T  E  n&#x7684; S  O  R  2  tensor1&#x3002;</em> tensor2  T  E  n&#x7684; S  O  R  1  &#x3002;  *  T  E  n&#x7684; S  O  R  2 </p>
</li>
<li><p><strong>tensor1</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x8981;&#x88AB;&#x4E58;&#x7684;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>tensor2</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x8981;&#x88AB;&#x4E58;&#x7684;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; t = torch.randn(1, 3)
&gt;&gt;&gt; t1 = torch.randn(3, 1)
&gt;&gt;&gt; t2 = torch.randn(1, 3)
&gt;&gt;&gt; torch.addcmul(t, 0.1, t1, t2)
tensor([[-0.8635, -0.6391,  1.6174],
        [-0.7617, -0.5879,  1.7388],
        [-0.8353, -0.6249,  1.6511]])
</code></pre><p><code>torch.``asin</code>( <em>input</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E0E;&#x8F93;&#x5165;&#x7684;<code>&#x7684;&#x5143;&#x7D20;</code>&#x7684;&#x53CD;&#x6B63;&#x5F26;&#x65B0;&#x5F20;&#x91CF;&#x3002;</p>
<p>outi=sin&#x2061;&#x2212;1(inputi)\text{out}<em>{i} = \sin^{-1}(\text{input}</em>{i})
outi&#x200B;=sin&#x2212;1(inputi&#x200B;)</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([-0.5962,  1.4985, -0.4396,  1.4525])
&gt;&gt;&gt; torch.asin(a)
tensor([-0.6387,     nan, -0.4552,     nan])
</code></pre><p><code>torch.``atan</code>( <em>input</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E0E;&#x8F93;&#x5165;&#x7684;<code>&#x7684;&#x5143;&#x7D20;</code>&#x7684;&#x53CD;&#x6B63;&#x5207;&#x4E00;&#x4E2A;&#x65B0;&#x7684;&#x5F20;&#x91CF;&#x3002;</p>
<p>outi=tan&#x2061;&#x2212;1(inputi)\text{out}<em>{i} = \tan^{-1}(\text{input}</em>{i})
outi&#x200B;=tan&#x2212;1(inputi&#x200B;)</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([ 0.2341,  0.2539, -0.6256, -0.6448])
&gt;&gt;&gt; torch.atan(a)
tensor([ 0.2299,  0.2487, -0.5591, -0.5727])
</code></pre><p><code>torch.``atan2</code>( <em>input</em> , <em>other</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E0E;&#x8F93;&#x5165;&#x7684;<code>&#x548C;&#x5143;&#x7D20; ``&#x5176;&#x4ED6;</code>&#x7684;&#x53CD;&#x6B63;&#x5207;&#x4E00;&#x4E2A;&#x65B0;&#x7684;&#x5F20;&#x91CF;&#x3002;</p>
<p>The shapes of <code>input</code>and <code>other</code>must be
<a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a>.</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the first input tensor</p>
</li>
<li><p><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the second input tensor</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([ 0.9041,  0.0196, -0.3108, -2.4423])
&gt;&gt;&gt; torch.atan2(a, torch.randn(4))
tensor([ 0.9833,  0.0811, -1.9743, -1.4151])
</code></pre><p><code>torch.``ceil</code>( <em>input</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E0E;&#x8F93;&#x5165;&#x7684;<code>&#x7684;&#x5143;&#x7D20;</code>&#x7684;&#x6700;&#x5C0F;&#x6574;&#x6570;&#x5927;&#x4E8E;&#x6216;&#x7B49;&#x4E8E;&#x6BCF;&#x4E2A;&#x5143;&#x4EF6;&#x7684;&#x5C0F;&#x533A;&#x4E00;&#x4E2A;&#x65B0;&#x7684;&#x5F20;&#x91CF;&#x3002;</p>
<p>outi=&#x2308;inputi&#x2309;=&#x230A;inputi&#x230B;+1\text{out}<em>{i} = \left\lceil \text{input}</em>{i}
\right\rceil = \left\lfloor \text{input}_{i} \right\rfloor + 1
outi&#x200B;=&#x2308;inputi&#x200B;&#x2309;=&#x230A;inputi&#x200B;&#x230B;+1</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([-0.6341, -1.4208, -1.0900,  0.5826])
&gt;&gt;&gt; torch.ceil(a)
tensor([-0., -1., -1.,  1.])
</code></pre><p><code>torch.``clamp</code>( <em>input</em> , <em>min</em> , <em>max</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x5939;&#x4F4F;<code>&#x8F93;&#x5165;</code>&#x7684;&#x6240;&#x6709;&#x5143;&#x7D20;&#x4E3A;&#x8303;&#x56F4; [ <code>&#x5206;&#x949F;HTG9]</code>&#xFF0C; <code>MAX</code>&#xFF0C;&#x5E76;&#x8FD4;&#x56DE;&#x6240;&#x5F97;&#x5230;&#x7684;&#x5F20;&#x91CF;&#xFF1A;</p>
<p>yi={minif xi<minxiif min≤xi≤maxmaxif="" xi="">maxy_i = \begin{cases} \text{min} &amp;
\text{if } x_i &lt; \text{min} \\ x_i &amp; \text{if } \text{min} \leq x_i \leq
\text{max} \\ \text{max} &amp; \text{if } x_i &gt; \text{max} \end{cases}
yi&#x200B;=&#x23A9;&#x23AA;&#x23A8;&#x23AA;&#x23A7;&#x200B;minxi&#x200B;max&#x200B;if xi&#x200B;<minif min≤xi​≤maxif="" xi​="">max&#x200B;</minif></minxiif></p>
<p>&#x5982;&#x679C;<code>&#x8F93;&#x5165;</code>&#x662F;&#x5F0F; FloatTensor &#x6216; DoubleTensor &#xFF0C;ARGS<code>&#x5206;&#x949F;HTG11]</code>&#x548C; <code>MAX</code>
&#x5FC5;&#x987B;&#x662F;&#x5B9E;&#x6570;&#xFF0C;&#x5426;&#x5219;&#x4ED6;&#x4EEC;&#x5E94;&#x8BE5;&#x662F;&#x6574;&#x6570;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>&#x5206;&#x949F;HTG1]&#xFF08; <em>&#x53F7;&#x7801;</em> &#xFF09; - &#x7ED3;&#x5408;&#x7684;&#x8F83;&#x4F4E;&#x7684;&#x8303;&#x56F4;&#x5185;&#x7684;&#x88AB;&#x5939;&#x7D27;&#x5230;</strong></p>
</li>
<li><p><strong>MAX</strong> &#xFF08; <em>&#x53F7;&#x7801;</em> &#xFF09; - &#x4E0A;&#x9650;&#x7684;&#x8303;&#x56F4;&#x5185;&#x7684;&#x88AB;&#x5939;&#x7D27;&#x5230;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([-1.7120,  0.1734, -0.0478, -0.0922])
&gt;&gt;&gt; torch.clamp(a, min=-0.5, max=0.5)
tensor([-0.5000,  0.1734, -0.0478, -0.0922])
</code></pre><p><code>torch.``clamp</code>( <em>input</em> , <em>*</em> , <em>min</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x5939;&#x5177;&#x5728;<code>&#x8F93;&#x5165;</code>&#x4E3A;&#x5927;&#x4E8E;&#x6216;&#x7B49;&#x4E8E; <code>&#x5206;&#x949F;HTG7]</code>&#x7684;&#x6240;&#x6709;&#x5143;&#x7D20;&#x3002;</p>
<p>&#x5982;&#x679C;<code>&#x8F93;&#x5165;</code>&#x662F;&#x5F0F; FloatTensor &#x6216; DoubleTensor &#xFF0C;<code>&#x503C;</code>&#x5E94;&#x8BE5;&#x662F;&#x4E00;&#x4E2A;&#x771F;&#x6B63;&#x53F7;&#xFF0C;&#x5426;&#x5219;&#x5B83;&#x5E94;&#x8BE5;&#x662F;&#x6574;&#x6570;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>&#x503C;</strong> &#xFF08; <em>&#x53F7;&#x7801;</em> &#xFF09; - &#x5728;&#x8F93;&#x51FA;&#x4E2D;&#x7684;&#x5404;&#x5143;&#x7D20;&#x7684;&#x6700;&#x5C0F;&#x503C;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([-0.0299, -2.3184,  2.1593, -0.8883])
&gt;&gt;&gt; torch.clamp(a, min=0.5)
tensor([ 0.5000,  0.5000,  2.1593,  0.5000])
</code></pre><p><code>torch.``clamp</code>( <em>input</em> , <em>*</em> , <em>max</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x5939;&#x5177;&#x5728;<code>&#x8F93;&#x5165;&#x7684;&#x6240;&#x6709;&#x5143;&#x7D20;</code>&#x662F;&#x5C0F;&#x4E8E;&#x6216;&#x7B49;&#x4E8E; <code>MAX</code>&#x3002;</p>
<p>If <code>input</code>is of type FloatTensor or DoubleTensor, <code>value</code>should be a real
number, otherwise it should be an integer.</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>&#x503C;</strong> &#xFF08; <em>&#x53F7;&#x7801;</em> &#xFF09; - &#x5728;&#x8F93;&#x51FA;&#x4E2D;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x6700;&#x5927;&#x503C;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([ 0.7753, -0.4702, -0.4599,  1.1899])
&gt;&gt;&gt; torch.clamp(a, max=0.5)
tensor([ 0.5000, -0.4702, -0.4599,  0.5000])
</code></pre><p><code>torch.``cos</code>( <em>input</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E0E;&#x8F93;&#x5165;&#x7684;<code>&#x7684;&#x5143;&#x7D20;</code>&#x7684;&#x4F59;&#x5F26;&#x4E00;&#x4E2A;&#x65B0;&#x7684;&#x5F20;&#x91CF;&#x3002;</p>
<p>outi=cos&#x2061;(inputi)\text{out}<em>{i} = \cos(\text{input}</em>{i}) outi&#x200B;=cos(inputi&#x200B;)</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([ 1.4309,  1.2706, -0.8562,  0.9796])
&gt;&gt;&gt; torch.cos(a)
tensor([ 0.1395,  0.2957,  0.6553,  0.5574])
</code></pre><p><code>torch.``cosh</code>( <em>input</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E0E;&#x8F93;&#x5165;&#x7684;<code>&#x7684;&#x5143;&#x7D20;</code>&#x7684;&#x53CC;&#x66F2;&#x4F59;&#x5F26;&#x4E00;&#x4E2A;&#x65B0;&#x7684;&#x5F20;&#x91CF;&#x3002;</p>
<p>outi=cosh&#x2061;(inputi)\text{out}<em>{i} = \cosh(\text{input}</em>{i}) outi&#x200B;=cosh(inputi&#x200B;)</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([ 0.1632,  1.1835, -0.6979, -0.7325])
&gt;&gt;&gt; torch.cosh(a)
tensor([ 1.0133,  1.7860,  1.2536,  1.2805])
</code></pre><p><code>torch.``div</code>()</p>
<p><code>torch.``div</code>( <em>input</em> , <em>other</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x5C06;&#x8F93;&#x5165;<code>&#x8F93;&#x5165;</code>&#x4E2D;&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x4E0E;&#x6240;&#x8FF0;&#x6807;&#x91CF;<code>&#x5176;&#x4ED6;</code>&#x5E76;&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x65B0;&#x4EA7;&#x751F;&#x7684;&#x5F20;&#x91CF;&#x3002;</p>
<p>outi=inputiother\text{out}_i = \frac{\text{input}_i}{\text{other}}
outi&#x200B;=otherinputi&#x200B;&#x200B;</p>
<p>&#x5982;&#x679C;<code>&#x8F93;&#x5165;</code>&#x662F;&#x5F0F; FloatTensor &#x6216; DoubleTensor &#xFF0C;<code>&#x5176;&#x4ED6;</code>&#x5E94;&#x8BE5;&#x662F;&#x4E00;&#x4E2A;&#x771F;&#x6B63;&#x53F7;&#xFF0C;&#x5426;&#x5219;&#x5E94;&#x8BE5;&#x662F;&#x4E00;&#x4E2A;&#x6574;&#x6570;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>&#x5176;&#x4ED6;</strong> &#xFF08; <em>&#x53F7;&#x7801;</em> &#xFF09; - &#x7684;&#x6570;&#x91CF;&#x88AB;&#x5212;&#x5206;&#x4E3A;<code>&#x8F93;&#x5165;</code>&#x7684;&#x5404;&#x8981;&#x7D20;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(5)
&gt;&gt;&gt; a
tensor([ 0.3810,  1.2774, -0.2972, -0.3719,  0.4637])
&gt;&gt;&gt; torch.div(a, 0.5)
tensor([ 0.7620,  2.5548, -0.5944, -0.7439,  0.9275])
</code></pre><p><code>torch.``div</code>( <em>input</em> , <em>other</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x5F20;&#x91CF;<code>&#x8F93;&#x5165;</code>&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x7531;&#x5F20;&#x91CF;<code>&#x5176;&#x4ED6;</code>&#x7684;&#x5404;&#x8981;&#x7D20;&#x5206;&#x5272;&#x3002;&#x5F97;&#x5230;&#x7684;&#x5F20;&#x91CF;&#x8FD4;&#x56DE;&#x3002;&#x8F93;&#x5165;&#x7684;<code>&#x5F62;&#x72B6;</code>&#x548C;<code>&#x5176;&#x4ED6;</code>&#x5FC5;&#x987B;<a href="notes/broadcasting.html#broadcasting-semantics"> broadcastable
</a>&#x3002;</p>
<p>outi=inputiotheri\text{out}_i = \frac{\text{input}_i}{\text{other}_i}
outi&#x200B;=otheri&#x200B;inputi&#x200B;&#x200B;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x5206;&#x5B50;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>&#x5176;&#x4ED6;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x5206;&#x6BCD;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a
tensor([[-0.3711, -1.9353, -0.4605, -0.2917],
        [ 0.1815, -1.0111,  0.9805, -1.5923],
        [ 0.1062,  1.4581,  0.7759, -1.2344],
        [-0.1830, -0.0313,  1.1908, -1.4757]])
&gt;&gt;&gt; b = torch.randn(4)
&gt;&gt;&gt; b
tensor([ 0.8032,  0.2930, -0.8113, -0.2308])
&gt;&gt;&gt; torch.div(a, b)
tensor([[-0.4620, -6.6051,  0.5676,  1.2637],
        [ 0.2260, -3.4507, -1.2086,  6.8988],
        [ 0.1322,  4.9764, -0.9564,  5.3480],
        [-0.2278, -0.1068, -1.4678,  6.3936]])
</code></pre><p><code>torch.``digamma</code>( <em>input</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8BA1;&#x7B97;&#x5728;&#x8F93;&#x5165;&#x4F3D;&#x9A6C;&#x51FD;&#x6570;&#x7684;&#x5BF9;&#x6570;&#x5BFC;&#x6570;&#x3002;</p>
<p>&#x3C8;(x)=ddxln&#x2061;(&#x393;(x))=&#x393;&#x2032;(x)&#x393;(x)\psi(x) = \frac{d}{dx}
\ln\left(\Gamma\left(x\right)\right) = \frac{\Gamma&apos;(x)}{\Gamma(x)}
&#x3C8;(x)=dxd&#x200B;ln(&#x393;(x))=&#x393;(x)&#x393;&#x2032;(x)&#x200B;</p>
<p>Parameters</p>
<p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x5F20;&#x91CF;&#x6765;&#x8BA1;&#x7B97;&#x5BF9;&#x53CC;&#x4F3D;&#x739B;&#x51FD;&#x6570;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.tensor([1, 0.5])
&gt;&gt;&gt; torch.digamma(a)
tensor([-0.5772, -1.9635])
</code></pre><p><code>torch.``erf</code>( <em>input</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8BA1;&#x7B97;&#x6BCF;&#x4E2A;&#x5143;&#x4EF6;&#x7684;&#x8BEF;&#x5DEE;&#x51FD;&#x6570;&#x3002;&#x8BEF;&#x5DEE;&#x51FD;&#x6570;&#x5B9A;&#x4E49;&#x5982;&#x4E0B;&#xFF1A;</p>
<p>erf(x)=2&#x3C0;&#x222B;0xe&#x2212;t2dt\mathrm{erf}(x) = \frac{2}{\sqrt{\pi}} \int_{0}^{x} e^{-t^2}
dt erf(x)=&#x3C0;&#x200B;2&#x200B;&#x222B;0x&#x200B;e&#x2212;t2dt</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.erf(torch.tensor([0, -1., 10.]))
tensor([ 0.0000, -0.8427,  1.0000])
</code></pre><p><code>torch.``erfc</code>( <em>input</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8BA1;&#x7B97;&#x7684;<code>&#x8F93;&#x5165;</code>&#x7684;&#x5404;&#x5143;&#x7D20;&#x7684;&#x4E92;&#x8865;&#x8BEF;&#x5DEE;&#x51FD;&#x6570;&#x3002;&#x4E92;&#x8865;&#x8BEF;&#x5DEE;&#x51FD;&#x6570;&#x5B9A;&#x4E49;&#x5982;&#x4E0B;&#xFF1A;</p>
<p>erfc(x)=1&#x2212;2&#x3C0;&#x222B;0xe&#x2212;t2dt\mathrm{erfc}(x) = 1 - \frac{2}{\sqrt{\pi}} \int_{0}^{x}
e^{-t^2} dt erfc(x)=1&#x2212;&#x3C0;&#x200B;2&#x200B;&#x222B;0x&#x200B;e&#x2212;t2dt</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x5F20;&#x91CF;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x8F93;&#x5165;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.erfc(torch.tensor([0, -1., 10.]))
tensor([ 1.0000, 1.8427,  0.0000])
</code></pre><p><code>torch.``erfinv</code>( <em>input</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8BA1;&#x7B97;&#x7684;<code>&#x8F93;&#x5165;</code>&#x7684;&#x5404;&#x5143;&#x7D20;&#x7684;&#x9006;&#x8BEF;&#x5DEE;&#x51FD;&#x6570;&#x3002;  1  [HTG16 - &#x9006;&#x8BEF;&#x5DEE;&#x51FD;&#x6570;&#x7684;&#x8303;&#x56F4;&#x5728; &#xFF08; &#x4E2D;&#x5B9A;&#x4E49;]&#xFF0C; 1  &#xFF09; &#xFF08;-1&#xFF0C;1&#xFF09; &#xFF08; -  1  &#xFF0C; 1
&#xFF09;  &#x4E3A;&#xFF1A;</p>
<p>erfinv(erf(x))=x\mathrm{erfinv}(\mathrm{erf}(x)) = x erfinv(erf(x))=x</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.erfinv(torch.tensor([0, 0.5, -1.]))
tensor([ 0.0000,  0.4769,    -inf])
</code></pre><p><code>torch.``exp</code>( <em>input</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E0E;&#x8F93;&#x5165;&#x5F20;&#x91CF;<code>&#x8F93;&#x5165;</code>&#x7684;&#x5143;&#x7D20;&#x7684;&#x6307;&#x6570;&#x4E00;&#x4E2A;&#x65B0;&#x7684;&#x5F20;&#x91CF;&#x3002;</p>
<p>yi=exiy<em>{i} = e^{x</em>{i}} yi&#x200B;=exi&#x200B;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.exp(torch.tensor([0, math.log(2.)]))
tensor([ 1.,  2.])
</code></pre><p><code>torch.``expm1</code>( <em>input</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E0E;&#x5143;&#x4EF6;&#x7684;&#x6307;&#x6570;&#x51CF;&#x53BB;<code>&#x8F93;&#x5165;</code>1&#x65B0;&#x7684;&#x5F20;&#x91CF;&#x3002;</p>
<p>yi=exi&#x2212;1y<em>{i} = e^{x</em>{i}} - 1 yi&#x200B;=exi&#x200B;&#x2212;1</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.expm1(torch.tensor([0, math.log(2.)]))
tensor([ 0.,  1.])
</code></pre><p><code>torch.``floor</code>( <em>input</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E0E;&#x8F93;&#x5165;&#x7684;<code>&#x7684;&#x5143;&#x7D20;</code>&#x7684;&#x6700;&#x5927;&#x6574;&#x6570;&#x662F;&#x5C0F;&#x4E8E;&#x6216;&#x7B49;&#x4E8E;&#x6BCF;&#x4E00;&#x4E2A;&#x5143;&#x4EF6;&#x7684;&#x5E95;&#x677F;&#x4E2D;&#x7684;&#x65B0;&#x5F20;&#x91CF;&#x3002;</p>
<p>outi=&#x230A;inputi&#x230B;\text{out}<em>{i} = \left\lfloor \text{input}</em>{i} \right\rfloor
outi&#x200B;=&#x230A;inputi&#x200B;&#x230B;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([-0.8166,  1.5308, -0.2530, -0.2091])
&gt;&gt;&gt; torch.floor(a)
tensor([-1.,  1., -1., -1.])
</code></pre><p><code>torch.``fmod</code>( <em>input</em> , <em>other</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8BA1;&#x7B97;&#x4E8B;&#x4E1A;&#x90E8;&#x7684;&#x5143;&#x7D20;&#x65B9;&#x9762;&#x7684;&#x5269;&#x4F59;&#x90E8;&#x5206;&#x3002;</p>
<p>&#x8BE5;&#x88AB;&#x9664;&#x6570;&#x548C;&#x9664;&#x6570;&#x53EF;&#x4EE5;&#x540C;&#x65F6;&#x5305;&#x542B;&#x4E86;&#x6574;&#x6570;&#x548C;&#x6D6E;&#x70B9;&#x6570;&#x3002;&#x5176;&#x4F59;&#x5177;&#x6709;&#x76F8;&#x540C;&#x7684;&#x7B26;&#x53F7;&#x4F5C;&#x4E3A;&#x88AB;&#x9664;&#x6570;<code>&#x8F93;&#x5165;</code>&#x3002;</p>
<p>&#x5F53;<code>&#x5176;&#x4ED6;</code>&#x662F;&#x4E00;&#x4E2A;&#x5F20;&#x91CF;&#xFF0C;<code>&#x8F93;&#x5165;</code>&#x548C;<code>&#x5176;&#x4ED6;</code>&#x5FC5;&#x987B;&#x7684;&#x5F62;&#x72B6;<a href="notes/broadcasting.html#broadcasting-semantics"> broadcastable
</a>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x88AB;&#x9664;&#x6570;</p>
</li>
<li><p><strong>&#x5176;&#x4ED6;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a> <em>&#x6216;</em> <a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a>&#xFF09; - &#x9664;&#x6570;&#xFF0C;&#x5176;&#x53EF;&#x4EE5;&#x662F;&#x6570;&#x5B57;&#x6216;&#x4F5C;&#x4E3A;&#x88AB;&#x9664;&#x6570;&#x7684;&#x76F8;&#x540C;&#x5F62;&#x72B6;&#x7684;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.fmod(torch.tensor([-3., -2, -1, 1, 2, 3]), 2)
tensor([-1., -0., -1.,  1.,  0.,  1.])
&gt;&gt;&gt; torch.fmod(torch.tensor([1., 2, 3, 4, 5]), 1.5)
tensor([ 1.0000,  0.5000,  0.0000,  1.0000,  0.5000])
</code></pre><p><code>torch.``frac</code>( <em>input</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8BA1;&#x7B97;&#x6BCF;&#x4E2A;&#x5143;&#x4EF6;&#x7684;&#x5728;<code>&#x8F93;&#x5165;</code>&#x7684;&#x5C0F;&#x6570;&#x90E8;&#x5206;&#x3002;</p>
<p>outi=inputi&#x2212;&#x230A;&#x2223;inputi&#x2223;&#x230B;&#x2217;sgn&#x2061;(inputi)\text{out}<em>{i} = \text{input}</em>{i} -
\left\lfloor |\text{input}<em>{i}| \right\rfloor *
\operatorname{sgn}(\text{input}</em>{i}) outi&#x200B;=inputi&#x200B;&#x2212;&#x230A;&#x2223;inputi&#x200B;&#x2223;&#x230B;&#x2217;sgn(inputi&#x200B;)</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.frac(torch.tensor([1, 2.5, -3.2]))
tensor([ 0.0000,  0.5000, -0.2000])
</code></pre><p><code>torch.``lerp</code>( <em>input</em> , <em>end</em> , <em>weight</em> , <em>out=None</em> )</p>
<p>&#x505A;&#x4E24;&#x5F20;&#x91CF;&#x7684;&#x7EBF;&#x6027;&#x5185;&#x63D2;<code>&#x5F00;&#x59CB;</code>&#xFF08;&#x7531;<code>&#x8F93;&#x5165;</code>&#x4E2D;&#x7ED9;&#x51FA;&#xFF09;&#x548C;<code>&#x7ED3;&#x675F; [HTG11&#x57FA;&#x4E8E;&#x6807;&#x91CF;&#x6216;&#x5F20;&#x91CF;</code>&#x91CD;&#x91CF; <code>]&#x5E76;&#x8FD4;&#x56DE;&#x751F;&#x6210;&#x7684;</code>OUT<code>&#x5F20;&#x91CF;&#x3002;</code></p>
<p>outi=starti+weighti&#xD7;(endi&#x2212;starti)\text{out}_i = \text{start}_i +
\text{weight}_i \times (\text{end}_i - \text{start}_i)
outi&#x200B;=starti&#x200B;+weighti&#x200B;&#xD7;(endi&#x200B;&#x2212;starti&#x200B;)</p>
<p>&#x7684;<code>&#x5F62;&#x72B6;&#x5F00;&#x59CB;</code>&#x548C;<code>&#x7ED3;&#x675F;</code>&#x5FC5;&#x987B;<a href="notes/broadcasting.html#broadcasting-%0Asemantics"> broadcastable  </a>&#x3002;&#x5982;&#x679C;<code>&#x91CD;&#x91CF;</code>&#x662F;&#x4E00;&#x4E2A;&#x5F20;&#x91CF;&#xFF0C;&#x5219;<code>&#x5F62;&#x72B6;&#x91CD;&#x91CF;</code>&#xFF0C;<code>&#x5F00;&#x59CB;</code>&#x548C;<code>&#x7ED3;&#x675F;</code>&#x5FC5;&#x987B;<a href="notes/broadcasting.html#broadcasting-semantics"> broadcastable
</a>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#xFF0C;&#x8D77;&#x70B9;&#x7684;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>&#x7AEF;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x4E0E;&#x8BE5;&#x7ED3;&#x675F;&#x70B9;&#x7684;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>&#x91CD;&#x91CF;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a> <em>&#x6216;</em> <em>&#x5F20;&#x91CF;</em> &#xFF09; - &#x7528;&#x4E8E;&#x5185;&#x63D2;&#x516C;&#x5F0F;&#x7684;&#x6743;&#x91CD;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; start = torch.arange(1., 5.)
&gt;&gt;&gt; end = torch.empty(4).fill_(10)
&gt;&gt;&gt; start
tensor([ 1.,  2.,  3.,  4.])
&gt;&gt;&gt; end
tensor([ 10.,  10.,  10.,  10.])
&gt;&gt;&gt; torch.lerp(start, end, 0.5)
tensor([ 5.5000,  6.0000,  6.5000,  7.0000])
&gt;&gt;&gt; torch.lerp(start, end, torch.full_like(start, 0.5))
tensor([ 5.5000,  6.0000,  6.5000,  7.0000])
</code></pre><p><code>torch.``log</code>( <em>input</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E0E;&#x8F93;&#x5165;&#x7684;<code>&#x7684;&#x5143;&#x7D20;</code>&#x7684;&#x81EA;&#x7136;&#x5BF9;&#x6570;&#x65B0;&#x7684;&#x5F20;&#x91CF;&#x3002;</p>
<p>yi=log&#x2061;e(xi)y<em>{i} = \log</em>{e} (x_{i}) yi&#x200B;=loge&#x200B;(xi&#x200B;)</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(5)
&gt;&gt;&gt; a
tensor([-0.7168, -0.5471, -0.8933, -1.4428, -0.1190])
&gt;&gt;&gt; torch.log(a)
tensor([ nan,  nan,  nan,  nan,  nan])
</code></pre><p><code>torch.``log10</code>( <em>input</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E0E;&#x5BF9;&#x6570;&#x65B0;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x4EE5;&#x8F93;&#x5165;&#x7684;<code>&#x7684;&#x5143;&#x7D20;</code>&#x7684;&#x57FA;&#x6781;10&#x3002;</p>
<p>yi=log&#x2061;10(xi)y<em>{i} = \log</em>{10} (x_{i}) yi&#x200B;=log10&#x200B;(xi&#x200B;)</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.rand(5)
&gt;&gt;&gt; a
tensor([ 0.5224,  0.9354,  0.7257,  0.1301,  0.2251])


&gt;&gt;&gt; torch.log10(a)
tensor([-0.2820, -0.0290, -0.1392, -0.8857, -0.6476])
</code></pre><p><code>torch.``log1p</code>( <em>input</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E0E;&#xFF08;1 + <code>&#x8F93;&#x5165;</code>&#xFF09;&#x7684;&#x81EA;&#x7136;&#x5BF9;&#x6570;&#x65B0;&#x7684;&#x5F20;&#x91CF;&#x3002;</p>
<p>yi=log&#x2061;e(xi+1)y<em>i = \log</em>{e} (x_i + 1) yi&#x200B;=loge&#x200B;(xi&#x200B;+1)</p>
<p>Note</p>
<p>&#x6B64;&#x51FD;&#x6570;&#x662F;&#x66F4;&#x7CBE;&#x786E;&#x7684;&#x6BD4; <code>torch.log&#xFF08;&#xFF09;</code>&#x4E3A;<code>&#x8F93;&#x5165;</code>&#x7684;&#x503C;&#x8F83;&#x5C0F;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(5)
&gt;&gt;&gt; a
tensor([-1.0090, -0.9923,  1.0249, -0.5372,  0.2492])
&gt;&gt;&gt; torch.log1p(a)
tensor([    nan, -4.8653,  0.7055, -0.7705,  0.2225])
</code></pre><p><code>torch.``log2</code>( <em>input</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E0E;&#x5BF9;&#x6570;&#x65B0;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x4EE5;&#x8F93;&#x5165;&#x7684;<code>&#x7684;&#x5143;&#x7D20;</code>&#x7684;&#x57FA;&#x4F53;2&#x3002;</p>
<p>yi=log&#x2061;2(xi)y<em>{i} = \log</em>{2} (x_{i}) yi&#x200B;=log2&#x200B;(xi&#x200B;)</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.rand(5)
&gt;&gt;&gt; a
tensor([ 0.8419,  0.8003,  0.9971,  0.5287,  0.0490])


&gt;&gt;&gt; torch.log2(a)
tensor([-0.2483, -0.3213, -0.0042, -0.9196, -4.3504])
</code></pre><p><code>torch.``mul</code>()</p>
<p><code>torch.``mul</code>( <em>input</em> , <em>other</em> , <em>out=None</em> )</p>
<p>&#x4E58;&#x4EE5;&#x6240;&#x8FF0;&#x6807;&#x91CF;<code>&#x5176;&#x4ED6;</code>&#x8F93;&#x5165;<code>&#x8F93;&#x5165;</code>&#x4E2D;&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#xFF0C;&#x5E76;&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x65B0;&#x4EA7;&#x751F;&#x7684;&#x5F20;&#x91CF;&#x3002;</p>
<p>outi=other&#xD7;inputi\text{out}_i = \text{other} \times \text{input}_i
outi&#x200B;=other&#xD7;inputi&#x200B;</p>
<p>If <code>input</code>is of type FloatTensor or DoubleTensor, <code>other</code>should be a real
number, otherwise it should be an integer</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>&#x5176;&#x4ED6;</strong> &#xFF08; <em>&#x53F7;&#x7801;</em> &#xFF09; - &#x6570;&#x76F8;&#x4E58;&#x4EE5;<code>&#x8F93;&#x5165;</code>&#x7684;&#x5404;&#x8981;&#x7D20;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(3)
&gt;&gt;&gt; a
tensor([ 0.2015, -0.4255,  2.6087])
&gt;&gt;&gt; torch.mul(a, 100)
tensor([  20.1494,  -42.5491,  260.8663])
</code></pre><p><code>torch.``mul</code>( <em>input</em> , <em>other</em> , <em>out=None</em> )</p>
<p>&#x5F20;&#x91CF;<code>&#x8F93;&#x5165;</code>&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x662F;&#x7531;&#x76F8;&#x5E94;&#x7684;&#x5143;&#x7D20;&#x76F8;&#x4E58;&#x7684;&#x5F20;&#x91CF;<code>&#x5176;&#x4ED6;</code>&#x3002;&#x5F97;&#x5230;&#x7684;&#x5F20;&#x91CF;&#x8FD4;&#x56DE;&#x3002;</p>
<p>The shapes of <code>input</code>and <code>other</code>must be
<a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a>.</p>
<p>outi=inputi&#xD7;otheri\text{out}_i = \text{input}_i \times \text{other}_i
outi&#x200B;=inputi&#x200B;&#xD7;otheri&#x200B;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x7B2C;&#x4E00;&#x88AB;&#x4E58;&#x6570;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>&#x5176;&#x4ED6;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x7B2C;&#x4E8C;&#x88AB;&#x4E58;&#x6570;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 1)
&gt;&gt;&gt; a
tensor([[ 1.1207],
        [-0.3137],
        [ 0.0700],
        [ 0.8378]])
&gt;&gt;&gt; b = torch.randn(1, 4)
&gt;&gt;&gt; b
tensor([[ 0.5146,  0.1216, -0.5244,  2.2382]])
&gt;&gt;&gt; torch.mul(a, b)
tensor([[ 0.5767,  0.1363, -0.5877,  2.5083],
        [-0.1614, -0.0382,  0.1645, -0.7021],
        [ 0.0360,  0.0085, -0.0367,  0.1567],
        [ 0.4312,  0.1019, -0.4394,  1.8753]])
</code></pre><p><code>torch.``mvlgamma</code>( <em>input</em> , <em>p</em> ) &#x2192; Tensor</p>
<p>&#x4E0E;&#x5C3A;&#x5BF8; P  <a href="https://en.wikipedia.org/wiki/Multivariate_gamma_function" target="_blank">HTG11&#x8BA1;&#x7B97;&#x591A;&#x5143;&#x5BF9;&#x6570;&#x4F3D;&#x739B;&#x51FD;&#x6570;&#xFF08;[ [&#x53C2;&#x8003;&#x6587;&#x732E;]
</a>&#xFF09; ] p  p  &#x9010;&#x5143;&#x7D20;&#xFF0C;&#x7ED9;&#x51FA;&#x901A;&#x8FC7;</p>
<p>log&#x2061;(&#x393;p(a))=C+&#x2211;i=1plog&#x2061;(&#x393;(a&#x2212;i&#x2212;12))\log(\Gamma<em>{p}(a)) = C + \displaystyle
\sum</em>{i=1}^{p} \log\left(\Gamma\left(a - \frac{i - 1}{2}\right)\right)
log(&#x393;p&#x200B;(a))=C+i=1&#x2211;p&#x200B;log(&#x393;(a&#x2212;2i&#x2212;1&#x200B;))</p>
<p>&#x5176;&#x4E2D; C  =  &#x65E5;&#x5FD7; &#x2061; &#xFF08; &#x3C0; &#xFF09; &#xD7; p  &#xFF08; p  -  1  &#xFF09; 4  C = \&#x65E5;&#x5FD7;&#xFF08;\ PI&#xFF09; \&#x500D;\&#x538B;&#x88C2;{&#x5BF9; - &#xFF08;&#x5BF9;1&#xFF09;} {4}  C
=  LO  G  &#xFF08; &#x3C0; &#xFF09; &#xD7; 4  P  &#xFF08; P  -  1  &#xFF09; &#x548C; &#x393; &#xFF08; &#x22C5; &#xFF09; \&#x4F3D;&#x739B;&#xFF08;\ CDOT&#xFF09; &#x393; &#xFF08; &#x22C5; &#xFF09; &#x662F;Gamma&#x51FD;&#x6570;&#x3002;</p>
<p>&#x5982;&#x679C;&#x4EFB;&#x4F55;&#x5143;&#x4EF6;&#x90FD;&#x5C0F;&#x4E8E;&#x6216;&#x7B49;&#x4E8E; P  -  1  2  \&#x538B;&#x88C2;{&#x5BF9; - 1} {2}  2  p  -  1  &#xFF0C;&#x7136;&#x540E;&#x5F15;&#x53D1;&#x9519;&#x8BEF;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x5F20;&#x91CF;&#x6765;&#x8BA1;&#x7B97;&#x591A;&#x53D8;&#x91CF;&#x6570;&#x4F3D;&#x739B;&#x51FD;&#x6570;</p>
</li>
<li><p><strong>P</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x7EF4;&#x6570;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.empty(2, 3).uniform_(1, 2)
&gt;&gt;&gt; a
tensor([[1.6835, 1.8474, 1.1929],
        [1.0475, 1.7162, 1.4180]])
&gt;&gt;&gt; torch.mvlgamma(a, 2)
tensor([[0.3928, 0.4007, 0.7586],
        [1.0311, 0.3901, 0.5049]])
</code></pre><p><code>torch.``neg</code>( <em>input</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E0E;&#x8D1F;&#x8F93;&#x5165;&#x7684;<code>&#x7684;&#x5143;&#x7D20;</code>&#x7684;&#x65B0;&#x7684;&#x5F20;&#x91CF;&#x3002;</p>
<p>out=&#x2212;1&#xD7;input\text{out} = -1 \times \text{input} out=&#x2212;1&#xD7;input</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(5)
&gt;&gt;&gt; a
tensor([ 0.0090, -0.2262, -0.0682, -0.2866,  0.3940])
&gt;&gt;&gt; torch.neg(a)
tensor([-0.0090,  0.2262,  0.0682,  0.2866, -0.3940])
</code></pre><p><code>torch.``pow</code>()</p>
<p><code>torch.``pow</code>( <em>input</em> , <em>exponent</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x6CE8;&#x610F;&#x5230;&#x6BCF;&#x4E2A;&#x5143;&#x4EF6;&#x7684;&#x529F;&#x7387;&#x5728;<code>&#x8F93;&#x5165;</code>&#x4E0E;<code>&#x6307;&#x6570;</code>&#xFF0C;&#x5E76;&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x5F20;&#x91CF;&#x7684;&#x7ED3;&#x679C;&#x3002;</p>
<p><code>&#x6307;&#x6570;</code>&#x53EF;&#x4EE5;&#x662F;&#x4E00;&#x4E2A;&#x5355;&#x4E00;&#x7684;<code>&#x6D6E;&#x52A8;</code>&#x53F7;&#x7801;&#x6216;&#x5F20;&#x91CF;&#x5177;&#x6709;&#x76F8;&#x540C;&#x6570;&#x91CF;&#x7684;&#x5143;&#x7D20;[&#x7684;HTG10 ] &#x8F93;&#x5165; &#x3002;</p>
<p>&#x5F53;<code>&#x6307;&#x6570;</code>&#x662F;&#x4E00;&#x4E2A;&#x6807;&#x91CF;&#x503C;&#xFF0C;&#x6240;&#x65BD;&#x52A0;&#x7684;&#x64CD;&#x4F5C;&#xFF1A;</p>
<p>outi=xiexponent\text{out}_i = x_i ^ \text{exponent} outi&#x200B;=xiexponent&#x200B;</p>
<p>&#x5F53;<code>&#x6307;&#x6570;</code>&#x662F;&#x4E00;&#x4E2A;&#x5F20;&#x91CF;&#xFF0C;&#x6240;&#x65BD;&#x52A0;&#x7684;&#x64CD;&#x4F5C;&#xFF1A;</p>
<p>outi=xiexponenti\text{out}_i = x_i ^ {\text{exponent}_i} outi&#x200B;=xiexponenti&#x200B;&#x200B;</p>
<p>&#x5F53;<code>&#x6307;&#x6570;</code>&#x662F;&#x4E00;&#x4E2A;&#x5F20;&#x91CF;&#xFF0C;<code>&#x8F93;&#x5165;</code>&#x548C;<code>&#x6307;&#x6570;</code>&#x5FC5;&#x987B;&#x7684;&#x5F62;&#x72B6;<a href="notes/broadcasting.html#broadcasting-semantics"> broadcastable
</a>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>&#x6307;&#x6570;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a> <em>&#x6216;</em> <em>&#x5F20;&#x91CF;</em> &#xFF09; - &#x6307;&#x6570;&#x503C;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([ 0.4331,  1.2475,  0.6834, -0.2791])
&gt;&gt;&gt; torch.pow(a, 2)
tensor([ 0.1875,  1.5561,  0.4670,  0.0779])
&gt;&gt;&gt; exp = torch.arange(1., 5.)

&gt;&gt;&gt; a = torch.arange(1., 5.)
&gt;&gt;&gt; a
tensor([ 1.,  2.,  3.,  4.])
&gt;&gt;&gt; exp
tensor([ 1.,  2.,  3.,  4.])
&gt;&gt;&gt; torch.pow(a, exp)
tensor([   1.,    4.,   27.,  256.])
</code></pre><p><code>torch.``pow</code>( <em>self</em> , <em>exponent</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p><code>&#x81EA;</code>&#x662F;&#x4E00;&#x4E2A;&#x6807;&#x91CF;<code>&#x6D6E;&#x52A8;</code>&#x503C;&#x548C;<code>&#x6307;&#x6570;</code>&#x662F;&#x4E00;&#x4E2A;&#x5F20;&#x91CF;&#x3002;&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;<code>OUT</code>&#x662F;&#x76F8;&#x540C;&#x7684;&#x5F62;&#x72B6;&#x7684;&#x4F5C;&#x4E3A;<code>&#x6307;&#x6570;</code></p>
<p>&#x6240;&#x65BD;&#x52A0;&#x7684;&#x64CD;&#x4F5C;&#x662F;&#xFF1A;</p>
<p>outi=selfexponenti\text{out}_i = \text{self} ^ {\text{exponent}_i}
outi&#x200B;=selfexponenti&#x200B;</p>
<p>Parameters</p>
<ul>
<li><p>&#x4E3A;&#x7535;&#x6E90;&#x64CD;&#x4F5C;&#x7684;&#x6807;&#x91CF;&#x57FA;&#x503C; - <strong>&#x81EA;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a>&#xFF09;</p>
</li>
<li><p><strong>&#x6307;&#x6570;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x6307;&#x6570;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; exp = torch.arange(1., 5.)
&gt;&gt;&gt; base = 2
&gt;&gt;&gt; torch.pow(base, exp)
tensor([  2.,   4.,   8.,  16.])
</code></pre><p><code>torch.``reciprocal</code>( <em>input</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x65B0;&#x7684;&#x5F20;&#x91CF;<code>&#x8F93;&#x5165;</code>&#x7684;&#x5143;&#x7D20;&#x7684;&#x5012;&#x6570;</p>
<p>outi=1inputi\text{out}<em>{i} = \frac{1}{\text{input}</em>{i}} outi&#x200B;=inputi&#x200B;1&#x200B;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([-0.4595, -2.1219, -1.4314,  0.7298])
&gt;&gt;&gt; torch.reciprocal(a)
tensor([-2.1763, -0.4713, -0.6986,  1.3702])
</code></pre><p><code>torch.``remainder</code>( <em>input</em> , <em>other</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>Computes the element-wise remainder of division.</p>
<p>&#x8BE5;&#x9664;&#x6570;&#x548C;&#x88AB;&#x9664;&#x6570;&#x53EF;&#x4EE5;&#x540C;&#x65F6;&#x5305;&#x542B;&#x4E86;&#x6574;&#x6570;&#x548C;&#x6D6E;&#x70B9;&#x6570;&#x3002;&#x5176;&#x4F59;&#x6709;&#x76F8;&#x540C;&#x7684;&#x7B26;&#x53F7;&#x4E0E;&#x9664;&#x6570;&#x3002;</p>
<p>When <code>other</code>is a tensor, the shapes of <code>input</code>and <code>other</code>must be
<a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a>.</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the dividend</p>
</li>
<li><p><strong>&#x5176;&#x4ED6;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a> <em>&#x6216;</em> <a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a>&#xFF09; - &#x5176;&#x53EF;&#x4EE5;&#x662F;&#x9664;&#x6570;&#x6570;&#x6216;&#x4F5C;&#x4E3A;&#x88AB;&#x9664;&#x6570;&#x7684;&#x76F8;&#x540C;&#x5F62;&#x72B6;&#x7684;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.remainder(torch.tensor([-3., -2, -1, 1, 2, 3]), 2)
tensor([ 1.,  0.,  1.,  1.,  0.,  1.])
&gt;&gt;&gt; torch.remainder(torch.tensor([1., 2, 3, 4, 5]), 1.5)
tensor([ 1.0000,  0.5000,  0.0000,  1.0000,  0.5000])
</code></pre><p>&#x4E5F;&#x53EF;&#x4EE5;&#x770B;&#x770B;</p>
<p><code>torch.fmod&#xFF08;&#xFF09;</code>&#xFF0C;&#x5B83;&#x7B49;&#x6548;&#x8BA1;&#x7B97;&#x5206;&#x5272;&#x7684;&#x9010;&#x5143;&#x7D20;&#x5176;&#x4F59;&#x90E8;&#x5206;C&#x5E93;&#x51FD;&#x6570;<code>FMOD&#xFF08;&#xFF09;</code>&#x3002;</p>
<p><code>torch.``round</code>( <em>input</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E0E;&#x6BCF;&#x4E2A;<code>&#x7684;&#x5143;&#x7D20;&#x7684;&#x4E00;&#x4E2A;&#x65B0;&#x7684;&#x5F20;&#x91CF;</code>&#x820D;&#x5165;&#x5230;&#x6700;&#x63A5;&#x8FD1;&#x7684;&#x6574;&#x6570;&#x7684;&#x8F93;&#x5165;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([ 0.9920,  0.6077,  0.9734, -1.0362])
&gt;&gt;&gt; torch.round(a)
tensor([ 1.,  1.,  1., -1.])
</code></pre><p><code>torch.``rsqrt</code>( <em>input</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E0E;&#x6BCF;&#x4E2A;&#x8F93;&#x5165;&#x7684;<code>&#x7684;&#x5143;&#x7D20;</code>&#x7684;&#x5E73;&#x65B9;&#x6839;&#x7684;&#x5012;&#x6570;&#x7684;&#x65B0;&#x5F20;&#x91CF;&#x3002;</p>
<p>outi=1inputi\text{out}<em>{i} = \frac{1}{\sqrt{\text{input}</em>{i}}}
outi&#x200B;=inputi&#x200B;&#x200B;1&#x200B;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([-0.0370,  0.2970,  1.5420, -0.9105])
&gt;&gt;&gt; torch.rsqrt(a)
tensor([    nan,  1.8351,  0.8053,     nan])
</code></pre><p><code>torch.``sigmoid</code>( <em>input</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E0E;&#x8F93;&#x5165;&#x7684;<code>&#x7684;&#x5143;&#x7D20;</code>&#x7684;&#x4E59;&#x72B6;&#x7ED3;&#x80A0;&#x65B0;&#x5F20;&#x91CF;&#x3002;</p>
<p>outi=11+e&#x2212;inputi\text{out}<em>{i} = \frac{1}{1 + e^{-\text{input}</em>{i}}}
outi&#x200B;=1+e&#x2212;inputi&#x200B;1&#x200B;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([ 0.9213,  1.0887, -0.8858, -1.7683])
&gt;&gt;&gt; torch.sigmoid(a)
tensor([ 0.7153,  0.7481,  0.2920,  0.1458])
</code></pre><p><code>torch.``sign</code>( <em>input</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E0E;&#x8F93;&#x5165;&#x7684;<code>&#x7684;&#x5143;&#x7D20;</code>&#x7684;&#x6807;&#x5FD7;&#x65B0;&#x7684;&#x5F20;&#x91CF;&#x3002;</p>
<p>outi=sgn&#x2061;(inputi)\text{out}<em>{i} = \operatorname{sgn}(\text{input}</em>{i})
outi&#x200B;=sgn(inputi&#x200B;)</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.tensor([0.7, -1.2, 0., 2.3])
&gt;&gt;&gt; a
tensor([ 0.7000, -1.2000,  0.0000,  2.3000])
&gt;&gt;&gt; torch.sign(a)
tensor([ 1., -1.,  0.,  1.])
</code></pre><p><code>torch.``sin</code>( <em>input</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E0E;&#x8F93;&#x5165;&#x7684;<code>&#x7684;&#x5143;&#x7D20;</code>&#x7684;&#x6B63;&#x5F26;&#x4E00;&#x4E2A;&#x65B0;&#x7684;&#x5F20;&#x91CF;&#x3002;</p>
<p>outi=sin&#x2061;(inputi)\text{out}<em>{i} = \sin(\text{input}</em>{i}) outi&#x200B;=sin(inputi&#x200B;)</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([-0.5461,  0.1347, -2.7266, -0.2746])
&gt;&gt;&gt; torch.sin(a)
tensor([-0.5194,  0.1343, -0.4032, -0.2711])
</code></pre><p><code>torch.``sinh</code>( <em>input</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E0E;&#x8F93;&#x5165;&#x7684;<code>&#x7684;&#x5143;&#x7D20;</code>&#x7684;&#x53CC;&#x66F2;&#x6B63;&#x5F26;&#x4E00;&#x4E2A;&#x65B0;&#x7684;&#x5F20;&#x91CF;&#x3002;</p>
<p>outi=sinh&#x2061;(inputi)\text{out}<em>{i} = \sinh(\text{input}</em>{i}) outi&#x200B;=sinh(inputi&#x200B;)</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([ 0.5380, -0.8632, -0.1265,  0.9399])
&gt;&gt;&gt; torch.sinh(a)
tensor([ 0.5644, -0.9744, -0.1268,  1.0845])
</code></pre><p><code>torch.``sqrt</code>( <em>input</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E0E;&#x8F93;&#x5165;&#x7684;<code>&#x7684;&#x5143;&#x7D20;</code>&#x7684;&#x5E73;&#x65B9;&#x6839;&#x4E00;&#x4E2A;&#x65B0;&#x7684;&#x5F20;&#x91CF;&#x3002;</p>
<p>outi=inputi\text{out}<em>{i} = \sqrt{\text{input}</em>{i}} outi&#x200B;=inputi&#x200B;&#x200B;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([-2.0755,  1.0226,  0.0831,  0.4806])
&gt;&gt;&gt; torch.sqrt(a)
tensor([    nan,  1.0112,  0.2883,  0.6933])
</code></pre><p><code>torch.``tan</code>( <em>input</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E0E;&#x8F93;&#x5165;&#x7684;<code>&#x7684;&#x5143;&#x7D20;</code>&#x7684;&#x5207;&#x7EBF;&#x4E00;&#x4E2A;&#x65B0;&#x7684;&#x5F20;&#x91CF;&#x3002;</p>
<p>outi=tan&#x2061;(inputi)\text{out}<em>{i} = \tan(\text{input}</em>{i}) outi&#x200B;=tan(inputi&#x200B;)</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([-1.2027, -1.7687,  0.4412, -1.3856])
&gt;&gt;&gt; torch.tan(a)
tensor([-2.5930,  4.9859,  0.4722, -5.3366])
</code></pre><p><code>torch.``tanh</code>( <em>input</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E0E;&#x8F93;&#x5165;&#x7684;<code>&#x7684;&#x5143;&#x7D20;</code>&#x7684;&#x53CC;&#x66F2;&#x6B63;&#x5207;&#x4E00;&#x4E2A;&#x65B0;&#x7684;&#x5F20;&#x91CF;&#x3002;</p>
<p>outi=tanh&#x2061;(inputi)\text{out}<em>{i} = \tanh(\text{input}</em>{i}) outi&#x200B;=tanh(inputi&#x200B;)</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([ 0.8986, -0.7279,  1.1745,  0.2611])
&gt;&gt;&gt; torch.tanh(a)
tensor([ 0.7156, -0.6218,  0.8257,  0.2553])
</code></pre><p><code>torch.``trunc</code>( <em>input</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E0E;&#x8F93;&#x5165;&#x7684;<code>&#x7684;&#x5143;&#x7D20;</code>&#x7684;&#x622A;&#x5C3E;&#x6574;&#x6570;&#x503C;&#x7684;&#x65B0;&#x5F20;&#x91CF;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([ 3.4742,  0.5466, -0.8008, -0.9079])
&gt;&gt;&gt; torch.trunc(a)
tensor([ 3.,  0., -0., -0.])
</code></pre><h3 id="&#x8FD8;&#x539F;&#x884C;&#x52A8;">&#x8FD8;&#x539F;&#x884C;&#x52A8;</h3>
<p><code>torch.``argmax</code>()</p>
<p><code>torch.``argmax</code>( <em>input</em> ) &#x2192; LongTensor</p>
<p>&#x8FD4;&#x56DE;&#x5728;<code>&#x8F93;&#x5165;</code>&#x5F20;&#x91CF;&#x7684;&#x6240;&#x6709;&#x5143;&#x7D20;&#x7684;&#x6700;&#x5927;&#x503C;&#x7684;&#x7D22;&#x5F15;&#x3002;</p>
<p>&#x8FD9;&#x662F;&#x901A;&#x8FC7; <code>torch.max&#xFF08;&#xFF09;</code>&#x8FD4;&#x56DE;&#x7684;&#x7B2C;&#x4E8C;&#x503C;&#x3002;&#x770B;&#x5230;&#x5B83;&#x7684;&#x6587;&#x6863;&#xFF0C;&#x8FD9;&#x79CD;&#x65B9;&#x6CD5;&#x7684;&#x51C6;&#x786E;&#x8BED;&#x4E49;&#x3002;</p>
<p>Parameters</p>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input
tensor</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a
tensor([[ 1.3398,  0.2663, -0.2686,  0.2450],
        [-0.7401, -0.8805, -0.3402, -1.1936],
        [ 0.4907, -1.3948, -1.0691, -0.3132],
        [-1.6092,  0.5419, -0.2993,  0.3195]])
&gt;&gt;&gt; torch.argmax(a)
tensor(0)
</code></pre><p><code>torch.``argmax</code>( <em>input</em> , <em>dim</em> , <em>keepdim=False</em> ) &#x2192; LongTensor</p>
<p>&#x8FD4;&#x56DE;&#x8DE8;&#x5C3A;&#x5EA6;&#x5F20;&#x91CF;&#x7684;&#x6700;&#x5927;&#x503C;&#x7684;&#x6307;&#x6807;&#x3002;</p>
<p>This is the second value returned by <code>torch.max()</code>. See its documentation for
the exact semantics of this method.</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x7684;&#x5C3A;&#x5BF8;&#xFF0C;&#x4EE5;&#x51CF;&#x5C11;&#x3002;&#x5982;&#x679C;<code>&#x65E0;</code>&#xFF0C;&#x5219;&#x8FD4;&#x56DE;&#x6241;&#x5E73;&#x8F93;&#x5165;&#x7684;argmax&#x3002;</p>
</li>
<li><p><strong>keepdim</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a>&#xFF09; - &#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x662F;&#x5426;&#x6709;<code>&#x6697;&#x6DE1;</code>&#x4FDD;&#x7559;&#x6216;&#x6CA1;&#x6709;&#x3002;&#x5982;&#x679C;<code>&#x6697;&#x6DE1;=&#x65E0;</code>&#x5FFD;&#x7565;&#x3002;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a
tensor([[ 1.3398,  0.2663, -0.2686,  0.2450],
        [-0.7401, -0.8805, -0.3402, -1.1936],
        [ 0.4907, -1.3948, -1.0691, -0.3132],
        [-1.6092,  0.5419, -0.2993,  0.3195]])
&gt;&gt;&gt; torch.argmax(a, dim=1)
tensor([ 0,  2,  0,  1])
</code></pre><p><code>torch.``argmin</code>()</p>
<p><code>torch.``argmin</code>( <em>input</em> ) &#x2192; LongTensor</p>
<p>&#x8FD4;&#x56DE;&#x5728;<code>&#x8F93;&#x5165;</code>&#x5F20;&#x91CF;&#x7684;&#x6240;&#x6709;&#x5143;&#x7D20;&#x7684;&#x6700;&#x5C0F;&#x503C;&#x7684;&#x7D22;&#x5F15;&#x3002;</p>
<p>&#x8FD9;&#x662F;&#x901A;&#x8FC7; <code>torch.min&#x8FD4;&#x56DE;&#x7684;&#x7B2C;&#x4E8C;&#x503C;&#xFF08;&#xFF09;</code>&#x3002;&#x770B;&#x5230;&#x5B83;&#x7684;&#x6587;&#x6863;&#xFF0C;&#x8FD9;&#x79CD;&#x65B9;&#x6CD5;&#x7684;&#x51C6;&#x786E;&#x8BED;&#x4E49;&#x3002;</p>
<p>Parameters</p>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input
tensor</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a
tensor([[ 0.1139,  0.2254, -0.1381,  0.3687],
        [ 1.0100, -1.1975, -0.0102, -0.4732],
        [-0.9240,  0.1207, -0.7506, -1.0213],
        [ 1.7809, -1.2960,  0.9384,  0.1438]])
&gt;&gt;&gt; torch.argmin(a)
tensor(13)
</code></pre><p><code>torch.``argmin</code>( <em>input</em> , <em>dim</em> , <em>keepdim=False</em> , <em>out=None</em> ) &#x2192;
LongTensor</p>
<p>&#x8FD4;&#x56DE;&#x8DE8;&#x5C3A;&#x5EA6;&#x5F20;&#x91CF;&#x7684;&#x6700;&#x4F4E;&#x503C;&#x7684;&#x7D22;&#x5F15;&#x3002;</p>
<p>This is the second value returned by <code>torch.min()</code>. See its documentation for
the exact semantics of this method.</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x7684;&#x5C3A;&#x5BF8;&#xFF0C;&#x4EE5;&#x51CF;&#x5C11;&#x3002;&#x5982;&#x679C;<code>&#x65E0;</code>&#xFF0C;&#x5219;&#x8FD4;&#x56DE;&#x6241;&#x5E73;&#x8F93;&#x5165;&#x7684;argmin&#x3002;</p>
</li>
<li><p><strong>keepdim</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a>) &#x2013; whether the output tensors have <code>dim</code>retained or not. Ignored if <code>dim=None</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a
tensor([[ 0.1139,  0.2254, -0.1381,  0.3687],
        [ 1.0100, -1.1975, -0.0102, -0.4732],
        [-0.9240,  0.1207, -0.7506, -1.0213],
        [ 1.7809, -1.2960,  0.9384,  0.1438]])
&gt;&gt;&gt; torch.argmin(a, dim=1)
tensor([ 2,  1,  3,  1])
</code></pre><p><code>torch.``cumprod</code>( <em>input</em> , <em>dim</em> , <em>out=None</em> , <em>dtype=None</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x8F93;&#x5165; &#x7684;<code>&#x5143;&#x4EF6;&#x7684;&#x7D2F;&#x79EF;&#x4EA7;&#x7269;&#x5728;&#x5C3A;&#x5BF8;</code>&#x6697;&#x6DE1; <code>&#x3002;</code></p>
<p>&#x4F8B;&#x5982;&#xFF0C;&#x5982;&#x679C;<code>&#x8F93;&#x5165;</code>&#x662F;&#x5927;&#x5C0F;&#x4E3A;N&#x7684;&#x5411;&#x91CF;&#xFF0C;&#x5176;&#x7ED3;&#x679C;&#x4E5F;&#x5C06;&#x5927;&#x5C0F;&#x4E3A;N&#x7684;&#x5411;&#x91CF;&#xFF0C;&#x5305;&#x542B;&#x5143;&#x7D20;&#x3002;</p>
<p>yi=x1&#xD7;x2&#xD7;x3&#xD7;&#x22EF;&#xD7;xiy_i = x_1 \times x_2\times x_3\times \dots \times x_i
yi&#x200B;=x1&#x200B;&#xD7;x2&#x200B;&#xD7;x3&#x200B;&#xD7;&#x22EF;&#xD7;xi&#x200B;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x7684;&#x5C3A;&#x5BF8;&#x5230;&#x8D85;&#x8FC7;&#x505A;&#x624B;&#x672F;</p>
</li>
<li><p><strong>DTYPE</strong> &#xFF08;<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"> <code>torch.dtype</code></a>&#xFF0C;&#x53EF;&#x9009;&#xFF09; - &#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x7684;&#x6240;&#x5E0C;&#x671B;&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x3002;&#x5982;&#x679C;&#x6307;&#x5B9A;&#xFF0C;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x6D47;&#x94F8;&#x5230;<code>&#x5728;&#x6267;&#x884C;&#x64CD;&#x4F5C;&#x4E4B;&#x524D;D&#x578B;&#x7EC6;&#x80DE;</code>&#x3002;&#x8FD9;&#x662F;&#x4E3A;&#x4E86;&#x9632;&#x6B62;&#x6570;&#x636E;&#x6EA2;&#x51FA;&#x578B;&#x6709;&#x7528;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;&#x65E0;&#x3002;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(10)
&gt;&gt;&gt; a
tensor([ 0.6001,  0.2069, -0.1919,  0.9792,  0.6727,  1.0062,  0.4126,
        -0.2129, -0.4206,  0.1968])
&gt;&gt;&gt; torch.cumprod(a, dim=0)
tensor([ 0.6001,  0.1241, -0.0238, -0.0233, -0.0157, -0.0158, -0.0065,
         0.0014, -0.0006, -0.0001])

&gt;&gt;&gt; a[5] = 0.0
&gt;&gt;&gt; torch.cumprod(a, dim=0)
tensor([ 0.6001,  0.1241, -0.0238, -0.0233, -0.0157, -0.0000, -0.0000,
         0.0000, -0.0000, -0.0000])
</code></pre><p><code>torch.``cumsum</code>( <em>input</em> , <em>dim</em> , <em>out=None</em> , <em>dtype=None</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x5728;&#x5C3A;&#x5BF8;&#x8F93;&#x5165; &#x7684;<code>&#x5143;&#x7D20;&#x7684;&#x7D2F;&#x79EF;&#x548C;</code>&#x6697;&#x6DE1; <code>&#x3002;</code></p>
<p>For example, if <code>input</code>is a vector of size N, the result will also be a
vector of size N, with elements.</p>
<p>yi=x1+x2+x3+&#x22EF;+xiy_i = x_1 + x_2 + x_3 + \dots + x_i yi&#x200B;=x1&#x200B;+x2&#x200B;+x3&#x200B;+&#x22EF;+xi&#x200B;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a>) &#x2013; the dimension to do the operation over</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned tensor. If specified, the input tensor is casted to <code>dtype</code>before the operation is performed. This is useful for preventing data type overflows. Default: None.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(10)
&gt;&gt;&gt; a
tensor([-0.8286, -0.4890,  0.5155,  0.8443,  0.1865, -0.1752, -2.0595,
         0.1850, -1.1571, -0.4243])
&gt;&gt;&gt; torch.cumsum(a, dim=0)
tensor([-0.8286, -1.3175, -0.8020,  0.0423,  0.2289,  0.0537, -2.0058,
        -1.8209, -2.9780, -3.4022])
</code></pre><p><code>torch.``dist</code>( <em>input</em> , <em>other</em> , <em>p=2</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x7684;p&#x8303;&#x6570;&#xFF08;<code>&#x8F93;&#x5165;</code>- <code>&#x5176;&#x4ED6;</code>&#xFF09;</p>
<p>The shapes of <code>input</code>and <code>other</code>must be
<a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a>.</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>&#x5176;&#x4ED6;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x7684;&#x53F3;&#x624B;&#x4FA7;&#x7684;&#x8F93;&#x5165;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>P</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x8303;&#x6570;&#x88AB;&#x8BA1;&#x7B97;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(4)
&gt;&gt;&gt; x
tensor([-1.5393, -0.8675,  0.5916,  1.6321])
&gt;&gt;&gt; y = torch.randn(4)
&gt;&gt;&gt; y
tensor([ 0.0967, -1.0511,  0.6295,  0.8360])
&gt;&gt;&gt; torch.dist(x, y, 3.5)
tensor(1.6727)
&gt;&gt;&gt; torch.dist(x, y, 3)
tensor(1.6973)
&gt;&gt;&gt; torch.dist(x, y, 0)
tensor(inf)
&gt;&gt;&gt; torch.dist(x, y, 1)
tensor(2.6537)
</code></pre><p><code>torch.``logsumexp</code>( <em>input</em> , <em>dim</em> , <em>keepdim=False</em> , <em>out=None</em> )</p>
<p>&#x8FD4;&#x56DE;<code>&#x8F93;&#x5165;</code>&#x5F20;&#x91CF;&#x4E2D;&#x7684;&#x6BCF;&#x4E00;&#x884C;&#x7684;&#x6C42;&#x548C;&#x6307;&#x6570;&#x7684;&#x65E5;&#x5FD7;&#x4E2D;&#x7684;&#x7ED9;&#x5B9A;&#x7EF4;&#x5EA6;<code>&#x6697;&#x6DE1;</code>&#x3002;&#x8BA1;&#x7B97;&#x662F;&#x6570;&#x503C;&#x4E0A;&#x7A33;&#x5B9A;&#x3002;</p>
<p>&#x7528;&#x4E8E;&#x6C42;&#x548C;&#x6307;&#x6570; [HTG6&#xFF1A;J  [HTG9&#xFF1A;J  [HTG18&#xFF1A;J  &#x7531;&#x6697;&#x6DE1;&#x548C;&#x7ED9;&#x5B9A;&#x7684;&#x5176;&#x4ED6;&#x6307;&#x6570; i&#x7684; i&#x7684; i&#x7684; &#xFF0C;&#x5176;&#x7ED3;&#x679C;&#x662F;</p>
<blockquote>
<p>logsumexp  &#xFF08; &#xD7; &#xFF09; i&#x7684; =  &#x65E5;&#x5FD7; &#x2061; &#x3A3; [HTG29&#xFF1A;J  &#x5B9E;&#x9A8C;&#x503C; &#x2061; &#xFF08; &#xD7; i&#x7684; f]  &#xFF09;
\&#x6587;&#x672C;{logsumexp}&#xFF08;x&#xFF09;&#x7684;<em> {I} = \ LOG \ sum_j \ EXP&#xFF08;X</em> {IJ}&#xFF09; logsumexp  &#xFF08; &#xD7; &#xFF09; i&#x7684;
[HTG10 0] =  LO  G  [HTG122&#xFF1A;J  &#x3A3; EXP  &#xFF08; &#xD7; i&#x7684; [HTG166&#xFF1A;J  &#xFF09;</p>
</blockquote>
<p>&#x5982;&#x679C;<code>keepdim</code>&#x662F;<code>&#x771F;</code>&#xFF0C;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x662F;&#x76F8;&#x540C;&#x7684;&#x5927;&#x5C0F;&#x4E3A;<code>&#x8F93;&#x5165;</code>&#x9664;&#x4E86;&#x5728;&#x5C3A;&#x5BF8;&#xFF08;s&#xFF09;&#x5B9E;&#x65BD;HTG12] &#x6697;&#x6DE1; &#x5176;&#x4E2D;&#x5B83;&#x662F;&#x5C3A;&#x5BF8;1&#x7684;&#x5426;&#x5219;&#xFF0C;<code>&#x6697;&#x6DE1;</code>&#x88AB;&#x6324;&#x51FA;&#xFF08;&#x89C1; <code>torch.squeeze&#xFF08;&#xFF09;</code>&#xFF09;&#xFF0C;&#x5BFC;&#x81F4;&#x5177;&#x6709;1&#x7684;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#xFF08;&#x6216;<code>LEN&#xFF08;&#x6697;&#xFF09;</code>&#xFF09;&#x8F83;&#x5C11;&#x7684;&#x7EF4;&#xFF08;S&#xFF09;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#x6216;</em> <em>&#x87D2;&#x7684;&#x5143;&#x7EC4;&#xFF1A;&#x6574;&#x6570;</em> &#xFF09; - &#x7684;&#x5C3A;&#x5BF8;&#x6216;&#x5C3A;&#x5BF8;&#xFF0C;&#x4EE5;&#x51CF;&#x5C11;</p>
</li>
<li><p><strong>keepdim</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a>&#xFF09; - &#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x662F;&#x5426;&#x6709;<code>&#x6697;&#x6DE1;</code>&#x4FDD;&#x7559;&#x6216;&#x4E0D;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(3, 3)
&gt;&gt;&gt; torch.logsumexp(a, 1)
tensor([ 0.8442,  1.4322,  0.8711])
</code></pre><p><code>torch.``mean</code>()</p>
<p><code>torch.``mean</code>( <em>input</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x6240;&#x6709;&#x5143;&#x7D20;&#x7684;&#x5728;<code>&#x8F93;&#x5165;</code>&#x5F20;&#x91CF;&#x7684;&#x5E73;&#x5747;&#x503C;&#x3002;</p>
<p>Parameters</p>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input
tensor</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a
tensor([[ 0.2294, -0.5481,  1.3288]])
&gt;&gt;&gt; torch.mean(a)
tensor(0.3367)
</code></pre><p><code>torch.``mean</code>( <em>input</em> , <em>dim</em> , <em>keepdim=False</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x5728;&#x7ED9;&#x5B9A;&#x7EF4;&#x5EA6;&#x7684;<code>&#x8F93;&#x5165;</code>&#x5F20;&#x91CF;&#x4E2D;&#x7684;&#x6BCF;&#x4E00;&#x884C;&#x7684;&#x5E73;&#x5747;&#x503C;<code>&#x6697;&#x6DE1;</code>&#x3002;&#x5982;&#x679C;<code>&#x6697;&#x6DE1;</code>&#x4E3A;&#x7EF4;&#x5EA6;&#x7684;&#x5217;&#x8868;&#xFF0C;&#x51CF;&#x5C11;&#x8FC7;&#x5EA6;&#x6240;&#x6709;&#x7684;&#x4EBA;&#x3002;</p>
<p>If <code>keepdim</code>is <code>True</code>, the output tensor is of the same size as <code>input</code>
except in the dimension(s) <code>dim</code>where it is of size 1. Otherwise, <code>dim</code>is
squeezed (see <code>torch.squeeze()</code>), resulting in the output tensor having 1 (or
<code>len(dim)</code>) fewer dimension(s).</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>or</em> <em>tuple of python:ints</em> ) &#x2013; the dimension or dimensions to reduce</p>
</li>
<li><p><strong>keepdim</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a>) &#x2013; whether the output tensor has <code>dim</code>retained or not</p>
</li>
<li><p><strong>OUT</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x8F93;&#x51FA;&#x5F20;&#x91CF;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a
tensor([[-0.3841,  0.6320,  0.4254, -0.7384],
        [-0.9644,  1.0131, -0.6549, -1.4279],
        [-0.2951, -1.3350, -0.7694,  0.5600],
        [ 1.0842, -0.9580,  0.3623,  0.2343]])
&gt;&gt;&gt; torch.mean(a, 1)
tensor([-0.0163, -0.5085, -0.4599,  0.1807])
&gt;&gt;&gt; torch.mean(a, 1, True)
tensor([[-0.0163],
        [-0.5085],
        [-0.4599],
        [ 0.1807]])
</code></pre><p><code>torch.``median</code>()</p>
<p><code>torch.``median</code>( <em>input</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x6240;&#x6709;&#x5143;&#x7D20;&#x7684;&#x5728;<code>&#x8F93;&#x5165;</code>&#x5F20;&#x91CF;&#x7684;&#x4E2D;&#x503C;&#x3002;</p>
<p>Parameters</p>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input
tensor</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a
tensor([[ 1.5219, -1.5212,  0.2202]])
&gt;&gt;&gt; torch.median(a)
tensor(0.2202)
</code></pre><p><code>torch.``median</code>( <em>input</em> , <em>dim=-1</em> , <em>keepdim=False</em> , <em>values=None</em> ,
<em>indices=None) - &gt; (Tensor</em>, <em>LongTensor</em> )</p>
<p>&#x8FD4;&#x56DE;namedtuple <code>&#xFF08;&#x503C; &#x7D22;&#x5F15;&#xFF09;</code>&#x5176;&#x4E2D;<code>&#x503C;</code>&#x4E3A;&#x5404;&#x884C;&#x7684;&#x4E2D;&#x503C;&#x7684;<code>&#x8F93;&#x5165;</code>&#x5F20;&#x91CF;&#x5728;&#x7ED9;&#x5B9A;&#x7684;&#x5C3A;&#x5BF8;<code>&#x6697;&#x6DE1;</code>&#x3002;&#x548C;<code>&#x6307;&#x6570;</code>&#x662F;&#x627E;&#x5230;&#x7684;&#x6BCF;&#x4E2A;&#x4E2D;&#x95F4;&#x503C;&#x7684;&#x7D22;&#x5F15;&#x4F4D;&#x7F6E;&#x3002;</p>
<p>&#x9ED8;&#x8BA4;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;<code>&#x6697;&#x6DE1;</code>&#x4E3A;<code>&#x8F93;&#x5165;</code>&#x5F20;&#x91CF;&#x7684;&#x6700;&#x540E;&#x7EF4;&#x5EA6;&#x3002;</p>
<p>&#x5982;&#x679C;<code>keepdim</code>&#x662F;<code>&#x771F;</code>&#xFF0C;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x662F;&#x76F8;&#x540C;&#x5927;&#x5C0F;&#x7684;&#x4F5C;&#x4E3A;<code>&#x8F93;&#x5165;</code>&#x9664;&#x4E86;&#x5728;&#x5C3A;&#x5BF8;<code>&#x6697;&#x6DE1;</code>&#x5176;&#x4E2D;&#x5B83;&#x4EEC;&#x662F;&#x5C3A;&#x5BF8;1&#x7684;&#x5426;&#x5219;&#xFF0C;<code>&#x6697;&#x6DE1;</code>&#x88AB;&#x6324;&#x51FA;&#xFF08;&#x89C1; <code>torch.squeeze&#xFF08;&#xFF09;</code>&#xFF09;&#xFF0C;&#x5BFC;&#x81F4;&#x5177;&#x6709;&#x6BD4;1&#x79CD;<code>&#x8F93;&#x5165;</code>&#x66F4;&#x5C11;&#x5C3A;&#x5BF8;&#x7684;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x7684;&#x5C3A;&#x5BF8;&#xFF0C;&#x4EE5;&#x51CF;&#x5C11;</p>
</li>
<li><p><strong>keepdim</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a>&#xFF09; - &#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x662F;&#x5426;&#x6709;<code>&#x6697;&#x6DE1;</code>&#x4FDD;&#x7559;&#x6216;&#x4E0D;</p>
</li>
<li><p><strong>&#x503C;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x8F93;&#x51FA;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>&#x6307;&#x6570;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x8F93;&#x51FA;&#x7D22;&#x5F15;&#x5F20;&#x91CF;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 5)
&gt;&gt;&gt; a
tensor([[ 0.2505, -0.3982, -0.9948,  0.3518, -1.3131],
        [ 0.3180, -0.6993,  1.0436,  0.0438,  0.2270],
        [-0.2751,  0.7303,  0.2192,  0.3321,  0.2488],
        [ 1.0778, -1.9510,  0.7048,  0.4742, -0.7125]])
&gt;&gt;&gt; torch.median(a, 1)
torch.return_types.median(values=tensor([-0.3982,  0.2270,  0.2488,  0.4742]), indices=tensor([1, 4, 4, 3]))
</code></pre><p><code>torch.``mode</code>( <em>input</em> , <em>dim=-1</em> , <em>keepdim=False</em> , <em>values=None</em> ,
<em>indices=None) - &gt; (Tensor</em>, <em>LongTensor</em> )</p>
<p>&#x8FD4;&#x56DE;namedtuple <code>&#xFF08;&#x503C; &#x7D22;&#x5F15;&#xFF09;</code>&#x5176;&#x4E2D;<code>&#x503C;</code>&#x662F;&#x6BCF;&#x884C;&#x7684;&#x6A21;&#x5F0F;&#x503C;&#x7684;<code>&#x8F93;&#x5165;</code>&#x5F20;&#x91CF;&#x5728;&#x7ED9;&#x5B9A;&#x7684;&#x5C3A;&#x5BF8;<code>&#x6697;&#x6DE1;</code>&#xFF0C;&#x5373;&#xFF0C;&#x6700;&#x7ECF;&#x5E38;&#x51FA;&#x73B0;&#x5728;&#x8BE5;&#x884C;&#x4E2D;&#x7684;&#x503C;&#xFF0C;&#x5E76;<code>&#x7D22;&#x5F15;</code>&#x662F;&#x627E;&#x5230;&#x7684;&#x6BCF;&#x4E2A;&#x6A21;&#x5F0F;&#x503C;&#x7684;&#x7D22;&#x5F15;&#x4F4D;&#x7F6E;&#x3002;</p>
<p>By default, <code>dim</code>is the last dimension of the <code>input</code>tensor.</p>
<p>&#x5982;&#x679C;<code>keepdim</code>&#x662F;<code>&#x771F;</code>&#xFF0C;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x662F;&#x76F8;&#x540C;&#x5927;&#x5C0F;&#x7684;&#x4F5C;&#x4E3A;<code>&#x8F93;&#x5165;</code>&#x9664;&#x4E86;&#x5728;&#x5C3A;&#x5BF8;<code>&#x6697;&#x6DE1;</code>&#x5176;&#x4E2D;&#x5B83;&#x4EEC;&#x662F;&#x5C3A;&#x5BF8;1&#x7684;&#x5426;&#x5219;&#xFF0C;<code>&#x6697;&#x6DE1;</code>&#x88AB;&#x6324;&#x51FA;&#xFF08;&#x89C1; <code>torch.squeeze&#xFF08;&#xFF09;</code>&#xFF09;&#xFF0C;&#x5BFC;&#x81F4;&#x5177;&#x6709;&#x6BD4;1&#x79CD;<code>&#x8F93;&#x5165;</code>&#x66F4;&#x5C11;&#x5C3A;&#x5BF8;&#x7684;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x3002;</p>
<p>Note</p>
<p>&#x6B64;&#x529F;&#x80FD;&#x8FD8;&#x6CA1;&#x6709;&#x4E3A;<code>torch.cuda.Tensor</code>&#x4E2D;&#x5B9A;&#x4E49;&#x7231;&#x597D;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a>) &#x2013; the dimension to reduce</p>
</li>
<li><p><strong>keepdim</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a>) &#x2013; whether the output tensors have <code>dim</code>retained or not</p>
</li>
<li><p><strong>values</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
<li><p><strong>indices</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output index tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randint(10, (5,))
&gt;&gt;&gt; a
tensor([6, 5, 1, 0, 2])
&gt;&gt;&gt; b = a + (torch.randn(50, 1) * 5).long()
&gt;&gt;&gt; torch.mode(b, 0)
torch.return_types.mode(values=tensor([6, 5, 1, 0, 2]), indices=tensor([2, 2, 2, 2, 2]))
</code></pre><p><code>torch.``norm</code>( <em>input</em> , <em>p=&apos;fro&apos;</em> , <em>dim=None</em> , <em>keepdim=False</em> ,
<em>out=None</em> , <em>dtype=None</em> )<a href="_modules/torch/functional.html#norm">[source]</a></p>
<p>&#x8FD4;&#x56DE;&#x7ED9;&#x5B9A;&#x5F20;&#x91CF;&#x7684;&#x77E9;&#x9635;&#x8303;&#x6570;&#x6216;&#x5411;&#x91CF;&#x8303;&#x6570;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>P</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a> <em>&#xFF0C;</em> <em>INF</em> <em>&#xFF0C;</em> <em>-INF</em> <em>&#xFF0C;</em> <em>&apos;&#x6765;&#x56DE;&apos;</em> <em>&#xFF0C;</em> <em>&apos;NUC&apos;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - </p>
</li>
</ul>
<p>&#x89C4;&#x8303;&#x7684;&#x79E9;&#x5E8F;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&apos;&#x6765;&#x56DE;&apos;</code>&#x53EF;&#x4EE5;&#x8BA1;&#x7B97;&#x4EE5;&#x4E0B;&#x89C4;&#x8303;&#xFF1A;</p>
<p>ORD</p>
<p>|</p>
<p>&#x77E9;&#x9635;&#x8303;</p>
<p>|</p>
<p>&#x5411;&#x91CF;&#x6A21;  </p>
<p>---|---|---  </p>
<p>&#x6CA1;&#x6709;</p>
<p>|</p>
<p>&#x5F17;&#x6D1B;&#x6BD4;&#x5C3C;&#x8303;&#x6570;</p>
<p>|</p>
<p>2&#x8303;  </p>
<p>&#x201C;&#x56DE;&#x56DE;&#x201D;</p>
<p>|</p>
<p>Frobenius norm</p>
<p>|</p>
<p>-  </p>
<p>&#x201C;&#x56FD;&#x7EDF;&#x4F1A;&#x201D;</p>
<p>|</p>
<p>&#x6838;&#x6807;&#x51C6;</p>
<p>|</p>
<p>&#x2013;  </p>
<p>&#x5176;&#x4ED6;</p>
<p>|</p>
<p>&#x4F5C;&#x4E3A;VEC&#x89C4;&#x8303;&#x65F6;&#xFF0C;&#x660F;&#x6697;&#x7684;&#x662F;&#x65E0;</p>
<p>|</p>
<p>&#x603B;&#x548C;&#xFF08;ABS&#xFF08;X&#xFF09;<strong> ORD&#xFF09;</strong>&#xFF08;1./ord&#xFF09;  </p>
<ul>
<li><p><strong>&#x6697;&#x6DE1;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <em>&#x87D2;&#x7684;2&#x5143;&#x7EC4;&#xFF1A;&#x6574;&#x578B;</em> <em>&#xFF0C;</em> <em>&#x87D2;2-&#x5217;&#x8868;&#xFF1A;&#x6574;&#x6570;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5982;&#x679C;&#x5B83;&#x662F;&#x4E00;&#x4E2A;int&#xFF0C;&#x77E2;&#x91CF;&#x8303;&#x6570;&#x5C06;&#x88AB;&#x8BA1;&#x7B97;&#xFF0C;&#x5982;&#x679C;&#x662F;&#x6574;&#x6570;&#x7684;2&#x5143;&#x7EC4;&#xFF0C;&#x77E9;&#x9635;&#x8303;&#x6570;&#x5C06;&#x88AB;&#x8BA1;&#x7B97;&#x3002;&#x5982;&#x679C;&#x8BE5;&#x503C;&#x662F;&#x65E0;&#xFF0C;&#x5F53;&#x8F93;&#x5165;&#x4EC5;&#x5F20;&#x91CF;&#x5177;&#x6709;&#x4E24;&#x4E2A;&#x7EF4;&#x5EA6;&#x77E9;&#x9635;&#x8303;&#x6570;&#x5C06;&#x88AB;&#x8BA1;&#x7B97;&#xFF0C;&#x5F53;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x53EA;&#x6709;&#x4E00;&#x4E2A;&#x7EF4;&#x5EA6;&#x77E2;&#x91CF;&#x8303;&#x6570;&#x5C06;&#x88AB;&#x8BA1;&#x7B97;&#x3002;&#x5982;&#x679C;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x5177;&#x6709;&#x591A;&#x4E8E;&#x4E24;&#x4E2A;&#x5C3A;&#x5BF8;&#xFF0C;&#x77E2;&#x91CF;&#x8303;&#x6570;&#x5C06;&#x88AB;&#x5E94;&#x7528;&#x5230;&#x6700;&#x540E;&#x5C3A;&#x5BF8;&#x3002;</p>
</li>
<li><p><strong>keepdim</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x662F;&#x5426;&#x6709;<code>&#x6697;&#x6DE1;</code>&#x4FDD;&#x7559;&#x6216;&#x6CA1;&#x6709;&#x3002;&#x5FFD;&#x7565;&#x5982;&#x679C;<code>&#x6697;&#x6DE1;</code>= <code>&#x65E0;</code>&#x548C;<code>OUT</code>= <code>&#x65E0;</code>&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x5047;</code></p>
</li>
<li><p><strong>OUT</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x3002;&#x5FFD;&#x7565;&#x5982;&#x679C;<code>&#x6697;&#x6DE1;</code>= <code>&#x65E0;</code>&#x548C;<code>OUT</code>= <code>&#x65E0;</code>&#x3002;</p>
</li>
<li><p><strong>DTYPE</strong> &#xFF08;<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"> <code>torch.dtype</code></a>&#xFF0C;&#x53EF;&#x9009;&#xFF09; - &#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x7684;&#x6240;&#x5E0C;&#x671B;&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x3002;&#x6267;&#x884C;&#x64CD;&#x4F5C;&#x201C;D&#x578B;&#x201D;&#xFF1A;&#x5982;&#x679C;&#x6307;&#x5B9A;&#xFF0C;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x6D47;&#x94F8;&#x5230;&#xFF1A;ATTR&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;&#x65E0;&#x3002;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; import torch
&gt;&gt;&gt; a = torch.arange(9, dtype= torch.float) - 4
&gt;&gt;&gt; b = a.reshape((3, 3))
&gt;&gt;&gt; torch.norm(a)
tensor(7.7460)
&gt;&gt;&gt; torch.norm(b)
tensor(7.7460)
&gt;&gt;&gt; torch.norm(a, float(&apos;inf&apos;))
tensor(4.)
&gt;&gt;&gt; torch.norm(b, float(&apos;inf&apos;))
tensor(4.)
&gt;&gt;&gt; c = torch.tensor([[ 1, 2, 3],[-1, 1, 4]] , dtype= torch.float)
&gt;&gt;&gt; torch.norm(c, dim=0)
tensor([1.4142, 2.2361, 5.0000])
&gt;&gt;&gt; torch.norm(c, dim=1)
tensor([3.7417, 4.2426])
&gt;&gt;&gt; torch.norm(c, p=1, dim=1)
tensor([6., 6.])
&gt;&gt;&gt; d = torch.arange(8, dtype= torch.float).reshape(2,2,2)
&gt;&gt;&gt; torch.norm(d, dim=(1,2))
tensor([ 3.7417, 11.2250])
&gt;&gt;&gt; torch.norm(d[0, :, :]), torch.norm(d[1, :, :])
(tensor(3.7417), tensor(11.2250))
</code></pre><p><code>torch.``prod</code>()</p>
<p><code>torch.``prod</code>( <em>input</em> , <em>dtype=None</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x5728;<code>&#x8F93;&#x5165;</code>&#x5F20;&#x91CF;&#x7684;&#x6240;&#x6709;&#x5143;&#x7D20;&#x7684;&#x4E58;&#x79EF;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned tensor. If specified, the input tensor is casted to <code>dtype</code>before the operation is performed. This is useful for preventing data type overflows. Default: None.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a
tensor([[-0.8020,  0.5428, -1.5854]])
&gt;&gt;&gt; torch.prod(a)
tensor(0.6902)
</code></pre><p><code>torch.``prod</code>( <em>input</em> , <em>dim</em> , <em>keepdim=False</em> , <em>dtype=None</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;<code>&#x8F93;&#x5165;</code>&#x5F20;&#x91CF;&#x7684;&#x5404;&#x884C;&#x7684;&#x4EA7;&#x54C1;&#x5728;&#x7ED9;&#x5B9A;&#x7684;&#x5C3A;&#x5BF8;<code>&#x6697;&#x6DE1;</code>&#x3002;</p>
<p>&#x5982;&#x679C;<code>keepdim</code>&#x662F;<code>&#x771F;</code>&#xFF0C;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x662F;&#x76F8;&#x540C;&#x7684;&#x5927;&#x5C0F;&#x4E3A;<code>&#x8F93;&#x5165;</code>&#x9664;&#x4E86;&#x5728;&#x5C3A;&#x5BF8;<code>&#x6697;&#x6DE1;</code>&#x5176;&#x4E2D;&#x5B83;&#x662F;&#x5C3A;&#x5BF8;1&#x7684;&#x5426;&#x5219;&#xFF0C;<code>&#x6697;&#x6DE1;</code>&#x88AB;&#x6324;&#x51FA;&#xFF08;&#x89C1; <code>torch.squeeze&#xFF08;&#xFF09;</code>&#xFF09;&#xFF0C;&#x5BFC;&#x81F4;&#x5177;&#x6709;&#x6BD4;1&#x79CD;<code>&#x8F93;&#x5165;</code>&#x66F4;&#x5C11;&#x5C3A;&#x5BF8;&#x7684;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a>) &#x2013; the dimension to reduce</p>
</li>
<li><p><strong>keepdim</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a>) &#x2013; whether the output tensor has <code>dim</code>retained or not</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned tensor. If specified, the input tensor is casted to <code>dtype</code>before the operation is performed. This is useful for preventing data type overflows. Default: None.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 2)
&gt;&gt;&gt; a
tensor([[ 0.5261, -0.3837],
        [ 1.1857, -0.2498],
        [-1.1646,  0.0705],
        [ 1.1131, -1.0629]])
&gt;&gt;&gt; torch.prod(a, 1)
tensor([-0.2018, -0.2962, -0.0821, -1.1831])
</code></pre><p><code>torch.``std</code>()</p>
<p><code>torch.``std</code>( <em>input</em> , <em>unbiased=True</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x5728;<code>&#x8F93;&#x5165;</code>&#x5F20;&#x91CF;&#x7684;&#x6240;&#x6709;&#x5143;&#x7D20;&#x7684;&#x6807;&#x51C6;&#x504F;&#x5DEE;&#x3002;</p>
<p>&#x5982;&#x679C;<code>&#x65E0;&#x504F;</code>&#x662F;<code>&#x5047;</code>&#xFF0C;&#x5219;&#x6807;&#x51C6;&#x504F;&#x5DEE;&#x5C06;&#x88AB;&#x7ECF;&#x7531;&#x504F;&#x4F30;&#x8BA1;&#x8BA1;&#x7B97;&#x3002;&#x5426;&#x5219;&#xFF0C;&#x8D1D;&#x585E;&#x5C14;&#x4FEE;&#x6B63;&#x5C06;&#x88AB;&#x4F7F;&#x7528;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>&#x65E0;&#x504F;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a>&#xFF09; - &#x662F;&#x5426;&#x4F7F;&#x7528;&#x65E0;&#x504F;&#x4F30;&#x8BA1;&#x6216;&#x4E0D;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a
tensor([[-0.8166, -1.3802, -0.3560]])
&gt;&gt;&gt; torch.std(a)
tensor(0.5130)
</code></pre><p><code>torch.``std</code>( <em>input</em> , <em>dim</em> , <em>keepdim=False</em> , <em>unbiased=True</em> ,
<em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x7EF4;&#x5EA6;&#x4E2D;&#x7684;<code>&#x8F93;&#x5165;</code>&#x5F20;&#x91CF;&#x7684;&#x5404;&#x884C;&#x7684;&#x6807;&#x51C6;&#x504F;&#x5DEE;<code>&#x6697;&#x6DE1;</code>&#x3002;&#x5982;&#x679C;<code>&#x6697;&#x6DE1;</code>&#x4E3A;&#x7EF4;&#x5EA6;&#x7684;&#x5217;&#x8868;&#xFF0C;&#x51CF;&#x5C11;&#x8FC7;&#x5EA6;&#x6240;&#x6709;&#x7684;&#x4EBA;&#x3002;</p>
<p>If <code>keepdim</code>is <code>True</code>, the output tensor is of the same size as <code>input</code>
except in the dimension(s) <code>dim</code>where it is of size 1. Otherwise, <code>dim</code>is
squeezed (see <code>torch.squeeze()</code>), resulting in the output tensor having 1 (or
<code>len(dim)</code>) fewer dimension(s).</p>
<p>If <code>unbiased</code>is <code>False</code>, then the standard-deviation will be calculated via
the biased estimator. Otherwise, Bessel&#x2019;s correction will be used.</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>or</em> <em>tuple of python:ints</em> ) &#x2013; the dimension or dimensions to reduce</p>
</li>
<li><p><strong>keepdim</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a>) &#x2013; whether the output tensor has <code>dim</code>retained or not</p>
</li>
<li><p><strong>unbiased</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a>) &#x2013; whether to use the unbiased estimation or not</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a
tensor([[ 0.2035,  1.2959,  1.8101, -0.4644],
        [ 1.5027, -0.3270,  0.5905,  0.6538],
        [-1.5745,  1.3330, -0.5596, -0.6548],
        [ 0.1264, -0.5080,  1.6420,  0.1992]])
&gt;&gt;&gt; torch.std(a, dim=1)
tensor([ 1.0311,  0.7477,  1.2204,  0.9087])
</code></pre><p><code>torch.``std_mean</code>()</p>
<p><code>torch.``std_mean</code>( <em>input</em> , <em>unbiased=True) - &gt; (Tensor</em>, <em>Tensor</em> )</p>
<p>&#x8FD4;&#x56DE;&#x5728;<code>&#x8F93;&#x5165;</code>&#x5F20;&#x91CF;&#x7684;&#x6240;&#x6709;&#x5143;&#x7D20;&#x7684;&#x6807;&#x51C6;&#x504F;&#x5DEE;&#x548C;&#x5E73;&#x5747;&#x503C;&#x3002;</p>
<p>If <code>unbiased</code>is <code>False</code>, then the standard-deviation will be calculated via
the biased estimator. Otherwise, Bessel&#x2019;s correction will be used.</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>unbiased</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a>) &#x2013; whether to use the unbiased estimation or not</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a
tensor([[0.3364, 0.3591, 0.9462]])
&gt;&gt;&gt; torch.std_mean(a)
(tensor(0.3457), tensor(0.5472))
</code></pre><p><code>torch.``std</code>( <em>input</em> , <em>dim</em> , <em>keepdim=False</em> , <em>unbiased=True) - &gt;
(Tensor</em>, <em>Tensor</em> )</p>
<p>&#x8FD4;&#x56DE;&#x7EF4;&#x5EA6;&#x4E2D;&#x7684;<code>&#x8F93;&#x5165;</code>&#x5F20;&#x91CF;&#x7684;&#x5404;&#x884C;&#x7684;&#x6807;&#x51C6;&#x504F;&#x5DEE;&#x548C;&#x5E73;&#x5747;&#x503C;<code>&#x6697;&#x6DE1;</code>&#x3002;&#x5982;&#x679C;<code>&#x6697;&#x6DE1;</code>&#x4E3A;&#x7EF4;&#x5EA6;&#x7684;&#x5217;&#x8868;&#xFF0C;&#x51CF;&#x5C11;&#x8FC7;&#x5EA6;&#x6240;&#x6709;&#x7684;&#x4EBA;&#x3002;</p>
<p>If <code>keepdim</code>is <code>True</code>, the output tensor is of the same size as <code>input</code>
except in the dimension(s) <code>dim</code>where it is of size 1. Otherwise, <code>dim</code>is
squeezed (see <code>torch.squeeze()</code>), resulting in the output tensor having 1 (or
<code>len(dim)</code>) fewer dimension(s).</p>
<p>If <code>unbiased</code>is <code>False</code>, then the standard-deviation will be calculated via
the biased estimator. Otherwise, Bessel&#x2019;s correction will be used.</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>or</em> <em>tuple of python:ints</em> ) &#x2013; the dimension or dimensions to reduce</p>
</li>
<li><p><strong>keepdim</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a>) &#x2013; whether the output tensor has <code>dim</code>retained or not</p>
</li>
<li><p><strong>unbiased</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a>) &#x2013; whether to use the unbiased estimation or not</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a
tensor([[ 0.5648, -0.5984, -1.2676, -1.4471],
        [ 0.9267,  1.0612,  1.1050, -0.6014],
        [ 0.0154,  1.9301,  0.0125, -1.0904],
        [-1.9711, -0.7748, -1.3840,  0.5067]])
&gt;&gt;&gt; torch.std_mean(a, 1)
(tensor([0.9110, 0.8197, 1.2552, 1.0608]), tensor([-0.6871,  0.6229,  0.2169, -0.9058]))
</code></pre><p><code>torch.``sum</code>()</p>
<p><code>torch.``sum</code>( <em>input</em> , <em>dtype=None</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x5728;<code>&#x8F93;&#x5165;</code>&#x5F20;&#x91CF;&#x7684;&#x6240;&#x6709;&#x5143;&#x7D20;&#x7684;&#x603B;&#x548C;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned tensor. If specified, the input tensor is casted to <code>dtype</code>before the operation is performed. This is useful for preventing data type overflows. Default: None.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a
tensor([[ 0.1133, -0.9567,  0.2958]])
&gt;&gt;&gt; torch.sum(a)
tensor(-0.5475)
</code></pre><p><code>torch.``sum</code>( <em>input</em> , <em>dim</em> , <em>keepdim=False</em> , <em>dtype=None</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x5728;&#x7ED9;&#x5B9A;&#x7EF4;&#x5EA6;&#x7684;<code>&#x8F93;&#x5165;</code>&#x5F20;&#x91CF;&#x7684;&#x6BCF;&#x4E00;&#x884C;&#x7684;&#x603B;&#x548C;<code>&#x6697;&#x6DE1;</code>&#x3002;&#x5982;&#x679C;<code>&#x6697;&#x6DE1;</code>&#x4E3A;&#x7EF4;&#x5EA6;&#x7684;&#x5217;&#x8868;&#xFF0C;&#x51CF;&#x5C11;&#x8FC7;&#x5EA6;&#x6240;&#x6709;&#x7684;&#x4EBA;&#x3002;</p>
<p>If <code>keepdim</code>is <code>True</code>, the output tensor is of the same size as <code>input</code>
except in the dimension(s) <code>dim</code>where it is of size 1. Otherwise, <code>dim</code>is
squeezed (see <code>torch.squeeze()</code>), resulting in the output tensor having 1 (or
<code>len(dim)</code>) fewer dimension(s).</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>or</em> <em>tuple of python:ints</em> ) &#x2013; the dimension or dimensions to reduce</p>
</li>
<li><p><strong>keepdim</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a>) &#x2013; whether the output tensor has <code>dim</code>retained or not</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned tensor. If specified, the input tensor is casted to <code>dtype</code>before the operation is performed. This is useful for preventing data type overflows. Default: None.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a
tensor([[ 0.0569, -0.2475,  0.0737, -0.3429],
        [-0.2993,  0.9138,  0.9337, -1.6864],
        [ 0.1132,  0.7892, -0.1003,  0.5688],
        [ 0.3637, -0.9906, -0.4752, -1.5197]])
&gt;&gt;&gt; torch.sum(a, 1)
tensor([-0.4598, -0.1381,  1.3708, -2.6217])
&gt;&gt;&gt; b = torch.arange(4 * 5 * 6).view(4, 5, 6)
&gt;&gt;&gt; torch.sum(b, (2, 1))
tensor([  435.,  1335.,  2235.,  3135.])
</code></pre><p><code>torch.``unique</code>( <em>input</em> , <em>sorted=True</em> , <em>return_inverse=False</em> ,
<em>return_counts=False</em> , <em>dim=None</em>
)<a href="_modules/torch/functional.html#unique">[source]</a></p>
<p>&#x8FD4;&#x56DE;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x7684;&#x72EC;&#x7279;&#x5143;&#x7D20;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>&#x6392;&#x5E8F;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a>&#xFF09; - &#x662F;&#x5426;&#x8FD4;&#x56DE;&#x4F5C;&#x4E3A;&#x8F93;&#x51FA;&#x4E4B;&#x524D;&#x6309;&#x5347;&#x5E8F;&#x5BF9;&#x72EC;&#x7279;&#x7684;&#x5143;&#x7D20;&#x8FDB;&#x884C;&#x6392;&#x5E8F;&#x3002;</p>
</li>
<li><p><strong>return_inverse</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a>&#xFF09; - &#x662F;&#x5426;&#x4E5F;&#x8FD4;&#x56DE;&#x5982;&#x5728;&#x539F;&#x6709;&#x7684;&#x8F93;&#x5165;&#x5143;&#x7D20;&#x5728;&#x8FD4;&#x56DE;&#x7684;&#x552F;&#x4E00;&#x5217;&#x8868;&#x7ED3;&#x675F;&#x4E86;&#x7D22;&#x5F15;&#x3002;</p>
</li>
<li><p><strong>return_counts</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a>&#xFF09; - &#x662F;&#x5426;&#x4E5F;&#x8FD4;&#x56DE;&#x7684;&#x8BA1;&#x6570;&#x4E3A;&#x6BCF;&#x4E2A;&#x552F;&#x4E00;&#x7684;&#x5143;&#x4EF6;&#x3002;</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x7EF4;&#x5EA6;&#x5E94;&#x7528;&#x662F;&#x552F;&#x4E00;&#x7684;&#x3002;&#x5982;&#x679C;<code>&#x65E0;</code>&#xFF0C;&#x72EC;&#x7279;&#x7684;&#x6241;&#x5E73;&#x8F93;&#x5165;&#x7684;&#x88AB;&#x8FD4;&#x56DE;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x65E0;</code></p>
</li>
</ul>
<p>Returns</p>
<p>&#x7684;&#x5F20;&#x91CF;&#x6216;&#x5F20;&#x91CF;&#x7684;&#x542B;&#x6709;&#x4E00;&#x4E2A;&#x5143;&#x7EC4;</p>
<blockquote>
<ul>
<li><strong>&#x8F93;&#x51FA;</strong> &#xFF08; <em>&#x5F20;&#x91CF;</em> &#xFF09;&#xFF1A;&#x552F;&#x4E00;&#x7684;&#x6807;&#x91CF;&#x5143;&#x4EF6;&#x7684;&#x8F93;&#x51FA;&#x5217;&#x8868;&#x3002;</li>
</ul>
</blockquote>
<p>&gt;</p>
<blockquote>
<ul>
<li><strong>inverse_indices</strong> &#xFF08; <em>&#x5F20;&#x91CF;</em> &#xFF09;&#xFF1A;&#xFF08;&#x53EF;&#x9009;&#xFF09;&#x5982;&#x679C;<code>return_inverse</code>&#x4E3A;True&#xFF0C;&#x5C06;&#x6709;&#x4E00;&#x4E2A;&#x9644;&#x52A0;&#x7684;&#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#xFF08;&#x76F8;&#x540C;&#x7684;&#x5F62;&#x72B6;&#x4F5C;&#x4E3A;&#x8F93;&#x5165;&#xFF09;&#x8868;&#x793A;&#x7528;&#x4E8E;&#x5176;&#x4E2D;&#x5728;&#x539F;&#x59CB;&#x8F93;&#x5165;&#x5730;&#x56FE;&#x5143;&#x7D20;&#x5230;&#x8F93;&#x51FA;&#x7D22;&#x5F15;;&#x5426;&#x5219;&#xFF0C;&#x8BE5;&#x51FD;&#x6570;&#x5C06;&#x53EA;&#x8FD4;&#x56DE;&#x5355;&#x4E2A;&#x5F20;&#x91CF;&#x3002;</li>
</ul>
</blockquote>
<p>&gt;</p>
<blockquote>
<ul>
<li><strong>&#x8BA1;&#x6570;</strong> &#xFF08; <em>&#x5F20;&#x91CF;</em> &#xFF09;&#xFF1A;&#xFF08;&#x53EF;&#x9009;&#xFF09;&#x5982;&#x679C;<code>return_counts</code>&#x4E3A;True&#xFF0C;&#x5C06;&#x6709;&#x4E00;&#x4E2A;&#x9644;&#x52A0;&#x7684;&#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#xFF08;&#x76F8;&#x540C;&#x7684;&#x5F62;&#x72B6;&#x7684;&#x8F93;&#x51FA;&#x6216;output.size&#xFF08;DIM&#xFF09;&#xFF0C;&#x5982;&#x679C;&#x8C03;&#x5149;&#x88AB;&#x6307;&#x5B9A;&#xFF09;&#x4EE3;&#x8868;&#x51FA;&#x73B0;&#x7684;&#x6BCF;&#x4E2A;&#x552F;&#x4E00;&#x503C;&#xFF0C;&#x6216;&#x5F20;&#x91CF;&#x7684;&#x6570;&#x76EE;&#x3002;</li>
</ul>
</blockquote>
<p>&gt;</p>
<p>&gt;</p>
<p>Return type</p>
<p>&#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor">&#x5F20;&#x91CF;</a>&#xFF0C;<a href="tensors.html#torch.Tensor" title="torch.Tensor">&#x5F20;&#x91CF;</a>&#xFF08;&#x53EF;&#x9009;&#xFF09;&#xFF0C;<a href="tensors.html#torch.Tensor" title="torch.Tensor">&#x5F20;&#x91CF;</a>&#xFF08;&#x53EF;&#x9009;&#xFF09;&#xFF09;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; output = torch.unique(torch.tensor([1, 3, 2, 3], dtype=torch.long))
&gt;&gt;&gt; output
tensor([ 2,  3,  1])

&gt;&gt;&gt; output, inverse_indices = torch.unique(
        torch.tensor([1, 3, 2, 3], dtype=torch.long), sorted=True, return_inverse=True)
&gt;&gt;&gt; output
tensor([ 1,  2,  3])
&gt;&gt;&gt; inverse_indices
tensor([ 0,  2,  1,  2])

&gt;&gt;&gt; output, inverse_indices = torch.unique(
        torch.tensor([[1, 3], [2, 3]], dtype=torch.long), sorted=True, return_inverse=True)
&gt;&gt;&gt; output
tensor([ 1,  2,  3])
&gt;&gt;&gt; inverse_indices
tensor([[ 0,  2],
        [ 1,  2]])
</code></pre><p><code>torch.``unique_consecutive</code>( <em>input</em> , <em>return_inverse=False</em> ,
<em>return_counts=False</em> , <em>dim=None</em>
)<a href="_modules/torch/functional.html#unique_consecutive">[source]</a></p>
<p>&#x6D88;&#x9664;&#x4E86;&#x6240;&#x6709;&#x7684;&#x4F46;&#x7B49;&#x6548;&#x4ECE;&#x5143;&#x4EF6;&#x7684;&#x6BCF;&#x4E2A;&#x8FDE;&#x7EED;&#x7EC4;&#x7684;&#x7B2C;&#x4E00;&#x4E2A;&#x5143;&#x7D20;&#x3002;</p>
<p>Note</p>
<p>&#x6B64;&#x529F;&#x80FD;&#x662F;&#x7531;&#x4E0D;&#x540C;<code>torch.unique&#xFF08;&#xFF09;</code>&#x5728;&#x8FD9;&#x4E2A;&#x610F;&#x4E49;&#x4E0A;&#xFF0C;&#x6B64;&#x529F;&#x80FD;&#x4EC5;&#x6D88;&#x9664;&#x8FDE;&#x7EED;&#x91CD;&#x590D;&#x7684;&#x503C;&#x3002;&#x8FD9;&#x4E2A;&#x8BED;&#x4E49;&#x7C7B;&#x4F3C;&#x4E8E;&#x7684;std ::&#x7528;C&#x72EC;&#x7279;&#x7684; ++&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>return_inverse</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a>) &#x2013; Whether to also return the indices for where elements in the original input ended up in the returned unique list.</p>
</li>
<li><p><strong>return_counts</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a>) &#x2013; Whether to also return the counts for each unique element.</p>
</li>
<li><p><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a>) &#x2013; the dimension to apply unique. If <code>None</code>, the unique of the flattened input is returned. default: <code>None</code></p>
</li>
</ul>
<p>Returns</p>
<p>A tensor or a tuple of tensors containing</p>
<blockquote>
<ul>
<li><strong>output</strong> ( <em>Tensor</em> ): the output list of unique scalar elements.</li>
</ul>
</blockquote>
<p>&gt;</p>
<blockquote>
<ul>
<li><strong>inverse_indices</strong> ( <em>Tensor</em> ): (optional) if <code>return_inverse</code>is
True, there will be an additional returned tensor (same shape as input)
representing the indices for where elements in the original input map to in
the output; otherwise, this function will only return a single tensor.</li>
</ul>
</blockquote>
<p>&gt;</p>
<blockquote>
<ul>
<li><strong>counts</strong> ( <em>Tensor</em> ): (optional) if <code>return_counts</code>is True, there
will be an additional returned tensor (same shape as output or
output.size(dim), if dim was specified) representing the number of occurrences
for each unique value or tensor.</li>
</ul>
</blockquote>
<p>&gt;</p>
<p>&gt;</p>
<p>Return type</p>
<p>(<a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>,
<a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a> (optional),
<a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a> (optional))</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.tensor([1, 1, 2, 2, 3, 1, 1, 2])
&gt;&gt;&gt; output = torch.unique_consecutive(x)
&gt;&gt;&gt; output
tensor([1, 2, 3, 1, 2])

&gt;&gt;&gt; output, inverse_indices = torch.unique_consecutive(x, return_inverse=True)
&gt;&gt;&gt; output
tensor([1, 2, 3, 1, 2])
&gt;&gt;&gt; inverse_indices
tensor([0, 0, 1, 1, 2, 3, 3, 4])

&gt;&gt;&gt; output, counts = torch.unique_consecutive(x, return_counts=True)
&gt;&gt;&gt; output
tensor([1, 2, 3, 1, 2])
&gt;&gt;&gt; counts
tensor([2, 2, 1, 2, 1])
</code></pre><p><code>torch.``var</code>()</p>
<p><code>torch.``var</code>( <em>input</em> , <em>unbiased=True</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x6240;&#x6709;&#x5143;&#x7D20;&#x7684;&#x5728;<code>&#x8F93;&#x5165;</code>&#x5F20;&#x91CF;&#x7684;&#x65B9;&#x5DEE;&#x3002;</p>
<p>&#x5982;&#x679C;<code>&#x65E0;&#x504F;</code>&#x662F;<code>&#x5047;</code>&#xFF0C;&#x7136;&#x540E;&#x65B9;&#x5DEE;&#x5C06;&#x7ECF;&#x7531;&#x504F;&#x4F30;&#x8BA1;&#x8BA1;&#x7B97;&#x3002;&#x5426;&#x5219;&#xFF0C;&#x8D1D;&#x585E;&#x5C14;&#x4FEE;&#x6B63;&#x5C06;&#x88AB;&#x4F7F;&#x7528;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>unbiased</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a>) &#x2013; whether to use the unbiased estimation or not</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a
tensor([[-0.3425, -1.2636, -0.4864]])
&gt;&gt;&gt; torch.var(a)
tensor(0.2455)
</code></pre><p><code>torch.``var</code>( <em>input</em> , <em>dim</em> , <em>keepdim=False</em> , <em>unbiased=True</em> ,
<em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;<code>&#x8F93;&#x5165;</code>&#x5F20;&#x91CF;&#x4E2D;&#x7684;&#x6BCF;&#x4E00;&#x884C;&#x7684;&#x65B9;&#x5DEE;&#x5728;&#x7ED9;&#x5B9A;&#x7684;&#x5C3A;&#x5BF8;<code>&#x6697;&#x6DE1;</code>&#x3002;</p>
<p>If <code>keepdim</code>is <code>True</code>, the output tensor is of the same size as <code>input</code>
except in the dimension(s) <code>dim</code>where it is of size 1. Otherwise, <code>dim</code>is
squeezed (see <code>torch.squeeze()</code>), resulting in the output tensor having 1 (or
<code>len(dim)</code>) fewer dimension(s).</p>
<p>If <code>unbiased</code>is <code>False</code>, then the variance will be calculated via the biased
estimator. Otherwise, Bessel&#x2019;s correction will be used.</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>or</em> <em>tuple of python:ints</em> ) &#x2013; the dimension or dimensions to reduce</p>
</li>
<li><p><strong>keepdim</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a>) &#x2013; whether the output tensor has <code>dim</code>retained or not</p>
</li>
<li><p><strong>unbiased</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a>) &#x2013; whether to use the unbiased estimation or not</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a
tensor([[-0.3567,  1.7385, -1.3042,  0.7423],
        [ 1.3436, -0.1015, -0.9834, -0.8438],
        [ 0.6056,  0.1089, -0.3112, -1.4085],
        [-0.7700,  0.6074, -0.1469,  0.7777]])
&gt;&gt;&gt; torch.var(a, 1)
tensor([ 1.7444,  1.1363,  0.7356,  0.5112])
</code></pre><p><code>torch.``var_mean</code>()</p>
<p><code>torch.``var_mean</code>( <em>input</em> , <em>unbiased=True) - &gt; (Tensor</em>, <em>Tensor</em> )</p>
<p>&#x8FD4;&#x56DE;&#x6240;&#x6709;&#x5143;&#x7D20;&#x7684;&#x5728;<code>&#x8F93;&#x5165;</code>&#x5F20;&#x91CF;&#x7684;&#x65B9;&#x5DEE;&#x548C;&#x5E73;&#x5747;&#x503C;&#x3002;</p>
<p>If <code>unbiased</code>is <code>False</code>, then the variance will be calculated via the biased
estimator. Otherwise, Bessel&#x2019;s correction will be used.</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>unbiased</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a>) &#x2013; whether to use the unbiased estimation or not</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a
tensor([[0.0146, 0.4258, 0.2211]])
&gt;&gt;&gt; torch.var_mean(a)
(tensor(0.0423), tensor(0.2205))
</code></pre><p><code>torch.``var_mean</code>( <em>input</em> , <em>dim</em> , <em>keepdim=False</em> , <em>unbiased=True) - &gt;
(Tensor</em>, <em>Tensor</em> )</p>
<p>&#x8FD4;&#x56DE;<code>&#x8F93;&#x5165;</code>&#x5F20;&#x91CF;&#x4E2D;&#x7684;&#x6BCF;&#x4E00;&#x884C;&#x7684;&#x65B9;&#x5DEE;&#x548C;&#x5E73;&#x5747;&#x503C;&#x5728;&#x7ED9;&#x5B9A;&#x7684;&#x5C3A;&#x5BF8;<code>&#x6697;&#x6DE1;</code>&#x3002;</p>
<p>If <code>keepdim</code>is <code>True</code>, the output tensor is of the same size as <code>input</code>
except in the dimension(s) <code>dim</code>where it is of size 1. Otherwise, <code>dim</code>is
squeezed (see <code>torch.squeeze()</code>), resulting in the output tensor having 1 (or
<code>len(dim)</code>) fewer dimension(s).</p>
<p>If <code>unbiased</code>is <code>False</code>, then the variance will be calculated via the biased
estimator. Otherwise, Bessel&#x2019;s correction will be used.</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>or</em> <em>tuple of python:ints</em> ) &#x2013; the dimension or dimensions to reduce</p>
</li>
<li><p><strong>keepdim</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a>) &#x2013; whether the output tensor has <code>dim</code>retained or not</p>
</li>
<li><p><strong>unbiased</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a>) &#x2013; whether to use the unbiased estimation or not</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a
tensor([[-1.5650,  2.0415, -0.1024, -0.5790],
        [ 0.2325, -2.6145, -1.6428, -0.3537],
        [-0.2159, -1.1069,  1.2882, -1.3265],
        [-0.6706, -1.5893,  0.6827,  1.6727]])
&gt;&gt;&gt; torch.var_mean(a, 1)
(tensor([2.3174, 1.6403, 1.4092, 2.0791]), tensor([-0.0512, -1.0946, -0.3403,  0.0239]))
</code></pre><h3 id="&#x6BD4;&#x8F83;&#x884C;&#x52A8;">&#x6BD4;&#x8F83;&#x884C;&#x52A8;</h3>
<p><code>torch.``allclose</code>( <em>input</em> , <em>other</em> , <em>rtol=1e-05</em> , <em>atol=1e-08</em> ,
<em>equal_nan=False</em> ) &#x2192; bool</p>
<p>&#x6B64;&#x529F;&#x80FD;&#x68C0;&#x67E5;&#x662F;&#x5426;&#x6240;&#x6709;<code>&#x8F93;&#x5165;</code>&#x548C;<code>&#x5176;&#x4ED6;</code>&#x6EE1;&#x8DB3;&#x6761;&#x4EF6;&#xFF1A;</p>
<p>&#x2223;input&#x2212;other&#x2223;&#x2264;atol+rtol&#xD7;&#x2223;other&#x2223;\lvert \text{input} - \text{other} \rvert \leq
\texttt{atol} + \texttt{rtol} \times \lvert \text{other} \rvert
&#x2223;input&#x2212;other&#x2223;&#x2264;atol+rtol&#xD7;&#x2223;other&#x2223;</p>
<p>&#x7684;elementwise&#xFF0C;&#x5BF9;&#x4E8E; &#x548C;&#x8F93;&#x5165;&#x7684;<code>&#x6240;&#x6709;&#x5143;&#x7D20;</code>&#x5176;&#x5B83; <code>&#x3002;&#x6B64;&#x51FD;&#x6570;&#x7684;&#x884C;&#x4E3A;&#x7C7B;&#x4F3C;&#x4E8E;[ numpy.allclose
](https://docs.scipy.org/doc/numpy/reference/generated/numpy.allclose.html)</code></p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x7B2C;&#x4E00;&#x4F38;&#x5F20;&#x5668;&#xFF0C;&#x4EE5;&#x6BD4;&#x8F83;</p>
</li>
<li><p><strong>&#x5176;&#x4ED6;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x7B2C;&#x4E8C;&#x4F38;&#x5F20;&#x5668;&#xFF0C;&#x4EE5;&#x6BD4;&#x8F83;</p>
</li>
<li><p><strong>&#x8482;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x7EDD;&#x5BF9;&#x516C;&#x5DEE;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;1E-08</p>
</li>
<li><p><strong>RTOL</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x76F8;&#x5BF9;&#x516C;&#x5DEE;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;1E-05</p>
</li>
<li><p><strong>equal_nan</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5982;&#x679C;<code>&#x771F;</code>&#xFF0C;&#x7136;&#x540E;2 <code>&#x7684;NaN</code>S&#x5C06;&#x88AB;&#x76F8;&#x7B49;&#x6BD4;&#x8F83;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x5047;</code></p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.allclose(torch.tensor([10000., 1e-07]), torch.tensor([10000.1, 1e-08]))
False
&gt;&gt;&gt; torch.allclose(torch.tensor([10000., 1e-08]), torch.tensor([10000.1, 1e-09]))
True
&gt;&gt;&gt; torch.allclose(torch.tensor([1.0, float(&apos;nan&apos;)]), torch.tensor([1.0, float(&apos;nan&apos;)]))
False
&gt;&gt;&gt; torch.allclose(torch.tensor([1.0, float(&apos;nan&apos;)]), torch.tensor([1.0, float(&apos;nan&apos;)]), equal_nan=True)
True
</code></pre><p><code>torch.``argsort</code>( <em>input</em> , <em>dim=-1</em> , <em>descending=False</em> , <em>out=None</em> ) &#x2192;
LongTensor</p>
<p>&#x8FD4;&#x56DE;&#x7531;&#x503C;&#x5347;&#x5E8F;&#x6392;&#x5217;&#x6CBF;&#x7ED9;&#x5B9A;&#x5C3A;&#x5BF8;&#x7684;&#x5F20;&#x91CF;&#x7684;&#x6307;&#x6570;&#x3002;</p>
<p>&#x8FD9;&#x662F;&#x901A;&#x8FC7; <code>torch.sort&#xFF08;&#xFF09;</code>&#x8FD4;&#x56DE;&#x7684;&#x7B2C;&#x4E8C;&#x503C;&#x3002;&#x770B;&#x5230;&#x5B83;&#x7684;&#x6587;&#x6863;&#xFF0C;&#x8FD9;&#x79CD;&#x65B9;&#x6CD5;&#x7684;&#x51C6;&#x786E;&#x8BED;&#x4E49;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x7684;&#x5C3A;&#x5BF8;&#x8FDB;&#x884C;&#x6392;&#x5E8F;&#x6CBF;</p>
</li>
<li><p><strong>&#x964D;&#x5E8F;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x63A7;&#x5236;&#x6240;&#x8FF0;&#x6392;&#x5E8F;&#x987A;&#x5E8F;&#xFF08;&#x5347;&#x5E8F;&#x6216;&#x964D;&#x5E8F;&#xFF09;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a
tensor([[ 0.0785,  1.5267, -0.8521,  0.4065],
        [ 0.1598,  0.0788, -0.0745, -1.2700],
        [ 1.2208,  1.0722, -0.7064,  1.2564],
        [ 0.0669, -0.2318, -0.8229, -0.9280]])


&gt;&gt;&gt; torch.argsort(a, dim=1)
tensor([[2, 0, 3, 1],
        [3, 2, 1, 0],
        [2, 1, 0, 3],
        [3, 2, 1, 0]])
</code></pre><p><code>torch.``eq</code>( <em>input</em> , <em>other</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8BA1;&#x7B97;&#x5143;&#x7D20;&#x65B9;&#x9762;&#x7684;&#x5E73;&#x7B49;</p>
<p>&#x7B2C;&#x4E8C;&#x4E2A;&#x53C2;&#x6570;&#x53EF;&#x4EE5;&#x662F;&#x4E00;&#x4E2A;&#x6570;&#x5B57;&#x6216;&#x4E00;&#x4E2A;&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x5F62;&#x72B6;&#x4E3A;<a href="notes/broadcasting.html#broadcasting-%0Asemantics"> broadcastable  </a>&#x7684;&#x7B2C;&#x4E00;&#x4E2A;&#x53C2;&#x6570;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x5F20;&#x91CF;&#x6765;&#x6BD4;&#x8F83;</p>
</li>
<li><p><strong>&#x5176;&#x4ED6;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a> <em>&#x6216;</em> <a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a>&#xFF09; - &#x5F20;&#x91CF;&#x6216;&#x503C;&#x8FDB;&#x884C;&#x6BD4;&#x8F83;</p>
</li>
<li><p><strong>OUT</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x3002;&#x5FC5;&#x987B;&#x662F; BoolTensor </p>
</li>
</ul>
<p>Returns</p>
<p>A <code>torch.BoolTensor  [HTG3&#x542B;&#x6709;&#x4E00;&#x4E2A;True&#x5728;&#x6BCF;&#x4E2A;&#x4F4D;&#x7F6E;&#x5904;&#xFF0C;&#x5176;&#x4E2D;&#x6BD4;&#x8F83;&#x7ED3;&#x679C;&#x4E3A;&#x771F;</code></p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.eq(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))
tensor([[True, False], [False, True]])
</code></pre><p><code>torch.``equal</code>( <em>input</em> , <em>other</em> ) &#x2192; bool</p>
<p><code>&#x771F;</code>&#x5982;&#x679C;&#x4E24;&#x4E2A;&#x5F20;&#x91CF;&#x5177;&#x6709;&#x76F8;&#x540C;&#x7684;&#x5C3A;&#x5BF8;&#x548C;&#x5143;&#x7D20;&#xFF0C;<code>&#x5047;</code>&#x5426;&#x5219;&#x3002;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.equal(torch.tensor([1, 2]), torch.tensor([1, 2]))
True
</code></pre><p><code>torch.``ge</code>( <em>input</em> , <em>other</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8BA1;&#x7B97; &#x8F93;&#x5165; &#x2265; &#x5176;&#x4ED6; \&#x6587;&#x672C;{&#x8F93;&#x5165;} \ GEQ \&#x6587;&#x672C;{&#x5176;&#x5B83;}  &#x8F93;&#x5165; &#x2265; &#x5176;&#x4ED6; &#x9010;&#x5143;&#x7D20;&#x3002;</p>
<p>The second argument can be a number or a tensor whose shape is
<a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a> with the first
argument.</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the tensor to compare</p>
</li>
<li><p><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>or</em><a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>float</em></a>) &#x2013; the tensor or value to compare</p>
</li>
<li><p><strong>OUT</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x5FC5;&#x987B;&#x662F;&#x4E00;&#x4E2A; BoolTensor </p>
</li>
</ul>
<p>Returns</p>
<p>A <code>torch.BoolTensor</code>containing a True at each location where comparison is
true</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.ge(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))
tensor([[True, True], [False, True]])
</code></pre><p><code>torch.``gt</code>( <em>input</em> , <em>other</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8BA1;&#x7B97; &#x8F93;&#x5165; &amp; GT ;  &#x5176;&#x4ED6; \ {&#x6587;&#x672C;&#x8F93;&#x5165;} &amp; GT ; \ {&#x6587;&#x672C;&#x5176;&#x4ED6;}  &#x8F93;&#x5165; &amp; GT ;  &#x5176;&#x4ED6; &#x9010;&#x5143;&#x7D20;&#x3002;</p>
<p>The second argument can be a number or a tensor whose shape is
<a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a> with the first
argument.</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the tensor to compare</p>
</li>
<li><p><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>or</em><a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>float</em></a>) &#x2013; the tensor or value to compare</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor that must be a BoolTensor</p>
</li>
</ul>
<p>Returns</p>
<p>A <code>torch.BoolTensor</code>containing a True at each location where comparison is
true</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.gt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))
tensor([[False, True], [False, False]])
</code></pre><p><code>torch.``isfinite</code>( <em>tensor</em>
)<a href="_modules/torch/functional.html#isfinite">[source]</a></p>
<p>&#x8FD4;&#x56DE;&#x4E0E;&#x8868;&#x793A;&#x5982;&#x679C;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x662F;&#x6709;&#x9650;&#x6216;&#x4E0D;&#x5E03;&#x5C14;&#x5143;&#x7D20;&#x7684;&#x65B0;&#x5F20;&#x91CF;&#x3002;</p>
<p>Parameters</p>
<p><strong>&#x5F20;&#x91CF;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x7532;&#x5F20;&#x91CF;&#xFF0C;&#x4EE5;&#x68C0;&#x67E5;</p>
<p>Returns</p>
<p><code>A  torch.Tensor  &#x4E0E; DTYPE  torch.bool  [HTG11&#x542B;&#x6709;&#x4E00;&#x4E2A;True&#x5728;&#x6BCF;&#x4E2A;&#x6709;&#x9650;&#x5143;&#x7D20;&#x548C;&#x5047;&#x7684;&#x4F4D;&#x7F6E;&#xFF0C;&#x5426;&#x5219;</code></p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.isfinite(torch.tensor([1, float(&apos;inf&apos;), 2, float(&apos;-inf&apos;), float(&apos;nan&apos;)]))
tensor([True,  False,  True,  False,  False])
</code></pre><p><code>torch.``isinf</code>( <em>tensor</em> )<a href="_modules/torch/functional.html#isinf">[source]</a></p>
<p>&#x8FD4;&#x56DE;&#x4E0E;&#x8868;&#x793A;&#x5982;&#x679C;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x662F; +/- INF &#x6216;&#x4E0D;&#x5E03;&#x5C14;&#x5143;&#x7D20;&#x7684;&#x65B0;&#x5F20;&#x91CF;&#x3002;</p>
<p>Parameters</p>
<p><strong>tensor</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; A tensor
to check</p>
<p>Returns</p>
<p><code>A  torch.Tensor  &#x4E0E; DTYPE  torch.bool  [HTG11&#x542B;&#x6709;&#x4E00;&#x4E2A;True&#x5728;&#x6BCF;&#x4E2A; +/- INF &#x5143;&#x7D20;&#x548C;&#x5047;&#x7684;&#x4F4D;&#x7F6E;&#xFF0C;&#x5426;&#x5219;</code></p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.isinf(torch.tensor([1, float(&apos;inf&apos;), 2, float(&apos;-inf&apos;), float(&apos;nan&apos;)]))
tensor([False,  True,  False,  True,  False])
</code></pre><p><code>torch.``isnan</code>()</p>
<p>&#x8FD4;&#x56DE;&#x4E0E;&#x8868;&#x793A;&#x5982;&#x679C;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x662F;&#x7684;NaN &#x6216;&#x4E0D;&#x5E03;&#x5C14;&#x5143;&#x7D20;&#x7684;&#x65B0;&#x5F20;&#x91CF;&#x3002;</p>
<p>Parameters</p>
<p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x7532;&#x5F20;&#x91CF;&#xFF0C;&#x4EE5;&#x68C0;&#x67E5;</p>
<p>Returns</p>
<p>A <code>torch.BoolTensor</code>&#x542B;&#x6709;&#x5728;&#x771F;NaN&#x7684;&#x5143;&#x7D20;&#x6BCF;&#x4E2A;&#x4F4D;&#x7F6E;&#x3002;</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.isnan(torch.tensor([1, float(&apos;nan&apos;), 2]))
tensor([False, True, False])
</code></pre><p><code>torch.``kthvalue</code>( <em>input</em> , <em>k</em> , <em>dim=None</em> , <em>keepdim=False</em> , _out=None)</p>
<ul>
<li><blockquote>
<p>(Tensor<em>, _LongTensor</em> )</p>
</blockquote>
</li>
</ul>
<p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;namedtuple <code>&#xFF08;&#x503C; &#x7D22;&#x5F15;&#xFF09;</code>&#x5176;&#x4E2D;<code>&#x503C;</code>&#x5728;<code>K</code>&#x5728;&#x7ED9;&#x5B9A;&#x7684;&#x7EF4;&#x5EA6;&#x4E0A;&#x7684;<code>&#x8F93;&#x5165;</code>&#x5F20;&#x91CF;&#x4E2D;&#x7684;&#x6BCF;&#x4E00;&#x884C;&#x7684;&#x7B2C;&#x6700;&#x5C0F;&#x5143;&#x7D20;<code>&#x6697;&#x6DE1;</code>&#x3002;&#x548C;<code>&#x6307;&#x6570;</code>&#x662F;&#x627E;&#x5230;&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x7D22;&#x5F15;&#x4F4D;&#x7F6E;&#x3002;</p>
<p>&#x5982;&#x679C;<code>&#x6697;&#x6DE1;</code>&#x6CA1;&#x6709;&#x7ED9;&#x51FA;&#xFF0C;&#x5219;&#x8F93;&#x5165;&#x7684;&#x6700;&#x540E;&#x5C3A;&#x5BF8;&#x88AB;&#x9009;&#x62E9;&#x3002;</p>
<p>&#x5982;&#x679C;<code>keepdim</code>&#x662F;<code>&#x771F;</code>&#xFF0C;&#x65E0;&#x8BBA;&#x662F;<code>&#x503C;</code>&#x548C;<code>&#x6307;&#x6570;</code>&#x5F20;&#x91CF;&#x662F;&#x76F8;&#x540C;&#x7684;&#x5927;&#x5C0F;&#x4E3A;<code>&#x8F93;&#x5165;</code>&#xFF0C;&#x9664;&#x4E86;&#x5728;&#x5C3A;&#x5BF8;<code>&#x6697;&#x6DE1;</code>&#x5176;&#x4E2D;&#x5B83;&#x4EEC;&#x7684;&#x5C3A;&#x5BF8;1&#xFF0C;&#x5426;&#x5219;<code>&#x6697;&#x6DE1;</code>&#x88AB;&#x6324;&#x51FA;&#xFF08;&#x89C1; <code>torch.squeeze&#xFF08;&#xFF09;</code>&#xFF09;&#xFF0C;&#x4ECE;&#x800C;&#x5BFC;&#x81F4;&#x5728;&#x4E24;&#x4E2A;<code>&#x503C;</code>&#x548C;<code>&#x6307;&#x6570;</code>&#x5177;&#x6709;1&#x540D;&#x6BD4;<code>&#x8F93;&#x5165;</code>&#x5F20;&#x91CF;&#x8F83;&#x5C11;&#x7EF4;&#x5F20;&#x91CF;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>K</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - K&#x4E3A;&#x7B2C;k&#x4E2A;&#x6700;&#x5C0F;&#x7684;&#x5143;&#x7D20;</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x7684;&#x5C3A;&#x5BF8;&#x6CBF;&#x7740;&#x627E;&#x5230;&#x7684;&#x7B2C;k&#x4E2A;&#x503C;</p>
</li>
<li><p><strong>keepdim</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a>) &#x2013; whether the output tensors have <code>dim</code>retained or not</p>
</li>
<li><p><strong>OUT</strong> &#xFF08;<a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>&#x5143;&#x7EC4;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#xFF08;&#x5F20;&#x91CF;&#xFF0C;LongTensor&#xFF09;&#x7684;&#x8F93;&#x51FA;&#x5143;&#x7EC4;&#x53EF;&#x4EFB;&#x9009;&#x5730;&#x7ED9;&#x5B9A;&#x7684;&#x4EE5;&#x7528;&#x4F5C;&#x8F93;&#x51FA;&#x7F13;&#x51B2;&#x5668;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.arange(1., 6.)
&gt;&gt;&gt; x
tensor([ 1.,  2.,  3.,  4.,  5.])
&gt;&gt;&gt; torch.kthvalue(x, 4)
torch.return_types.kthvalue(values=tensor(4.), indices=tensor(3))

&gt;&gt;&gt; x=torch.arange(1.,7.).resize_(2,3)
&gt;&gt;&gt; x
tensor([[ 1.,  2.,  3.],
        [ 4.,  5.,  6.]])
&gt;&gt;&gt; torch.kthvalue(x, 2, 0, True)
torch.return_types.kthvalue(values=tensor([[4., 5., 6.]]), indices=tensor([[1, 1, 1]]))
</code></pre><p><code>torch.``le</code>( <em>input</em> , <em>other</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8BA1;&#x7B97; &#x8F93;&#x5165; &#x2264; &#x5176;&#x4ED6; \&#x6587;&#x672C;{&#x8F93;&#x5165;} \&#x5F53;&#x91CF;\&#x6587;&#x672C;{&#x5176;&#x5B83;}  &#x8F93;&#x5165; &#x2264; &#x5176;&#x4ED6; &#x9010;&#x5143;&#x7D20;&#x3002;</p>
<p>The second argument can be a number or a tensor whose shape is
<a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a> with the first
argument.</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the tensor to compare</p>
</li>
<li><p><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>or</em><a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>float</em></a>) &#x2013; the tensor or value to compare</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor that must be a BoolTensor</p>
</li>
</ul>
<p>Returns</p>
<p>A <code>torch.BoolTensor</code>containing a True at each location where comparison is
true</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.le(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))
tensor([[True, False], [True, True]])
</code></pre><p><code>torch.``lt</code>( <em>input</em> , <em>other</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8BA1;&#x7B97; &#x8F93;&#x5165; &amp; LT ;  &#x5176;&#x4ED6; \ {&#x6587;&#x672C;&#x8F93;&#x5165;} &amp; LT ; \ {&#x6587;&#x672C;&#x5176;&#x4ED6;}  &#x8F93;&#x5165; &amp; LT ;  &#x5176;&#x4ED6; &#x9010;&#x5143;&#x7D20;&#x3002;</p>
<p>The second argument can be a number or a tensor whose shape is
<a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a> with the first
argument.</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the tensor to compare</p>
</li>
<li><p><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>or</em><a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>float</em></a>) &#x2013; the tensor or value to compare</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor that must be a BoolTensor</p>
</li>
</ul>
<p>Returns</p>
<p>A  torch.BoolTensor [HTG1&#x542B;&#x6709;&#x4E00;&#x4E2A;True&#x5728;&#x6BCF;&#x4E2A;&#x4F4D;&#x7F6E;&#x5904;&#xFF0C;&#x5176;&#x4E2D;&#x6BD4;&#x8F83;&#x7ED3;&#x679C;&#x4E3A;&#x771F;</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.lt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))
tensor([[False, False], [True, False]])
</code></pre><p><code>torch.``max</code>()</p>
<p><code>torch.``max</code>( <em>input</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x6240;&#x6709;&#x5143;&#x7D20;&#x7684;&#x5728;<code>&#x8F93;&#x5165;</code>&#x5F20;&#x91CF;&#x7684;&#x6700;&#x5927;&#x503C;&#x3002;</p>
<p>Parameters</p>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input
tensor</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a
tensor([[ 0.6763,  0.7445, -2.2369]])
&gt;&gt;&gt; torch.max(a)
tensor(0.7445)
</code></pre><p><code>torch.``max</code>( <em>input</em> , <em>dim</em> , <em>keepdim=False</em> , <em>out=None) - &gt; (Tensor</em>,
<em>LongTensor</em> )</p>
<p>&#x8FD4;&#x56DE;namedtuple <code>&#xFF08;&#x503C; &#x7D22;&#x5F15;&#xFF09;</code>&#x5176;&#x4E2D;<code>&#x503C;</code>&#x662F;&#x6BCF;&#x884C;&#x7684;&#x6700;&#x5927;&#x503C;<code>&#x8F93;&#x5165;</code>&#x5F20;&#x91CF;&#x5728;&#x7ED9;&#x5B9A;&#x7684;&#x5C3A;&#x5BF8;<code>&#x6697;&#x6DE1;</code>&#x3002;&#x548C;<code>&#x6307;&#x6570;</code>&#x662F;&#x627E;&#x5230;&#x7684;&#x6BCF;&#x4E2A;&#x6700;&#x5927;&#x503C;&#x7684;&#x7D22;&#x5F15;&#x4F4D;&#x7F6E;&#xFF08;argmax&#xFF09;&#x3002;</p>
<p>&#x5982;&#x679C;<code>keepdim</code>&#x662F;<code>&#x771F;</code>&#xFF0C;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x662F;&#x76F8;&#x540C;&#x5927;&#x5C0F;&#x7684;&#x4F5C;&#x4E3A;<code>&#x8F93;&#x5165;</code>&#x9664;&#x4E86;&#x5728;&#x5C3A;&#x5BF8;<code>&#x6697;&#x6DE1;</code>&#x5176;&#x4E2D;&#x5B83;&#x4EEC;&#x662F;&#x5C3A;&#x5BF8;1&#x7684;&#x5426;&#x5219;&#xFF0C;<code>&#x6697;&#x6DE1;</code>&#x88AB;&#x6324;&#x51FA;&#xFF08;&#x89C1; <code>torch.squeeze&#xFF08;&#xFF09;</code>&#xFF09;&#xFF0C;&#x5BFC;&#x81F4;&#x5177;&#x6709;&#x6BD4;1&#x79CD;<code>&#x8F93;&#x5165;</code>&#x66F4;&#x5C11;&#x5C3A;&#x5BF8;&#x7684;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a>) &#x2013; the dimension to reduce</p>
</li>
<li><p><strong>keepdim</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x662F;&#x5426;&#x6709;<code>&#x6697;&#x6DE1;</code>&#x4FDD;&#x7559;&#x6216;&#x6CA1;&#x6709;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x5047; [HTG17&#x3002;</code></p>
</li>
<li><p><strong>OUT</strong> &#xFF08;<a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>&#x5143;&#x7EC4;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x4E24;&#x4E2A;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684;&#x7ED3;&#x679C;&#x5143;&#x7EC4;&#xFF08;&#x6700;&#x5927;&#xFF0C;max_indices&#xFF09;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a
tensor([[-1.2360, -0.2942, -0.1222,  0.8475],
        [ 1.1949, -1.1127, -2.2379, -0.6702],
        [ 1.5717, -0.9207,  0.1297, -1.8768],
        [-0.6172,  1.0036, -0.6060, -0.2432]])
&gt;&gt;&gt; torch.max(a, 1)
torch.return_types.max(values=tensor([0.8475, 1.1949, 1.5717, 1.0036]), indices=tensor([3, 0, 0, 1]))
</code></pre><p><code>torch.``max</code>( <em>input</em> , <em>other</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x5F20;&#x91CF;<code>&#x8F93;&#x5165;</code>&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x4E0E;&#x5F20;&#x529B;<code>&#x5176;&#x4ED6;</code>&#x548C;&#x9010;&#x5143;&#x7D20;&#x6700;&#x5927;&#x53D6;&#x7684;&#x5BF9;&#x5E94;&#x5143;&#x7D20;&#x8FDB;&#x884C;&#x6BD4;&#x8F83;&#x3002;</p>
<p><code>&#x8F93;&#x5165;</code>&#x548C;<code>&#x7B49;</code>&#x4E0D;&#x9700;&#x8981;&#x5339;&#x914D;&#xFF0C;&#x4F46;&#x662F;&#x4ED6;&#x4EEC;&#x5FC5;&#x987B;<a href="notes/broadcasting.html#broadcasting-semantics"> broadcastable [HTG10&#x7684;&#x5F62;&#x72B6;]
</a>&#x3002;</p>
<p>outi=max&#x2061;(tensori,otheri)\text{out}_i = \max(\text{tensor}_i, \text{other}_i)
outi&#x200B;=max(tensori&#x200B;,otheri&#x200B;)</p>
<p>Note</p>
<p>&#x5F53;&#x5F62;&#x72B6;&#x4E0D;&#x5339;&#x914D;&#xFF0C;&#x5219;&#x8FD4;&#x56DE;&#x7684;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684;&#x5F62;&#x72B6;&#x9075;&#x5FAA;<a href="notes/broadcasting.html#broadcasting-semantics"> &#x5E7F;&#x64AD;&#x89C4;&#x5219; </a>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the second input tensor</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([ 0.2942, -0.7416,  0.2653, -0.1584])
&gt;&gt;&gt; b = torch.randn(4)
&gt;&gt;&gt; b
tensor([ 0.8722, -1.7421, -0.4141, -0.5055])
&gt;&gt;&gt; torch.max(a, b)
tensor([ 0.8722, -0.7416,  0.2653, -0.1584])
</code></pre><p><code>torch.``min</code>()</p>
<p><code>torch.``min</code>( <em>input</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x6240;&#x6709;&#x5143;&#x7D20;&#x7684;&#x5728;<code>&#x8F93;&#x5165;</code>&#x5F20;&#x91CF;&#x7684;&#x6700;&#x5C0F;&#x503C;&#x3002;</p>
<p>Parameters</p>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input
tensor</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a
tensor([[ 0.6750,  1.0857,  1.7197]])
&gt;&gt;&gt; torch.min(a)
tensor(0.6750)
</code></pre><p><code>torch.``min</code>( <em>input</em> , <em>dim</em> , <em>keepdim=False</em> , <em>out=None) - &gt; (Tensor</em>,
<em>LongTensor</em> )</p>
<p>&#x8FD4;&#x56DE;namedtuple <code>&#xFF08;&#x503C; &#x7D22;&#x5F15;&#xFF09;</code>&#x5176;&#x4E2D;<code>&#x503C;</code>&#x662F;&#x6BCF;&#x884C;&#x7684;&#x6700;&#x5C0F;&#x503C;<code>&#x8F93;&#x5165;</code>&#x5F20;&#x91CF;&#x5728;&#x7ED9;&#x5B9A;&#x7684;&#x5C3A;&#x5BF8;<code>&#x6697;&#x6DE1;</code>&#x3002;&#x548C;<code>&#x6307;&#x6570;</code>&#x662F;&#x53D1;&#x73B0;&#xFF08;argmin&#xFF09;&#x5404;&#x6700;&#x5C0F;&#x503C;&#x7684;&#x7D22;&#x5F15;&#x4F4D;&#x7F6E;&#x3002;</p>
<p>&#x5982;&#x679C;<code>keepdim</code>&#x662F;<code>&#x771F;</code>&#xFF0C;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x662F;&#x76F8;&#x540C;&#x5927;&#x5C0F;&#x7684;&#x4F5C;&#x4E3A;<code>&#x8F93;&#x5165;</code>&#x9664;&#x4E86;&#x5728;&#x5C3A;&#x5BF8;<code>&#x6697;&#x6DE1;</code>&#x5176;&#x4E2D;&#x5B83;&#x4EEC;&#x662F;&#x5C3A;&#x5BF8;1&#x7684;&#x5426;&#x5219;&#xFF0C;<code>&#x6697;&#x6DE1;</code>&#x88AB;&#x6324;&#x51FA;&#xFF08;&#x89C1; <code>torch.squeeze&#xFF08;&#xFF09;</code>&#xFF09;&#xFF0C;&#x5BFC;&#x81F4;&#x5177;&#x6709;&#x6BD4;1&#x79CD;<code>&#x8F93;&#x5165;</code>&#x66F4;&#x5C11;&#x5C3A;&#x5BF8;&#x7684;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a>) &#x2013; the dimension to reduce</p>
</li>
<li><p><strong>keepdim</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a>) &#x2013; whether the output tensors have <code>dim</code>retained or not</p>
</li>
<li><p><strong>OUT</strong> &#xFF08;<a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>&#x5143;&#x7EC4;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x4E24;&#x4E2A;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684;&#x5143;&#x7EC4;&#xFF08;&#x5206;&#x949F;&#xFF0C;min_indices&#xFF09;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a
tensor([[-0.6248,  1.1334, -1.1899, -0.2803],
        [-1.4644, -0.2635, -0.3651,  0.6134],
        [ 0.2457,  0.0384,  1.0128,  0.7015],
        [-0.1153,  2.9849,  2.1458,  0.5788]])
&gt;&gt;&gt; torch.min(a, 1)
torch.return_types.min(values=tensor([-1.1899, -1.4644,  0.0384, -0.1153]), indices=tensor([2, 0, 1, 0]))
</code></pre><p><code>torch.``min</code>( <em>input</em> , <em>other</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x5F20;&#x91CF;<code>&#x8F93;&#x5165;</code>&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x4E0E;&#x5F20;&#x529B;<code>&#x5176;&#x4ED6;</code>&#x548C;&#x88AB;&#x53D6;&#x9010;&#x5143;&#x7D20;&#x7684;&#x6700;&#x5C0F;&#x7684;&#x5BF9;&#x5E94;&#x5143;&#x7D20;&#x8FDB;&#x884C;&#x6BD4;&#x8F83;&#x3002;&#x5F97;&#x5230;&#x7684;&#x5F20;&#x91CF;&#x8FD4;&#x56DE;&#x3002;</p>
<p>The shapes of <code>input</code>and <code>other</code>don&#x2019;t need to match, but they must be
<a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a>.</p>
<p>outi=min&#x2061;(tensori,otheri)\text{out}_i = \min(\text{tensor}_i, \text{other}_i)
outi&#x200B;=min(tensori&#x200B;,otheri&#x200B;)</p>
<p>Note</p>
<p>When the shapes do not match, the shape of the returned output tensor follows
the <a href="notes/broadcasting.html#broadcasting-semantics">broadcasting rules</a>.</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the second input tensor</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([ 0.8137, -1.1740, -0.6460,  0.6308])
&gt;&gt;&gt; b = torch.randn(4)
&gt;&gt;&gt; b
tensor([-0.1369,  0.1555,  0.4019, -0.1929])
&gt;&gt;&gt; torch.min(a, b)
tensor([-0.1369, -1.1740, -0.6460, -0.1929])
</code></pre><p><code>torch.``ne</code>( <em>input</em> , <em>other</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8BA1;&#x7B97; i&#x7684; n&#x7684; P  U  T  &#x2260; O  T  H  E  R  &#x8F93;&#x5165;\ NEQ&#x5176;&#x4ED6; i&#x7684; n&#x7684; p  U  T  &#xE020; =  O  T  H  E  R
[HT G105]&#x9010;&#x5143;&#x7D20;&#x3002;</p>
<p>The second argument can be a number or a tensor whose shape is
<a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a> with the first
argument.</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the tensor to compare</p>
</li>
<li><p><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>or</em><a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>float</em></a>) &#x2013; the tensor or value to compare</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor that must be a BoolTensor</p>
</li>
</ul>
<p>Returns</p>
<p>A <code>torch.BoolTensor</code>&#x542B;&#x6709;&#x5728;&#x771F;&#x5176;&#x4E2D;&#x6BD4;&#x8F83;&#x4E3A;&#x771F;&#x6BCF;&#x4E2A;&#x4F4D;&#x7F6E;&#x3002;</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.ne(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))
tensor([[False, True], [True, False]])
</code></pre><p><code>torch.``sort</code>( <em>input</em> , <em>dim=-1</em> , <em>descending=False</em> , <em>out=None) - &gt;
(Tensor</em>, <em>LongTensor</em> )</p>
<p>&#x6392;&#x5E8F;<code>&#x8F93;&#x5165;</code>&#x5F20;&#x91CF;&#x7684;&#x6CBF;&#x7ED9;&#x5B9A;&#x7684;&#x7EF4;&#x5EA6;&#x4EE5;&#x5347;&#x5E8F;&#x901A;&#x8FC7;&#x503C;&#x7684;&#x5143;&#x7D20;&#x3002;</p>
<p>If <code>dim</code>is not given, the last dimension of the input is chosen.</p>
<p>&#x5982;&#x679C;<code>&#x964D;&#x5E8F;</code>&#x662F;<code>&#x771F;</code>&#x7136;&#x540E;&#x5C06;&#x5143;&#x4EF6;&#x5728;&#x7531;&#x503C;&#x964D;&#x5E8F;&#x6392;&#x5217;&#x3002;</p>
<p>&#xFF08;&#x503C;&#xFF0C;&#x7D22;&#x5F15;&#xFF09;&#x7684;namedtuple&#x88AB;&#x8FD4;&#x56DE;&#xFF0C;&#x5176;&#x4E2D;&#xFF0C;&#x6240;&#x8FF0;&#x503C;&#x662F;&#x6392;&#x5E8F;&#x7684;&#x503C;&#x548C;&#x6307;&#x6570;&#x662F;&#x5728;&#x539F;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x7684;&#x5143;&#x7D20;&#x7684;&#x7D22;&#x5F15;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>,</em> <em>optional</em> ) &#x2013; the dimension to sort along</p>
</li>
<li><p><strong>descending</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; controls the sorting order (ascending or descending)</p>
</li>
<li><p><strong>OUT</strong> &#xFF08;<a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>&#x5143;&#x7EC4;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x7684;&#xFF08;&#x5F20;&#x91CF;&#x7684;&#x8F93;&#x51FA;&#x5143;&#x7EC4;&#xFF0C;  LongTensor &#xFF09;&#xFF0C;&#x5176;&#x53EF;&#x4EFB;&#x9009;&#x5730;&#x7ED9;&#x5B9A;&#x7684;&#x7528;&#x4F5C;&#x8F93;&#x51FA;&#x7F13;&#x51B2;&#x5668;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(3, 4)
&gt;&gt;&gt; sorted, indices = torch.sort(x)
&gt;&gt;&gt; sorted
tensor([[-0.2162,  0.0608,  0.6719,  2.3332],
        [-0.5793,  0.0061,  0.6058,  0.9497],
        [-0.5071,  0.3343,  0.9553,  1.0960]])
&gt;&gt;&gt; indices
tensor([[ 1,  0,  2,  3],
        [ 3,  1,  0,  2],
        [ 0,  3,  1,  2]])

&gt;&gt;&gt; sorted, indices = torch.sort(x, 0)
&gt;&gt;&gt; sorted
tensor([[-0.5071, -0.2162,  0.6719, -0.5793],
        [ 0.0608,  0.0061,  0.9497,  0.3343],
        [ 0.6058,  0.9553,  1.0960,  2.3332]])
&gt;&gt;&gt; indices
tensor([[ 2,  0,  0,  1],
        [ 0,  1,  1,  2],
        [ 1,  2,  2,  0]])
</code></pre><p><code>torch.``topk</code>( <em>input</em> , <em>k</em> , <em>dim=None</em> , <em>largest=True</em> , <em>sorted=True</em> ,
<em>out=None) - &gt; (Tensor</em>, <em>LongTensor</em> )</p>
<p>&#x8FD4;&#x56DE;&#x6CBF;&#x7ED9;&#x5B9A;&#x7684;&#x7EF4;&#x5EA6;&#x4E0A;&#x7684;&#x7ED9;&#x5B9A;&#x7684;<code>&#x8F93;&#x5165;</code>&#x5F20;&#x91CF;&#x7684;<code>K</code>&#x6700;&#x5927;&#x5143;&#x7D20;&#x3002;</p>
<p>If <code>dim</code>is not given, the last dimension of the input is chosen.</p>
<p>&#x5982;&#x679C;<code>&#x5927;</code>&#x662F;<code>&#x5047;</code>&#x7136;&#x540E;&#x6309; K &#x8FD4;&#x56DE;&#x6700;&#x5C0F;&#x7684;&#x5143;&#x7D20;&#x3002;</p>
<p>&#x7684;&#x7532;namedtuple&#xFF08;&#x503C;&#xFF0C;&#x7D22;&#x5F15;&#xFF09;&#x88AB;&#x8FD4;&#x56DE;&#xFF0C;&#x5176;&#x4E2D;&#xFF0C;&#x6240;&#x8FF0;&#x6307;&#x6570;&#x662F;&#x5728;&#x539F;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x7684;&#x5143;&#x7D20;&#x7684;&#x7D22;&#x5F15;&#x3002;</p>
<p><code>&#x6392;&#x5E8F;&#x7684;&#x5E03;&#x5C14;&#x9009;&#x9879;</code>&#x5982;&#x679C;<code>&#x771F;</code>&#xFF0C;&#x5C06;&#x786E;&#x4FDD;&#x8FD4;&#x56DE; K &#x5143;&#x7D20;&#x672C;&#x8EAB;&#x4E5F;&#x662F;&#x5206;&#x7C7B;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>K</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x5728;&#x201C;&#x524D;k&#x201D;&#x7B2C;k</p>
</li>
<li><p><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>,</em> <em>optional</em> ) &#x2013; the dimension to sort along</p>
</li>
<li><p><strong>&#x6700;&#x5927;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x63A7;&#x5236;&#x662F;&#x5426;&#x8FD4;&#x56DE;&#x6700;&#x5927;&#x6216;&#x6700;&#x5C0F;&#x7684;&#x5143;&#x7D20;</p>
</li>
<li><p><strong>&#x6392;&#x5E8F;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x63A7;&#x5236;&#x662F;&#x5426;&#x8FD4;&#x56DE;&#x6309;&#x6392;&#x5E8F;&#x987A;&#x5E8F;&#x4E2D;&#x7684;&#x5143;&#x7D20;</p>
</li>
<li><p><strong>OUT</strong> &#xFF08;<a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>&#x5143;&#x7EC4;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#xFF08;&#x5F20;&#x91CF;&#xFF0C;LongTensor&#xFF09;&#x7684;&#x8F93;&#x51FA;&#x5143;&#x7EC4;&#xFF0C;&#x53EF;&#x4EE5;&#x662F;&#x4EFB;&#x9009;&#x7ED9;&#x5B9A;&#x5C06;&#x88AB;&#x7528;&#x4F5C;&#x8F93;&#x51FA;&#x7F13;&#x51B2;&#x5668;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.arange(1., 6.)
&gt;&gt;&gt; x
tensor([ 1.,  2.,  3.,  4.,  5.])
&gt;&gt;&gt; torch.topk(x, 3)
torch.return_types.topk(values=tensor([5., 4., 3.]), indices=tensor([4, 3, 2]))
</code></pre><h3 id="&#x5149;&#x8C31;&#x884C;&#x52A8;">&#x5149;&#x8C31;&#x884C;&#x52A8;</h3>
<p><code>torch.``fft</code>( <em>input</em> , <em>signal_ndim</em> , <em>normalized=False</em> ) &#x2192; Tensor</p>
<p>&#x590D;&#x6742;&#x5230;&#x590D;&#x6742;&#x7684;&#x79BB;&#x6563;&#x5085;&#x7ACB;&#x53F6;&#x53D8;&#x6362;</p>
<p>&#x6B64;&#x65B9;&#x6CD5;&#x8BA1;&#x7B97;&#x590D;&#x6742;&#x5230;&#x590D;&#x6742;&#x7684;&#x79BB;&#x6563;&#x5085;&#x7ACB;&#x53F6;&#x53D8;&#x6362;&#x3002;&#x5FFD;&#x7565;&#x6279;&#x6B21;&#x5C3A;&#x5BF8;&#xFF0C;&#x5B83;&#x8BA1;&#x7B97;&#x4E0B;&#x9762;&#x7684;&#x8868;&#x8FBE;&#x5F0F;&#xFF1A;</p>
<p>X[&#x3C9;1,&#x2026;,&#x3C9;d]=&#x2211;n1=0N1&#x2212;1&#x22EF;&#x2211;nd=0Nd&#x2212;1x[n1,&#x2026;,nd]e&#x2212;j 2&#x3C0;&#x2211;i=0d&#x3C9;iniNi,X[\omega<em>1, \dots,
\omega_d] = \sum</em>{n<em>1=0}^{N_1-1} \dots \sum</em>{n<em>d=0}^{N_d-1} x[n_1, \dots, n_d]
e^{-j\ 2 \pi \sum</em>{i=0}^d \frac{\omega_i n_i}{N_i}},
X[&#x3C9;1&#x200B;,&#x2026;,&#x3C9;d&#x200B;]=n1&#x200B;=0&#x2211;N1&#x200B;&#x2212;1&#x200B;&#x22EF;nd&#x200B;=0&#x2211;Nd&#x200B;&#x2212;1&#x200B;x[n1&#x200B;,&#x2026;,nd&#x200B;]e&#x2212;j 2&#x3C0;&#x2211;i=0d&#x200B;Ni&#x200B;&#x3C9;i&#x200B;ni&#x200B;&#x200B;,</p>
<p>&#x5176;&#x4E2D; d  d  d  = <code>signal_ndim</code>&#x662F;&#x5C3A;&#x5BF8;&#x4E3A;&#x4FE1;&#x53F7;&#x6570;&#x76EE;&#xFF0C;&#x548C; N  i&#x7684; n_i&#x4E2A; N  i&#x7684; &#x662F;&#x4FE1;&#x53F7;&#x7EF4;&#x5EA6; i&#x7684; [&#x5927;&#x5C0F;HTG90 ]
i&#x7684; [HT G99]  i&#x7684; &#x3002;</p>
<p>&#x6B64;&#x65B9;&#x6CD5;&#x652F;&#x6301;&#x4E00;&#x7EF4;&#xFF0C;&#x4E8C;&#x7EF4;&#x548C;&#x4E09;&#x7EF4;&#x590D;&#x6742;&#x5230;&#x590D;&#x6742;&#x7684;&#x53D8;&#x6362;&#xFF0C;&#x7531;<code>signal_ndim</code>&#x8868;&#x793A;&#x3002; <code>&#x8F93;&#x5165;</code>&#x5FC5;&#x987B;&#x4E0E;&#x5C3A;&#x5BF8;2&#xFF0C;&#x4EE3;&#x8868;&#x590D;&#x6570;&#x7684;&#x5B9E;&#x548C;&#x865A;&#x5206;&#x91CF;&#x7684;&#x6700;&#x540E;&#x4E00;&#x7EF4;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x5E76;&#x5E94;&#x5177;&#x6709;&#x81F3;&#x5C11;[H&#x200B;&#x200B;TG8]  signal<em>ndim  +  1
&#x5C3A;&#x5BF8;&#x4E0E;&#x9886;&#x5148;&#x7684;&#x6279;&#x91CF;&#x5C3A;&#x5BF8;&#x7684;&#x4EFB;&#x9009;&#x4EFB;&#x610F;&#x6570;&#x91CF;&#x3002;&#x5982;&#x679C;<code>&#x5F52;</code>&#x88AB;&#x8BBE;&#x5B9A;&#x4E3A;<code>&#x771F;</code>&#xFF0C;&#x8FD9;&#x901A;&#x8FC7;&#x7528; [HTG27&#x9664;&#x4EE5;&#x5F52;&#x4E00;&#x5316;&#x7684;&#x7ED3;&#x679C;]  &#x3A0; i&#x7684; =  1  K  N  i&#x7684; \
SQRT {\ prod</em> {I = 1}-1K-n_i&#x4E2A;}  &#x3A0; i&#x7684; =  1  K  [HT G112]  N  i&#x7684; &#xFF0C;&#x4F7F;&#x5F97;&#x64CD;&#x4F5C;&#x8005;&#x662F;&#x4E00;&#x4F53;&#x7684;&#x3002;</p>
<p>&#x8FD4;&#x56DE;&#x7684;&#x5B9E;&#x90E8;&#x548C;&#x865A;&#x90E8;&#x7684;<code>&#x8F93;&#x5165;</code>&#x76F8;&#x540C;&#x7684;&#x5F62;&#x72B6;&#x7684;&#x8054;&#x4E3A;&#x4E00;&#x4F53;&#x5F20;&#x91CF;&#x3002;</p>
<p>&#x6B64;&#x51FD;&#x6570;&#x7684;&#x9006;&#x662F; <code>IFFT&#xFF08;&#xFF09;</code>&#x3002;</p>
<p>Note</p>
<p>&#x5BF9;&#x4E8E;CUDA&#x5F20;&#x91CF;&#xFF0C;&#x4E00;&#x4E2A;LRU&#x7F13;&#x5B58;&#x7528;&#x4E8E;CUFFT&#x8BA1;&#x5212;&#x52A0;&#x5FEB;&#x4E0E;&#x76F8;&#x540C;&#x914D;&#x7F6E;&#x76F8;&#x540C;&#x7684;&#x51E0;&#x4F55;&#x5F62;&#x72B6;&#x7684;&#x5F20;&#x91CF;&#x91CD;&#x590D;&#x8FD0;&#x884C;FFT&#x65B9;&#x6CD5;&#x3002;&#x53C2;&#x89C1;[ CUFFT&#x8BA1;&#x5212;&#x7F13;&#x5B58;
<a href="notes/cuda.html#cufft-plan-cache">HTG3&#x5BF9;&#x4E8E;&#x5982;&#x4F55;&#x76D1;&#x89C6;&#x548C;&#x63A7;&#x5236;&#x7F13;&#x5B58;&#x7684;&#x66F4;&#x591A;&#x7EC6;&#x8282;&#x3002;</a></p>
<p>Warning</p>
<p>&#x5BF9;&#x4E8E;CPU&#x5F20;&#x91CF;&#xFF0C;&#x8FD9;&#x79CD;&#x65B9;&#x6CD5;&#x76EE;&#x524D;&#x53EA;&#x9002;&#x7528;&#x4E8E;MKL&#x3002;&#x4F7F;&#x7528;<code>torch.backends.mkl.is_available&#xFF08;&#xFF09;</code>&#x68C0;&#x67E5;&#x662F;&#x5426;&#x5B89;&#x88C5;MKL&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x7684;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x81F3;&#x5C11;<code>signal_ndim``+  1</code>&#x5C3A;&#x5BF8;</p>
</li>
<li><p><strong>signal_ndim</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x4E2D;&#x7684;&#x6BCF;&#x4E2A;&#x4FE1;&#x53F7;&#x7684;&#x7EF4;&#x6570;&#x3002; <code>signal_ndim</code>&#x53EA;&#x80FD;&#x662F;1&#xFF0C;2&#x6216;3&#x4E2A;</p>
</li>
<li><p><strong>&#x5F52;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x63A7;&#x5236;&#x662F;&#x5426;&#x8FD4;&#x56DE;&#x5F52;&#x4E00;&#x5316;&#x7ED3;&#x679C;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x5047;</code></p>
</li>
</ul>
<p>Returns</p>
<p>&#x5C06;&#x542B;&#x6709;&#x590D;&#x5408;&#x5230;&#x590D;&#x6570;&#x5085;&#x7ACB;&#x53F6;&#x53D8;&#x6362;&#x7ED3;&#x679C;&#x5F20;&#x91CF;</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; # unbatched 2D FFT
&gt;&gt;&gt; x = torch.randn(4, 3, 2)
&gt;&gt;&gt; torch.fft(x, 2)
tensor([[[-0.0876,  1.7835],
         [-2.0399, -2.9754],
         [ 4.4773, -5.0119]],

        [[-1.5716,  2.7631],
         [-3.8846,  5.2652],
         [ 0.2046, -0.7088]],

        [[ 1.9938, -0.5901],
         [ 6.5637,  6.4556],
         [ 2.9865,  4.9318]],

        [[ 7.0193,  1.1742],
         [-1.3717, -2.1084],
         [ 2.0289,  2.9357]]])
&gt;&gt;&gt; # batched 1D FFT
&gt;&gt;&gt; torch.fft(x, 1)
tensor([[[ 1.8385,  1.2827],
         [-0.1831,  1.6593],
         [ 2.4243,  0.5367]],

        [[-0.9176, -1.5543],
         [-3.9943, -2.9860],
         [ 1.2838, -2.9420]],

        [[-0.8854, -0.6860],
         [ 2.4450,  0.0808],
         [ 1.3076, -0.5768]],

        [[-0.1231,  2.7411],
         [-0.3075, -1.7295],
         [-0.5384, -2.0299]]])
&gt;&gt;&gt; # arbitrary number of batch dimensions, 2D FFT
&gt;&gt;&gt; x = torch.randn(3, 3, 5, 5, 2)
&gt;&gt;&gt; y = torch.fft(x, 2)
&gt;&gt;&gt; y.shape
torch.Size([3, 3, 5, 5, 2])
</code></pre><p><code>torch.``ifft</code>( <em>input</em> , <em>signal_ndim</em> , <em>normalized=False</em> ) &#x2192; Tensor</p>
<p>&#x590D;&#x6742;&#x5230;&#x590D;&#x6742;&#x7684;&#x79BB;&#x6563;&#x5085;&#x7ACB;&#x53F6;&#x9006;&#x53D8;&#x6362;</p>
<p>&#x6B64;&#x65B9;&#x6CD5;&#x8BA1;&#x7B97;&#x590D;&#x6742;&#x5230;&#x590D;&#x6742;&#x9006;&#x79BB;&#x6563;&#x5085;&#x7ACB;&#x53F6;&#x53D8;&#x6362;&#x3002;&#x5FFD;&#x7565;&#x6279;&#x6B21;&#x5C3A;&#x5BF8;&#xFF0C;&#x5B83;&#x8BA1;&#x7B97;&#x4E0B;&#x9762;&#x7684;&#x8868;&#x8FBE;&#x5F0F;&#xFF1A;</p>
<p>X[&#x3C9;1,&#x2026;,&#x3C9;d]=1&#x220F;i=1dNi&#x2211;n1=0N1&#x2212;1&#x22EF;&#x2211;nd=0Nd&#x2212;1x[n1,&#x2026;,nd]e j 2&#x3C0;&#x2211;i=0d&#x3C9;iniNi,X[\omega<em>1,
\dots, \omega_d] = \frac{1}{\prod</em>{i=1}^d N<em>i} \sum</em>{n<em>1=0}^{N_1-1} \dots
\sum</em>{n<em>d=0}^{N_d-1} x[n_1, \dots, n_d] e^{\ j\ 2 \pi \sum</em>{i=0}^d
\frac{\omega_i n_i}{N_i}},
X[&#x3C9;1&#x200B;,&#x2026;,&#x3C9;d&#x200B;]=&#x220F;i=1d&#x200B;Ni&#x200B;1&#x200B;n1&#x200B;=0&#x2211;N1&#x200B;&#x2212;1&#x200B;&#x22EF;nd&#x200B;=0&#x2211;Nd&#x200B;&#x2212;1&#x200B;x[n1&#x200B;,&#x2026;,nd&#x200B;]e j
2&#x3C0;&#x2211;i=0d&#x200B;Ni&#x200B;&#x3C9;i&#x200B;ni&#x200B;&#x200B;,</p>
<p>where ddd = <code>signal_ndim</code>is number of dimensions for the signal, and NiN_iNi&#x200B;
is the size of signal dimension iii .</p>
<p>&#x7684;&#x53C2;&#x6570;&#x89C4;&#x683C;&#x662F;&#x4E0E; <code>&#x51E0;&#x4E4E;&#x76F8;&#x540C;&#x7684;FFT&#xFF08;&#xFF09;</code>&#x3002;&#x7136;&#x800C;&#xFF0C;&#x5982;&#x679C;<code>&#x5F52;</code>&#x88AB;&#x8BBE;&#x5B9A;&#x4E3A;<code>&#x771F;</code>&#xFF0C;&#x8FD9;&#x800C;&#x662F;&#x8FD4;&#x56DE;&#x4E58;&#x4EE5; [&#x7ED3;&#x679C;HTG17]  &#x3A0; i&#x7684; =  1  d  N
i&#x7684; \ SQRT {\ prod_ {I = 1} ^ d n_i&#x4E2A;}  &#x3A0; i&#x7684; =  1  d  N  i&#x7684; &#xFF0C;&#x6210;&#x4E3A;&#x4E00;&#x4E2A;&#x6574;&#x4F53;&#x7684;&#x64CD;&#x4F5C;&#x3002;&#x56E0;&#x6B64;&#xFF0C;&#x53CD;&#x8F6C;&#x4E00;&#x4E2A; <code>FFT&#xFF08;&#xFF09;</code>&#x65F6;&#xFF0C;<code>&#x5F52;</code>&#x53C2;&#x6570;&#x5E94;&#x8BE5;&#x88AB;&#x76F8;&#x540C;&#x5730;&#x7528;&#x4E8E;&#x8BBE;&#x7F6E; <code>FFT&#xFF08;&#xFF09;</code>&#x3002;</p>
<p>Returns the real and the imaginary parts together as one tensor of the same
shape of <code>input</code>.</p>
<p>&#x6B64;&#x51FD;&#x6570;&#x7684;&#x9006;&#x662F; <code>FFT&#xFF08;&#xFF09;</code>&#x3002;</p>
<p>Note</p>
<p>For CUDA tensors, an LRU cache is used for cuFFT plans to speed up repeatedly
running FFT methods on tensors of same geometry with same configuration. See
<a href="notes/cuda.html#cufft-plan-cache">cuFFT plan cache</a> for more details on how
to monitor and control the cache.</p>
<p>Warning</p>
<p>For CPU tensors, this method is currently only available with MKL. Use
<code>torch.backends.mkl.is_available()</code>to check if MKL is installed.</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor of at least <code>signal_ndim``+ 1</code>dimensions</p>
</li>
<li><p><strong>signal_ndim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a>) &#x2013; the number of dimensions in each signal. <code>signal_ndim</code>can only be 1, 2 or 3</p>
</li>
<li><p><strong>normalized</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; controls whether to return normalized results. Default: <code>False</code></p>
</li>
</ul>
<p>Returns</p>
<p>&#x5C06;&#x542B;&#x6709;&#x590D;&#x5408;&#x7269;&#x5230;&#x590D;&#x6742;&#x5085;&#x7ACB;&#x53F6;&#x9006;&#x53D8;&#x6362;&#x7ED3;&#x679C;&#x5F20;&#x91CF;</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(3, 3, 2)
&gt;&gt;&gt; x
tensor([[[ 1.2766,  1.3680],
         [-0.8337,  2.0251],
         [ 0.9465, -1.4390]],

        [[-0.1890,  1.6010],
         [ 1.1034, -1.9230],
         [-0.9482,  1.0775]],

        [[-0.7708, -0.8176],
         [-0.1843, -0.2287],
         [-1.9034, -0.2196]]])
&gt;&gt;&gt; y = torch.fft(x, 2)
&gt;&gt;&gt; torch.ifft(y, 2)  # recover x
tensor([[[ 1.2766,  1.3680],
         [-0.8337,  2.0251],
         [ 0.9465, -1.4390]],

        [[-0.1890,  1.6010],
         [ 1.1034, -1.9230],
         [-0.9482,  1.0775]],

        [[-0.7708, -0.8176],
         [-0.1843, -0.2287],
         [-1.9034, -0.2196]]])
</code></pre><p><code>torch.``rfft</code>( <em>input</em> , <em>signal_ndim</em> , <em>normalized=False</em> , <em>onesided=True</em>
) &#x2192; Tensor</p>
<p>&#x771F;&#x6B63;&#x5230;&#x590D;&#x6742;&#x7684;&#x79BB;&#x6563;&#x5085;&#x7ACB;&#x53F6;&#x53D8;&#x6362;</p>
<p>&#x8BE5;&#x65B9;&#x6CD5;&#x53EF;&#x4EE5;&#x8BA1;&#x7B97;&#x771F;&#x6B63;&#x5230;&#x590D;&#x6742;&#x7684;&#x79BB;&#x6563;&#x5085;&#x7ACB;&#x53F6;&#x53D8;&#x6362;&#x3002;&#x5B83;&#x4E0E; <code>&#x6570;&#x5B66;&#x4E0A;&#x662F;&#x7B49;&#x6548;&#x7684;fft&#xFF08;&#xFF09;</code>&#x4EC5;&#x5728;&#x8F93;&#x5165;&#x548C;&#x8F93;&#x51FA;&#x7684;&#x683C;&#x5F0F;&#x7684;&#x5DEE;&#x5F02;&#x3002;</p>
<p>&#x6B64;&#x65B9;&#x6CD5;&#x652F;&#x6301;1D&#xFF0C;2D&#x548C;3D&#x771F;&#x5B9E;&#x5230;&#x590D;&#x6742;&#x7684;&#x53D8;&#x6362;&#xFF0C;&#x7531;<code>signal_ndim</code>&#x8868;&#x793A;&#x3002; <code>&#x8F93;&#x5165;</code>&#x5FC5;&#x987B;&#x4E0E;&#x81F3;&#x5C11;<code>&#x4E0E;&#x9886;&#x5148;&#x7684;&#x6279;&#x91CF;&#x5C3A;&#x5BF8;&#x7684;&#x4EFB;&#x9009;&#x4EFB;&#x610F;&#x6570;&#x91CF;signal_ndim</code>&#x5C3A;&#x5BF8;&#x7684;&#x5F20;&#x91CF;&#x3002;&#x5982;&#x679C;<code>&#x5F52;</code>&#x88AB;&#x8BBE;&#x5B9A;&#x4E3A;<code>&#x771F;</code>&#xFF0C;&#x8FD9;&#x901A;&#x8FC7;&#x7528; [HTG23&#x9664;&#x4EE5;&#x5F52;&#x4E00;&#x5316;&#x7684;&#x7ED3;&#x679C;]  &#x3A0; i&#x7684;
=  1  K  N  i&#x7684; \ SQRT {\ prod_ {I = 1}-1K-n_i&#x4E2A;}  &#x3A0; i&#x7684; =  1  K  N  i&#x7684;
&#xFF0C;&#x4F7F;&#x5F97;&#x64CD;&#x4F5C;&#x8005;&#x662F;&#x5355;&#x4E00;&#x7684;&#xFF0C;&#x5176;&#x4E2D; N  i&#x7684; n_i&#x4E2A; N  i&#x7684; &#x662F;&#x4FE1;&#x53F7;&#x7EF4;&#x5EA6; i&#x7684; i&#x7684; [HTG233&#x7684;&#x5927;&#x5C0F;]  i&#x7684; &#x3002;</p>
<p>&#x771F;&#x6B63;&#x5230;&#x590D;&#x6742;&#x7684;&#x5085;&#x7ACB;&#x53F6;&#x53D8;&#x6362;&#x7ED3;&#x679C;&#x5982;&#x4E0B;&#x5171;&#x8F6D;&#x5BF9;&#x79F0;&#xFF1A;</p>
<p>X[&#x3C9;1,&#x2026;,&#x3C9;d]=X&#x2217;[N1&#x2212;&#x3C9;1,&#x2026;,Nd&#x2212;&#x3C9;d],X[\omega_1, \dots, \omega_d] = X^*[N_1 -
\omega_1, \dots, N_d - \omega_d], X[&#x3C9;1&#x200B;,&#x2026;,&#x3C9;d&#x200B;]=X&#x2217;[N1&#x200B;&#x2212;&#x3C9;1&#x200B;,&#x2026;,Nd&#x200B;&#x2212;&#x3C9;d&#x200B;],</p>
<p>&#x5176;&#x4E2D;&#x7D22;&#x5F15;&#x7B97;&#x672F;&#x8BA1;&#x7B97;&#x6A21;&#x91CF;&#x7684;&#x5BF9;&#x5E94;&#x5C3A;&#x5BF8;&#x7684;&#x5C3A;&#x5BF8;&#xFF0C; <em>  \ ^ </em>  <em>  &#x4E3A;&#x5171;&#x8F6D;&#x7B97;&#x5B50;&#xFF0C;&#x5E76; d  d  d  = <code>signal_ndim  [ HTG73&#x3002;</code>
&#x7247;&#x9762; <code>&#x6807;&#x5FD7;&#x63A7;&#x5236;&#xFF0C;&#x4EE5;&#x907F;&#x514D;&#x5728;&#x8F93;&#x51FA;&#x7ED3;&#x679C;&#x7684;&#x5197;&#x4F59;&#x3002;&#x5982;&#x679C;&#x8BBE;&#x7F6E;&#x4E3A;</code>&#x771F; `&#xFF08;&#x9ED8;&#x8BA4;&#xFF09;&#x4E2D;&#xFF0C;&#x8F93;&#x51FA;&#x5C06;&#x4E0D;&#x4F1A;&#x88AB;&#x7684;&#x5F62;&#x72B6; [HTG88&#x5168;&#x590D;&#x6570;&#x7ED3;&#x679C;]&#xFF08;  </em>  &#xFF0C; 2  &#xFF09; &#xFF08;<em>&#xFF0C;2&#xFF09;
&#xFF08; </em>  &#xFF0C; 2  &#xFF09; &#xFF0C;&#x5176;&#x4E2D; <em>  </em>  *  &#x4E3A;&#x8F93;&#x5165;&#x7684;<code>&#x5F62;&#x72B6;</code>&#xFF0C;&#x800C;&#x662F;&#x6700;&#x540E;&#x5C3A;&#x5BF8;&#x5C06;&#x88AB;&#x51CF;&#x534A;&#x4E86;&#x4F5C;&#x4E3A;&#x5C3A;&#x5BF8; &#x7684;&#x230A; [H TG160]  N  d  2  &#x230B; +
1  \ lfloor \&#x538B;&#x88C2;{N_d} {2} \ rfloor + 1  &#x230A; 2  N  d  &#x230B; +  1  &#x3002;`</p>
<p>&#x6B64;&#x51FD;&#x6570;&#x7684;&#x9006;&#x662F; <code>irfft&#xFF08;&#xFF09;</code>&#x3002;</p>
<p>Note</p>
<p>For CUDA tensors, an LRU cache is used for cuFFT plans to speed up repeatedly
running FFT methods on tensors of same geometry with same configuration. See
<a href="notes/cuda.html#cufft-plan-cache">cuFFT plan cache</a> for more details on how
to monitor and control the cache.</p>
<p>Warning</p>
<p>For CPU tensors, this method is currently only available with MKL. Use
<code>torch.backends.mkl.is_available()</code>to check if MKL is installed.</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x7684;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x81F3;&#x5C11;<code>signal_ndim</code>&#x5C3A;&#x5BF8;</p>
</li>
<li><p><strong>signal_ndim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a>) &#x2013; the number of dimensions in each signal. <code>signal_ndim</code>can only be 1, 2 or 3</p>
</li>
<li><p><strong>normalized</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; controls whether to return normalized results. Default: <code>False</code></p>
</li>
<li><p><strong>&#x7247;&#x9762;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x63A7;&#x5236;&#x662F;&#x5426;&#x8FD4;&#x56DE;&#x4E00;&#x534A;&#x7684;&#x7ED3;&#x679C;&#xFF0C;&#x4EE5;&#x907F;&#x514D;&#x5197;&#x4F59;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x771F;</code></p>
</li>
</ul>
<p>Returns</p>
<p>&#x5C06;&#x542B;&#x6709;&#x5B9E;&#x6570;&#x5230;&#x590D;&#x6570;&#x5085;&#x7ACB;&#x53F6;&#x53D8;&#x6362;&#x7ED3;&#x679C;&#x5F20;&#x91CF;</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(5, 5)
&gt;&gt;&gt; torch.rfft(x, 2).shape
torch.Size([5, 3, 2])
&gt;&gt;&gt; torch.rfft(x, 2, onesided=False).shape
torch.Size([5, 5, 2])
</code></pre><p><code>torch.``irfft</code>( <em>input</em> , <em>signal_ndim</em> , <em>normalized=False</em> ,
<em>onesided=True</em> , <em>signal_sizes=None</em> ) &#x2192; Tensor</p>
<p>&#x590D;&#x6742;&#x5230;&#x771F;&#x6B63;&#x7684;&#x79BB;&#x6563;&#x5085;&#x7ACB;&#x53F6;&#x9006;&#x53D8;&#x6362;</p>
<p>&#x6B64;&#x65B9;&#x6CD5;&#x8BA1;&#x7B97;&#x590D;&#x6742;&#x5230;&#x5B9E;&#x9006;&#x79BB;&#x6563;&#x5085;&#x7ACB;&#x53F6;&#x53D8;&#x6362;&#x3002;&#x5B83;&#x4E0E;&#x6570;&#x5B66;&#x4E0A;&#x7B49;&#x6548;<code>IFFT&#xFF08;&#xFF09;</code>&#x4EC5;&#x5728;&#x8F93;&#x5165;&#x548C;&#x8F93;&#x51FA;&#x7684;&#x683C;&#x5F0F;&#x7684;&#x5DEE;&#x5F02;&#x3002;</p>
<p>&#x7684;&#x53C2;&#x6570;&#x89C4;&#x683C;&#x662F;&#x4E0E;&#x51E0;&#x4E4E;&#x76F8;&#x540C;<code>IFFT&#xFF08;&#xFF09;</code>&#x3002;&#x7C7B;&#x4F3C;&#x4E8E; <code>IFFT&#xFF08;&#xFF09;</code>&#x65F6;&#xFF0C;&#x5982;&#x679C;<code>&#x5F52;</code>&#x88AB;&#x8BBE;&#x5B9A;&#x4E3A;<code>&#x771F;</code>&#xFF0C;&#x8FD9;&#x901A;&#x8FC7;&#x7528; &#x4E58;&#x4EE5;&#x5F52;&#x4E00;&#x5316;&#x7ED3;&#x679C;&#x3A0; i&#x7684; =  1
K  N  i&#x7684; \ SQRT {\ prod_ {I = 1}-1K-n_i&#x4E2A;}  &#x3A0; i&#x7684; =  1  K  [HTG1 01]  N  i&#x7684;
&#xFF0C;&#x4F7F;&#x5F97;&#x64CD;&#x4F5C;&#x8005;&#x662F;&#x5355;&#x4E00;&#x7684;&#xFF0C;&#x5176;&#x4E2D; N  i&#x7684; n_i&#x4E2A; N  i&#x7684; &#x662F;&#x4FE1;&#x53F7;&#x7EF4;&#x5EA6; [&#x5927;&#x5C0F;HTG226 ]  i&#x7684; i&#x7684; i&#x7684; &#x3002;</p>
<p>Note</p>
<p>&#x7531;&#x4E8E;&#x5171;&#x8F6D;&#x5BF9;&#x79F0;&#xFF0C;<code>&#x8F93;&#x5165;</code>&#x4E0D;&#x9700;&#x8981;&#x5305;&#x542B;&#x5B8C;&#x6574;&#x7684;&#x590D;&#x9891;&#x7387;&#x503C;&#x3002;&#x5927;&#x81F4;&#x7684;&#x503C;&#x7684;&#x4E00;&#x534A;&#x5C06;&#x662F;&#x8DB3;&#x591F;&#x7684;&#xFF0C;&#x56E0;&#x4E3A;&#x662F;&#x5F53;<code>&#x8F93;&#x5165;</code>&#x7531; <code>rfft&#x7ED9;&#x5B9A;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#xFF08;&#xFF09;</code>&#x4E0E;<code>rfft&#xFF08;&#x4FE1;&#x53F7;&#xFF0C; &#x7247;&#x9762;=&#x771F;&#xFF09;</code>&#x3002;&#x5728;&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x8BBE;&#x7F6E;&#x6B64;&#x65B9;&#x6CD5;&#x7684;&#x4E3A;<code>&#x771F;</code>&#x4E2D;&#x7684;<code>&#x7247;&#x9762;</code>&#x53C2;&#x6570;&#x3002;&#x6B64;&#x5916;&#xFF0C;&#x539F;&#x6765;&#x7684;&#x4FE1;&#x53F7;&#x5F62;&#x72B6;&#x7684;&#x4FE1;&#x606F;&#x6709;&#x65F6;&#x4F1A;&#x4E22;&#x5931;&#xFF0C;&#x4EFB;&#x610F;&#x8BBE;&#x5B9A;<code>signal_sizes</code>&#x662F;&#x539F;&#x59CB;&#x4FE1;&#x53F7;&#x7684;&#x5927;&#x5C0F;&#xFF08;&#x65E0;&#x6279;&#x6B21;&#x5C3A;&#x5BF8;&#xFF0C;&#x5982;&#x679C;&#x5728;&#x6210;&#x6279;&#x6A21;&#x5F0F;&#xFF09;&#x4E0E;&#x6B63;&#x786E;&#x6062;&#x590D;&#x5B83;&#x5F62;&#x72B6;&#x3002;</p>
<p>&#x56E0;&#x6B64;&#xFF0C;&#x53CD;&#x8F6C;&#x7684; <code>rfft&#xFF08;&#xFF09;</code>&#xFF0C;&#x5219;&#x5F52;&#x4E00;&#x5316;<code>`&#x548C;</code>&#x7247;&#x9762; <code>&#x53C2;&#x6570;&#x5E94;&#x8BE5;&#x88AB;&#x76F8;&#x540C;&#x5730;&#x8BBE;&#x5B9A;&#x4E3A;</code>irfft&#xFF08;&#xFF09; <code>&#x548C;preferrably&#x4E00;&#x4E2A;</code>
signal_sizes`&#x662F;&#x9274;&#x4E8E;&#x4EE5;&#x907F;&#x514D;&#x5927;&#x5C0F;&#x4E0D;&#x5339;&#x914D;&#x3002;&#x53C2;&#x89C1;&#x5C3A;&#x5BF8;&#x4E0D;&#x5339;&#x914D;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#x7684;&#x4F8B;&#x5B50;&#x3002;</p>
<p>&#x53C2;&#x89C1; <code>rfft&#xFF08;&#xFF09;</code>&#x5173;&#x4E8E;&#x5171;&#x8F6D;&#x5BF9;&#x79F0;&#x7684;&#x7EC6;&#x8282;&#x3002;</p>
<p>&#x6B64;&#x51FD;&#x6570;&#x7684;&#x9006;&#x662F; <code>rfft&#xFF08;&#xFF09;</code>&#x3002;</p>
<p>Warning</p>
<p>&#x4E00;&#x822C;&#x6765;&#x8BF4;&#xFF0C;&#x8F93;&#x5165;&#x8FD9;&#x4E2A;&#x529F;&#x80FD;&#x5E94;&#x8BE5;&#x5305;&#x542B;&#x4EE5;&#x4E0B;&#x7684;&#x5171;&#x8F6D;&#x5BF9;&#x79F0;&#x6027;&#x503C;&#x3002;&#x9700;&#x8981;&#x6CE8;&#x610F;&#x7684;&#x662F;&#x7247;&#x9762; &#x5373;&#x4F7F;<code>&#x662F;</code>&#x771F; <code>&#xFF0C;&#x5E38;&#x4E3A;&#x5BF9;&#x79F0;&#x6027;&#x4E0A;&#x4ECD;&#x7136;&#x9700;&#x8981;&#x4E00;&#x4E9B;&#x90E8;&#x5206;&#x3002;&#x5F53;&#x8BE5;&#x8981;&#x6C42;&#x4E0D;&#x88AB;&#x6EE1;&#x8DB3;&#xFF0C;&#x7684;</code>
&#x884C;&#x4E3A;irfft&#xFF08;&#xFF09; <code>&#x662F;&#x672A;&#x5B9A;&#x4E49;&#x7684;&#x3002;&#x7531;&#x4E8E;[</code>torch.autograd.gradcheck&#xFF08;&#xFF09; <code>](autograd.html#torch.autograd.gradcheck
&quot;torch.autograd.gradcheck&quot;)&#x4F30;&#x8BA1;&#x6570;&#x503C;&#x96C5;&#x53EF;&#x6BD4;&#x4E0E;&#x70B9;&#x6270;&#x52A8;&#xFF0C;</code>irfft&#xFF08;&#xFF09; <code>&#x51E0;&#x4E4E;&#x80AF;&#x5B9A;&#x4F1A;&#x5931;&#x8D25;&#x7684;&#x68C0;&#x67E5;&#x3002;</code></p>
<p>Note</p>
<p>For CUDA tensors, an LRU cache is used for cuFFT plans to speed up repeatedly
running FFT methods on tensors of same geometry with same configuration. See
<a href="notes/cuda.html#cufft-plan-cache">cuFFT plan cache</a> for more details on how
to monitor and control the cache.</p>
<p>Warning</p>
<p>For CPU tensors, this method is currently only available with MKL. Use
<code>torch.backends.mkl.is_available()</code>to check if MKL is installed.</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor of at least <code>signal_ndim``+ 1</code>dimensions</p>
</li>
<li><p><strong>signal_ndim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a>) &#x2013; the number of dimensions in each signal. <code>signal_ndim</code>can only be 1, 2 or 3</p>
</li>
<li><p><strong>normalized</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; controls whether to return normalized results. Default: <code>False</code></p>
</li>
<li><p><strong>&#x7247;&#x9762;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x63A7;&#x5236;&#x662F;&#x5426;<code>&#x8F93;&#x5165;</code>&#x88AB;&#x51CF;&#x534A;&#x4E86;&#x907F;&#x514D;&#x5197;&#x4F59;&#xFF0C;&#x4F8B;&#x5982;&#xFF0C;&#x901A;&#x8FC7; <code>rfft&#xFF08;&#xFF09;</code>&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x771F;</code></p>
</li>
<li><p><strong>signal_sizes</strong> &#xFF08;&#x5217;&#x8868;&#x6216;<code>torch.Size</code>&#xFF0C;&#x53EF;&#x9009;&#xFF09; - &#x539F;&#x59CB;&#x4FE1;&#x53F7;&#xFF08;&#x65E0;&#x6279;&#x6B21;&#x5C3A;&#x5BF8;&#xFF09;&#x7684;&#x5C3A;&#x5BF8;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x65E0;</code></p>
</li>
</ul>
<p>Returns</p>
<p>&#x5C06;&#x542B;&#x6709;&#x590D;&#x5408;&#x7269;&#x5230;&#x5B9E;&#x5085;&#x7ACB;&#x53F6;&#x9006;&#x53D8;&#x6362;&#x7ED3;&#x679C;&#x5F20;&#x91CF;</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(4, 4)
&gt;&gt;&gt; torch.rfft(x, 2, onesided=True).shape
torch.Size([4, 3, 2])
&gt;&gt;&gt;
&gt;&gt;&gt; # notice that with onesided=True, output size does not determine the original signal size
&gt;&gt;&gt; x = torch.randn(4, 5)

&gt;&gt;&gt; torch.rfft(x, 2, onesided=True).shape
torch.Size([4, 3, 2])
&gt;&gt;&gt;
&gt;&gt;&gt; # now we use the original shape to recover x
&gt;&gt;&gt; x
tensor([[-0.8992,  0.6117, -1.6091, -0.4155, -0.8346],
        [-2.1596, -0.0853,  0.7232,  0.1941, -0.0789],
        [-2.0329,  1.1031,  0.6869, -0.5042,  0.9895],
        [-0.1884,  0.2858, -1.5831,  0.9917, -0.8356]])
&gt;&gt;&gt; y = torch.rfft(x, 2, onesided=True)
&gt;&gt;&gt; torch.irfft(y, 2, onesided=True, signal_sizes=x.shape)  # recover x
tensor([[-0.8992,  0.6117, -1.6091, -0.4155, -0.8346],
        [-2.1596, -0.0853,  0.7232,  0.1941, -0.0789],
        [-2.0329,  1.1031,  0.6869, -0.5042,  0.9895],
        [-0.1884,  0.2858, -1.5831,  0.9917, -0.8356]])
</code></pre><p><code>torch.``stft</code>( <em>input</em> , <em>n_fft</em> , <em>hop_length=None</em> , <em>win_length=None</em> ,
<em>window=None</em> , <em>center=True</em> , <em>pad_mode=&apos;reflect&apos;</em> , <em>normalized=False</em> ,
<em>onesided=True</em> )<a href="_modules/torch/functional.html#stft">[source]</a></p>
<p>&#x77ED;&#x65F6;&#x5085;&#x7ACB;&#x53F6;&#x53D8;&#x6362;&#xFF08;STFT&#xFF09;&#x3002;</p>
<p>&#x5FFD;&#x7565;&#x53EF;&#x9009;&#x6279;&#x6B21;&#x5C3A;&#x5BF8;&#xFF0C;&#x6B64;&#x65B9;&#x6CD5;&#x8BA1;&#x7B97;&#x4E0B;&#x5217;&#x8868;&#x8FBE;&#x5F0F;&#xFF1A;</p>
<p>X[m,&#x3C9;]=&#x2211;k=0win<em>length-1window[k] input[m&#xD7;hop_length+k]
exp&#x2061;(&#x2212;j2&#x3C0;&#x22C5;&#x3C9;kwin_length),X[m, \omega] = \sum</em>{k = 0}^{\text{win\_length-1}}%
\text{window}[k]\ \text{input}[m \times \text{hop\_length} + k]\ %
\exp\left(- j \frac{2 \pi \cdot \omega k}{\text{win\_length}}\right),
X[m,&#x3C9;]=k=0&#x2211;win_length-1&#x200B;window[k] input[m&#xD7;hop_length+k]
exp(&#x2212;jwin_length2&#x3C0;&#x22C5;&#x3C9;k&#x200B;),</p>
<p>&#x5176;&#x4E2D; M  M  M  &#x5728;&#x6ED1;&#x52A8;&#x7A97;&#x53E3;&#x7684;&#x7D22;&#x5F15;&#xFF0C;&#x548C; &#x3C9; \&#x7684;&#x3C9; &#x3C9; &#x662F;&#x9891;&#x7387; 0  &#x2264; &#x3C9; &amp; LT ;  N_FFT  0 \&#x5F53;&#x91CF;\&#x6B27;&#x7C73;&#x52A0;&amp; LT ; \&#x6587;&#x672C;{N
\ _fft}  0  &#x2264; &#x3C9; &amp; LT ;  [ H T G94]  N_FFT  &#x3002;&#x5F53;<code>&#x7247;&#x9762;</code>&#x4E3A;&#x9ED8;&#x8BA4;&#x503C;<code>&#x771F;</code></p>
<ul>
<li><p><code>&#x8F93;&#x5165;</code>&#x5FC5;&#x987B;&#x662F;1-d&#x7684;&#x65F6;&#x95F4;&#x5E8F;&#x5217;&#x6216;2- d&#x6279;&#x6B21;&#x65F6;&#x95F4;&#x5E8F;&#x5217;&#x3002;</p>
</li>
<li><p>&#x5982;&#x679C;<code>hop_length</code>&#x662F;<code>&#x65E0;</code>&#xFF08;&#x9ED8;&#x8BA4;&#xFF09;&#xFF0C;&#x5B83;&#x88AB;&#x89C6;&#x4E3A;&#x7B49;&#x4E8E;<code>&#x5730;&#x677F;&#xFF08;N_FFT  /  4&#xFF09;</code>&#x3002;</p>
</li>
<li><p>&#x5982;&#x679C;<code>win_length</code>&#x662F;<code>&#x65E0;</code>&#xFF08;&#x9ED8;&#x8BA4;&#xFF09;&#xFF0C;&#x5B83;&#x88AB;&#x89C6;&#x4E3A;&#x7B49;&#x4E8E;<code>N_FFT</code>&#x3002;</p>
</li>
<li><p><code>&#x7A97;&#x53E3;</code>&#x53EF;&#x4EE5;&#x662F;&#x5927;&#x5C0F;<code>win_length</code>&#xFF0C;&#x4F8B;&#x5982;1-d&#x5F20;&#x91CF;&#xFF0C;&#x7531; <code>torch&#x3002; hann_window&#xFF08;&#xFF09;</code>&#x3002;&#x5982;&#x679C;<code>&#x7A97;&#x53E3;</code>&#x662F;<code>&#x65E0;</code>&#xFF08;&#x9ED8;&#x8BA4;&#xFF09;&#xFF0C;&#x5B83;&#x88AB;&#x89C6;&#x4E3A;&#x597D;&#x50CF;&#x5177;&#x6709; 1  1  1  &#x65E0;&#x5904;&#x4E0D;&#x5728;&#x7684;&#x7A97;&#x53E3;&#x3002;&#x5982;&#x679C; win_length  &amp; LT ;  N_FFT  \ {&#x6587;&#x672C;&#x8D62;&#x5F97;\ _length} &amp; LT ; \&#x6587;&#x672C;{N \ _fft}  win_length  &amp; LT ;  N_FFT  &#xFF0C;<code>&#x7A97;&#x53E3;</code>&#x5C06;&#x5728;&#x4E24;&#x4FA7;&#x957F;&#x5EA6;&#x88AB;&#x586B;&#x5145;<code>N_FFT</code>&#x4E4B;&#x524D;&#x88AB;&#x65BD;&#x52A0;&#x3002;</p>
</li>
<li><p>&#x5982;&#x679C;<code>&#x4E2D;&#x5FC3;</code>&#x662F;<code>&#x771F;</code>&#xFF08;&#x9ED8;&#x8BA4;&#xFF09;&#xFF0C;<code>&#x8F93;&#x5165;</code>&#x5C06;&#x5728;&#x4E24;&#x4E2A;&#x586B;&#x5145;&#x4FA7;&#xFF0C;&#x4F7F;&#x5F97;&#x6240;&#x8FF0; T  T  T  &#x4E2A;&#x5E27;&#x5728;&#x65F6;&#x95F4; [HTG40&#x4E2D;&#x5FC3;]  T  &#xD7; hop_length  &#x5428;\&#x500D;\&#x6587;&#x672C;{&#x4E00;&#x8DF3;\ _length}  T  &#xD7; hop_length  &#x3002;&#x5426;&#x5219;&#xFF0C; T  T  T  &#x4E2A;&#x5E27;&#x5F00;&#x59CB;&#x4E8E;&#x65F6;&#x95F4; T  &#xD7; hop_length  &#x5428;\&#x500D;\&#x6587;&#x672C;{&#x4E00;&#x8DF3;\ _length}  T  &#xD7; hop_length  &#x3002;</p>
</li>
<li><p><code>pad_mode</code>&#x786E;&#x5B9A;&#x5728;<code>&#x8F93;&#x5165;</code>&#x4E2D;&#x4F7F;&#x7528;&#x7684;&#x586B;&#x8865;&#x65B9;&#x6CD5;&#xFF0C;&#x5F53;<code>&#x4E2D;&#x5FC3;</code>&#x662F;<code>&#x771F;</code>&#x3002;&#x53C2;&#x89C1;<a href="nn.functional.html#torch.nn.functional.pad" title="torch.nn.functional.pad"> <code>torch.nn.functional.pad&#xFF08;&#xFF09;</code></a>&#x6240;&#x6709;&#x53EF;&#x7528;&#x7684;&#x9009;&#x9879;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#x662F;<code>&#x201C;&#x53CD;&#x6620;&#x201D;</code>&#x3002;</p>
</li>
<li><p>&#x5982;&#x679C;<code>&#x7247;&#x9762;</code>&#x662F;<code>&#x771F;</code>&#xFF08;&#x9ED8;&#x8BA4;&#xFF09;&#x4E2D;&#xFF0C;&#x4EC5;&#x503C; &#x3C9; \&#x7684;&#x3C9; &#x3C9; &#x5728; [ 0  &#xFF0C; 1  &#xFF0C; 2  &#xFF0C; ...  &#xFF0C; &#x230A;  N_FFT  2  &#x230B; +  1  ]  \&#x5DE6;[0&#xFF0C;1&#xFF0C;2&#xFF0C;\&#x70B9;&#xFF0C;\&#x5DE6;\ lfloor \&#x538B;&#x88C2;{\&#x6587;&#x672C;{N \ _fft}} {2} \&#x53F3;\ rfloor + 1 \&#x53F3;]  [ 0  1  &#xFF0C; 2  &#xFF0C; ...  &#xFF0C; &#x230A; 2  N_FFT  &#x230B; +  1  &#x88AB;&#x8FD4;&#x56DE;&#x56E0;&#x4E3A;&#x771F;&#x6B63;&#x7684;&#x5230;&#x590D;&#x6742;&#x7684;&#x5085;&#x7ACB;&#x53F6;&#x53D8;&#x6362;&#x6EE1;&#x8DB3;&#x5171;&#x8F6D;&#x5BF9;&#x79F0;&#x7684;&#xFF0C;&#x5373;&#xFF0C; X  [  M  &#xFF0C; &#x3C9; =  X  [ M  &#xFF0C; N_FFT  -  &#x3C9; <em>  X [&#x7C73;&#xFF0C;\&#x3C9;= X [&#x7C73;&#xFF0C;\&#x6587;&#x672C;{N \ _fft} - \&#x3C9;-^ </em>  X  [ M  &#xFF0C; &#x3C9;  =  X  [ &#x200B;&#x200B; M  [HTG27 2]&#xFF0C; N_FFT  -  &#x3C9; *  &#x3002;</p>
</li>
<li><p>&#x5982;&#x679C;<code>&#x5F52;</code>&#x662F;<code>&#x771F;</code>&#xFF08;&#x9ED8;&#x8BA4;&#x8BBE;&#x5B9A;&#x4E3A;<code>&#x5047;</code>&#xFF09;&#xFF0C;&#x8BE5;&#x51FD;&#x6570;&#x8FD4;&#x56DE;&#x5F52;&#x4E00;&#x5316;&#x7684;STFT&#x7684;&#x7ED3;&#x679C;&#xFF0C;&#x5373;&#xFF0C;&#x4E58;&#x4EE5; &#xFF08; &#x5E27;<em>  &#xFF09; -  0.5  &#xFF08;\&#x6587;&#x672C;{&#x5E27;\ _length}&#xFF09;^ { - 0.5}  &#xFF08; &#x5E27;</em>  &#xFF09; -  0  &#x3002;  5  &#x3002;</p>
</li>
</ul>
<p>&#x8FD4;&#x56DE;&#x7684;&#x5B9E;&#x90E8;&#x548C;&#x865A;&#x90E8;&#x4E00;&#x8D77;&#x4F5C;&#x4E3A;&#x5927;&#x5C0F; &#xFF08; <em>  &#xD7;[&#x4E4B;&#x4E00;&#x5F20;&#x91CF;HTG11]  N  &#xD7; T  &#xD7; 2  &#xFF09; &#xFF08;</em> \&#x6B21;&#x6570;N \&#x65F6;&#x95F4;T \&#x500D;2&#xFF09; &#xFF08; <em>  &#xD7; N  &#xD7;
T  &#xD7; 2  &#xFF09; &#xFF0C;&#x5176;&#x4E2D; </em>  <em>  [H TG96]  </em>  &#x662F;<code>&#x53EF;&#x9009;&#x7684;&#x6279;&#x91CF;&#x5927;&#x5C0F;&#x8F93;&#x5165;</code>&#xFF0C; N  N  N  &#x662F;&#x5176;&#x4E2D;&#x5E94;&#x7528;STFT&#x7684;&#x9891;&#x7387;&#x7684;&#x6570;&#x91CF;&#xFF0C; T  T
T  &#x662F;&#x4F7F;&#x7528;&#x7684;&#x5E27;&#x7684;&#x603B;&#x6570;&#x91CF;&#xFF0C;&#x5E76;&#x4E14;&#x6BCF;&#x5BF9;&#x5728;&#x6700;&#x540E;&#x4E00;&#x7EF4;&#x8868;&#x793A;&#x590D;&#x6570;&#x4F5C;&#x4E3A;&#x5B9E;&#x90E8;&#x548C;&#x865A;&#x90E8;&#x3002;</p>
<p>Warning</p>
<p>&#x6B64;&#x529F;&#x80FD;&#x5728;0.4.1&#x7248;&#x672C;&#x4E2D;&#x66F4;&#x6539;&#x7B7E;&#x540D;&#x3002;&#x4E0E;&#x5148;&#x524D;&#x7684;&#x7B7E;&#x540D;&#x8C03;&#x7528;&#x53EF;&#x80FD;&#x4F1A;&#x5BFC;&#x81F4;&#x9519;&#x8BEF;&#x6216;&#x8FD4;&#x56DE;&#x4E0D;&#x6B63;&#x786E;&#x7684;&#x7ED3;&#x679C;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>N_FFT</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x5085;&#x7ACB;&#x53F6;&#x53D8;&#x6362;&#x5927;&#x5C0F;</p>
</li>
<li><p><strong>hop_length</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x76F8;&#x90BB;&#x6ED1;&#x52A8;&#x7A97;&#x5E27;&#x4E4B;&#x95F4;&#x7684;&#x8DDD;&#x79BB;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x65E0;</code>&#xFF08;&#x89C6;&#x4E3A;&#x7B49;&#x4E8E;<code>&#x5730;&#x677F;&#xFF08;N_FFT  /  4&#xFF09;</code>&#xFF09;</p>
</li>
<li><p><strong>win_length</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x7A97;&#x6846;&#x548C;STFT&#x6EE4;&#x6CE2;&#x5668;&#x7684;&#x5C3A;&#x5BF8;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x65E0;</code>&#xFF08;&#x89C6;&#x4E3A;&#x7B49;&#x4E8E;<code>N_FFT</code>&#xFF09;</p>
</li>
<li><p><strong>&#x7A97;&#x53E3;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x53EF;&#x9009;&#x7684;&#x7A97;&#x53E3;&#x51FD;&#x6570;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x65E0;</code>&#xFF08;&#x89C6;&#x4F5C;&#x7684;&#x7A97;&#x53E3;&#x4E2D;&#x7684;&#x6240;&#x6709; 1  1  1  S&#xFF09;</p>
</li>
<li><p><strong>&#x4E2D;&#x5FC3;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x662F;&#x5426;&#x57AB;<code>&#x8F93;&#x5165;</code>&#x5728;&#x4E24;&#x4FA7;&#xFF0C;&#x4F7F;&#x5F97; T  T  T  &#x4E2A;&#x5E27;&#x5728;&#x65F6;&#x95F4;&#x5C45;&#x4E2D; T  &#xD7; hop_length  &#x5428;\&#x500D;\&#x6587;&#x672C;{&#x4E00;&#x8DF3;\ _length}  T  &#xD7; hop_length  &#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x771F;</code></p>
</li>
<li><p><strong>pad_mode</strong> &#xFF08; <em>&#x4E32;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x63A7;&#x5236;&#x6240;&#x4F7F;&#x7528;&#x7684;&#x586B;&#x8865;&#x65B9;&#x6CD5;&#xFF0C;&#x5F53;<code>&#x4E2D;&#x5FC3;</code>&#x662F;<code>&#x771F;</code>&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x201C;&#x53CD;&#x6620;&#x201D;</code></p>
</li>
<li><p><strong>&#x5F52;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x63A7;&#x5236;&#x662F;&#x5426;&#x8FD4;&#x56DE;&#x7684;&#x5F52;&#x4E00;&#x5316;STFT&#x7ED3;&#x679C;&#x9ED8;&#x8BA4;&#xFF1A;<code>&#x5047;</code></p>
</li>
<li><p><strong>&#x7247;&#x9762;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x63A7;&#x5236;&#x662F;&#x5426;&#x8FD4;&#x56DE;&#x4E00;&#x534A;&#x7684;&#x7ED3;&#x679C;&#xFF0C;&#x4EE5;&#x907F;&#x514D;&#x5197;&#x4F59;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x771F;</code></p>
</li>
</ul>
<p>Returns</p>
<p>&#x5982;&#x4E0A;&#x6240;&#x8FF0;&#x5305;&#x542B;&#x4E0E;&#x5F62;&#x72B6;STFT&#x7ED3;&#x679C;&#x5F20;&#x91CF;</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p><code>torch.``bartlett_window</code>( <em>window_length</em> , <em>periodic=True</em> , <em>dtype=None</em> ,
<em>layout=torch.strided</em> , <em>device=None</em> , <em>requires_grad=False</em> ) &#x2192; Tensor</p>
<p>&#x5DF4;&#x7279;&#x5229;&#x7279;&#x7A97;&#x51FD;&#x6570;&#x3002;</p>
<p>w[n]=1&#x2212;&#x2223;2nN&#x2212;1&#x2212;1&#x2223;={2nN&#x2212;1if 0&#x2264;n&#x2264;N&#x2212;122&#x2212;2nN&#x2212;1if N&#x2212;12&lt;n&lt;N,w[n] = 1 - \left|
\frac{2n}{N-1} - 1 \right| = \begin{cases} \frac{2n}{N - 1} &amp; \text{if } 0
\leq n \leq \frac{N - 1}{2} \\ 2 - \frac{2n}{N - 1} &amp; \text{if } \frac{N -
1}{2} &lt; n &lt; N \\ \end{cases}, w[n]=1&#x2212;&#x2223;&#x2223;&#x2223;&#x2223;&#x200B;N&#x2212;12n&#x200B;&#x2212;1&#x2223;&#x2223;&#x2223;&#x2223;&#x200B;={N&#x2212;12n&#x200B;2&#x2212;N&#x2212;12n&#x200B;&#x200B;if
0&#x2264;n&#x2264;2N&#x2212;1&#x200B;if 2N&#x2212;1&#x200B;&lt;n&lt;N&#x200B;,</p>
<p>&#x5176;&#x4E2D; N  N  N  &#x662F;&#x5168;&#x7A97;&#x53E3;&#x5927;&#x5C0F;&#x3002;</p>
<p>&#x8F93;&#x5165;<code>window_length</code>&#x662F;&#x6B63;&#x6574;&#x6570;&#x63A7;&#x5236;&#x8FD4;&#x56DE;&#x7A97;&#x53E3;&#x5927;&#x5C0F;&#x3002; <code>&#x5468;&#x671F;&#x6027;</code>&#x6807;&#x5FD7;&#x786E;&#x5B9A;&#x6240;&#x8FD4;&#x56DE;&#x7684;&#x7A97;&#x53E3;&#x662F;&#x5426;&#x526A;&#x6389;&#x4ECE;&#x5BF9;&#x79F0;&#x7A97;&#x53E3;&#x4E2D;&#x7684;&#x6700;&#x540E;&#x91CD;&#x590D;&#x7684;&#x503C;&#xFF0C;&#x5E76;&#x51C6;&#x5907;&#x7528;&#x4F5C;&#x5468;&#x671F;&#x6027;&#x7A97;&#x53E3;&#x4E2D;&#x5305;&#x542B; <code>[HTG10&#x529F;&#x80FD;] torch.stft&#xFF08;&#xFF09;</code>&#x3002;&#x56E0;&#x6B64;&#xFF0C;&#x5982;&#x679C;<code>&#x5468;&#x671F;&#x6027;</code>&#x4E3A;&#x771F;&#xFF0C;&#x5219; N  N  N  &#x5728;&#x4E0A;&#x8FF0;&#x5F0F;&#x4E2D;&#x4E8B;&#x5B9E;&#x4E0A; window_length  +  1  \&#x6587;&#x672C;{&#x7A97;&#x53E3;\ _length} + 1
window_length  +  1  &#x3002;&#x53E6;&#x5916;&#xFF0C;&#x6211;&#x4EEC;&#x59CB;&#x7EC8;&#x6709;<code>torch.bartlett_window&#xFF08;L&#xFF0C; &#x5468;&#x671F;&#x6027;=&#x771F;&#xFF09;</code>&#x7B49;&#x4E8E;<code>torch.bartlett_window&#xFF08;L  +  1&#xFF0C; &#x5468;&#x671F;&#x6027;=&#x5047;&#xFF09;[&#xFF1A; - 1]&#xFF09;</code>&#x3002;</p>
<p>Note</p>
<p>&#x5982;&#x679C;<code>window_length</code>=  1  = 1  =  1  &#xFF0C;&#x8FD4;&#x56DE;&#x7684;&#x7A97;&#x53E3;&#x5305;&#x542B;&#x4E00;&#x4E2A;&#x503C;1&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>window_length</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x8FD4;&#x56DE;&#x7684;&#x7A97;&#x53E3;&#x7684;&#x5927;&#x5C0F;</p>
</li>
<li><p><strong>&#x5468;&#x671F;&#x6027;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5982;&#x679C;&#x4E3A;True&#xFF0C;&#x8FD4;&#x56DE;&#x5230;&#x88AB;&#x7528;&#x4F5C;&#x5468;&#x671F;&#x51FD;&#x6570;&#x7684;&#x7A97;&#x53E3;&#x3002;&#x5982;&#x679C;&#x4E3A;False&#xFF0C;&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x5BF9;&#x79F0;&#x7A97;&#x53E3;&#x3002;</p>
</li>
<li><p><strong>DTYPE</strong> &#xFF08;<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"> <code>torch.dtype</code></a>&#xFF0C;&#x53EF;&#x9009;&#xFF09; - &#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x7684;&#x6240;&#x5E0C;&#x671B;&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;&#x5982;&#x679C;<code>&#x65E0;</code>&#xFF0C;&#x4F7F;&#x7528;&#x5168;&#x5C40;&#x9ED8;&#x8BA4;&#x8BBE;&#x7F6E;&#xFF08;&#x89C1; <code>torch.set_default_tensor_type&#xFF08;&#xFF09;</code>&#xFF09;&#x3002;&#x53EA;&#x6709;&#x6D6E;&#x70B9;&#x7C7B;&#x578B;&#x7684;&#x652F;&#x6301;&#x3002;</p>
</li>
<li><p><strong>&#x5E03;&#x5C40;</strong> &#xFF08;<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"> <code>torch.layout</code></a>&#xFF0C;&#x53EF;&#x9009;&#xFF09; - &#x8FD4;&#x56DE;&#x7684;&#x7A97;&#x53E3;&#x5F20;&#x91CF;&#x7684;&#x6240;&#x9700;&#x5E03;&#x5C40;&#x3002;&#x53EA;&#x6709;<code>torch.strided</code>&#xFF08;&#x5BC6;&#x96C6;&#x5E03;&#x5C40;&#xFF09;&#x88AB;&#x652F;&#x6491;&#x3002;</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <code>torch.set_default_tensor_type()</code>). <code>device</code>will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li><p><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Returns</p>
<p>&#x7684;A 1-d&#x5F20;&#x91CF;&#x5927;&#x5C0F; &#xFF08; window_length  &#xFF0C; &#xFF09;  &#xFF08;\&#x6587;&#x672C;{&#x7A97;&#x53E3;\ _length}&#xFF0C;&#xFF09; &#xFF08; window_length  &#xFF0C; &#xFF09; &#x5305;&#x542B;&#x7A97;&#x53E3;</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p><code>torch.``blackman_window</code>( <em>window_length</em> , <em>periodic=True</em> , <em>dtype=None</em> ,
<em>layout=torch.strided</em> , <em>device=None</em> , <em>requires_grad=False</em> ) &#x2192; Tensor</p>
<p>&#x5E03;&#x83B1;&#x514B;&#x66FC;&#x7A97;&#x51FD;&#x6570;&#x3002;</p>
<p>w[n]=0.42&#x2212;0.5cos&#x2061;(2&#x3C0;nN&#x2212;1)+0.08cos&#x2061;(4&#x3C0;nN&#x2212;1)w[n] = 0.42 - 0.5 \cos \left(
\frac{2 \pi n}{N - 1} \right) + 0.08 \cos \left( \frac{4 \pi n}{N - 1} \right)
w[n]=0.42&#x2212;0.5cos(N&#x2212;12&#x3C0;n&#x200B;)+0.08cos(N&#x2212;14&#x3C0;n&#x200B;)</p>
<p>where NNN is the full window size.</p>
<p>&#x8F93;&#x5165;<code>window_length</code>&#x662F;&#x6B63;&#x6574;&#x6570;&#x63A7;&#x5236;&#x8FD4;&#x56DE;&#x7A97;&#x53E3;&#x5927;&#x5C0F;&#x3002; <code>&#x5468;&#x671F;&#x6027;</code>&#x6807;&#x5FD7;&#x786E;&#x5B9A;&#x6240;&#x8FD4;&#x56DE;&#x7684;&#x7A97;&#x53E3;&#x662F;&#x5426;&#x526A;&#x6389;&#x4ECE;&#x5BF9;&#x79F0;&#x7A97;&#x53E3;&#x4E2D;&#x7684;&#x6700;&#x540E;&#x91CD;&#x590D;&#x7684;&#x503C;&#xFF0C;&#x5E76;&#x51C6;&#x5907;&#x7528;&#x4F5C;&#x5468;&#x671F;&#x6027;&#x7A97;&#x53E3;&#x4E2D;&#x5305;&#x542B; <code>[HTG10&#x529F;&#x80FD;] torch.stft&#xFF08;&#xFF09;</code>&#x3002;&#x56E0;&#x6B64;&#xFF0C;&#x5982;&#x679C;<code>&#x5468;&#x671F;&#x6027;</code>&#x4E3A;&#x771F;&#xFF0C;&#x5219; N  N  N  &#x5728;&#x4E0A;&#x8FF0;&#x5F0F;&#x4E2D;&#x4E8B;&#x5B9E;&#x4E0A; window_length  +  1  \&#x6587;&#x672C;{&#x7A97;&#x53E3;\ _length} + 1
window_length  +  1  &#x3002;&#x53E6;&#x5916;&#xFF0C;&#x6211;&#x4EEC;&#x59CB;&#x7EC8;&#x6709;<code>torch.blackman_window&#xFF08;L&#xFF0C; &#x5468;&#x671F;&#x6027;=&#x771F;&#xFF09;</code>&#x7B49;&#x4E8E;<code>torch.blackman_window&#xFF08;L  +  1&#xFF0C; &#x5468;&#x671F;&#x6027;=&#x5047;&#xFF09;[&#xFF1A; - 1]&#xFF09;</code>&#x3002;</p>
<p>Note</p>
<p>If <code>window_length</code>=1=1=1 , the returned window contains a single value 1.</p>
<p>Parameters</p>
<ul>
<li><p><strong>window_length</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a>) &#x2013; the size of returned window</p>
</li>
<li><p><strong>periodic</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; If True, returns a window to be used as periodic function. If False, return a symmetric window.</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <code>torch.set_default_tensor_type()</code>). Only floating point types are supported.</p>
</li>
<li><p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned window tensor. Only <code>torch.strided</code>(dense layout) is supported.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <code>torch.set_default_tensor_type()</code>). <code>device</code>will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li><p><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Returns</p>
<p>A 1-D tensor of size (window_length,)(\text{window\_length},)(window_length,)
containing the window</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p><code>torch.``hamming_window</code>( <em>window_length</em> , <em>periodic=True</em> , <em>alpha=0.54</em> ,
<em>beta=0.46</em> , <em>dtype=None</em> , <em>layout=torch.strided</em> , <em>device=None</em> ,
<em>requires_grad=False</em> ) &#x2192; Tensor</p>
<p>&#x6D77;&#x660E;&#x7A97;&#x51FD;&#x6570;&#x3002;</p>
<p>w[n]=&#x3B1;&#x2212;&#x3B2; cos&#x2061;(2&#x3C0;nN&#x2212;1),w[n] = \alpha - \beta\ \cos \left( \frac{2 \pi n}{N - 1}
\right), w[n]=&#x3B1;&#x2212;&#x3B2; cos(N&#x2212;12&#x3C0;n&#x200B;),</p>
<p>where NNN is the full window size.</p>
<p>&#x8F93;&#x5165;<code>window_length</code>&#x662F;&#x6B63;&#x6574;&#x6570;&#x63A7;&#x5236;&#x8FD4;&#x56DE;&#x7A97;&#x53E3;&#x5927;&#x5C0F;&#x3002; <code>&#x5468;&#x671F;&#x6027;</code>&#x6807;&#x5FD7;&#x786E;&#x5B9A;&#x6240;&#x8FD4;&#x56DE;&#x7684;&#x7A97;&#x53E3;&#x662F;&#x5426;&#x526A;&#x6389;&#x4ECE;&#x5BF9;&#x79F0;&#x7A97;&#x53E3;&#x4E2D;&#x7684;&#x6700;&#x540E;&#x91CD;&#x590D;&#x7684;&#x503C;&#xFF0C;&#x5E76;&#x51C6;&#x5907;&#x7528;&#x4F5C;&#x5468;&#x671F;&#x6027;&#x7A97;&#x53E3;&#x4E2D;&#x5305;&#x542B; <code>[HTG10&#x529F;&#x80FD;] torch.stft&#xFF08;&#xFF09;</code>&#x3002;&#x56E0;&#x6B64;&#xFF0C;&#x5982;&#x679C;<code>&#x5468;&#x671F;&#x6027;</code>&#x4E3A;&#x771F;&#xFF0C;&#x5219; N  N  N  &#x5728;&#x4E0A;&#x8FF0;&#x5F0F;&#x4E2D;&#x4E8B;&#x5B9E;&#x4E0A; window_length  +  1  \&#x6587;&#x672C;{&#x7A97;&#x53E3;\ _length} + 1
window_length  +  1  &#x3002;&#x53E6;&#x5916;&#xFF0C;&#x6211;&#x4EEC;&#x59CB;&#x7EC8;&#x6709;<code>torch.hamming_window&#xFF08;L&#xFF0C; &#x5468;&#x671F;&#x6027;=&#x771F;&#xFF09;</code>&#x7B49;&#x4E8E;<code>torch.hamming_window&#xFF08;L  +  1&#xFF0C; &#x5468;&#x671F;&#x6027;=&#x5047;&#xFF09;[&#xFF1A; - 1]&#xFF09;</code>&#x3002;</p>
<p>Note</p>
<p>If <code>window_length</code>=1=1=1 , the returned window contains a single value 1.</p>
<p>Note</p>
<p>&#x8FD9;&#x662F; <code>&#x7684;&#x4E00;&#x822C;&#x5316;&#x7248;&#x672C;torch.hann_window&#xFF08;&#xFF09;</code>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>window_length</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a>) &#x2013; the size of returned window</p>
</li>
<li><p><strong>periodic</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; If True, returns a window to be used as periodic function. If False, return a symmetric window.</p>
</li>
<li><p><strong>&#x963F;&#x5C14;&#x6CD5;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x7CFB;&#x6570; &#x3B1; \&#x963F;&#x5C14;&#x6CD5; &#x3B1;  &#x5728;&#x4E0A;&#x9762;&#x7684;&#x7B49;&#x5F0F;</p>
</li>
<li><p><strong>&#x7684;&#x3B2;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x7CFB;&#x6570; &#x3B2; \&#x7684;&#x3B2; &#x3B2;  &#x5728;&#x4E0A;&#x9762;&#x7684;&#x7B49;&#x5F0F;</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <code>torch.set_default_tensor_type()</code>). Only floating point types are supported.</p>
</li>
<li><p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned window tensor. Only <code>torch.strided</code>(dense layout) is supported.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <code>torch.set_default_tensor_type()</code>). <code>device</code>will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li><p><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Returns</p>
<p>A 1-D tensor of size (window_length,)(\text{window\_length},)(window_length,)
containing the window</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p><code>torch.``hann_window</code>( <em>window_length</em> , <em>periodic=True</em> , <em>dtype=None</em> ,
<em>layout=torch.strided</em> , <em>device=None</em> , <em>requires_grad=False</em> ) &#x2192; Tensor</p>
<p>Hann&#x7A97;&#x51FD;&#x6570;&#x3002;</p>
<p>w[n]=12 [1&#x2212;cos&#x2061;(2&#x3C0;nN&#x2212;1)]=sin&#x2061;2(&#x3C0;nN&#x2212;1),w[n] = \frac{1}{2}\ \left[1 - \cos
\left( \frac{2 \pi n}{N - 1} \right)\right] = \sin^2 \left( \frac{\pi n}{N -
1} \right), w[n]=21&#x200B; [1&#x2212;cos(N&#x2212;12&#x3C0;n&#x200B;)]=sin2(N&#x2212;1&#x3C0;n&#x200B;),</p>
<p>where NNN is the full window size.</p>
<p>&#x8F93;&#x5165;<code>window_length</code>&#x662F;&#x6B63;&#x6574;&#x6570;&#x63A7;&#x5236;&#x8FD4;&#x56DE;&#x7A97;&#x53E3;&#x5927;&#x5C0F;&#x3002; <code>&#x5468;&#x671F;&#x6027;</code>&#x6807;&#x5FD7;&#x786E;&#x5B9A;&#x6240;&#x8FD4;&#x56DE;&#x7684;&#x7A97;&#x53E3;&#x662F;&#x5426;&#x526A;&#x6389;&#x4ECE;&#x5BF9;&#x79F0;&#x7A97;&#x53E3;&#x4E2D;&#x7684;&#x6700;&#x540E;&#x91CD;&#x590D;&#x7684;&#x503C;&#xFF0C;&#x5E76;&#x51C6;&#x5907;&#x7528;&#x4F5C;&#x5468;&#x671F;&#x6027;&#x7A97;&#x53E3;&#x4E2D;&#x5305;&#x542B; <code>[HTG10&#x529F;&#x80FD;] torch.stft&#xFF08;&#xFF09;</code>&#x3002;&#x56E0;&#x6B64;&#xFF0C;&#x5982;&#x679C;<code>&#x5468;&#x671F;&#x6027;</code>&#x4E3A;&#x771F;&#xFF0C;&#x5219; N  N  N  &#x5728;&#x4E0A;&#x8FF0;&#x5F0F;&#x4E2D;&#x4E8B;&#x5B9E;&#x4E0A; window_length  +  1  \&#x6587;&#x672C;{&#x7A97;&#x53E3;\ _length} + 1
window_length  +  1  &#x3002;&#x53E6;&#x5916;&#xFF0C;&#x6211;&#x4EEC;&#x59CB;&#x7EC8;&#x6709;<code>torch.hann_window&#xFF08;L&#xFF0C; &#x5468;&#x671F;&#x6027;=&#x771F;&#xFF09;</code>&#x7B49;&#x4E8E;<code>torch.hann_window&#xFF08;L  +  1&#xFF0C; &#x5468;&#x671F;&#x6027;=&#x5047;&#xFF09;[&#xFF1A; - 1]&#xFF09;</code>&#x3002;</p>
<p>Note</p>
<p>If <code>window_length</code>=1=1=1 , the returned window contains a single value 1.</p>
<p>Parameters</p>
<ul>
<li><p><strong>window_length</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a>) &#x2013; the size of returned window</p>
</li>
<li><p><strong>periodic</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; If True, returns a window to be used as periodic function. If False, return a symmetric window.</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <code>torch.set_default_tensor_type()</code>). Only floating point types are supported.</p>
</li>
<li><p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned window tensor. Only <code>torch.strided</code>(dense layout) is supported.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <code>torch.set_default_tensor_type()</code>). <code>device</code>will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li><p><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Returns</p>
<p>A 1-D tensor of size (window_length,)(\text{window\_length},)(window_length,)
containing the window</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<h3 id="&#x5176;&#x4ED6;&#x64CD;&#x4F5C;">&#x5176;&#x4ED6;&#x64CD;&#x4F5C;</h3>
<p><code>torch.``bincount</code>( <em>input</em> , <em>weights=None</em> , <em>minlength=0</em> ) &#x2192; Tensor</p>
<p>&#x8BA1;&#x6570;&#x6BCF;&#x4E2A;&#x503C;&#x7684;&#x9891;&#x7387;&#x5728;&#x975E;&#x8D1F;&#x6574;&#x6570;&#x7684;&#x6570;&#x7EC4;&#x3002;</p>
<p>&#xFF08;&#x5C3A;&#x5BF8;1&#xFF09;&#x6BB5;&#x7684;&#x6570;&#x76EE;&#x4F1A;&#x6BD4;<code>&#x6700;&#x5927;&#x503C;&#x8F83;&#x5927;&#x7684;&#x4E00;&#x4E2A;&#x8F93;&#x5165;</code>&#x9664;&#x975E;<code>&#x8F93;&#x5165;</code>&#x662F;&#x7A7A;&#x7684;&#xFF0C;&#x5728;&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;&#x7ED3;&#x679C;&#x662F;&#x5927;&#x5C0F;&#x4E3A;0&#x7684;&#x5F20;&#x91CF;&#x5982;&#x679C;<code>&#x4E2D;&#x6307;&#x5B9A;</code>&#x65F6;MINLENGTH&#xFF0C;&#x7BB1;&#x67DC;&#x7684;&#x6570;&#x76EE;&#x81F3;&#x5C11;&#x4E3A;<code>MINLENGTH</code>&#x5982;&#x679C;<code>&#x8F93;&#x5165;</code>&#x662F;&#x7A7A;&#x7684;&#xFF0C;&#x90A3;&#x4E48;&#x7ED3;&#x679C;&#x662F;&#x5927;&#x5C0F;<code>&#x586B;&#x5145; MINLENGTH</code>&#x7528;&#x96F6;&#x7684;&#x5F20;&#x91CF;&#x3002;&#x5982;&#x679C;<code>n&#x7684;</code>&#x5728;&#x4F4D;&#x7F6E;&#x503C;<code>i&#x7684;</code>&#xFF0C;<code>OUT [N]  + =  &#x6743;&#x91CD;[I]</code>&#x5982;&#x679C;<code>&#x7684;&#x6743;&#x91CD;&#x88AB;&#x522B;&#x7684;&#x6307;&#x5B9A; ``OUT [N]  + =  1</code>&#x3002;</p>
<p>Note</p>
<p>&#x5F53;&#x4F7F;&#x7528;CUDA&#x540E;&#x7AEF;&#xFF0C;&#x8BE5;&#x64CD;&#x4F5C;&#x53EF;&#x4EE5;&#x8BF1;&#x5BFC;&#x975E;&#x786E;&#x5B9A;&#x6027;&#x7684;&#x884C;&#x4E3A;&#x662F;&#x4E0D;&#x5BB9;&#x6613;&#x65AD;&#x5F00;&#x3002;&#x8BF7;&#x53C2;&#x9605;<a href="notes/randomness.html"> &#x91CD;&#x590D;&#x6027; </a>&#x4E3A;&#x80CC;&#x666F;&#x7684;&#x97F3;&#x7B26;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - 1-d INT&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>&#x6743;&#x91CD;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x53EF;&#x9009;&#x7684;&#xFF0C;&#x91CD;&#x91CF;&#x4E3A;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x7684;&#x6BCF;&#x4E2A;&#x503C;&#x3002;&#x5E94;&#x8BE5;&#x662F;&#x76F8;&#x540C;&#x7684;&#x5927;&#x5C0F;&#x4F5C;&#x4E3A;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x3002;</p>
</li>
<li><p><strong>MINLENGTH</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x4ED3;&#x7684;&#x53EF;&#x9009;&#x7684;&#xFF0C;&#x6700;&#x5C0F;&#x6570;&#x91CF;&#x3002;&#x5E94;&#x4E3A;&#x975E;&#x8D1F;&#x3002;</p>
</li>
</ul>
<p>Returns</p>
<p>&#x5F62;&#x72B6;<code>&#x5927;&#x5C0F;&#x7684;&#x5F20;&#x91CF;&#xFF08;[&#x6700;&#x5927;&#x503C;&#xFF08;&#x8F93;&#x5165;&#xFF09; +  1]&#xFF09;</code>&#x5982;&#x679C;<code>&#x8F93;&#x5165;</code>&#x975E;&#x7A7A;&#xFF0C;&#x5426;&#x5219;<code>&#x5C3A;&#x5BF8;&#xFF08;0&#xFF09;</code></p>
<p>Return type</p>
<p>&#x8F93;&#x51FA;&#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor">&#x5F20;&#x91CF;</a>&#xFF09;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; input = torch.randint(0, 8, (5,), dtype=torch.int64)
&gt;&gt;&gt; weights = torch.linspace(0, 1, steps=5)
&gt;&gt;&gt; input, weights
(tensor([4, 3, 6, 3, 4]),
 tensor([ 0.0000,  0.2500,  0.5000,  0.7500,  1.0000])

&gt;&gt;&gt; torch.bincount(input)
tensor([0, 0, 0, 2, 2, 0, 1])

&gt;&gt;&gt; input.bincount(weights)
tensor([0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.5000])
</code></pre><p><code>torch.``broadcast_tensors</code>( <em>*tensors</em> ) &#x2192; List of
Tensors<a href="_modules/torch/functional.html#broadcast_tensors">[source]</a></p>
<p>&#x6839;&#x636E;<a href="notes/broadcasting.html#broadcasting-semantics"> &#x5E7F;&#x64AD;&#x8BED;&#x4E49; </a>&#x5E7F;&#x64AD;&#x7ED9;&#x5B9A;&#x7684;&#x5F20;&#x91CF;&#x3002;</p>
<p>Parameters</p>
<p><strong>*&#x5F20;&#x91CF;</strong> - &#x4EFB;&#x4F55;&#x6570;&#x91CF;&#x7684;&#x76F8;&#x540C;&#x7C7B;&#x578B;&#x7684;&#x5F20;&#x91CF;&#x7684;</p>
<p>Warning</p>
<p>&#x5E7F;&#x64AD;&#x7684;&#x5F20;&#x91CF;&#x7684;&#x591A;&#x4E8E;&#x4E00;&#x4E2A;&#x7684;&#x5143;&#x4EF6;&#x53EF;&#x6307;&#x4EE3;&#x5355;&#x4E2A;&#x5B58;&#x50A8;&#x5668;&#x4F4D;&#x7F6E;&#x3002;&#x5176;&#x7ED3;&#x679C;&#x662F;&#xFF0C;&#x5C31;&#x5730;&#x64CD;&#x4F5C;&#xFF08;&#x7279;&#x522B;&#x662F;&#x90A3;&#x4E9B;&#x6709;&#x91CF;&#x5316;&#x7684;&#xFF09;&#x53EF;&#x80FD;&#x4F1A;&#x5BFC;&#x81F4;&#x4E0D;&#x6B63;&#x786E;&#x7684;&#x884C;&#x4E3A;&#x3002;&#x5982;&#x679C;&#x4F60;&#x9700;&#x8981;&#x5199;&#x5F20;&#x91CF;&#xFF0C;&#x8BF7;&#x5148;&#x514B;&#x9686;&#x5B83;&#x4EEC;&#x3002;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.arange(3).view(1, 3)
&gt;&gt;&gt; y = torch.arange(2).view(2, 1)
&gt;&gt;&gt; a, b = torch.broadcast_tensors(x, y)
&gt;&gt;&gt; a.size()
torch.Size([2, 3])
&gt;&gt;&gt; a
tensor([[0, 1, 2],
        [0, 1, 2]])
</code></pre><p><code>torch.``cartesian_prod</code>( <em>*tensors</em>
)<a href="_modules/torch/functional.html#cartesian_prod">[source]</a></p>
<p>&#x505A;&#x5F20;&#x91CF;&#x7684;&#x5B9A;&#x5E8F;&#x5217;&#x7684;&#x7B1B;&#x5361;&#x5C14;&#x4E58;&#x79EF;&#x3002;&#x8BE5;&#x884C;&#x4E3A;&#x7C7B;&#x4F3C;&#x4E8E;Python&#x7684; itertools.product [HTG1&#x3002;</p>
<p>Parameters</p>
<p><strong>*&#x5F20;&#x91CF;</strong> - &#x4EFB;&#x4F55;&#x6570;&#x91CF;&#x7684;1&#x7EF4;&#x5F20;&#x91CF;&#x3002;</p>
<p>Returns</p>
<p>A tensor equivalent to converting all the input tensors into lists,</p>
<p>&#x505A; itertools.product &#x5728;&#x8FD9;&#x4E9B;&#x540D;&#x5355;&#xFF0C;&#x6700;&#x540E;&#x7ED3;&#x679C;&#x5217;&#x8868;&#x8F6C;&#x6362;&#x6210;&#x5F20;&#x91CF;&#x3002;</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = [1, 2, 3]
&gt;&gt;&gt; b = [4, 5]
&gt;&gt;&gt; list(itertools.product(a, b))
[(1, 4), (1, 5), (2, 4), (2, 5), (3, 4), (3, 5)]
&gt;&gt;&gt; tensor_a = torch.tensor(a)
&gt;&gt;&gt; tensor_b = torch.tensor(b)
&gt;&gt;&gt; torch.cartesian_prod(tensor_a, tensor_b)
tensor([[1, 4],
        [1, 5],
        [2, 4],
        [2, 5],
        [3, 4],
        [3, 5]])
</code></pre><p><code>torch.``combinations</code>( <em>input</em> , <em>r=2</em> , <em>with_replacement=False</em> ) &#x2192; seq</p>
<p>&#x957F;&#x5EA6; R  R  [HTG14&#x7684;&#x8BA1;&#x7B97;&#x7EC4;&#x5408;]  R  [HTG23&#x7ED9;&#x5B9A;&#x7684;&#x5F20;&#x91CF;&#x3002;&#x8BE5;&#x884C;&#x4E3A;&#x7C7B;&#x4F3C;&#x4E8E;Python&#x7684; itertools.combinations &#x5F53;
with_replacement &#x8BBE;&#x7F6E;&#x4E3A;&#x5047;&#x548C; itertools.combinations_with_replacement &#x5F53;
with_replacement &#x8BBE;&#x7F6E;&#x4E3A;&#x771F;[HTG35&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - 1D&#x5411;&#x91CF;&#x3002;</p>
</li>
<li><p><strong>R</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5143;&#x7D20;&#x7684;&#x6570;&#x76EE;&#x76F8;&#x7ED3;&#x5408;</p>
</li>
<li><p><strong>with_replacement</strong> &#xFF08; <em>&#x5E03;&#x5C14;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x662F;&#x5426;&#x5141;&#x8BB8;&#x5728;&#x7EC4;&#x5408;&#x7684;&#x91CD;&#x590D;</p>
</li>
</ul>
<p>Returns</p>
<p>&#x5F20;&#x91CF;&#x76F8;&#x5F53;&#x4E8E;&#x5C06;&#x6240;&#x6709;&#x7684;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x6210;&#x5217;&#x8868;&#xFF0C;&#x6267;&#x884C; itertools.combinations &#x6216;
itertools.combinations_with_replacement &#x5728;&#x8FD9;&#x4E9B;&#x540D;&#x5355;&#xFF0C;&#x6700;&#x540E;&#x7ED3;&#x679C;&#x5217;&#x8868;&#x8F6C;&#x6362;&#x6210;&#x5F20;&#x91CF;&#x3002;</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = [1, 2, 3]
&gt;&gt;&gt; list(itertools.combinations(a, r=2))
[(1, 2), (1, 3), (2, 3)]
&gt;&gt;&gt; list(itertools.combinations(a, r=3))
[(1, 2, 3)]
&gt;&gt;&gt; list(itertools.combinations_with_replacement(a, r=2))
[(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)]
&gt;&gt;&gt; tensor_a = torch.tensor(a)
&gt;&gt;&gt; torch.combinations(tensor_a)
tensor([[1, 2],
        [1, 3],
        [2, 3]])
&gt;&gt;&gt; torch.combinations(tensor_a, r=3)
tensor([[1, 2, 3]])
&gt;&gt;&gt; torch.combinations(tensor_a, with_replacement=True)
tensor([[1, 1],
        [1, 2],
        [1, 3],
        [2, 2],
        [2, 3],
        [3, 3]])
</code></pre><p><code>torch.``cross</code>( <em>input</em> , <em>other</em> , <em>dim=-1</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x5411;&#x91CF;&#x7684;&#x53C9;&#x79EF;&#x7684;&#x5C3A;&#x5BF8;<code>&#x6697;&#x6DE1;</code>&#x7684;<code>&#x8F93;&#x5165;</code>&#x548C;<code>&#x5176;&#x4ED6;</code>&#x3002;</p>
<p><code>&#x8F93;&#x5165;</code>&#x548C;<code>&#x5176;&#x4ED6;</code>&#x5FC5;&#x987B;&#x5177;&#x6709;&#x76F8;&#x540C;&#x7684;&#x5C3A;&#x5BF8;&#xFF0C;&#x5E76;&#x4E14;&#x5B83;&#x4EEC;&#x7684;<code>&#x6697;&#x6DE1; [HTG11&#x7684;&#x5927;&#x5C0F;]&#x7EF4;&#x5E94;&#x8BE5;&#x662F;3&#x3002;</code></p>
<p>&#x5982;&#x679C;<code>&#x6697;&#x6DE1;</code>&#x6CA1;&#x6709;&#x7ED9;&#x51FA;&#xFF0C;&#x5219;&#x9ED8;&#x8BA4;&#x4E3A;&#x4E0E;&#x5C3A;&#x5BF8;3&#x627E;&#x5230;&#x7684;&#x7B2C;&#x4E00;&#x4E2A;&#x7EF4;&#x5EA6;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the second input tensor</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x7684;&#x5C3A;&#x5BF8;&#x53D6;&#x8DE8;&#x4EA7;&#x54C1;&#x82F1;&#x5BF8;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 3)
&gt;&gt;&gt; a
tensor([[-0.3956,  1.1455,  1.6895],
        [-0.5849,  1.3672,  0.3599],
        [-1.1626,  0.7180, -0.0521],
        [-0.1339,  0.9902, -2.0225]])
&gt;&gt;&gt; b = torch.randn(4, 3)
&gt;&gt;&gt; b
tensor([[-0.0257, -1.4725, -1.2251],
        [-1.1479, -0.7005, -1.9757],
        [-1.3904,  0.3726, -1.1836],
        [-0.9688, -0.7153,  0.2159]])
&gt;&gt;&gt; torch.cross(a, b, dim=1)
tensor([[ 1.0844, -0.5281,  0.6120],
        [-2.4490, -1.5687,  1.9792],
        [-0.8304, -1.3037,  0.5650],
        [-1.2329,  1.9883,  1.0551]])
&gt;&gt;&gt; torch.cross(a, b)
tensor([[ 1.0844, -0.5281,  0.6120],
        [-2.4490, -1.5687,  1.9792],
        [-0.8304, -1.3037,  0.5650],
        [-1.2329,  1.9883,  1.0551]])
</code></pre><p><code>torch.``diag</code>( <em>input</em> , <em>diagonal=0</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<ul>
<li><p>&#x5982;&#x679C;<code>&#x8F93;&#x5165;</code>&#x662F;&#x77E2;&#x91CF;&#xFF08;1-d&#x5F20;&#x91CF;&#xFF09;&#xFF0C;&#x7136;&#x540E;&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;2-d&#x5E73;&#x65B9;&#x5F20;&#x91CF;&#x4E0E;<code>&#x8F93;&#x5165;</code>&#x4E3A;&#x4E00;&#x4F53;&#x7684;&#x5143;&#x4EF6;&#x5BF9;&#x89D2;&#x7EBF;&#x3002;</p>
</li>
<li><p>&#x5982;&#x679C;<code>&#x8F93;&#x5165;</code>&#x662F;&#x77E9;&#x9635;&#xFF08;2- d&#x5F20;&#x91CF;&#xFF09;&#xFF0C;&#x5219;&#x8FD4;&#x56DE;1-d&#x5F20;&#x91CF;&#x4E0E;&#x8F93;&#x5165;&#x7684;``&#x7684;&#x5BF9;&#x89D2;&#x5143;&#x7D20;&#x3002;</p>
</li>
</ul>
<p>&#x7684;&#x53C2;&#x6570; <code>&#x5BF9;&#x89D2;&#x7EBF;</code>&#x63A7;&#x5236;&#x4EE5;&#x8003;&#x8651;&#x5176;&#x5BF9;&#x89D2;&#xFF1A;</p>
<ul>
<li><p>&#x5982;&#x679C; <code>&#x5BF9;&#x89D2;&#x7EBF;</code>= 0&#xFF0C;&#x5B83;&#x662F;&#x4E3B;&#x5BF9;&#x89D2;&#x7EBF;&#x3002;</p>
</li>
<li><p>&#x5982;&#x679C; <code>&#x5BF9;&#x89D2;&#x7EBF;</code>&amp; GT ; 0&#xFF0C;&#x5B83;&#x4E0A;&#x9762;&#x7684;&#x4E3B;&#x5BF9;&#x89D2;&#x7EBF;&#x3002;</p>
</li>
<li><p>&#x5982;&#x679C; <code>&#x5BF9;&#x89D2;&#x7EBF;</code>&amp; LT ; 0&#xFF0C;&#x5B83;&#x662F;&#x4E0B;&#x9762;&#x7684;&#x4E3B;&#x5BF9;&#x89D2;&#x7EBF;&#x3002;</p>
</li>
</ul>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>&#x5BF9;&#x89D2;&#x7EBF;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5BF9;&#x89D2;&#x7EBF;&#x8003;&#x8651;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>See also</p>
<p><code>torch.diagonal&#xFF08;&#xFF09;</code>&#x603B;&#x662F;&#x8FD4;&#x56DE;&#x5BF9;&#x89D2;&#x7EBF;&#x5176;&#x8F93;&#x5165;&#x3002;</p>
<p><code>torch.diagflat&#xFF08;&#xFF09;</code>&#x59CB;&#x7EC8;&#x6784;&#x6210;&#x4E0E;&#x7531;&#x8F93;&#x5165;&#x6307;&#x5B9A;&#x7684;&#x5BF9;&#x89D2;&#x5143;&#x7D20;&#x7684;&#x5F20;&#x91CF;&#x3002;</p>
<p>Examples:</p>
<p>&#x83B7;&#x53D6;&#x65B9;&#x9635;&#xFF0C;&#x5176;&#x4E2D;&#x8F93;&#x5165;&#x5411;&#x91CF;&#x4E3A;&#x5BF9;&#x89D2;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(3)
&gt;&gt;&gt; a
tensor([ 0.5950,-0.0872, 2.3298])
&gt;&gt;&gt; torch.diag(a)
tensor([[ 0.5950, 0.0000, 0.0000],
        [ 0.0000,-0.0872, 0.0000],
        [ 0.0000, 0.0000, 2.3298]])
&gt;&gt;&gt; torch.diag(a, 1)
tensor([[ 0.0000, 0.5950, 0.0000, 0.0000],
        [ 0.0000, 0.0000,-0.0872, 0.0000],
        [ 0.0000, 0.0000, 0.0000, 2.3298],
        [ 0.0000, 0.0000, 0.0000, 0.0000]])
</code></pre><p>&#x83B7;&#x53D6;&#x7ED9;&#x5B9A;&#x77E9;&#x9635;&#x7684;&#x7B2C;k&#x4E2A;&#x5BF9;&#x89D2;&#x7EBF;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(3, 3)
&gt;&gt;&gt; a
tensor([[-0.4264, 0.0255,-0.1064],
        [ 0.8795,-0.2429, 0.1374],
        [ 0.1029,-0.6482,-1.6300]])
&gt;&gt;&gt; torch.diag(a, 0)
tensor([-0.4264,-0.2429,-1.6300])
&gt;&gt;&gt; torch.diag(a, 1)
tensor([ 0.0255, 0.1374])
</code></pre><p><code>torch.``diag_embed</code>( <em>input</em> , <em>offset=0</em> , <em>dim1=-2</em> , <em>dim2=-1</em> ) &#x2192; Tensor</p>
<p>&#x521B;&#x5EFA;&#x4E00;&#x4E2A;&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x4E00;&#x5B9A;&#x7684;2D&#x5E73;&#x9762;&#x7684;&#x5BF9;&#x89D2;&#x7EBF;&#xFF08;&#x7531;<code>&#x6307;&#x5B9A;DIM1</code>&#x548C;<code>DIM2</code>&#xFF09;&#x7531;<code>&#x8F93;&#x5165;&#x586B;&#x5145;</code>&#x3002;&#x4E3A;&#x4E86;&#x4FBF;&#x4E8E;&#x521B;&#x5EFA;&#x6279;&#x5904;&#x7406;&#x5BF9;&#x89D2;&#x77E9;&#x9635;&#xFF0C;&#x7528;&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x7684;&#x6700;&#x540E;&#x4E24;&#x4E2A;&#x7EF4;&#x5EA6;&#x5F62;&#x6210;&#x4E8C;&#x7EF4;&#x5E73;&#x9762;&#x9ED8;&#x8BA4;&#x9009;&#x4E2D;&#x3002;</p>
<p>&#x7684;&#x53C2;&#x6570;<code>&#x504F;&#x79FB;HTG2]</code>&#x63A7;&#x5236;&#x4EE5;&#x8003;&#x8651;&#x5176;&#x5BF9;&#x89D2;&#xFF1A;</p>
<ul>
<li><p>&#x5982;&#x679C;offset = 0 <code>&#xFF0C;&#x5B83;&#x662F;&#x4E3B;&#x5BF9;&#x89D2;&#x7EBF;&#x3002;</code></p>
</li>
<li><p>&#x5982;&#x679C;<code>&#x504F;&#x79FB;HTG2]</code>&amp; GT ; 0&#xFF0C;&#x5B83;&#x4E0A;&#x9762;&#x7684;&#x4E3B;&#x5BF9;&#x89D2;&#x7EBF;&#x3002;</p>
</li>
<li><p>&#x5982;&#x679C;<code>&#x504F;&#x79FB;HTG2]</code>&amp; LT ; 0&#xFF0C;&#x5B83;&#x662F;&#x4E0B;&#x9762;&#x7684;&#x4E3B;&#x5BF9;&#x89D2;&#x7EBF;&#x3002;</p>
</li>
</ul>
<p>&#x65B0;&#x77E9;&#x9635;&#x7684;&#x5927;&#x5C0F;&#x5C06;&#x88AB;&#x8BA1;&#x7B97;&#x4E3A;&#x4F7F;&#x6700;&#x540E;&#x8F93;&#x5165;&#x5C3A;&#x5BF8;&#x5927;&#x5C0F;&#x7684;&#x6307;&#x5B9A;&#x5BF9;&#x89D2;&#x7EBF;&#x3002;&#x6CE8;&#x610F;&#xFF0C;<code>&#x504F;&#x79FB;</code>&#x9664; 0  0  0  &#xFF0C;&#x7684;&#x987A;&#x5E8F;<code>DIM1</code>&#x548C;<code>DIM2</code>&#x4E8B;&#x9879;&#x3002;&#x4EA4;&#x6362;&#x5B83;&#x4EEC;&#x662F;&#x7B49;&#x6548;&#x4E8E;&#x6539;&#x53D8;&#x7684;<code>&#x504F;&#x79FB;HTG38]</code>&#x7B26;&#x53F7;&#x3002;</p>
<p>&#x65BD;&#x52A0; <code>torch.diagonal&#xFF08;&#xFF09;</code>&#x8BE5;&#x51FD;&#x6570;&#x7684;&#x8F93;&#x51FA;&#x4E0E;&#x76F8;&#x540C;&#x53C2;&#x6570;&#x4EA7;&#x751F;&#x76F8;&#x540C;&#x7684;&#x8F93;&#x5165;&#x7684;&#x77E9;&#x9635;&#x3002;&#x7136;&#x800C;&#xFF0C; <code>torch.diagonal&#xFF08;&#xFF09;</code>
&#x5177;&#x6709;&#x4E0D;&#x540C;&#x7684;&#x9ED8;&#x8BA4;&#x5C3A;&#x5BF8;&#xFF0C;&#x56E0;&#x6B64;&#x8FD9;&#x4E9B;&#x9700;&#x8981;&#x88AB;&#x660E;&#x786E;&#x6307;&#x5B9A;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x8F93;&#x5165;&#x5F20;&#x91CF;&#x3002;&#x5FC5;&#x987B;&#x81F3;&#x5C11;&#x4E3A;1&#x7EF4;&#x7684;&#x3002;</p>
</li>
<li><p><strong>&#x504F;&#x79FB;HTG1]&#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x8003;&#x8651;&#x54EA;&#x4E9B;&#x5BF9;&#x89D2;&#x7EBF;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;0&#xFF08;&#x4E3B;&#x5BF9;&#x89D2;&#x7EBF;&#xFF09;&#x3002;</strong></p>
</li>
<li><p><strong>DIM1</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x76F8;&#x5BF9;&#x4E8E;&#x7B2C;&#x4E00;&#x7EF4;&#x5EA6;&#xFF0C;&#x5176;&#x91C7;&#x53D6;&#x5BF9;&#x89D2;&#x7EBF;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;-2&#x3002;</p>
</li>
<li><p><strong>DIM2</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x76F8;&#x5BF9;&#x4E8E;&#x7B2C;&#x4E8C;&#x5C3A;&#x5BF8;&#xFF0C;&#x5176;&#x91C7;&#x53D6;&#x5BF9;&#x89D2;&#x7EBF;&#x3002;&#x7F3A;&#x7701;&#x503C;&#xFF1A;-1&#x3002;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(2, 3)
&gt;&gt;&gt; torch.diag_embed(a)
tensor([[[ 1.5410,  0.0000,  0.0000],
         [ 0.0000, -0.2934,  0.0000],
         [ 0.0000,  0.0000, -2.1788]],

        [[ 0.5684,  0.0000,  0.0000],
         [ 0.0000, -1.0845,  0.0000],
         [ 0.0000,  0.0000, -1.3986]]])

&gt;&gt;&gt; torch.diag_embed(a, offset=1, dim1=0, dim2=2)
tensor([[[ 0.0000,  1.5410,  0.0000,  0.0000],
         [ 0.0000,  0.5684,  0.0000,  0.0000]],

        [[ 0.0000,  0.0000, -0.2934,  0.0000],
         [ 0.0000,  0.0000, -1.0845,  0.0000]],

        [[ 0.0000,  0.0000,  0.0000, -2.1788],
         [ 0.0000,  0.0000,  0.0000, -1.3986]],

        [[ 0.0000,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  0.0000]]])
</code></pre><p><code>torch.``diagflat</code>( <em>input</em> , <em>offset=0</em> ) &#x2192; Tensor</p>
<ul>
<li><p>If <code>input</code>is a vector (1-D tensor), then returns a 2-D square tensor with the elements of <code>input</code>as the diagonal.</p>
</li>
<li><p>&#x5982;&#x679C;<code>&#x8F93;&#x5165;</code>&#x662F;&#x4E0E;&#x591A;&#x4E8E;&#x4E00;&#x4E2A;&#x7684;&#x7EF4;&#x5EA6;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x7136;&#x540E;&#x8FD4;&#x56DE;2-d&#x5F20;&#x91CF;&#x7684;&#x5BF9;&#x89D2;&#x5143;&#x7D20;&#x7B49;&#x4E8E;&#x6241;&#x5E73;<code>&#x8F93;&#x5165;</code>&#x3002;</p>
</li>
</ul>
<p>The argument <code>offset</code>controls which diagonal to consider:</p>
<ul>
<li><p>If <code>offset</code>= 0, it is the main diagonal.</p>
</li>
<li><p>If <code>offset</code>&gt; 0, it is above the main diagonal.</p>
</li>
<li><p>If <code>offset</code>&lt; 0, it is below the main diagonal.</p>
</li>
</ul>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>&#x504F;&#x79FB;HTG1]&#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5BF9;&#x89D2;&#x7EBF;&#x6765;&#x8003;&#x8651;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;0&#xFF08;&#x4E3B;&#x5BF9;&#x89D2;&#x7EBF;&#xFF09;&#x3002;</strong></p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(3)
&gt;&gt;&gt; a
tensor([-0.2956, -0.9068,  0.1695])
&gt;&gt;&gt; torch.diagflat(a)
tensor([[-0.2956,  0.0000,  0.0000],
        [ 0.0000, -0.9068,  0.0000],
        [ 0.0000,  0.0000,  0.1695]])
&gt;&gt;&gt; torch.diagflat(a, 1)
tensor([[ 0.0000, -0.2956,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.9068,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.1695],
        [ 0.0000,  0.0000,  0.0000,  0.0000]])

&gt;&gt;&gt; a = torch.randn(2, 2)
&gt;&gt;&gt; a
tensor([[ 0.2094, -0.3018],
        [-0.1516,  1.9342]])
&gt;&gt;&gt; torch.diagflat(a)
tensor([[ 0.2094,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.3018,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.1516,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  1.9342]])
</code></pre><p><code>torch.``diagonal</code>( <em>input</em> , <em>offset=0</em> , <em>dim1=0</em> , <em>dim2=1</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;<code>&#x8F93;&#x5165;</code>&#x4E0E;&#x5B83;&#x7684;&#x5BF9;&#x89D2;&#x5143;&#x7D20;&#x7684;&#x5C40;&#x90E8;&#x89C6;&#x56FE;&#x76F8;&#x5BF9;&#x4E8E;<code>DIM1</code>&#x548C;<code>DIM2</code>&#x4F5C;&#x4E3A;&#x9644;&#x52A0;&#x5728;&#x6240;&#x8FF0;&#x5F62;&#x72B6;&#x7684;&#x7AEF;&#x90E8;&#x7684;&#x5C3A;&#x5BF8;&#x3002;</p>
<p>The argument <code>offset</code>controls which diagonal to consider:</p>
<ul>
<li><p>If <code>offset</code>= 0, it is the main diagonal.</p>
</li>
<li><p>If <code>offset</code>&gt; 0, it is above the main diagonal.</p>
</li>
<li><p>If <code>offset</code>&lt; 0, it is below the main diagonal.</p>
</li>
</ul>
<p><code>torch.diag_embed&#x65BD;&#x52A0;&#xFF08;&#xFF09;</code>&#x8BE5;&#x51FD;&#x6570;&#x7684;&#x8F93;&#x51FA;&#x4E0E;&#x76F8;&#x540C;&#x7684;&#x53C2;&#x6570;&#x4EA7;&#x751F;&#x4E0E;&#x6240;&#x8FF0;&#x8F93;&#x5165;&#x7684;&#x5BF9;&#x89D2;&#x9879;&#x7684;&#x5BF9;&#x89D2;&#x77E9;&#x9635;&#x3002;&#x7136;&#x800C;&#xFF0C; <code>torch.diag_embed&#xFF08;&#xFF09;</code>&#x5177;&#x6709;&#x4E0D;&#x540C;&#x7684;&#x9ED8;&#x8BA4;&#x5C3A;&#x5BF8;&#xFF0C;&#x56E0;&#x6B64;&#x8FD9;&#x4E9B;&#x9700;&#x8981;&#x88AB;&#x660E;&#x786E;&#x6307;&#x5B9A;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x8F93;&#x5165;&#x5F20;&#x91CF;&#x3002;&#x5FC5;&#x987B;&#x81F3;&#x5C11;2&#x7EF4;&#x7684;&#x3002;</p>
</li>
<li><p><strong>offset</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>,</em> <em>optional</em> ) &#x2013; which diagonal to consider. Default: 0 (main diagonal).</p>
</li>
<li><p><strong>DIM1</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x76F8;&#x5BF9;&#x4E8E;&#x7B2C;&#x4E00;&#x7EF4;&#x5EA6;&#xFF0C;&#x5176;&#x91C7;&#x53D6;&#x5BF9;&#x89D2;&#x7EBF;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;0&#x3002;</p>
</li>
<li><p><strong>DIM2</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x76F8;&#x5BF9;&#x4E8E;&#x7B2C;&#x4E8C;&#x5C3A;&#x5BF8;&#xFF0C;&#x5176;&#x91C7;&#x53D6;&#x5BF9;&#x89D2;&#x7EBF;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;1&#x3002;</p>
</li>
</ul>
<p>Note</p>
<p>&#x91C7;&#x53D6;&#x5206;&#x6279;&#x5BF9;&#x89D2;&#x7EBF;&#xFF0C;&#x4F20;&#x5165;DIM1 = -2&#xFF0C;DIM2 = -1&#x3002;</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(3, 3)
&gt;&gt;&gt; a
tensor([[-1.0854,  1.1431, -0.1752],
        [ 0.8536, -0.0905,  0.0360],
        [ 0.6927, -0.3735, -0.4945]])


&gt;&gt;&gt; torch.diagonal(a, 0)
tensor([-1.0854, -0.0905, -0.4945])


&gt;&gt;&gt; torch.diagonal(a, 1)
tensor([ 1.1431,  0.0360])


&gt;&gt;&gt; x = torch.randn(2, 5, 4, 2)
&gt;&gt;&gt; torch.diagonal(x, offset=-1, dim1=1, dim2=2)
tensor([[[-1.2631,  0.3755, -1.5977, -1.8172],
         [-1.1065,  1.0401, -0.2235, -0.7938]],

        [[-1.7325, -0.3081,  0.6166,  0.2335],
         [ 1.0500,  0.7336, -0.3836, -1.1015]]])
</code></pre><p><code>torch.``einsum</code>( <em>equation</em> , <em>*operands</em> ) &#x2192;
Tensor<a href="_modules/torch/functional.html#einsum">[source]</a></p>
<p>&#x8FD9;&#x4E2A;&#x529F;&#x80FD;&#x63D0;&#x4F9B;&#x8BA1;&#x7B97;&#x591A;&#x7EBF;&#x6027;&#x8868;&#x8FBE;&#x5F0F;&#x7684;&#x65B9;&#x5F0F;&#x4F7F;&#x7528;&#x7231;&#x56E0;&#x65AF;&#x5766;&#x6C42;&#x548C;&#x7EA6;&#x5B9A;&#xFF08;&#x5373;&#x4E58;&#x79EF;&#x7684;&#x548C;&#xFF09;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x65B9;&#x7A0B;</strong> &#xFF08; <em>&#x4E32;</em> &#xFF09; - &#x65B9;&#x7A0B;&#x5F0F;&#x4E2D;&#x7684;&#x5C0F;&#x5199;&#x5B57;&#x6BCD;&#xFF08;&#x7D22;&#x5F15;&#xFF09;&#x6765;&#x7ED9;&#x51FA;&#x5C06;&#x4E0E;&#x64CD;&#x4F5C;&#x6570;&#x548C;&#x7ED3;&#x679C;&#x7684;&#x6BCF;&#x4E2A;&#x7EF4;&#x5EA6;&#x76F8;&#x5173;&#x8054;&#x3002;&#x5DE6;&#x4FA7;&#x5217;&#x51FA;&#x4E86;&#x64CD;&#x4F5C;&#x6570;&#x7684;&#x5C3A;&#x5BF8;&#xFF0C;&#x7528;&#x9017;&#x53F7;&#x5206;&#x5F00;&#x3002;&#x5E94;&#x8BE5;&#x6709;&#x6BCF;&#x5F20;&#x91CF;&#x7EF4;&#x5EA6;&#x4E2D;&#x7684;&#x4E00;&#x4E2A;&#x7D22;&#x5F15;&#x5B57;&#x6BCD;&#x3002;&#x4E4B;&#x540E;&#x7684;&#x53F3;&#x624B;&#x8FB9;&#x5982;&#x4E0B; - &amp; GT ; &#xFF0C;&#x5E76;&#x7ED9;&#x51FA;&#x4E86;&#x6307;&#x6570;&#x7684;&#x8F93;&#x51FA;&#x3002;&#x5982;&#x679C; - [ - ] GT ; &#x548C;&#x53F3;&#x4FA7;&#x88AB;&#x7701;&#x7565;&#xFF0C;&#x5B83;&#x542B;&#x84C4;&#x5730;&#x5B9A;&#x4E49;&#x4E3A;&#x5DE6;&#x4FA7;&#x6070;&#x597D;&#x51FA;&#x73B0;&#x4E00;&#x6B21;&#x6240;&#x6709;&#x6307;&#x6570;&#x7684;&#x6309;&#x5B57;&#x6BCD;&#x987A;&#x5E8F;&#x6392;&#x5E8F;&#x5217;&#x8868;&#x3002;&#x5728;&#x8F93;&#x51FA;&#x4E0D;apprearing&#x8BE5;&#x6307;&#x6570;&#x7684;&#x64CD;&#x4F5C;&#x6570;&#x9879;&#x76F8;&#x4E58;&#x540E;&#x6C42;&#x548C;&#x3002;&#x5982;&#x679C;&#x6307;&#x6570;&#x51FA;&#x73B0;&#x51E0;&#x6B21;&#x540C;&#x6837;&#x7684;&#x64CD;&#x4F5C;&#xFF0C;&#x5BF9;&#x89D2;&#x7EBF;&#x53D6;&#x3002;&#x692D;&#x5706; ... &#x4EE3;&#x8868;&#x5C3A;&#x5BF8;&#x7684;&#x56FA;&#x5B9A;&#x6570;&#x76EE;&#x3002;&#x5982;&#x679C;&#x53F3;&#x4FA7;&#x63A8;&#x65AD;&#xFF0C;&#x7701;&#x7565;&#x53F7;&#x5C3A;&#x5BF8;&#x5728;&#x8F93;&#x51FA;&#x7684;&#x5F00;&#x59CB;&#x3002;</p>
</li>
<li><p><strong>&#x64CD;&#x4F5C;&#x6570;</strong> &#xFF08; <em>&#x5F20;&#x91CF;</em> &#x7684;&#x5217;&#x8868;&#x4E2D;&#xFF09; - &#x7684;&#x64CD;&#x4F5C;&#x6570;&#x6765;&#x8BA1;&#x7B97;&#x7684;&#x7231;&#x56E0;&#x65AF;&#x5766;&#x603B;&#x548C;&#x3002;</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(5)
&gt;&gt;&gt; y = torch.randn(4)
&gt;&gt;&gt; torch.einsum(&apos;i,j-&gt;ij&apos;, x, y)  # outer product
tensor([[-0.0570, -0.0286, -0.0231,  0.0197],
        [ 1.2616,  0.6335,  0.5113, -0.4351],
        [ 1.4452,  0.7257,  0.5857, -0.4984],
        [-0.4647, -0.2333, -0.1883,  0.1603],
        [-1.1130, -0.5588, -0.4510,  0.3838]])


&gt;&gt;&gt; A = torch.randn(3,5,4)
&gt;&gt;&gt; l = torch.randn(2,5)
&gt;&gt;&gt; r = torch.randn(2,4)
&gt;&gt;&gt; torch.einsum(&apos;bn,anm,bm-&gt;ba&apos;, l, A, r) # compare torch.nn.functional.bilinear
tensor([[-0.3430, -5.2405,  0.4494],
        [ 0.3311,  5.5201, -3.0356]])


&gt;&gt;&gt; As = torch.randn(3,2,5)
&gt;&gt;&gt; Bs = torch.randn(3,5,4)
&gt;&gt;&gt; torch.einsum(&apos;bij,bjk-&gt;bik&apos;, As, Bs) # batch matrix multiplication
tensor([[[-1.0564, -1.5904,  3.2023,  3.1271],
         [-1.6706, -0.8097, -0.8025, -2.1183]],

        [[ 4.2239,  0.3107, -0.5756, -0.2354],
         [-1.4558, -0.3460,  1.5087, -0.8530]],

        [[ 2.8153,  1.8787, -4.3839, -1.2112],
         [ 0.3728, -2.1131,  0.0921,  0.8305]]])

&gt;&gt;&gt; A = torch.randn(3, 3)
&gt;&gt;&gt; torch.einsum(&apos;ii-&gt;i&apos;, A) # diagonal
tensor([-0.7825,  0.8291, -0.1936])

&gt;&gt;&gt; A = torch.randn(4, 3, 3)
&gt;&gt;&gt; torch.einsum(&apos;...ii-&gt;...i&apos;, A) # batch diagonal
tensor([[-1.0864,  0.7292,  0.0569],
        [-0.9725, -1.0270,  0.6493],
        [ 0.5832, -1.1716, -1.5084],
        [ 0.4041, -1.1690,  0.8570]])

&gt;&gt;&gt; A = torch.randn(2, 3, 4, 5)
&gt;&gt;&gt; torch.einsum(&apos;...ij-&gt;...ji&apos;, A).shape # batch permute
torch.Size([2, 3, 5, 4])
</code></pre><p><code>torch.``flatten</code>( <em>input</em> , <em>start_dim=0</em> , <em>end_dim=-1</em> ) &#x2192; Tensor</p>
<p>&#x53D8;&#x5E73;&#x7684;&#x5F20;&#x91CF;DIMS&#x7684;&#x8FDE;&#x7EED;&#x8303;&#x56F4;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>start_dim</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x7B2C;&#x4E00;&#x6697;&#x6DE1;&#x53D8;&#x5E73;</p>
</li>
<li><p><strong>end_dim</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x6700;&#x540E;&#x6697;&#x6DE1;&#x53D8;&#x5E73;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; t = torch.tensor([[[1, 2],
                       [3, 4]],
                      [[5, 6],
                       [7, 8]]])
&gt;&gt;&gt; torch.flatten(t)
tensor([1, 2, 3, 4, 5, 6, 7, 8])
&gt;&gt;&gt; torch.flatten(t, start_dim=1)
tensor([[1, 2, 3, 4],
        [5, 6, 7, 8]])
</code></pre><p><code>torch.``flip</code>( <em>input</em> , <em>dims</em> ) &#x2192; Tensor</p>
<p>&#x53CD;&#x5411;&#x6CBF;DIMS&#x5B9A;&#x8F74;&#x7EBF;&#x6B63;d&#x5F20;&#x91CF;&#x7684;&#x987A;&#x5E8F;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>&#x53D8;&#x6697;</strong> &#xFF08; <em>&#x5217;&#x8868;</em> <em>&#x6216;</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>&#x5143;&#x7EC4;</em> </a>&#xFF09; - &#x8F74;&#x7FFB;&#x8F6C;&#x4E0A;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.arange(8).view(2, 2, 2)
&gt;&gt;&gt; x
tensor([[[ 0,  1],
         [ 2,  3]],

        [[ 4,  5],
         [ 6,  7]]])
&gt;&gt;&gt; torch.flip(x, [0, 1])
tensor([[[ 6,  7],
         [ 4,  5]],

        [[ 2,  3],
         [ 0,  1]]])
</code></pre><p><code>torch.``rot90</code>( <em>input</em> , <em>k</em> , <em>dims</em> ) &#x2192; Tensor</p>
<p>&#x5728;&#x901A;&#x8FC7;DIMS&#x8F74;&#x89C4;&#x5B9A;&#x7684;&#x5E73;&#x9762;&#x5185;&#x65CB;&#x8F6C;90&#x5EA6;&#x540E;&#x7684;&#x6B63;d&#x5F20;&#x91CF;&#x3002;&#x65CB;&#x8F6C;&#x65B9;&#x5411;&#x662F;&#x4ECE;&#x7B2C;&#x4E00;&#x5411;&#x7B2C;&#x4E8C;&#x8F74;&#x5982;&#x679C;k &amp; GT ; 0&#xFF0C;&#x5E76;&#x4E14;&#x4ECE;&#x7B2C;&#x4E8C;&#x5411;&#x7B2C;&#x4E00;&#x5BF9;&#x4E8E;k &amp; LT ; 0&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>K</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x6B21;&#x6570;&#x65CB;&#x8F6C;</p>
</li>
<li><p><strong>&#x53D8;&#x6697;</strong> &#xFF08; <em>&#x5217;&#x8868;</em> <em>&#x6216;</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>&#x5143;&#x7EC4;</em> </a>&#xFF09; - &#x8F74;&#x65CB;&#x8F6C;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.arange(4).view(2, 2)
&gt;&gt;&gt; x
tensor([[0, 1],
        [2, 3]])
&gt;&gt;&gt; torch.rot90(x, 1, [0, 1])
tensor([[1, 3],
        [0, 2]])

&gt;&gt;&gt; x = torch.arange(8).view(2, 2, 2)
&gt;&gt;&gt; x
tensor([[[0, 1],
         [2, 3]],

        [[4, 5],
         [6, 7]]])
&gt;&gt;&gt; torch.rot90(x, 1, [1, 2])
tensor([[[1, 3],
         [0, 2]],

        [[5, 7],
         [4, 6]]])
</code></pre><p><code>torch.``histc</code>( <em>input</em> , <em>bins=100</em> , <em>min=0</em> , <em>max=0</em> , <em>out=None</em> ) &#x2192;
Tensor</p>
<p>&#x8BA1;&#x7B97;&#x5F20;&#x91CF;&#x7684;&#x67F1;&#x72B6;&#x56FE;&#x3002;</p>
<p>&#x8FD9;&#x4E9B;&#x5143;&#x4EF6;&#x4E4B;&#x95F4; <code>&#x5206;&#x6210;&#x76F8;&#x7B49;&#x7684;&#x5BBD;&#x5EA6;&#x4ED3;&#x5206;&#x949F;HTG3]</code>&#x548C; <code>MAX</code>&#x3002;&#x5982;&#x679C; <code>&#x5206;&#x949F;HTG15]</code>&#x548C; <code>MAX</code>
&#x5747;&#x4E3A;&#x96F6;&#xFF0C;&#x6700;&#x5C0F;&#x548C;&#x7684;&#x6700;&#x5927;&#x503C;&#x7684;&#x6570;&#x636E;&#x88AB;&#x4F7F;&#x7528;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>&#x4ED3;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x76F4;&#x65B9;&#x56FE;&#x533A;&#x95F4;&#x7684;&#x6570;</p>
</li>
<li><p><strong>&#x5206;&#x949F;HTG1]&#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x8BE5;&#x8303;&#x56F4;&#x7684;&#x4E0B;&#x7AEF;&#xFF08;&#x542B;&#xFF09;</strong></p>
</li>
<li><p><strong>MAX</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x8BE5;&#x8303;&#x56F4;&#x7684;&#x4E0A;&#x7AEF;&#xFF08;&#x542B;&#xFF09;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Returns</p>
<p>&#x76F4;&#x65B9;&#x56FE;&#x8868;&#x793A;&#x4E3A;&#x5F20;&#x91CF;</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.histc(torch.tensor([1., 2, 1]), bins=4, min=0, max=3)
tensor([ 0.,  2.,  1.,  0.])
</code></pre><p><code>torch.``meshgrid</code>( <em>*tensors</em> , <em>**kwargs</em>
)<a href="_modules/torch/functional.html#meshgrid">[source]</a></p>
<p>&#x53D6; N  N  N  &#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x4E2D;&#x7684;&#x6BCF;&#x4E00;&#x4E2A;&#x53EF;&#x4EE5;&#x662F;&#x6807;&#x91CF;&#x6216;1&#x7EF4;&#x5411;&#x91CF;&#xFF0C;&#x5E76;&#x521B;&#x5EFA; N  N  N  N&#x7EF4;&#x7F51;&#x683C;&#xFF0C;&#x5176;&#x4E2D;&#xFF0C;&#x6240;&#x8FF0; i&#x7684; i&#x7684; i&#x7684; &#x7B2C;&#x7F51;&#x683C;&#x7531;&#x6269;&#x5927; i&#x7684; i&#x7684;
[HTG85&#x5B9A;&#x4E49;]  i&#x7684;[H TG93]  &#x7B2C;&#x5728;&#x7531;&#x5176;&#x4ED6;&#x8F93;&#x5165;&#x6765;&#x5B9A;&#x4E49;&#x7684;&#x5C3A;&#x5BF8;&#x8F93;&#x5165;&#x3002;</p>
<blockquote>
<p>Args:</p>
<p>&#x5F20;&#x91CF;&#xFF08;&#x5F20;&#x91CF;&#x7684;&#x5217;&#x8868;&#xFF09;&#xFF1A;&#x6807;&#x91CF;&#x6216;1&#x7EF4;&#x7684;&#x5F20;&#x91CF;&#x7684;&#x5217;&#x8868;&#x3002;&#x6807;&#x91CF;&#x5C06;&#x88AB;&#x89C6;&#x4E3A;&#x7684;&#x5F20;&#x91CF;&#x5927;&#x5C0F; &#xFF08; 1  &#xFF0C; &#xFF09; &#xFF08;1&#xFF09; &#xFF08; 1  &#xFF0C; &#xFF09; &#x81EA;&#x52A8;</p>
</blockquote>
<p>&gt;</p>
<blockquote>
<p>Returns:</p>
<p>SEQ&#xFF08;&#x5F20;&#x91CF;&#x7684;&#x5E8F;&#x5217;&#xFF09;&#xFF1A;&#x5982;&#x679C;&#x8F93;&#x5165;&#x5177;&#x6709; K  K  K  &#x5927;&#x5C0F;&#x7684;&#x5F20;&#x91CF; &#xFF08; N  1  &#xFF0C; &#xFF09; &#xFF0C; &#xFF08; N  2  &#xFF0C; &#xFF09; ...  &#xFF0C; &#xFF08; N  K  &#xFF0C;
&#xFF09; &#xFF08;N_1&#xFF0C;&#xFF09;&#xFF0C;&#xFF08;N_2&#xFF0C;&#xFF09;&#xFF0C;\ ldots&#xFF0C;&#xFF08;N_k&#xFF0C;&#xFF09; &#xFF08; N  1  &#xFF09; &#xFF0C; &#xFF08; N  2  &#xFF0C; &#xFF09; &#xFF0C; ...  &#xFF0C; &#xFF08; N  K  &#xFF0C; &#xFF09;
&#x5219;&#x8F93;&#x51FA;&#x4E5F;&#x5C06;&#x5177;&#x6709; K  K  K  &#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x4E2D;&#x6240;&#x6709;&#x7684;&#x5F20;&#x91CF;&#x7684;&#x5927;&#x5C0F; &#xFF08; N  1  &#xFF0C; N  2  &#xFF0C; ...  &#xFF0C; N  K  &#x200B;&#x200B;&#xFF09; &#xFF08;N_1&#xFF0C;N_2&#xFF0C;\
ldots&#xFF0C;N_k&#xFF09; &#xFF08; N  1  &#xFF0C; N  2  &#xFF0C; ...  &#xFF0C; N  K  [HT G382]  &#xFF09; &#x3002;</p>
</blockquote>
<p>&gt;</p>
<blockquote>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.tensor([1, 2, 3])

&gt;&gt;&gt; y = torch.tensor([4, 5, 6])

&gt;&gt;&gt; grid_x, grid_y = torch.meshgrid(x, y)

&gt;&gt;&gt; grid_x

tensor([[1, 1, 1],

        [2, 2, 2],

        [3, 3, 3]])

&gt;&gt;&gt; grid_y

tensor([[4, 5, 6],

        [4, 5, 6],

        [4, 5, 6]])
</code></pre></blockquote>
<p><code>torch.``renorm</code>( <em>input</em> , <em>p</em> , <em>dim</em> , <em>maxnorm</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x4E2D;&#x8F93;&#x5165;&#x7684;<code>&#x6BCF;&#x4E2A;&#x5B50;&#x5F20;&#x91CF;</code>&#x6CBF;&#x7740;&#x7EF4;&#x5EA6;<code>&#x6697;&#x6DE1;</code>&#x8FDB;&#x884C;&#x5F52;&#x4E00;&#x5316;&#xFF0C;&#x4F7F;&#x5F97; P  - &#x5B50;&#x5F20;&#x91CF;&#x7684;&#x8303;&#x6570;&#x4F4E;&#x4E8E;&#x503C;<code>maxnorm</code></p>
<p>Note</p>
<p>&#x5982;&#x679C;&#x884C;&#x7684;&#x8303;&#x6570;&#x5927;&#x4E8E; maxnorm &#x4E0B;&#xFF0C;&#x8BE5;&#x884C;&#x662F;&#x4E0D;&#x53D8;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>P</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a>&#xFF09; - &#x529F;&#x7387;&#x4E3A;&#x8303;&#x6570;&#x8BA1;&#x7B97;</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x7684;&#x5C3A;&#x5BF8;&#x5207;&#x8FC7;&#xFF0C;&#x4EE5;&#x83B7;&#x5F97;&#x5B50;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>maxnorm</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a>&#xFF09; - &#x6700;&#x5927;&#x8303;&#x4FDD;&#x6301;&#x6BCF;&#x4E2A;&#x5B50;&#x5F20;&#x91CF;&#x4E0B;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.ones(3, 3)
&gt;&gt;&gt; x[1].fill_(2)
tensor([ 2.,  2.,  2.])
&gt;&gt;&gt; x[2].fill_(3)
tensor([ 3.,  3.,  3.])
&gt;&gt;&gt; x
tensor([[ 1.,  1.,  1.],
        [ 2.,  2.,  2.],
        [ 3.,  3.,  3.]])
&gt;&gt;&gt; torch.renorm(x, 1, 0, 5)
tensor([[ 1.0000,  1.0000,  1.0000],
        [ 1.6667,  1.6667,  1.6667],
        [ 1.6667,  1.6667,  1.6667]])
</code></pre><p><code>torch.``repeat_interleave</code>()</p>
<p><code>torch.``repeat_interleave</code>( <em>input</em> , <em>repeats</em> , <em>dim=None</em> ) &#x2192; Tensor</p>
<p>&#x91CD;&#x590D;&#x5F20;&#x91CF;&#x5143;&#x7D20;&#x3002;</p>
<p>Warning</p>
<p>&#x8FD9;&#x662F;&#x4ECE;<code>torch.repeat&#xFF08;&#xFF09;</code>&#x4E0D;&#x540C;&#x4F46;&#x7C7B;&#x4F3C;&#x4E8E; numpy.repeat &#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x8F93;&#x5165;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>&#x91CD;&#x590D;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a> <em>&#x6216;</em> <a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x91CD;&#x590D;&#x7528;&#x4E8E;&#x6BCF;&#x4E2A;&#x5143;&#x4EF6;&#x7684;&#x6570;&#x76EE;&#x3002;&#x91CD;&#x590D;&#x5E7F;&#x64AD;&#x5230;&#x7B26;&#x5408;&#x7ED9;&#x5B9A;&#x7684;&#x8F74;&#x7684;&#x5F62;&#x72B6;&#x3002;</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x6CBF;&#x5176;&#x4EE5;&#x91CD;&#x590D;&#x7684;&#x503C;&#x7684;&#x5C3A;&#x5BF8;&#x3002;&#x7F3A;&#x7701;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x4F7F;&#x7528;&#x538B;&#x5E73;&#x8F93;&#x5165;&#x9635;&#x5217;&#xFF0C;&#x5E76;&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x5E73;&#x5766;&#x7684;&#x8F93;&#x51FA;&#x9635;&#x5217;&#x3002;</p>
</li>
</ul>
<p>Returns</p>
<p>Repeated tensor which has the same shape as input, except along the</p>
<p>&#x5B9A;&#x8F74;&#x3002;</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.tensor([1, 2, 3])
&gt;&gt;&gt; x.repeat_interleave(2)
tensor([1, 1, 2, 2, 3, 3])
&gt;&gt;&gt; y = torch.tensor([[1, 2], [3, 4]])
&gt;&gt;&gt; torch.repeat_interleave(y, 2)
tensor([1, 1, 2, 2, 3, 3, 4, 4])
&gt;&gt;&gt; torch.repeat_interleave(y, 3, dim=1)
tensor([[1, 1, 1, 2, 2, 2],
        [3, 3, 3, 4, 4, 4]])
&gt;&gt;&gt; torch.repeat_interleave(y, torch.tensor([1, 2]), dim=0)
tensor([[1, 2],
        [3, 4],
        [3, 4]])
</code></pre><p><code>torch.``repeat_interleave</code>( <em>repeats</em> ) &#x2192; Tensor</p>
<p>&#x5982;&#x679C;&#x91CD;&#x590D;&#x662F;&#x5F20;&#x91CF;&#xFF08;[N1&#xFF0C;N2&#xFF0C;N3&#xFF0C;...]&#xFF09;&#xFF0C;&#x90A3;&#x4E48;&#x8F93;&#x51FA;&#x5C06;&#x662F;&#x5F20;&#x91CF;&#xFF08;[0&#xFF0C;0&#xFF0C;...&#xFF0C;1&#xFF0C;1&#xFF0C; ...&#xFF0C;2,2&#xFF0C;...&#xFF0C;...] &#x5176;&#x4E2D; 0 &#x51FA;&#x73B0;&#xFF09; N1 &#xFF0C;
1 &#x51FA;&#x73B0;&#x500D; N 2 &#x6B21;&#xFF0C; 2 &#x51FA;&#x73B0; N3 &#x65F6;&#x95F4;&#x7B49;</p>
<p><code>torch.``roll</code>( <em>input</em> , <em>shifts</em> , <em>dims=None</em> ) &#x2192; Tensor</p>
<p>&#x4E00;&#x8D77;&#x6EDA;&#x52A8;&#x7ED9;&#x5B9A;&#x7684;&#x5C3A;&#x5BF8;&#xFF08;S&#xFF09;&#x7684;&#x5F20;&#x91CF;&#x3002;&#x5728;&#x7B2C;&#x4E00;&#x4F4D;&#x7F6E;&#x88AB;&#x79FB;&#x52A8;&#x8D85;&#x8FC7;&#x6700;&#x540E;&#x4E00;&#x4E2A;&#x4F4D;&#x7F6E;&#x5143;&#x7D20;&#x88AB;&#x91CD;&#x65B0;&#x5F15;&#x5165;&#x3002;&#x5982;&#x679C;&#x6CA1;&#x6709;&#x6307;&#x5B9A;&#x4E00;&#x4E2A;&#x5C3A;&#x5BF8;&#xFF0C;&#x5F20;&#x91CF;&#x5C06;&#x8F67;&#x5236;&#x524D;&#x7684;&#x88AB;&#x5E73;&#x5766;&#x5316;&#xFF0C;&#x7136;&#x540E;&#x6062;&#x590D;&#x5230;&#x539F;&#x6765;&#x7684;&#x5F62;&#x72B6;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>&#x79FB;&#x4F4D;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#x6216;</em> <em>&#x87D2;&#x7684;&#x5143;&#x7EC4;&#xFF1A;&#x6574;&#x6570;</em> &#xFF09; - &#x7684;&#x5730;&#x65B9;&#x6570;&#xFF0C;&#x901A;&#x8FC7;&#x8BE5;&#x5143;&#x4EF6;&#x5F20;&#x91CF;&#x79FB;&#x4F4D;&#x3002;&#x5982;&#x679C;&#x79FB;&#x4F4D;&#x662F;&#x4E00;&#x4E2A;&#x5143;&#x7EC4;&#xFF0C;DIMS&#x5FC5;&#x987B;&#x662F;&#x76F8;&#x540C;&#x5927;&#x5C0F;&#x7684;&#x5143;&#x7EC4;&#xFF0C;&#x5E76;&#x4E14;&#x6BCF;&#x4E2A;&#x7EF4;&#x5EA6;&#x5C06;&#x7531;&#x76F8;&#x5E94;&#x7684;&#x503C;&#x88AB;&#x5377;&#x8D77;</p>
</li>
<li><p><strong>&#x53D8;&#x6697;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#x6216;</em> <em>&#x87D2;&#x7684;&#x5143;&#x7EC4;&#xFF1A;&#x6574;&#x6570;</em> &#xFF09; - &#x8F74;&#x6CBF;&#x5176;&#x6EDA;&#x52A8;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8]).view(4, 2)
&gt;&gt;&gt; x
tensor([[1, 2],
        [3, 4],
        [5, 6],
        [7, 8]])
&gt;&gt;&gt; torch.roll(x, 1, 0)
tensor([[7, 8],
        [1, 2],
        [3, 4],
        [5, 6]])
&gt;&gt;&gt; torch.roll(x, -1, 0)
tensor([[3, 4],
        [5, 6],
        [7, 8],
        [1, 2]])
&gt;&gt;&gt; torch.roll(x, shifts=(2, 1), dims=(0, 1))
tensor([[6, 5],
        [8, 7],
        [2, 1],
        [4, 3]])
</code></pre><p><code>torch.``tensordot</code>( <em>a</em> , <em>b</em> , <em>dims=2</em>
)<a href="_modules/torch/functional.html#tensordot">[source]</a></p>
<p>&#x8FD4;&#x56DE;&#x7684;&#x6536;&#x7F29;&#x548C;b&#x5728;&#x591A;&#x4E2A;&#x7EF4;&#x5EA6;&#x3002;</p>
<p><code>tensordot</code>&#x5B9E;&#x73B0;&#x4E86;&#x4E00;&#x4E2A;&#x5E7F;&#x4E49;&#x77E9;&#x9635;&#x4E58;&#x79EF;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x4E00;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x5DE6;&#x5F20;&#x91CF;&#x6536;&#x7F29;</p>
</li>
<li><p><strong>B</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x53F3;&#x5F20;&#x91CF;&#x6536;&#x7F29;</p>
</li>
<li><p><strong>&#x53D8;&#x6697;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#x6216;</em> <em>&#x87D2;&#x4E24;&#x4E2A;&#x5217;&#x8868;&#x7684;&#x5143;&#x7EC4;&#xFF1A;&#x6574;&#x6570;</em> &#xFF09; - &#x7EF4;&#x6570;&#x6536;&#x7F29;&#x6216;&#x4E3A;<code>&#x4E00;</code>&#x548C;&#x5206;&#x522B;<code>b</code>&#x5C3A;&#x5BF8;&#x7684;&#x663E;&#x5F0F;&#x5217;&#x8868;</p>
</li>
</ul>
<p>&#x5F53;&#x4E0E;&#x4E00;&#x4E2A;&#x6574;&#x6570;&#x53C2;&#x6570;&#x79F0;&#x4E3A;<code>&#x53D8;&#x6697;</code>=  d  d  d  &#x548C;&#x6570;&#x91CF;&#x7684;&#x5C3A;&#x5BF8;<code>&#x4E00;</code>&#x548C;<code>b</code>&#x662F; M  M  M  &#x548C; n&#x7684; n&#x7684; n&#x7684; &#xFF0C;&#x5206;&#x522B;&#xFF0C;&#x5B83;&#x8BA1;&#x7B97;</p>
<p>ri0,...,im&#x2212;d,id,...,in=&#x2211;k0,...,kd&#x2212;1ai0,...,im&#x2212;d,k0,...,kd&#x2212;1&#xD7;bk0,...,kd&#x2212;1,id,...,in.r<em>{i_0,...,i</em>{m-d},
i<em>d,...,i_n} = \sum</em>{k<em>0,...,k</em>{d-1}} a<em>{i_0,...,i</em>{m-d},k<em>0,...,k</em>{d-1}}
\times b<em>{k_0,...,k</em>{d-1}, i_d,...,i_n}.
ri0&#x200B;,...,im&#x2212;d&#x200B;,id&#x200B;,...,in&#x200B;&#x200B;=k0&#x200B;,...,kd&#x2212;1&#x200B;&#x2211;&#x200B;ai0&#x200B;,...,im&#x2212;d&#x200B;,k0&#x200B;,...,kd&#x2212;1&#x200B;&#x200B;&#xD7;bk0&#x200B;,...,kd&#x2212;1&#x200B;,id&#x200B;,...,in&#x200B;&#x200B;.</p>
<p>&#x5F53;&#x4E0E;<code>&#x6240;&#x8C13;&#x53D8;&#x6697;</code>&#x5217;&#x8868;&#x7684;&#x5F62;&#x5F0F;&#xFF0C;&#x7ED9;&#x5B9A;&#x5C3A;&#x5BF8;&#x5C06;&#x53D6;&#x4EE3;&#x8FC7;&#x53BB;&#x7684; &#x627F;&#x5305; d  d  d  &#x7684;<code>&#x4E00;</code>&#x548C;&#x7B2C;&#x4E00; d  d  d  &#x7684; b  b  b
&#x3002;&#x5728;&#x8FD9;&#x4E9B;&#x7EF4;&#x5EA6;&#x7684;&#x5C3A;&#x5BF8;&#x5FC5;&#x987B;&#x5339;&#x914D;&#xFF0C;&#x4F46;&#x662F; <code>tensordot</code>&#x5C06;&#x5904;&#x7406;&#x5E7F;&#x64AD;&#x5C3A;&#x5BF8;&#x3002;</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; a = torch.arange(60.).reshape(3, 4, 5)
&gt;&gt;&gt; b = torch.arange(24.).reshape(4, 3, 2)
&gt;&gt;&gt; torch.tensordot(a, b, dims=([1, 0], [0, 1]))
tensor([[4400., 4730.],
        [4532., 4874.],
        [4664., 5018.],
        [4796., 5162.],
        [4928., 5306.]])

&gt;&gt;&gt; a = torch.randn(3, 4, 5, device=&apos;cuda&apos;)
&gt;&gt;&gt; b = torch.randn(4, 5, 6, device=&apos;cuda&apos;)
&gt;&gt;&gt; c = torch.tensordot(a, b, dims=2).cpu()
tensor([[ 8.3504, -2.5436,  6.2922,  2.7556, -1.0732,  3.2741],
        [ 3.3161,  0.0704,  5.0187, -0.4079, -4.3126,  4.8744],
        [ 0.8223,  3.9445,  3.2168, -0.2400,  3.4117,  1.7780]])
</code></pre><p><code>torch.``trace</code>( <em>input</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x5BF9;&#x89D2;&#x7EBF;&#x8F93;&#x5165;2-d&#x7684;&#x77E9;&#x9635;&#x7684;&#x5143;&#x7D20;&#x7684;&#x603B;&#x548C;&#x3002;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.arange(1., 10.).view(3, 3)
&gt;&gt;&gt; x
tensor([[ 1.,  2.,  3.],
        [ 4.,  5.,  6.],
        [ 7.,  8.,  9.]])
&gt;&gt;&gt; torch.trace(x)
tensor(15.)
</code></pre><p><code>torch.``tril</code>( <em>input</em> , <em>diagonal=0</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x77E9;&#x9635;&#xFF08;2- d&#x5F20;&#x91CF;&#xFF09;&#x6216;&#x5206;&#x6279;&#x77E9;&#x9635;<code>&#x8F93;&#x5165;&#x7684;&#x4E0B;&#x4E09;&#x89D2;&#x90E8;&#x5206;</code>&#xFF0C;&#x7ED3;&#x679C;&#x5F20;&#x91CF;<code>OUT &#x7684;&#x5176;&#x5B83;&#x5143;&#x4EF6;</code>&#x88AB;&#x8BBE;&#x7F6E;&#x4E3A;0&#x3002;</p>
<p>&#x77E9;&#x9635;&#x7684;&#x4E0B;&#x4E09;&#x89D2;&#x90E8;&#x5206;&#x5B9A;&#x4E49;&#x4E3A;&#x4E0A;&#x548C;&#x4E0B;&#x9762;&#x7684;&#x5BF9;&#x89D2;&#x7EBF;&#x7684;&#x5143;&#x7D20;&#x3002;</p>
<p>&#x7684;&#x53C2;&#x6570; <code>&#x5BF9;&#x89D2;&#x7EBF;</code>&#x63A7;&#x5236;&#x8003;&#x8651;&#x54EA;&#x4E9B;&#x5BF9;&#x89D2;&#x7EBF;&#x3002;&#x5982;&#x679C; <code>&#x5BF9;&#x89D2;&#x7EBF;</code>=
0&#xFF0C;&#x4E0A;&#x9762;&#x548C;&#x4E0B;&#x9762;&#x7684;&#x4E3B;&#x5BF9;&#x89D2;&#x7EBF;&#x7684;&#x6240;&#x6709;&#x5143;&#x7D20;&#x88AB;&#x4FDD;&#x7559;&#x3002;&#x6B63;&#x503C;&#x5305;&#x62EC;&#x6B63;&#x4E0A;&#x65B9;&#x7684;&#x4E3B;&#x5BF9;&#x89D2;&#x7EBF;&#x8BB8;&#x591A;&#x5BF9;&#x89D2;&#x7EBF;&#xFF0C;&#x5E76;&#x4E14;&#x7C7B;&#x4F3C;&#x5730;&#x8D1F;&#x503C;&#x6392;&#x9664;&#x6B63;&#x4E0B;&#x65B9;&#x7684;&#x4E3B;&#x5BF9;&#x89D2;&#x7EBF;&#x8BB8;&#x591A;&#x5BF9;&#x89D2;&#x7EBF;&#x3002;&#x4E3B;&#x5BF9;&#x89D2;&#x7EBF;&#x662F;&#x8BE5;&#x7EC4;&#x7684;&#x7D22;&#x5F15; { &#xFF08;
i&#x7684; i&#x7684; &#xFF09; }  \ lbrace&#xFF08;I&#xFF0C;i&#xFF09;&#x7684;\ rbrace  { &#xFF08; i&#x7684; &#xFF0C; i&#x7684; &#xFF09; }  &#x4E3A; i&#x7684; &#x2208; [ 0  &#xFF0C; &#x5206;&#x949F;HTG79] &#x2061; {
d  1  &#xFF0C; d  2  }  -  1  I \&#x5728;[0&#xFF0C;\&#x5206;&#x949F;\ {D<em> {1 }&#xFF0C;D</em> {2} \} - 1]  i&#x7684; &#x2208; [ 0  &#xFF0C;
&#x5206;&#x949F;HTG137]  { d  1  &#xFF0C; d  2  [H TG204]}  -  1  &#x5176;&#x4E2D; d  1  &#xFF0C; d  2  D<em> {1}&#xFF0C;D</em> {2}
d  1  &#x200B;&#x200B;  &#xFF0C; d  2  &#x662F;&#x77E9;&#x9635;&#x7684;&#x7EF4;&#x6570;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>diagonal</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>,</em> <em>optional</em> ) &#x2013; the diagonal to consider</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(3, 3)
&gt;&gt;&gt; a
tensor([[-1.0813, -0.8619,  0.7105],
        [ 0.0935,  0.1380,  2.2112],
        [-0.3409, -0.9828,  0.0289]])
&gt;&gt;&gt; torch.tril(a)
tensor([[-1.0813,  0.0000,  0.0000],
        [ 0.0935,  0.1380,  0.0000],
        [-0.3409, -0.9828,  0.0289]])

&gt;&gt;&gt; b = torch.randn(4, 6)
&gt;&gt;&gt; b
tensor([[ 1.2219,  0.5653, -0.2521, -0.2345,  1.2544,  0.3461],
        [ 0.4785, -0.4477,  0.6049,  0.6368,  0.8775,  0.7145],
        [ 1.1502,  3.2716, -1.1243, -0.5413,  0.3615,  0.6864],
        [-0.0614, -0.7344, -1.3164, -0.7648, -1.4024,  0.0978]])
&gt;&gt;&gt; torch.tril(b, diagonal=1)
tensor([[ 1.2219,  0.5653,  0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.4785, -0.4477,  0.6049,  0.0000,  0.0000,  0.0000],
        [ 1.1502,  3.2716, -1.1243, -0.5413,  0.0000,  0.0000],
        [-0.0614, -0.7344, -1.3164, -0.7648, -1.4024,  0.0000]])
&gt;&gt;&gt; torch.tril(b, diagonal=-1)
tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.4785,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
        [ 1.1502,  3.2716,  0.0000,  0.0000,  0.0000,  0.0000],
        [-0.0614, -0.7344, -1.3164,  0.0000,  0.0000,  0.0000]])
</code></pre><p><code>torch.``tril_indices</code>( <em>row</em> , <em>col</em> , <em>offset=0</em> , <em>dtype=torch.long</em> ,
<em>device=&apos;cpu&apos;</em> , <em>layout=torch.strided</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;<code>&#x884C;&#x7684;&#x4E0B;&#x4E09;&#x89D2;&#x90E8;&#x5206;&#x7684;&#x7D22;&#x5F15;</code>-by- <code>COL</code>&#x77E9;&#x9635;&#x5728;&#x4E00;&#x4E2A;2-N&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x4E2D;&#x7B2C;&#x4E00;&#x884C;&#x5305;&#x542B;&#x6240;&#x6709;&#x7D22;&#x5F15;&#x7684;&#x5217;&#x5750;&#x6807;&#x548C;&#x7B2C;&#x4E8C;&#x884C;&#x5305;&#x542B;&#x5217;&#x5750;&#x6807;&#x3002;&#x6307;&#x6570;&#x662F;&#x57FA;&#x4E8E;&#x884C;&#xFF0C;&#x7136;&#x540E;&#x5217;&#x6392;&#x5E8F;&#x3002;</p>
<p>The lower triangular part of the matrix is defined as the elements on and
below the diagonal.</p>
<p>&#x7684;&#x53C2;&#x6570;<code>&#x504F;&#x79FB;HTG2]</code>&#x63A7;&#x5236;&#x8003;&#x8651;&#x54EA;&#x4E9B;&#x5BF9;&#x89D2;&#x7EBF;&#x3002;&#x5982;&#x679C;<code>&#x504F;&#x79FB;HTG6]</code>=
0&#xFF0C;&#x4E0A;&#x9762;&#x548C;&#x4E0B;&#x9762;&#x7684;&#x4E3B;&#x5BF9;&#x89D2;&#x7EBF;&#x7684;&#x6240;&#x6709;&#x5143;&#x7D20;&#x88AB;&#x4FDD;&#x7559;&#x3002;&#x6B63;&#x503C;&#x5305;&#x62EC;&#x6B63;&#x4E0A;&#x65B9;&#x7684;&#x4E3B;&#x5BF9;&#x89D2;&#x7EBF;&#x8BB8;&#x591A;&#x5BF9;&#x89D2;&#x7EBF;&#xFF0C;&#x5E76;&#x4E14;&#x7C7B;&#x4F3C;&#x5730;&#x8D1F;&#x503C;&#x6392;&#x9664;&#x6B63;&#x4E0B;&#x65B9;&#x7684;&#x4E3B;&#x5BF9;&#x89D2;&#x7EBF;&#x8BB8;&#x591A;&#x5BF9;&#x89D2;&#x7EBF;&#x3002;&#x4E3B;&#x5BF9;&#x89D2;&#x7EBF;&#x662F;&#x8BE5;&#x7EC4;&#x7684;&#x7D22;&#x5F15; { &#xFF08;
i&#x7684; i&#x7684; &#xFF09; }  \ lbrace&#xFF08;I&#xFF0C;i&#xFF09;&#x7684;\ rbrace  { &#xFF08; i&#x7684; &#xFF0C; i&#x7684; &#xFF09; }  &#x4E3A; i&#x7684; &#x2208; [ 0  &#xFF0C; &#x5206;&#x949F;HTG75] &#x2061; {
d  1  &#xFF0C; d  2  }  -  1  I \&#x5728;[0&#xFF0C;\&#x5206;&#x949F;\ {D<em> {1 }&#xFF0C;{D</em> 2} \} - 1]  i&#x7684; &#x2208; [ 0  &#xFF0C; &#x5206;&#x949F; {
d  1  &#xFF0C; d  2  }  -  1  &#x5176;&#x4E2D; d  1  &#xFF0C; d  2  D<em> {1}&#xFF0C;D</em> {2}  d  1  &#x200B;&#x200B;  &#xFF0C; d  2
&#x662F;&#x77E9;&#x9635;&#x7684;&#x7EF4;&#x6570;&#x3002;</p>
<p>&#x6CE8;&#xFF1A;&#x5728; &apos;CUDA&apos; &#x8FD0;&#x884C;&#x65F6;&#xFF0C;&#x884C;* COL&#x5FC5;&#x987B;&#x4F4E;&#x4E8E; 2  59  2 ^ {59}  2  5  9  &#x53EF;&#x4EE5;&#x9632;&#x6B62;&#x8BA1;&#x7B97;&#x671F;&#x95F4;&#x7684;&#x6EA2;&#x51FA;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p>&#x5728;2 d&#x77E9;&#x9635;&#x4E2D;&#x7684;&#x884C;&#x6570; - <strong>&#x884C;</strong> &#xFF08;<code>INT</code>&#xFF09;&#x3002;</p>
</li>
<li><p><strong>COL</strong> &#xFF08;<code>INT</code>&#xFF09; - &#x5728;2-d&#x77E9;&#x9635;&#x7684;&#x5217;&#x6570;&#x3002;</p>
</li>
<li><p><strong>&#x504F;&#x79FB;HTG1]&#xFF08;<code>INT</code>&#xFF09; - &#x5BF9;&#x89D2;&#x7EBF;&#x4ECE;&#x6240;&#x8FF0;&#x4E3B;&#x5BF9;&#x89D2;&#x7EBF;&#x7684;&#x504F;&#x79FB;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;&#x5982;&#x679C;&#x6CA1;&#x6709;&#x63D0;&#x4F9B;&#xFF0C;0&#x3002;</strong></p>
</li>
<li><p><strong>DTYPE</strong> &#xFF08;<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"> <code>torch.dtype</code></a>&#xFF0C;&#x53EF;&#x9009;&#xFF09; - &#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x7684;&#x6240;&#x5E0C;&#x671B;&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;&#x5982;&#x679C;<code>&#x65E0;</code>&#xFF0C;<code>torch.long</code>&#x3002;</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <code>torch.set_default_tensor_type()</code>). <code>device</code>will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li><p><strong>&#x5E03;&#x5C40;</strong> &#xFF08;<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"> <code>torch.layout</code></a>&#xFF0C;&#x53EF;&#x9009;&#xFF09; - &#x76EE;&#x524D;&#x4EC5;&#x652F;&#x6301;<code>torch.strided</code>&#x3002;</p>
</li>
</ul>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; a = torch.tril_indices(3, 3)
&gt;&gt;&gt; a
tensor([[0, 1, 1, 2, 2, 2],
        [0, 0, 1, 0, 1, 2]])



&gt;&gt;&gt; a = torch.tril_indices(4, 3, -1)
&gt;&gt;&gt; a
tensor([[1, 2, 2, 3, 3, 3],
        [0, 0, 1, 0, 1, 2]])



&gt;&gt;&gt; a = torch.tril_indices(4, 3, 1)
&gt;&gt;&gt; a
tensor([[0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3],
        [0, 1, 0, 1, 2, 0, 1, 2, 0, 1, 2]])
</code></pre><p><code>torch.``triu</code>( <em>input</em> , <em>diagonal=0</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x77E9;&#x9635;&#xFF08;2- d&#x5F20;&#x91CF;&#xFF09;&#x6216;&#x5206;&#x6279;&#x77E9;&#x9635;<code>&#x8F93;&#x5165;&#x7684;&#x4E0A;&#x4E09;&#x89D2;&#x90E8;&#x5206;</code>&#xFF0C;&#x7ED3;&#x679C;&#x5F20;&#x91CF;<code>OUT &#x7684;&#x5176;&#x5B83;&#x5143;&#x4EF6;</code>&#x88AB;&#x8BBE;&#x7F6E;&#x4E3A;0&#x3002;</p>
<p>&#x77E9;&#x9635;&#x7684;&#x4E0A;&#x4E09;&#x89D2;&#x90E8;&#x5206;&#x5B9A;&#x4E49;&#x4E3A;&#x4E0A;&#x548C;&#x4E0A;&#x9762;&#x7684;&#x5BF9;&#x89D2;&#x7EBF;&#x7684;&#x5143;&#x7D20;&#x3002;</p>
<p>&#x7684;&#x53C2;&#x6570; <code>&#x5BF9;&#x89D2;&#x7EBF;</code>&#x63A7;&#x5236;&#x8003;&#x8651;&#x54EA;&#x4E9B;&#x5BF9;&#x89D2;&#x7EBF;&#x3002;&#x5982;&#x679C; <code>&#x5BF9;&#x89D2;&#x7EBF;</code>=
0&#xFF0C;&#x4E0A;&#x9762;&#x548C;&#x4E0B;&#x9762;&#x7684;&#x4E3B;&#x5BF9;&#x89D2;&#x7EBF;&#x7684;&#x6240;&#x6709;&#x5143;&#x7D20;&#x88AB;&#x4FDD;&#x7559;&#x3002;&#x6B63;&#x503C;&#x6392;&#x9664;&#x540C;&#x6837;&#x591A;&#x7684;&#x5BF9;&#x89D2;&#x7EBF;&#x4E0A;&#x65B9;&#x7684;&#x4E3B;&#x5BF9;&#x89D2;&#x7EBF;&#xFF0C;&#x5E76;&#x4E14;&#x7C7B;&#x4F3C;&#x5730;&#x8D1F;&#x503C;&#x5305;&#x62EC;&#x6B63;&#x4E0B;&#x65B9;&#x7684;&#x4E3B;&#x5BF9;&#x89D2;&#x7EBF;&#x8BB8;&#x591A;&#x5BF9;&#x89D2;&#x7EBF;&#x3002;&#x4E3B;&#x5BF9;&#x89D2;&#x7EBF;&#x662F;&#x8BE5;&#x7EC4;&#x7684;&#x7D22;&#x5F15; { &#xFF08;
i&#x7684; i&#x7684; &#xFF09; }  \ lbrace&#xFF08;I&#xFF0C;i&#xFF09;&#x7684;\ rbrace  { &#xFF08; i&#x7684; &#xFF0C; i&#x7684; &#xFF09; }  &#x4E3A; i&#x7684; &#x2208; [ 0  &#xFF0C; &#x5206;&#x949F;HTG79] &#x2061; {
d  1  &#xFF0C; d  2  }  -  1  I \&#x5728;[0&#xFF0C;\&#x5206;&#x949F;\ {D<em> {1 }&#xFF0C;D</em> {2} \} - 1]  i&#x7684; &#x2208; [ 0  &#xFF0C;
&#x5206;&#x949F;HTG137]  { d  1  &#xFF0C; d  2  [H TG204]}  -  1  &#x5176;&#x4E2D; d  1  &#xFF0C; d  2  D<em> {1}&#xFF0C;D</em> {2}
d  1  &#x200B;&#x200B;  &#xFF0C; d  2  &#x662F;&#x77E9;&#x9635;&#x7684;&#x7EF4;&#x6570;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>diagonal</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a> <em>,</em> <em>optional</em> ) &#x2013; the diagonal to consider</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(3, 3)
&gt;&gt;&gt; a
tensor([[ 0.2309,  0.5207,  2.0049],
        [ 0.2072, -1.0680,  0.6602],
        [ 0.3480, -0.5211, -0.4573]])
&gt;&gt;&gt; torch.triu(a)
tensor([[ 0.2309,  0.5207,  2.0049],
        [ 0.0000, -1.0680,  0.6602],
        [ 0.0000,  0.0000, -0.4573]])
&gt;&gt;&gt; torch.triu(a, diagonal=1)
tensor([[ 0.0000,  0.5207,  2.0049],
        [ 0.0000,  0.0000,  0.6602],
        [ 0.0000,  0.0000,  0.0000]])
&gt;&gt;&gt; torch.triu(a, diagonal=-1)
tensor([[ 0.2309,  0.5207,  2.0049],
        [ 0.2072, -1.0680,  0.6602],
        [ 0.0000, -0.5211, -0.4573]])

&gt;&gt;&gt; b = torch.randn(4, 6)
&gt;&gt;&gt; b
tensor([[ 0.5876, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],
        [-0.2447,  0.9556, -1.2919,  1.3378, -0.1768, -1.0857],
        [ 0.4333,  0.3146,  0.6576, -1.0432,  0.9348, -0.4410],
        [-0.9888,  1.0679, -1.3337, -1.6556,  0.4798,  0.2830]])
&gt;&gt;&gt; torch.triu(b, diagonal=1)
tensor([[ 0.0000, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],
        [ 0.0000,  0.0000, -1.2919,  1.3378, -0.1768, -1.0857],
        [ 0.0000,  0.0000,  0.0000, -1.0432,  0.9348, -0.4410],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.4798,  0.2830]])
&gt;&gt;&gt; torch.triu(b, diagonal=-1)
tensor([[ 0.5876, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],
        [-0.2447,  0.9556, -1.2919,  1.3378, -0.1768, -1.0857],
        [ 0.0000,  0.3146,  0.6576, -1.0432,  0.9348, -0.4410],
        [ 0.0000,  0.0000, -1.3337, -1.6556,  0.4798,  0.2830]])
</code></pre><p><code>torch.``triu_indices</code>( <em>row</em> , <em>col</em> , <em>offset=0</em> , <em>dtype=torch.long</em> ,
<em>device=&apos;cpu&apos;</em> , <em>layout=torch.strided</em> ) &#x2192; Tensor</p>
<p>&#x901A;&#x8FC7;<code>&#x8FD4;&#x56DE;</code>&#x884C; <code>&#x7684;&#x4E0A;&#x4E09;&#x89D2;&#x90E8;&#x5206;&#x7684;&#x7D22;&#x5F15;COL</code>&#x77E9;&#x9635;&#x5728;&#x4E00;&#x4E2A;2-N&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x4E2D;&#xFF0C;&#x6240;&#x8FF0;&#x7B2C;&#x4E00;&#x884C;&#x5305;&#x542B;&#x6240;&#x6709;&#x7D22;&#x5F15;&#x7684;&#x5217;&#x5750;&#x6807;&#x548C;&#x7B2C;&#x4E8C;&#x884C;&#x5305;&#x542B;&#x5217;&#x5750;&#x6807;&#x3002;&#x6307;&#x6570;&#x662F;&#x57FA;&#x4E8E;&#x884C;&#xFF0C;&#x7136;&#x540E;&#x5217;&#x6392;&#x5E8F;&#x3002;</p>
<p>The upper triangular part of the matrix is defined as the elements on and
above the diagonal.</p>
<p>&#x7684;&#x53C2;&#x6570;<code>&#x504F;&#x79FB;HTG2]</code>&#x63A7;&#x5236;&#x8003;&#x8651;&#x54EA;&#x4E9B;&#x5BF9;&#x89D2;&#x7EBF;&#x3002;&#x5982;&#x679C;<code>&#x504F;&#x79FB;HTG6]</code>=
0&#xFF0C;&#x4E0A;&#x548C;&#x4E0A;&#x65B9;&#x7684;&#x4E3B;&#x5BF9;&#x89D2;&#x7EBF;&#x7684;&#x6240;&#x6709;&#x5143;&#x7D20;&#x88AB;&#x4FDD;&#x7559;&#x3002;&#x6B63;&#x503C;&#x6392;&#x9664;&#x540C;&#x6837;&#x591A;&#x7684;&#x5BF9;&#x89D2;&#x7EBF;&#x4E0A;&#x65B9;&#x7684;&#x4E3B;&#x5BF9;&#x89D2;&#x7EBF;&#xFF0C;&#x5E76;&#x4E14;&#x7C7B;&#x4F3C;&#x5730;&#x8D1F;&#x503C;&#x5305;&#x62EC;&#x6B63;&#x4E0B;&#x65B9;&#x7684;&#x4E3B;&#x5BF9;&#x89D2;&#x7EBF;&#x8BB8;&#x591A;&#x5BF9;&#x89D2;&#x7EBF;&#x3002;&#x4E3B;&#x5BF9;&#x89D2;&#x7EBF;&#x662F;&#x8BE5;&#x7EC4;&#x7684;&#x7D22;&#x5F15; { &#xFF08;
i&#x7684; i&#x7684; &#xFF09; }  \ lbrace&#xFF08;I&#xFF0C;i&#xFF09;&#x7684;\ rbrace  { &#xFF08; i&#x7684; &#xFF0C; i&#x7684; &#xFF09; }  &#x4E3A; i&#x7684; &#x2208; [ 0  &#xFF0C; &#x5206;&#x949F;HTG75] &#x2061; {
d  1  &#xFF0C; d  2  }  -  1  I \&#x5728;[0&#xFF0C;\&#x5206;&#x949F;\ {D<em> {1 }&#xFF0C;{D</em> 2} \} - 1]  i&#x7684; &#x2208; [ 0  &#xFF0C; &#x5206;&#x949F; {
d  1  &#xFF0C; d  2  }  -  1  &#x5176;&#x4E2D; d  1  &#xFF0C; d  2  D<em> {1}&#xFF0C;D</em> {2}  d  1  &#x200B;&#x200B;  &#xFF0C; d  2
&#x662F;&#x77E9;&#x9635;&#x7684;&#x7EF4;&#x6570;&#x3002;</p>
<p>NOTE: when running on &#x2018;cuda&#x2019;, row * col must be less than 2592^{59}259 to
prevent overflow during calculation.</p>
<p>Parameters</p>
<ul>
<li><p><strong>row</strong> (<code>int</code>) &#x2013; number of rows in the 2-D matrix.</p>
</li>
<li><p><strong>col</strong> (<code>int</code>) &#x2013; number of columns in the 2-D matrix.</p>
</li>
<li><p><strong>offset</strong> (<code>int</code>) &#x2013; diagonal offset from the main diagonal. Default: if not provided, 0.</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned tensor. Default: if <code>None</code>, <code>torch.long</code>.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <code>torch.set_default_tensor_type()</code>). <code>device</code>will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li><p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; currently only support <code>torch.strided</code>.</p>
</li>
</ul>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; a = torch.triu_indices(3, 3)
&gt;&gt;&gt; a
tensor([[0, 0, 0, 1, 1, 2],
        [0, 1, 2, 1, 2, 2]])



&gt;&gt;&gt; a = torch.triu_indices(4, 3, -1)
&gt;&gt;&gt; a
tensor([[0, 0, 0, 1, 1, 1, 2, 2, 3],
        [0, 1, 2, 0, 1, 2, 1, 2, 2]])



&gt;&gt;&gt; a = torch.triu_indices(4, 3, 1)
&gt;&gt;&gt; a
tensor([[0, 0, 1],
        [1, 2, 2]])
</code></pre><h3 id="blas&#x548C;lapack&#x64CD;&#x4F5C;">BLAS&#x548C;LAPACK&#x64CD;&#x4F5C;</h3>
<p><code>torch.``addbmm</code>( <em>beta=1</em> , <em>input</em> , <em>alpha=1</em> , <em>batch1</em> , <em>batch2</em> ,
<em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x5BF9;&#x5B58;&#x50A8;&#x5728;<code>BATCH1</code>&#x548C;<code>batch2</code>&#xFF0C;&#x5177;&#x6709;&#x964D;&#x4F4E;&#x7684;&#x9644;&#x52A0;&#x6B65;&#x9AA4;&#xFF08;&#x6240;&#x6709;&#x7684;&#x77E9;&#x9635;&#x4E58;&#x6CD5;&#x5F97;&#x5230;&#x79EF;&#x7D2F;&#x77E9;&#x9635;&#x7684;&#x6279;&#x6B21;&#x77E9;&#x9635;&#x77E9;&#x9635;&#x79EF;&#x6CBF;&#x7740;&#x7B2C;&#x4E00;&#x7EF4;&#x5EA6;&#xFF09;&#x3002; <code>&#x8F93;&#x5165;</code>&#x52A0;&#x5230;&#x6700;&#x7EC8;&#x7ED3;&#x679C;&#x3002;</p>
<p><code>BATCH1</code>&#x548C;<code>batch2</code>&#x5FC5;&#x987B;&#x5404;&#x81EA;&#x542B;&#x6709;&#x76F8;&#x540C;&#x6570;&#x91CF;&#x7684;&#x77E9;&#x9635;&#x7684;3-d&#x5F20;&#x91CF;&#x3002;</p>
<p>&#x5982;&#x679C;<code>BATCH1</code>&#x662F; &#xFF08; B  &#xD7; n&#x7684; &#xD7; M  &#xFF09; &#xFF08;b \ n&#x6B21;\&#x4E58;&#x4EE5;m&#xFF09; &#xFF08; b  &#xD7; n&#x7684; &#xD7; M  &#xFF09; &#x5F20;&#x91CF;&#xFF0C;<code>batch2</code>&#x662F;
&#xFF08; b  &#xD7; M  &#xD7; p  &#xFF09; &#xFF08;b \&#x500D;&#x7C73;\&#x500D;p&#xFF09; [HT G100]  &#xFF08; B  &#xD7; M  &#xD7; p  &#xFF09; &#x5F20;&#x91CF;&#xFF0C;<code>&#x8F93;&#x5165;</code>&#x5FC5;&#x987B;<a href="notes/broadcasting.html#broadcasting-semantics">
broadcastable  </a>&#x4E0E; &#xFF08; n&#x7684; &#xD7; p  &#xFF09;
&#xFF08;N \&#x500D;p&#xFF09; &#xFF08; n&#x7684; &#xD7; p  &#xFF09; &#x51E0;&#x5341;&#x6216;&#x5E76;<code>OUT</code>&#x5C06;&#x662F; &#xFF08; n&#x7684; &#xD7; p  &#xFF09; &#xFF08;N \&#x500D;p&#xFF09; &#xFF08; n&#x7684; &#xD7; p  &#xFF09; &#x5F20;&#x91CF;&#x3002;</p>
<p>out=&#x3B2; input+&#x3B1; (&#x2211;i=0b&#x2212;1batch1i@batch2i)out = \beta\ \text{input} + \alpha\
(\sum_{i=0}^{b-1} \text{batch1}_i \mathbin{@} \text{batch2}_i) out=&#x3B2; input+&#x3B1;
(i=0&#x2211;b&#x2212;1&#x200B;batch1i&#x200B;@batch2i&#x200B;)</p>
<p>&#x5BF9;&#x4E8E;&#x7C7B;&#x578B;&#x7684;&#x8F93;&#x5165; FloatTensor &#x6216; DoubleTensor &#xFF0C;&#x81EA;&#x53D8;&#x91CF;<code>&#x7684;&#x3B2;</code>&#x548C;<code>&#x963F;&#x5C14;&#x6CD5;</code>&#x5FC5;&#x987B;&#x5B9E;&#x6570;&#xFF0C;&#x5426;&#x5219;&#x4ED6;&#x4EEC;&#x5E94;&#x8BE5;&#x662F;&#x6574;&#x6570;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x7684;&#x3B2;</strong> &#xFF08; <em>&#x53F7;&#x7801;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x4E58;&#x6570;<code>&#x8F93;&#x5165;</code>&#xFF08; &#x3B2; \&#x7684;&#x3B2; &#x3B2; &#xFF09;</p>
</li>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x8981;&#x6DFB;&#x52A0;&#x77E9;&#x9635;</p>
</li>
<li><p><strong>&#x963F;&#x5C14;&#x6CD5;</strong> &#xFF08; <em>&#x53F7;&#x7801;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x4E58;&#x6570; BATCH1 @ batch2 &#xFF08; &#x3B1; \&#x963F;&#x5C14;&#x6CD5; &#x3B1; &#xFF09;</p>
</li>
<li><p><strong>BATCH1</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x7B2C;&#x4E00;&#x6279;&#x77E9;&#x9635;&#x7684;&#x76F8;&#x4E58;</p>
</li>
<li><p><strong>batch2</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x7B2C;&#x4E8C;&#x6279;&#x77E9;&#x9635;&#x7684;&#x76F8;&#x4E58;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; M = torch.randn(3, 5)
&gt;&gt;&gt; batch1 = torch.randn(10, 3, 4)
&gt;&gt;&gt; batch2 = torch.randn(10, 4, 5)
&gt;&gt;&gt; torch.addbmm(M, batch1, batch2)
tensor([[  6.6311,   0.0503,   6.9768, -12.0362,  -2.1653],
        [ -4.8185,  -1.4255,  -6.6760,   8.9453,   2.5743],
        [ -3.8202,   4.3691,   1.0943,  -1.1109,   5.4730]])
</code></pre><p><code>torch.``addmm</code>( <em>beta=1</em> , <em>input</em> , <em>alpha=1</em> , <em>mat1</em> , <em>mat2</em> , <em>out=None</em>
) &#x2192; Tensor</p>
<p>&#x6267;&#x884C;&#x77E9;&#x9635;<code>MAT1</code>&#x548C;<code>MAT2</code>&#x7684;&#x77E9;&#x9635;&#x4E58;&#x6CD5;&#x3002;&#x77E9;&#x9635;<code>&#x8F93;&#x5165;</code>&#x52A0;&#x5230;&#x6700;&#x7EC8;&#x7ED3;&#x679C;&#x3002;</p>
<p>&#x5982;&#x679C;<code>MAT1</code>&#x662F; &#xFF08; n&#x7684; &#xD7; M  &#xFF09; &#xFF08;N \&#x4E58;&#x4EE5;m&#xFF09; &#xFF08; n&#x7684; &#xD7; M  &#xFF09; &#x5F20;&#x91CF;&#xFF0C;<code>MAT2</code>&#x662F; &#xFF08; M  &#xD7; p  &#xFF09; &#xFF08;M
\&#x500D;p&#xFF09; &#xFF08; M  &#xD7; p  &#xFF09; &#x5F20;&#x91CF;&#xFF0C;&#x7136;&#x540E;<code>&#x8F93;&#x5165;</code>&#x5FC5;&#x987B;<a href="notes/broadcasting.html#broadcasting-semantics"> broadcastable
</a>&#x4E0E; &#xFF08; n&#x7684; &#xD7; p  &#xFF09; &#xFF08;N \&#x500D;p &#xFF09; &#xFF08; n&#x7684; &#xD7;
p  &#xFF09; &#x5F20;&#x91CF;&#x548C;<code>OUT</code>&#x5C06;&#x662F; &#xFF08; n&#x7684; &#xD7;  p  &#xFF09; &#xFF08;N \&#x500D;p&#xFF09; &#xFF08; n&#x7684; &#xD7; P  &#xFF09; &#x5F20;&#x91CF;&#x3002;</p>
<p><code>&#x963F;&#x5C14;&#x6CD5;</code>&#x548C;<code>&#x7684;&#x3B2;</code>&#x7684;&#x4E4B;&#x95F4;<code>MAT1</code>&#x548C;&#x4E0A;&#x7684;&#x77E9;&#x9635;&#x77E2;&#x91CF;&#x4E58;&#x79EF;&#x7684;&#x7F29;&#x653E;&#x56E0;&#x5B50;<code>MAT2</code>&#x548C;&#x5206;&#x522B;&#x6DFB;&#x52A0;&#x7684;&#x77E9;&#x9635;<code>&#x8F93;&#x5165;</code>&#x3002;</p>
<p>out=&#x3B2; input+&#x3B1; (mat1i@mat2i)\text{out} = \beta\ \text{input} + \alpha\
(\text{mat1}_i \mathbin{@} \text{mat2}_i) out=&#x3B2; input+&#x3B1; (mat1i&#x200B;@mat2i&#x200B;)</p>
<p>&#x5BF9;&#x4E8E;&#x7C7B;&#x578B;&#x7684;&#x8F93;&#x5165; FloatTensor &#x6216; DoubleTensor &#xFF0C;&#x81EA;&#x53D8;&#x91CF;<code>&#x7684;&#x3B2;</code>&#x548C;<code>&#x963F;&#x5C14;&#x6CD5;</code>&#x5FC5;&#x987B;&#x5B9E;&#x6570;&#xFF0C;&#x5426;&#x5219;&#x4ED6;&#x4EEC;&#x5E94;&#x8BE5;&#x662F;&#x6574;&#x6570;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>beta</strong> ( <em>Number</em> <em>,</em> <em>optional</em> ) &#x2013; multiplier for <code>input</code>(&#x3B2;\beta&#x3B2; )</p>
</li>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; matrix to be added</p>
</li>
<li><p><strong>&#x963F;&#x5C14;&#x6CD5;</strong> &#xFF08; <em>&#x53F7;&#x7801;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x4E58;&#x6570; M  &#x4E00; T  1  @  M  &#x4E00; T  2  MAT1 @ MAT2  M  &#x4E00; T  1  @  M  &#x4E00; &#x5428; 2  &#xFF08; &#x3B1; \&#x963F;&#x5C14;&#x6CD5; &#x3B1; &#xFF09;</p>
</li>
<li><p><strong>MAT1</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x8981;&#x88AB;&#x76F8;&#x4E58;&#x7684;&#x7B2C;&#x4E00;&#x77E9;&#x9635;</p>
</li>
<li><p><strong>MAT2</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x7B2C;&#x4E8C;&#x77E9;&#x9635;&#x76F8;&#x4E58;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; M = torch.randn(2, 3)
&gt;&gt;&gt; mat1 = torch.randn(2, 3)
&gt;&gt;&gt; mat2 = torch.randn(3, 3)
&gt;&gt;&gt; torch.addmm(M, mat1, mat2)
tensor([[-4.8716,  1.4671, -1.3746],
        [ 0.7573, -3.9555, -2.8681]])
</code></pre><p><code>torch.``addmv</code>( <em>beta=1</em> , <em>input</em> , <em>alpha=1</em> , <em>mat</em> , <em>vec</em> , <em>out=None</em> )
&#x2192; Tensor</p>
<p>&#x6267;&#x884C;&#x77E9;&#x9635;<code>&#x57AB;</code>&#x548C;&#x7684;&#x77E9;&#x9635;&#x77E2;&#x91CF;&#x4E58;&#x79EF;&#x77E2;&#x91CF;<code>VEC</code>&#x3002;&#x77E2;&#x91CF;<code>&#x8F93;&#x5165;</code>&#x52A0;&#x5230;&#x6700;&#x7EC8;&#x7ED3;&#x679C;&#x3002;</p>
<p>&#x5982;&#x679C;<code>&#x57AB;</code>&#x662F; &#xFF08; n&#x7684; &#xD7; M  &#xFF09; &#xFF08;N \&#x4E58;&#x4EE5;m&#xFF09; &#xFF08; n&#x7684; &#xD7; M  &#xFF09; &#x5F20;&#x91CF;&#xFF0C;<code>VEC</code>&#x662F;&#x5927;&#x5C0F;[1-d&#x5F20;&#x91CF;HTG56] M &#xFF0C;&#x7136;&#x540E;<code>&#x8F93;&#x5165;</code>&#x5FC5;&#x987B;<a href="notes/broadcasting.html#broadcasting-semantics"> broadcastable  </a>&#x4E0E;&#x5927;&#x5C0F;&#x7684;1-d&#x5F20;&#x91CF;
n&#x7684;&#x548C;<code>OUT</code>&#x5C06;&#x5927;&#x5C0F; n&#x7684; 1- d&#x5F20;&#x91CF;&#x3002;</p>
<p><code>&#x963F;&#x5C14;&#x6CD5;</code>&#x548C;<code>&#x7684;&#x3B2;</code>&#x662F;&#x6BD4;&#x4F8B;&#x4E0A;&#x7684;&#x77E9;&#x9635;&#x77E2;&#x91CF;&#x4E58;&#x79EF;&#x56E0;&#x5B50;&#x4E4B;&#x95F4;<code>&#x57AB;</code>&#x548C;<code>VEC</code>&#x548C;&#x5206;&#x522B;&#x6DFB;&#x52A0;&#x7684;&#x5F20;&#x91CF;<code>&#x8F93;&#x5165;</code>&#x3002;</p>
<p>out=&#x3B2; input+&#x3B1; (mat@vec)\text{out} = \beta\ \text{input} + \alpha\ (\text{mat}
\mathbin{@} \text{vec}) out=&#x3B2; input+&#x3B1; (mat@vec)</p>
<p>&#x5BF9;&#x4E8E;&#x7C7B;&#x578B;&#x7684;&#x8F93;&#x5165; FloatTensor &#x6216; DoubleTensor &#xFF0C;&#x81EA;&#x53D8;&#x91CF;<code>&#x7684;&#x3B2;</code>&#x548C;<code>&#x963F;&#x5C14;&#x6CD5;</code>&#x5FC5;&#x987B;&#x5B9E;&#x6570;&#xFF0C;&#x5426;&#x5219;&#x4ED6;&#x4EEC;&#x5E94;&#x8BE5;&#x662F;&#x6574;&#x6570;</p>
<p>Parameters</p>
<ul>
<li><p><strong>beta</strong> ( <em>Number</em> <em>,</em> <em>optional</em> ) &#x2013; multiplier for <code>input</code>(&#x3B2;\beta&#x3B2; )</p>
</li>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x8981;&#x6DFB;&#x52A0;&#x77E2;&#x91CF;</p>
</li>
<li><p><strong>&#x963F;&#x5C14;&#x6CD5;</strong> &#xFF08; <em>&#x53F7;&#x7801;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x4E58;&#x6570; M  &#x4E00; T  @  [HTG22&#x3011;v  E  C  &#x57AB;@ VEC  M  &#x4E00; T  @  [HTG46&#x3011;v  E  C  &#xFF08; &#x3B1; \&#x963F;&#x5C14;&#x6CD5; &#x3B1; &#xFF09;</p>
</li>
<li><p><strong>&#x57AB;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x77E9;&#x9635;&#x76F8;&#x4E58;</p>
</li>
<li><p><strong>VEC</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x5411;&#x91CF;&#x76F8;&#x4E58;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; M = torch.randn(2)
&gt;&gt;&gt; mat = torch.randn(2, 3)
&gt;&gt;&gt; vec = torch.randn(3)
&gt;&gt;&gt; torch.addmv(M, mat, vec)
tensor([-0.3768, -5.5565])
</code></pre><p><code>torch.``addr</code>( <em>beta=1</em> , <em>input</em> , <em>alpha=1</em> , <em>vec1</em> , <em>vec2</em> , <em>out=None</em>
) &#x2192; Tensor</p>
<p>&#x6267;&#x884C;&#x7684;&#x77E2;&#x91CF;<code>VEC1</code>&#x548C;<code>VEC2</code>&#x5916;&#x4EA7;&#x7269;&#xFF0C;&#x5E76;&#x5C06;&#x5176;&#x6DFB;&#x52A0;&#x5230;&#x77E9;&#x9635;<code>&#x8F93;&#x5165;</code>&#x3002;</p>
<p>&#x53EF;&#x9009;&#x503C;<code>&#x7684;&#x3B2;</code>&#x548C;<code>&#x963F;&#x5C14;&#x6CD5;</code>&#x662F;&#x5728;&#x95F4;<code>&#x5916;&#x79EF;&#x7F29;&#x653E;&#x56E0;&#x5B50;VEC1</code>&#x548C;<code>VEC2</code>&#x548C;&#x5206;&#x522B;&#x6DFB;&#x52A0;&#x7684;&#x77E9;&#x9635;<code>&#x8F93;&#x5165;</code>&#x3002;</p>
<p>out=&#x3B2; input+&#x3B1; (vec1&#x2297;vec2)\text{out} = \beta\ \text{input} + \alpha\
(\text{vec1} \otimes \text{vec2}) out=&#x3B2; input+&#x3B1; (vec1&#x2297;vec2)</p>
<p>&#x5982;&#x679C;<code>VEC1</code>&#x662F;&#x5927;&#x5C0F; n&#x7684;&#x7684;&#x5411;&#x91CF;&#x548C;<code>VEC2</code>&#x662F;&#x5927;&#x5C0F; M [&#x5411;&#x91CF;HTG11]&#xFF0C;&#x7136;&#x540E;<code>&#x8F93;&#x5165;</code>&#x5FC5;&#x987B;<a href="notes/broadcasting.html#broadcasting-semantics"> broadcastable
</a>&#x7528;&#x7684;&#x5927;&#x5C0F; [&#x57FA;&#x8D28;HTG24]  &#xFF08; n&#x7684; &#xD7; M  &#xFF09;
&#xFF08;N \&#x500D;&#x7C73;&#xFF09; &#xFF08; n&#x7684; &#xD7;  M  &#xFF09; &#x548C;<code>OUT</code>&#x5C06;&#x5927;&#x5C0F;&#x7684;&#x77E9;&#x9635; &#xFF08; n&#x7684; &#xD7; M  &#xFF09; &#xFF08;N \&#x4E58;&#x4EE5;m&#xFF09;[HTG9 0]  &#xFF08; n&#x7684; &#xD7; M  &#xFF09;
&#x3002;</p>
<p>For inputs of type FloatTensor or DoubleTensor, arguments <code>beta</code>and <code>alpha</code>
must be real numbers, otherwise they should be integers</p>
<p>Parameters</p>
<ul>
<li><p><strong>beta</strong> ( <em>Number</em> <em>,</em> <em>optional</em> ) &#x2013; multiplier for <code>input</code>(&#x3B2;\beta&#x3B2; )</p>
</li>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; matrix to be added</p>
</li>
<li><p><strong>&#x963F;&#x5C14;&#x6CD5;</strong> &#xFF08; <em>&#x53F7;&#x7801;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x4E58;&#x6570; VEC1  &#x2297; VEC2  \&#x6587;&#x672C;{VEC1} \ otimes \&#x6587;&#x672C;{VEC2}  VEC1  &#x2297; VEC2  &#xFF08; &#x3B1; \&#x963F;&#x5C14;&#x6CD5; &#x3B1; &#xFF09;</p>
</li>
<li><p><strong>VEC1</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x5916;&#x79EF;&#x7684;&#x7B2C;&#x4E00;&#x5411;&#x91CF;</p>
</li>
<li><p><strong>VEC2</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x5916;&#x79EF;&#x7684;&#x7B2C;&#x4E8C;&#x77E2;&#x91CF;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; vec1 = torch.arange(1., 4.)
&gt;&gt;&gt; vec2 = torch.arange(1., 3.)
&gt;&gt;&gt; M = torch.zeros(3, 2)
&gt;&gt;&gt; torch.addr(M, vec1, vec2)
tensor([[ 1.,  2.],
        [ 2.,  4.],
        [ 3.,  6.]])
</code></pre><p><code>torch.``baddbmm</code>( <em>beta=1</em> , <em>input</em> , <em>alpha=1</em> , <em>batch1</em> , <em>batch2</em> ,
<em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x6267;&#x884C;&#x77E9;&#x9635;&#x7684;&#x6279;&#x6B21;&#x77E9;&#x9635;&#x77E9;&#x9635;&#x4EA7;&#x7269;<code>BATCH1</code>&#x548C;<code>batch2</code>&#x3002; <code>&#x8F93;&#x5165;</code>&#x52A0;&#x5230;&#x6700;&#x7EC8;&#x7ED3;&#x679C;&#x3002;</p>
<p><code>BATCH1</code>&#x548C;<code>batch2</code>&#x5FC5;&#x987B;&#x5404;&#x81EA;&#x542B;&#x6709;&#x76F8;&#x540C;&#x6570;&#x91CF;&#x7684;&#x77E9;&#x9635;&#x7684;3-d&#x5F20;&#x91CF;&#x3002;</p>
<p>&#x5982;&#x679C;<code>BATCH1</code>&#x662F; &#xFF08; B  &#xD7; n&#x7684; &#xD7; M  &#xFF09; &#xFF08;b \ n&#x6B21;\&#x4E58;&#x4EE5;m&#xFF09; &#xFF08; b  &#xD7; n&#x7684; &#xD7; M  &#xFF09; &#x5F20;&#x91CF;&#xFF0C;<code>batch2</code>&#x662F;
&#xFF08; b  &#xD7; M  &#xD7; p  &#xFF09; &#xFF08;b \&#x500D;&#x7C73;\&#x500D;p&#xFF09; [HT G100]  &#xFF08; B  &#xD7; M  &#xD7; p  &#xFF09; &#x5F20;&#x91CF;&#xFF0C;&#x7136;&#x540E;<code>&#x8F93;&#x5165;</code>&#x5FC5;&#x987B;<a href="notes/broadcasting.html#broadcasting-semantics">
broadcastable  </a>&#x4E0E; &#xFF08; b  &#xD7; n&#x7684; &#xD7;
p  &#xFF09; &#xFF08;b \ n&#x6B21;\&#x500D;p&#xFF09; &#xFF08; b  &#xD7; n&#x7684; &#xD7; P  &#xFF09; &#x5F20;&#x91CF;&#x548C;<code>OUT</code>&#x5C06;&#x662F; &#xFF08; b  &#xD7; n&#x7684; &#xD7; p  &#xFF09; &#xFF08;b \ n&#x6B21;\&#x500D;p&#xFF09; &#xFF08;
b  &#xD7; n&#x7684; &#xD7; p  &#x200B;&#x200B;&#xFF09; &#x5F20;&#x91CF;&#x3002;&#x65E2;<code>&#x963F;&#x5C14;&#x6CD5;</code>&#x548C;<code>&#x7684;&#x3B2;</code>&#x610F;&#x5473;&#x7740;&#x76F8;&#x540C;&#x4E8E; <code>torch.addbmm&#x6240;&#x4F7F;&#x7528;&#x7684;&#x6BD4;&#x4F8B;&#x56E0;&#x5B50;&#xFF08; &#xFF09;</code>&#x3002;</p>
<p>outi=&#x3B2; inputi+&#x3B1; (batch1i@batch2i)\text{out}_i = \beta\ \text{input}_i +
\alpha\ (\text{batch1}_i \mathbin{@} \text{batch2}_i) outi&#x200B;=&#x3B2; inputi&#x200B;+&#x3B1;
(batch1i&#x200B;@batch2i&#x200B;)</p>
<p>For inputs of type FloatTensor or DoubleTensor, arguments <code>beta</code>and <code>alpha</code>
must be real numbers, otherwise they should be integers.</p>
<p>Parameters</p>
<ul>
<li><p><strong>beta</strong> ( <em>Number</em> <em>,</em> <em>optional</em> ) &#x2013; multiplier for <code>input</code>(&#x3B2;\beta&#x3B2; )</p>
</li>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the tensor to be added</p>
</li>
<li><p><strong>&#x963F;&#x5C14;&#x6CD5;</strong> &#xFF08; <em>&#x53F7;&#x7801;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x4E58;&#x6570; BATCH1  @  batch2  \&#x6587;&#x672C;{BATCH1} \ mathbin {@} \&#x6587;&#x672C;{batch2}  BATCH1  @  batch2  &#xFF08; &#x3B1; \&#x963F;&#x5C14;&#x6CD5; &#x3B1; &#xFF09;</p>
</li>
<li><p><strong>batch1</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the first batch of matrices to be multiplied</p>
</li>
<li><p><strong>batch2</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the second batch of matrices to be multiplied</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; M = torch.randn(10, 3, 5)
&gt;&gt;&gt; batch1 = torch.randn(10, 3, 4)
&gt;&gt;&gt; batch2 = torch.randn(10, 4, 5)
&gt;&gt;&gt; torch.baddbmm(M, batch1, batch2).size()
torch.Size([10, 3, 5])
</code></pre><p><code>torch.``bmm</code>( <em>input</em> , <em>mat2</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x5BF9;&#x5B58;&#x50A8;&#x5728;<code>&#x8F93;&#x5165;</code>&#x548C;<code>MAT2</code>&#x77E9;&#x9635;&#x7684;&#x6279;&#x6B21;&#x77E9;&#x9635;&#x77E9;&#x9635;&#x4E58;&#x79EF;&#x3002;</p>
<p><code>&#x8F93;&#x5165;</code>&#x548C;<code>MAT2</code>&#x5FC5;&#x987B;&#x5404;&#x81EA;&#x542B;&#x6709;&#x76F8;&#x540C;&#x6570;&#x91CF;&#x7684;&#x77E9;&#x9635;&#x7684;3-d&#x5F20;&#x91CF;&#x3002;</p>
<p>&#x5982;&#x679C;<code>&#x8F93;&#x5165;</code>&#x662F; &#xFF08; B  &#xD7; n&#x7684; &#xD7; M  &#xFF09; &#xFF08;b \ n&#x6B21;\&#x4E58;&#x4EE5;m&#xFF09; &#xFF08; b  &#xD7; n&#x7684; &#xD7; M  &#xFF09; &#x5F20;&#x91CF;&#xFF0C;<code>MAT2</code>&#x662F; &#xFF08; b  &#xD7;
M  &#xD7; p  &#xFF09; &#xFF08;b \&#x500D;&#x7C73;\&#x500D;p&#xFF09; [HTG10 0]  &#xFF08; B  &#xD7; M  &#xD7; p  &#xFF09; &#x5F20;&#x91CF;&#xFF0C;<code>OUT</code>&#x5C06;&#x662F; &#xFF08; b  &#xD7; n&#x7684; &#xD7; p
&#xFF09; &#xFF08;b \ n&#x6B21;\&#x500D;p&#xFF09; &#xFF08; b  &#xD7; n&#x7684; &#xD7;  [HTG19 3]  P  &#xFF09; &#x5F20;&#x91CF;&#x3002;</p>
<p>outi=inputi@mat2i\text{out}_i = \text{input}_i \mathbin{@} \text{mat2}_i
outi&#x200B;=inputi&#x200B;@mat2i&#x200B;</p>
<p>Note</p>
<p>&#x6B64;&#x529F;&#x80FD;&#x4E0D;&#x64AD;[ <a href="notes/broadcasting.html#broadcasting-semantics">HTG3&#x3002;&#x7528;&#x4E8E;&#x5E7F;&#x64AD;&#x57FA;&#x8D28;&#x7684;&#x4EA7;&#x54C1;&#xFF0C;&#x89C1;</a> <code>torch.matmul&#xFF08;&#xFF09;</code>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x77E9;&#x9635;&#x7684;&#x7B2C;&#x4E00;&#x6279;&#x8981;&#x88AB;&#x4E58;</p>
</li>
<li><p><strong>MAT2</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x7B2C;&#x4E8C;&#x6279;&#x77E9;&#x9635;&#x7684;&#x76F8;&#x4E58;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; input = torch.randn(10, 3, 4)
&gt;&gt;&gt; mat2 = torch.randn(10, 4, 5)
&gt;&gt;&gt; res = torch.bmm(input, mat2)
&gt;&gt;&gt; res.size()
torch.Size([10, 3, 5])
</code></pre><p><code>torch.``bitwise_not</code>( <em>input</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8BA1;&#x7B97;&#x7ED9;&#x5B9A;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x7684;&#x4F4D;NOT&#x3002;&#x8F93;&#x5165;&#x5FC5;&#x987B;&#x662F;&#x6574;&#x6570;&#x6216;&#x5E03;&#x5C14;&#x7C7B;&#x578B;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example</p>
<pre><code>&gt;&gt;&gt; torch.bitwise_not(torch.tensor([-1, -2, 3], dtype=torch.int8))
tensor([ 0,  1, -4], dtype=torch.int8)
</code></pre><p><code>torch.``chain_matmul</code>( <em>*matrices</em>
)<a href="_modules/torch/functional.html#chain_matmul">[source]</a></p>
<p>&#x8FD4;&#x56DE;&#x7684;&#x77E9;&#x9635;&#x79EF;&#x7684; N  N  N  2-d&#x5F20;&#x91CF;&#x3002;&#x6B64;&#x4EA7;&#x7269;&#x7528;&#x5176;&#x9009;&#x62E9;&#x5176;&#x4E2D;&#x62DB;&#x81F4;&#x7B97;&#x672F;&#x64CD;&#x4F5C;&#x65B9;&#x9762;&#x7684;&#x6210;&#x672C;&#x6700;&#x4F4E;&#x7684;&#xFF08;<a href="https://mitpress.mit.edu/books/introduction-algorithms-third-
edition" target="_blank"> [CLRS]
</a>&#xFF09;&#x7684;&#x987A;&#x5E8F;&#x8FDB;&#x884C;&#x77E9;&#x9635;&#x94FE;&#x987A;&#x5E8F;&#x7B97;&#x6CD5;&#x6709;&#x6548;&#x5730;&#x8BA1;&#x7B97;&#x3002;&#x6CE8;&#x610F;&#xFF0C;&#x7531;&#x4E8E;&#x8FD9;&#x662F;&#x4E00;&#x4E2A;&#x51FD;&#x6570;&#x6765;&#x8BA1;&#x7B97;&#x7684;&#x4EA7;&#x7269;&#xFF0C; N  N  N  &#x9700;&#x8981;&#x4E3A;&#x5927;&#x4E8E;&#x6216;&#x7B49;&#x4E8E;2
;&#x5982;&#x679C;&#x7B49;&#x4E8E;2&#xFF0C;&#x5219;&#x4E00;&#x4E2A;&#x7B80;&#x5355;&#x7684;&#x77E9;&#x9635;&#x77E9;&#x9635;&#x4EA7;&#x54C1;&#x9000;&#x8FD8;&#x3002;&#x5982;&#x679C; N  N  N  &#x4E3A;1&#xFF0C;&#x90A3;&#x4E48;&#x8FD9;&#x662F;&#x4E00;&#x4E2A;&#x65E0;&#x64CD;&#x4F5C; - &#x539F;&#x59CB;&#x77E9;&#x9635;&#x8FD4;&#x56DE;&#x539F;&#x6837;&#x3002;</p>
<p>Parameters</p>
<p><strong>&#x77E9;&#x9635;</strong> &#xFF08; <em>&#x5F20;&#x91CF;...</em> &#xFF09; - 2&#x4EE5;&#x4E0A;2-d&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x4EA7;&#x7269;&#x662F;&#x5F85;&#x786E;&#x5B9A;&#x7684;&#x5E8F;&#x5217;&#x3002;</p>
<p>Returns</p>
<p>&#x5982;&#x679C; i&#x7684; T  H  I ^ {&#x4E2A;}  i&#x7684; T  H  &#x5F20;&#x91CF;&#x662F;&#x5C3A;&#x5BF8; p  i&#x7684; &#xD7; p  i&#x7684; +  1  P<em> {I} \&#x500D;P</em> {I + 1}
p  i&#x7684; &#xD7; p  i&#x7684; +  1  &#xFF0C;&#x5219;&#x4EA7;&#x54C1;&#x5C06;&#x662F;&#x5C3A;&#x5BF8; p&#x7684; 1  &#xD7; p  N  +  1  P<em> {1} \ TI MES P</em> {N + 1}
P  1  &#xD7; p  N  +  1  &#x200B;&#x200B;  &#x3002;</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(3, 4)
&gt;&gt;&gt; b = torch.randn(4, 5)
&gt;&gt;&gt; c = torch.randn(5, 6)
&gt;&gt;&gt; d = torch.randn(6, 7)
&gt;&gt;&gt; torch.chain_matmul(a, b, c, d)
tensor([[ -2.3375,  -3.9790,  -4.1119,  -6.6577,   9.5609, -11.5095,  -3.2614],
        [ 21.4038,   3.3378,  -8.4982,  -5.2457, -10.2561,  -2.4684,   2.7163],
        [ -0.9647,  -5.8917,  -2.3213,  -5.2284,  12.8615, -12.2816,  -2.5095]])
</code></pre><p><code>torch.``cholesky</code>( <em>input</em> , <em>upper=False</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8BA1;&#x7B97;&#x5BF9;&#x79F0;&#x6B63;&#x5B9A;&#x77E9;&#x9635; A  A  [&#x7684;Cholesky&#x5206;&#x89E3;HTG12]  A  &#x6216;&#x5BF9;&#x79F0;&#x6B63;&#x5B9A;&#x77E9;&#x9635;&#x7684;&#x6279;&#x6B21;&#x3002;</p>
<p>&#x5982;&#x679C;<code>&#x4E0A;</code>&#x662F;<code>&#x771F;</code>&#xFF0C;&#x8FD4;&#x56DE;&#x7684;&#x77E9;&#x9635;<code>U</code>&#x662F;&#x4E0A;&#x4E09;&#x89D2;&#xFF0C;&#x548C;&#x5206;&#x89E3;&#x7684;&#x5F62;&#x5F0F;&#x4E3A;&#xFF1A;</p>
<p>A=UTUA = U^TUA=UTU</p>
<p>&#x5982;&#x679C;<code>&#x4E0A;</code>&#x662F;<code>&#x5047;</code>&#xFF0C;&#x8FD4;&#x56DE;&#x7684;&#x77E9;&#x9635;<code>L</code>&#x662F;&#x4E0B;&#x4E09;&#x89D2;&#xFF0C;&#x548C;&#x5206;&#x89E3;&#x7684;&#x5F62;&#x5F0F;&#x4E3A;&#xFF1A;</p>
<p>A=LLTA = LL^TA=LLT</p>
<p>&#x5982;&#x679C;<code>&#x4E0A;</code>&#x662F;<code>&#x771F;</code>&#x548C; A  A  A  &#x662F;&#x5206;&#x6279;&#x5BF9;&#x79F0;&#x6B63;&#x5B9A;&#x77E9;&#x9635;&#xFF0C;&#x5219;&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x5C06;&#x7EC4;&#x6210;&#x6BCF;&#x4E2A;&#x5355;&#x72EC;&#x7684;&#x77E9;&#x9635;&#x7684;&#x4E0A;&#x4E09;&#x89D2;&#x7684;Cholesky&#x56E0;&#x7D20;&#x7684;&#x3002;&#x7C7B;&#x4F3C;&#x5730;&#xFF0C;&#x5F53;<code>&#x4E0A;</code>&#x662F;<code>&#x5047;</code>&#xFF0C;&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x5C06;&#x7EC4;&#x6210;&#x7684;&#x6BCF;&#x4E00;&#x4E2A;&#x5355;&#x72EC;&#x7684;&#x77E9;&#x9635;&#x7684;&#x4E0B;&#x4E09;&#x89D2;&#x7684;Cholesky&#x56E0;&#x7D20;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x8F93;&#x5165;&#x5F20;&#x91CF; A  A  A  &#x5927;&#x5C0F;&#x7684; &#xFF08; <em>  &#xFF0C; n&#x7684; &#xFF0C; n&#x7684; &#xFF09; &#xFF08;</em>&#xFF0C;N&#xFF0C;N&#xFF09; &#xFF08; <em>  &#xFF0C; n&#x7684; &#xFF0C; n&#x7684; &#xFF09; &#x5176;&#x4E2D; </em> &#x662F;&#x96F6;&#x4E2A;&#x6216;&#x591A;&#x4E2A;&#x9009;&#x81EA;&#x7531;&#x5BF9;&#x79F0;&#x6B63;&#x5B9A;&#x7684;&#x6279;&#x6B21;&#x5C3A;&#x5BF8;&#x77E9;&#x9635;&#x3002;</p>
</li>
<li><p><strong>&#x4E0A;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x6807;&#x5FD7;&#xFF0C;&#x6307;&#x793A;&#x662F;&#x5426;&#x4EE5;&#x8FD4;&#x56DE;&#x4E0A;&#x6216;&#x4E0B;&#x4E09;&#x89D2;&#x77E9;&#x9635;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x5047;</code></p>
</li>
<li><p><strong>OUT</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x8F93;&#x51FA;&#x77E9;&#x9635;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(3, 3)
&gt;&gt;&gt; a = torch.mm(a, a.t()) # make symmetric positive-definite
&gt;&gt;&gt; l = torch.cholesky(a)
&gt;&gt;&gt; a
tensor([[ 2.4112, -0.7486,  1.4551],
        [-0.7486,  1.3544,  0.1294],
        [ 1.4551,  0.1294,  1.6724]])
&gt;&gt;&gt; l
tensor([[ 1.5528,  0.0000,  0.0000],
        [-0.4821,  1.0592,  0.0000],
        [ 0.9371,  0.5487,  0.7023]])
&gt;&gt;&gt; torch.mm(l, l.t())
tensor([[ 2.4112, -0.7486,  1.4551],
        [-0.7486,  1.3544,  0.1294],
        [ 1.4551,  0.1294,  1.6724]])
&gt;&gt;&gt; a = torch.randn(3, 2, 2)
&gt;&gt;&gt; a = torch.matmul(a, a.transpose(-1, -2)) + 1e-03 # make symmetric positive-definite
&gt;&gt;&gt; l = torch.cholesky(a)
&gt;&gt;&gt; z = torch.matmul(l, l.transpose(-1, -2))
&gt;&gt;&gt; torch.max(torch.abs(z - a)) # Max non-zero
tensor(2.3842e-07)
</code></pre><p><code>torch.``cholesky_inverse</code>( <em>input</em> , <em>upper=False</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8BA1;&#x7B97;&#x5BF9;&#x79F0;&#x6B63;&#x5B9A;&#x77E9;&#x9635; A  &#x7684;&#x5012;&#x6570;A  A  &#x4F7F;&#x7528;&#x5176;&#x7684;Cholesky&#x56E0;&#x6570; U  U  U  &#xFF1A;&#x8FD4;&#x56DE;&#x77E9;&#x9635;<code>INV</code>&#x3002;&#x9006;&#x4F7F;&#x7528;LAPACK&#x4F8B;&#x7A0B;<code>dpotri&#x8BA1;&#x7B97;</code>&#x548C;<code>spotri</code>&#xFF08;&#x548C;&#x76F8;&#x5E94;&#x7684;MAGMA&#x4F8B;&#x7A0B;&#xFF09;&#x3002;</p>
<p>&#x5982;&#x679C;<code>&#x4E0A;</code>&#x662F;<code>&#x5047;</code>&#xFF0C; U  U  U  &#x662F;&#x4E0B;&#x4E09;&#x89D2;&#x4F7F;&#x5F97;&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x662F;</p>
<p>inv=(uuT)&#x2212;1inv = (uu^{T})^{-1} inv=(uuT)&#x2212;1</p>
<p>&#x5982;&#x679C;<code>&#x4E0A;</code>&#x662F;<code>&#x771F;</code>&#x6216;&#x6CA1;&#x6709;&#x63D0;&#x4F9B;&#xFF0C; U  U  U  &#x662F;&#x4E0A;&#x4E09;&#x89D2;&#x4F7F;&#x5F97;&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x662F;</p>
<p>inv=(uTu)&#x2212;1inv = (u^T u)^{-1} inv=(uTu)&#x2212;1</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x8F93;&#x5165;2-d&#x5F20;&#x91CF; U  U  U  &#xFF0C;&#x4E00;&#x4E2A;&#x4E0A;&#x6216;&#x4E0B;&#x4E09;&#x89D2;&#x7684;Cholesky&#x56E0;&#x6570;</p>
</li>
<li><p><strong>&#x4E0A;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x662F;&#x5426;&#x8FD4;&#x56DE;&#x4F4E;&#x7EA7;&#xFF08;&#x9ED8;&#x8BA4;&#xFF09;&#x6216;&#x4E0A;&#x4E09;&#x89D2;&#x77E9;&#x9635;</p>
</li>
<li><p><strong>OUT</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x4E3A; INV </p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(3, 3)
&gt;&gt;&gt; a = torch.mm(a, a.t()) + 1e-05 * torch.eye(3) # make symmetric positive definite
&gt;&gt;&gt; u = torch.cholesky(a)
&gt;&gt;&gt; a
tensor([[  0.9935,  -0.6353,   1.5806],
        [ -0.6353,   0.8769,  -1.7183],
        [  1.5806,  -1.7183,  10.6618]])
&gt;&gt;&gt; torch.cholesky_inverse(u)
tensor([[ 1.9314,  1.2251, -0.0889],
        [ 1.2251,  2.4439,  0.2122],
        [-0.0889,  0.2122,  0.1412]])
&gt;&gt;&gt; a.inverse()
tensor([[ 1.9314,  1.2251, -0.0889],
        [ 1.2251,  2.4439,  0.2122],
        [-0.0889,  0.2122,  0.1412]])
</code></pre><p><code>torch.``cholesky_solve</code>( <em>input</em> , <em>input2</em> , <em>upper=False</em> , <em>out=None</em> ) &#x2192;
Tensor</p>
<p>&#x89E3;&#x51B3;&#x65B9;&#x7A0B;&#x4E0E;&#x534A;&#x6B63;&#x5B9A;&#x77E9;&#x9635;&#x7684;&#x7EBF;&#x6027;&#x7CFB;&#x7EDF;&#x88AB;&#x5012;&#x7F6E;&#x7ED9;&#x4E88;&#x5176;&#x7684;Cholesky&#x56E0;&#x6570;&#x77E9;&#x9635; U  U  U  &#x3002;</p>
<p>&#x5982;&#x679C;<code>&#x4E0A;</code>&#x662F;<code>&#x5047;</code>&#xFF0C; U  U  U  &#x662F;&#x548C;&#x548C; C &#x88AB;&#x8FD4;&#x56DE;&#x4E0B;&#x4E09;&#x89D2;&#x4F7F;&#x5F97;&#xFF1A;</p>
<p>c=(uuT)&#x2212;1bc = (u u^T)^{-1} b c=(uuT)&#x2212;1b</p>
<p>&#x5982;&#x679C;<code>&#x4E0A;</code>&#x662F;<code>&#x771F;</code>&#x6216;&#x6CA1;&#x6709;&#x63D0;&#x4F9B;&#xFF0C; U  U  U  &#x662F;&#x4E0A;&#x4E09;&#x89D2;&#x548C; C &#x88AB;&#x8FD4;&#x56DE;&#xFF0C;&#x4F7F;&#x5F97;&#xFF1A;</p>
<p>c=(uTu)&#x2212;1bc = (u^T u)^{-1} b c=(uTu)&#x2212;1b</p>
<p>torch.cholesky_solve&#xFF08;B&#xFF0C;U&#xFF09;&#x53EF;&#x4EE5;&#x5728;2D&#x8F93;&#x5165; B&#xFF0C;U &#x6216;&#x662F;2D&#x77E9;&#x9635;&#x7684;&#x6279;&#x8F93;&#x5165;&#x3002;&#x5982;&#x679C;&#x8F93;&#x5165;&#x662F;&#x6279;&#x6B21;&#xFF0C;&#x7136;&#x540E;&#x8FD4;&#x56DE;&#x6210;&#x6279;&#x8F93;&#x51FA; C</p>
<p>Note</p>
<p>&#x7684;<code>OUT</code>&#x5173;&#x952E;&#x5B57;&#x4EC5;&#x652F;&#x6301;2D&#x77E9;&#x9635;&#x8F93;&#x5165;&#xFF0C;&#x5373;&#xFF0C; B&#xFF0C;U &#x5FC5;&#x987B;2D&#x77E9;&#x9635;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x8F93;&#x5165;&#x77E9;&#x9635; B  b  b  &#x5927;&#x5C0F;&#x7684; &#xFF08; <em>  &#xFF0C; M  &#xFF0C; K  &#xFF09; &#xFF08;</em>&#xFF0C;M&#xFF0C;K&#xFF09; &#xFF08; <em>  &#xFF0C; M  &#xFF0C; K  &#xFF09; &#xFF0C;&#x5176;&#x4E2D; </em>  <em>  </em>  [H TG102]  &#x662F;&#x96F6;&#x70B9;&#x6216;&#x591A;&#x4E2A;&#x6279;&#x6B21;&#x7684;&#x5C3A;&#x5BF8;</p>
</li>
<li><p><strong>&#x8F93;&#x5165;2</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x8F93;&#x5165;&#x77E9;&#x9635; U  U  U  &#x5927;&#x5C0F;&#x7684; &#xFF08; <em>  &#xFF0C; M  &#xFF0C; M  &#xFF09; &#xFF08;</em>&#xFF0C;M&#xFF0C;M&#xFF09; &#xFF08; <em>  &#xFF0C; M  &#xFF0C; M  &#xFF09; &#xFF0C;&#x5176;&#x4E2D; </em>  <em>  </em>  &#x4E3A;&#x4E0A;&#x9650;&#x6216;&#x4E0B;&#x4E09;&#x89D2;&#x7684;Cholesky&#x56E0;&#x6570;&#x7EC4;&#x6210;&#x591A;&#x4E2A;&#x6279;&#x5904;&#x7406;&#x5C3A;&#x5BF8;&#x7684;&#x96F6;</p>
</li>
<li><p><strong>&#x4E0A;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x662F;&#x5426;&#x8003;&#x8651;&#x7684;Cholesky&#x56E0;&#x6570;&#x4F5C;&#x4E3A;&#x4E0B;&#x6216;&#x4E0A;&#x4E09;&#x89D2;&#x77E9;&#x9635;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x5047; [HTG13&#x3002;</code></p>
</li>
<li><p><strong>OUT</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x4E3A; C </p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(3, 3)
&gt;&gt;&gt; a = torch.mm(a, a.t()) # make symmetric positive definite
&gt;&gt;&gt; u = torch.cholesky(a)
&gt;&gt;&gt; a
tensor([[ 0.7747, -1.9549,  1.3086],
        [-1.9549,  6.7546, -5.4114],
        [ 1.3086, -5.4114,  4.8733]])
&gt;&gt;&gt; b = torch.randn(3, 2)
&gt;&gt;&gt; b
tensor([[-0.6355,  0.9891],
        [ 0.1974,  1.4706],
        [-0.4115, -0.6225]])
&gt;&gt;&gt; torch.cholesky_solve(b, u)
tensor([[ -8.1625,  19.6097],
        [ -5.8398,  14.2387],
        [ -4.3771,  10.4173]])
&gt;&gt;&gt; torch.mm(a.inverse(), b)
tensor([[ -8.1626,  19.6097],
        [ -5.8398,  14.2387],
        [ -4.3771,  10.4173]])
</code></pre><p><code>torch.``dot</code>( <em>input</em> , <em>tensor</em> ) &#x2192; Tensor</p>
<p>&#x8BA1;&#x7B97;&#x4E24;&#x4E2A;&#x5F20;&#x91CF;&#x7684;&#x70B9;&#x79EF;&#xFF08;&#x5185;&#x79EF;&#xFF09;&#x3002;</p>
<p>Note</p>
<p>&#x6B64;&#x529F;&#x80FD;&#x4E0D;&#x64AD;[ <a href="notes/broadcasting.html#broadcasting-semantics">HTG3&#x3002;</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.dot(torch.tensor([2, 3]), torch.tensor([2, 1]))
tensor(7)
</code></pre><p><code>torch.``eig</code>( <em>input</em> , <em>eigenvectors=False</em> , <em>out=None) - &gt; (Tensor</em>,
<em>Tensor</em> )</p>
<p>&#x8BA1;&#x7B97;&#x5B9E;&#x65B9;&#x9635;&#x7684;&#x7279;&#x5F81;&#x503C;&#x548C;&#x7279;&#x5F81;&#x5411;&#x91CF;&#x3002;</p>
<p>Note</p>
<p>&#x56E0;&#x4E3A;&#x7279;&#x5F81;&#x5411;&#x91CF;&#x53EF;&#x80FD;&#x662F;&#x590D;&#x6742;&#x7684;&#xFF0C;&#x5411;&#x540E;&#x901A;&#x4EC5;&#x5BF9;&#x652F;&#x6301;<code>torch.symeig&#xFF08;&#xFF09;</code></p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x7684;&#x5F62;&#x72B6; &#x65B9;&#x9635;&#xFF08;  n&#x7684; &#xD7; n&#x7684; &#xFF09; &#xFF08;N \ n&#x6B21;&#xFF09; &#xFF08; n&#x7684; &#xD7; n&#x7684; &#xFF09; &#x7684;&#x91CF;&#xFF0C;&#x7279;&#x5F81;&#x5411;&#x91CF;&#x5C06;&#x88AB;&#x8BA1;&#x7B97;</p>
</li>
<li><p><strong>&#x672C;&#x5F81;&#x5411;&#x91CF;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a>&#xFF09; - <code>&#x771F;</code>&#x6765;&#x8BA1;&#x7B97;&#x4E24;&#x4E2A;&#x7279;&#x5F81;&#x5411;&#x91CF;;&#x5426;&#x5219;&#xFF0C;&#x53EA;&#x6709;&#x7279;&#x5F81;&#x503C;&#x5C06;&#x8BA1;&#x7B97;</p>
</li>
<li><p><strong>OUT</strong> &#xFF08;<a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>&#x5143;&#x7EC4;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x8F93;&#x51FA;&#x5F20;&#x91CF;</p>
</li>
</ul>
<p>Returns</p>
<p>&#x5C06;&#x542B;&#x6709;namedtuple&#xFF08;&#x672C;&#x5F81;&#x503C;&#xFF0C;&#x672C;&#x5F81;&#x77E2;&#x91CF;&#xFF09;</p>
<blockquote>
<ul>
<li><strong>&#x672C;&#x5F81;&#x503C;</strong> &#xFF08; <em>&#x5F20;&#x91CF;</em> &#xFF09;&#xFF1A;&#x5F62;&#x72B6; &#xFF08; n&#x7684; &#xD7; 2  &#xFF09; &#xFF08;N \&#x6B21;2&#xFF09; &#xFF08; n&#x7684; &#xD7; 2  &#xFF09; &#x3002;&#x6BCF;&#x4E00;&#x884C;&#x662F;&#x8F93;&#x5165;
&#xFF0C;&#x5176;&#x4E2D;&#xFF0C;&#x6240;&#x8FF0;&#x7B2C;&#x4E00;&#x5143;&#x4EF6;&#x662F;&#x5B9E;&#x90E8;&#x548C;&#x6240;&#x8FF0;&#x7B2C;&#x4E8C;&#x5143;&#x4EF6;&#x662F;&#x865A;&#x6570;&#x90E8;&#x5206;&#x7684;<code>&#x7684;&#x672C;&#x5F81;&#x503C;&#x3002;&#x7279;&#x5F81;&#x503C;&#x4E0D;&#x4E00;&#x5B9A;&#x6709;&#x5E8F;&#x3002;</code></li>
</ul>
</blockquote>
<p>&gt;</p>
<blockquote>
<ul>
<li><strong>&#x672C;&#x5F81;&#x5411;&#x91CF;</strong> &#xFF08; <em>&#x5F20;&#x91CF;</em> &#xFF09;&#xFF1A;&#x5982;&#x679C;<code>&#x672C;&#x5F81;&#x5411;&#x91CF;=&#x5047;</code>&#xFF0C;&#x5B83;&#x662F;&#x4E00;&#x4E2A;&#x7A7A;&#x7684;&#x5F20;&#x91CF;&#x3002;&#x5426;&#x5219;&#xFF0C;&#x8BE5;&#x5F20;&#x91CF;&#x7684;&#x5F62;&#x72B6; &#xFF08; n&#x7684; &#xD7; n&#x7684; &#xFF09; &#xFF08;N \ n&#x6B21;&#xFF09; &#xFF08;
n&#x7684; &#xD7; n&#x7684; &#xFF09; &#x53EF;&#x4EE5;&#x88AB;&#x7528;&#x4E8E;&#x8BA1;&#x7B97;&#x5F52;&#x4E00;&#x5316;&#x7684;&#xFF08;&#x5355;&#x5143;&#x957F;&#x5EA6;&#xFF09;&#x5982;&#x4E0B;&#x5BF9;&#x5E94;&#x7684;&#x7279;&#x5F81;&#x503C;&#x7684;&#x7279;&#x5F81;&#x5411;&#x91CF;&#x3002;&#x5982;&#x679C;&#x76F8;&#x5E94;&#x7684;&#x672C;&#x5F81;&#x503C;[J] &#x662F;&#x4E00;&#x4E2A;&#x5B9E;&#x6570;&#xFF0C;&#x67F1;&#x672C;&#x5F81;&#x5411;&#x91CF;[:,
j]&#x7684;&#x662F;&#x5BF9;&#x5E94;&#x4E8E;&#x672C;&#x5F81;&#x503C;[j]&#x7684;&#x672C;&#x5F81;&#x5411;&#x91CF;&#x3002;&#x5982;&#x679C;&#x76F8;&#x5E94;&#x7684;&#x672C;&#x5F81;&#x503C;[J] &#x548C;&#x672C;&#x5F81;&#x503C;[J + 1] &#x5F62;&#x6210;&#x7684;&#x590D;&#x5171;&#x8F6D;&#x5BF9;&#xFF0C;&#x5219;&#x771F;&#x6B63;&#x7684;&#x672C;&#x5F81;&#x5411;&#x91CF;&#x53EF;&#x88AB;&#x8BA1;&#x7B97;&#x4E3A; &#x771F;&#x5B9E;&#x7279;&#x5F81;&#x77E2;&#x91CF; [
[HTG76&#xFF1A;J  =  E  i&#x7684; &#x514B; E  n&#x7684; [HTG92&#x3011;v  E  C  T  O  R  S  [ &#xFF1A; &#xFF0C; [ HTG112&#xFF1A;J  +
i&#x7684; &#xD7; E  i&#x7684; G  E  n&#x7684; [HTG132&#x3011;v  E  C  T  &#x95EE;&#x9898;o  R  S  [ &#xFF1A; &#xFF0C; [HTG152&#xFF1A;J  +  1  \
{&#x6587;&#x672C;&#x7279;&#x5F81;&#x5411;&#x91CF;&#x771F;} [j]&#x7684;&#x672C;&#x5F81;&#x5411;&#x91CF;= [&#xFF1A;&#xFF0C;j]&#x7684;+ I \&#x500D;&#x7279;&#x5F81;&#x5411;&#x91CF;[&#xFF1A;&#xFF0C;J + 1]  &#x771F;&#x5B9E;&#x7279;&#x5F81;&#x77E2;&#x91CF; [ [HTG176&#xFF1A;J  =  E  i&#x7684; &#x514B;
E  n&#x7684; [HTG200&#x3011;v  E  C  T  O  R  S  [ &#xFF1A; &#xFF0C; [HTG226&#xFF1A;J  +  i&#x7684; &#xD7; E  i&#x7684; &#x514B; E  n&#x7684;
[HTG262&#x3011;v  E  C  T  &#x200B;&#x200B; O  R  S  [ &#xFF1A; &#xFF0C; [HTG288&#xFF1A;J  +  1  &#xFF0C; &#x771F;&#x5B9E;&#x7279;&#x5F81;&#x77E2;&#x91CF; [ [HTG318&#xFF1A;J
+  1  =  E  i&#x7684; &#x514B; E  n&#x7684; [HTG338&#x3011;V  E  C  T  O  R  S  [ &#xFF1A; &#xFF0C; [HTG358&#xFF1A;J  -  i&#x7684; &#xD7;
E  i&#x7684; G  E  n&#x7684; [HTG378&#x3011;v  E  C  T  O  R  S  [ &#xFF1A; &#xFF0C; [HTG398&#xFF1A;J  +  1  \
{&#x6587;&#x672C;&#x7279;&#x5F81;&#x5411;&#x91CF;&#x771F;} [J + 1] =&#x7279;&#x5F81;&#x5411;&#x91CF;[:, j]&#x7684; - I \&#x500D;&#x7279;&#x5F81;&#x5411;&#x91CF;[:, J + 1]  [HTG4 16] &#x771F;&#x5B9E;&#x7279;&#x5F81;&#x77E2;&#x91CF; [
[HTG422&#xFF1A;J  +  1  =  E  i&#x7684; &#x514B; E  n&#x7684; [HTG458&#x3011;v  E  C  T  O  R  S  [ &#xFF1A; &#xFF0C;
[HTG484&#xFF1A;J  -  i&#x7684; &#xD7; E  i&#x7684; &#x514B; E  n&#x7684; [HTG520&#x3011;V  E  C  T  O  R  S  [ &#xFF1A; &#xFF0C; [HTG546&#xFF1A;J
+  1  &#x3002;</li>
</ul>
</blockquote>
<p>&gt;</p>
<p>&gt;</p>
<p>Return type</p>
<p>&#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor">&#x5F20;&#x91CF;</a>&#xFF0C;<a href="tensors.html#torch.Tensor" title="torch.Tensor">&#x5F20;&#x91CF;</a>&#xFF09;</p>
<p><code>torch.``gels</code>( <em>input</em> , <em>A</em> , <em>out=None</em>
)<a href="_modules/torch/functional.html#gels">[source]</a></p>
<p>&#x8BA1;&#x7B97;&#x7684;&#x89E3;&#x6700;&#x5C0F;&#x4E8C;&#x4E58;&#x548C;&#x6700;&#x5C0F;&#x8303;&#x6570;&#x95EE;&#x9898;&#x4E3A;&#x6EE1;&#x79E9;&#x77E9;&#x9635; A  A  A  &#x5927;&#x5C0F;&#x7684; &#xFF08; M  &#xD7; n&#x7684; &#xFF09; &#xFF08;M \ n&#x6B21;&#xFF09; &#xFF08; M  &#xD7; n&#x7684; &#xFF09; &#x548C;&#x4E00;&#x4E2A;&#x77E9;&#x9635; B  B
B  &#x5927;&#x5C0F;&#x7684; &#xFF08; M  &#xD7; K  &#xFF09; &#xFF08;M \&#x500D;K&#xFF09; &#xFF08; M  &#xD7; K  &#xFF09; &#x3002;</p>
<p>&#x6709;&#x5173; <code>torch.gels&#xFF08;&#xFF09;&#x66F4;&#x591A;&#x4FE1;&#x606F;</code>&#xFF0C;&#x8BF7;&#x68C0;&#x67E5; <code>torch.lstsq&#xFF08;&#xFF09;</code>&#x3002;</p>
<p>Warning</p>
<p><code>torch.gels&#xFF08;&#xFF09;</code>&#x4EE5;&#x6709;&#x5229;&#x4E8E; <code>torch.lstsq&#x7684;&#xFF08;&#x5E9F;&#x5F03;&#xFF09;</code>&#xFF0C;&#x5C06;&#x5728;&#x672A;&#x6765;&#x7684;&#x7248;&#x672C;&#x4E2D;&#x5220;&#x9664;&#x3002;&#x8BF7;&#x4F7F;&#x7528; <code>torch.lstsq&#xFF08;&#xFF09;</code>
&#x4EE3;&#x66FF;&#x3002;</p>
<p><code>torch.``geqrf</code>( <em>input</em> , <em>out=None) - &gt; (Tensor</em>, <em>Tensor</em> )</p>
<p>&#x8FD9;&#x662F;&#x76F4;&#x63A5;&#x8C03;&#x7528;LAPACK&#x4E00;&#x4E2A;&#x4F4E;&#x7EA7;&#x522B;&#x7684;&#x529F;&#x80FD;&#x3002;&#x6B64;&#x51FD;&#x6570;&#x8FD4;&#x56DE;&#x5982;<a href="https://software.intel.com/en-
us/node/521004" target="_blank"> LAPACK&#x6587;&#x6863;&#x5B9A;&#x4E49;geqrf </a>&#x4E00;&#x4E2A;namedtuple&#xFF08;&#x4E00;&#xFF0C;tau&#x86CB;&#x767D;&#xFF09;&#x3002;</p>
<p>&#x60A8;&#x901A;&#x5E38;&#x9700;&#x8981;&#x4F7F;&#x7528; <code>torch.qr&#xFF08;&#xFF09;</code>&#x4EE3;&#x66FF;&#x3002;</p>
<p>&#x8BA1;&#x7B97;<code>&#x8F93;&#x5165;&#x7684;QR&#x5206;&#x89E3;</code>&#xFF0C;&#x4F46;&#x6CA1;&#x6709;&#x6784;&#x5EFA; Q  Q  Q  &#x548C; R  R  R  &#x4F5C;&#x4E3A;&#x660E;&#x786E;&#x7684;&#x5206;&#x79BB;&#x77E9;&#x9635;&#x3002;</p>
<p>&#x76F8;&#x53CD;&#xFF0C;&#x8BE5;&#x76F4;&#x63A5;&#x8C03;&#x7528;&#x5E95;&#x5C42;LAPACK&#x51FD;&#x6570; geqrf&#x4EA7;&#x751F;&#x201C;&#x57FA;&#x672C;&#x53CD;&#x5C04;&#x201D;&#x7684;&#x5E8F;&#x5217;&#x3002;</p>
<p>&#x53C2;&#x89C1;[HTG0&#x5BF9;&#x4E8E;geqrf &#x4E3A;&#x8FDB;&#x4E00;&#x6B65;&#x7684;&#x7EC6;&#x8282;LAPACK&#x6587;&#x6863;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x8F93;&#x5165;&#x77E9;&#x9635;</p>
</li>
<li><p><strong>OUT</strong> &#xFF08;<a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>&#x5143;&#x7EC4;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x7684;&#x8F93;&#x51FA;&#x5143;&#x7EC4;&#xFF08;&#x5F20;&#x91CF;&#xFF0C;&#x5F20;&#x91CF;&#xFF09;</p>
</li>
</ul>
<p><code>torch.``ger</code>( <em>input</em> , <em>vec2</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x7684;<code>&#x8F93;&#x5165;</code>&#x548C;<code>VEC2</code>&#x5916;&#x79EF;&#x3002;&#x5982;&#x679C;<code>&#x8F93;&#x5165;</code>&#x662F;&#x5927;&#x5C0F;&#x4E3A;&#x5411;&#x91CF; n&#x7684; N  n&#x7684; &#x548C;<code>VEC2</code>&#x662F;&#x5927;&#x5C0F;&#x4E3A;&#x5411;&#x91CF; M  M  M  &#xFF0C;&#x7136;&#x540E;<code>OUT</code>&#x5FC5;&#x987B;&#x7684;&#x5927;&#x5C0F; &#xFF08; n&#x7684; &#xD7; [HTG80&#x4E00;&#x4E2A;&#x77E9;&#x9635;] M &#xFF09; &#xFF08;N \&#x4E58;&#x4EE5;m&#xFF09; &#xFF08; n&#x7684; &#xD7; M  &#xFF09; &#x3002;</p>
<p>Note</p>
<p>This function does not <a href="notes/broadcasting.html#broadcasting-%0Asemantics">broadcast</a>.</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - 1-d&#x8F93;&#x5165;&#x77E2;&#x91CF;</p>
</li>
<li><p><strong>VEC2</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - 1-d&#x8F93;&#x5165;&#x77E2;&#x91CF;</p>
</li>
<li><p><strong>OUT</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x53EF;&#x9009;&#x7684;&#x8F93;&#x51FA;&#x77E9;&#x9635;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; v1 = torch.arange(1., 5.)
&gt;&gt;&gt; v2 = torch.arange(1., 4.)
&gt;&gt;&gt; torch.ger(v1, v2)
tensor([[  1.,   2.,   3.],
        [  2.,   4.,   6.],
        [  3.,   6.,   9.],
        [  4.,   8.,  12.]])
</code></pre><p><code>torch.``inverse</code>( <em>input</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x53D6;&#x6B63;&#x65B9;&#x5F62;&#x77E9;&#x9635;<code>&#x8F93;&#x5165;</code>&#x7684;&#x5012;&#x6570;&#x3002; <code>&#x8F93;&#x5165;</code>&#x53EF;&#x4EE5;&#x662F;2D&#x65B9;&#x5F62;&#x5F20;&#x91CF;&#xFF0C;&#x5728;&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x8FD9;&#x51FD;&#x6570;&#x5C06;&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x4E2A;&#x4F53;&#x9006;&#x7EC4;&#x6210;&#x5F20;&#x91CF;&#x7684;&#x6279;&#x6B21;&#x3002;</p>
<p>Note</p>
<p>&#x4E0D;&#x7BA1;&#x539F;&#x59CB;&#x8FDB;&#x5C55;&#x7684;&#xFF0C;&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x5C06;&#x88AB;&#x8F6C;&#x7F6E;&#xFF0C;&#x5373;&#x5177;&#x6709;&#x5982;&#x6B65;&#x5E45;input.contiguous&#xFF08;&#xFF09;&#x3002;&#x8F6C;&#x7F6E;&#xFF08;-2&#xFF0C;-1&#xFF09;.stride&#xFF08;&#xFF09;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x5927;&#x5C0F; &#x7684;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#xFF08;  <em>  &#xFF0C; n&#x7684; &#xFF0C; n&#x7684; &#xFF09; &#xFF08;</em>&#xFF0C;N&#xFF0C;N&#xFF09; &#xFF08; <em>  n&#x7684; &#xFF0C; n&#x7684; &#xFF09; &#x5176;&#x4E2D; </em> &#x662F;&#x96F6;&#x70B9;&#x6216;&#x591A;&#x4E2A;&#x6279;&#x6B21;&#x7684;&#x5C3A;&#x5BF8;</p>
</li>
<li><p><strong>OUT</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x53EF;&#x9009;&#x8F93;&#x51FA;&#x5F20;&#x91CF;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.rand(4, 4)
&gt;&gt;&gt; y = torch.inverse(x)
&gt;&gt;&gt; z = torch.mm(x, y)
&gt;&gt;&gt; z
tensor([[ 1.0000, -0.0000, -0.0000,  0.0000],
        [ 0.0000,  1.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.0000,  0.0000],
        [ 0.0000, -0.0000, -0.0000,  1.0000]])
&gt;&gt;&gt; torch.max(torch.abs(z - torch.eye(4))) # Max non-zero
tensor(1.1921e-07)
&gt;&gt;&gt; # Batched inverse example
&gt;&gt;&gt; x = torch.randn(2, 3, 4, 4)
&gt;&gt;&gt; y = torch.inverse(x)
&gt;&gt;&gt; z = torch.matmul(x, y)
&gt;&gt;&gt; torch.max(torch.abs(z - torch.eye(4).expand_as(x))) # Max non-zero
tensor(1.9073e-06)
</code></pre><p><code>torch.``det</code>( <em>input</em> ) &#x2192; Tensor</p>
<p>&#x8BA1;&#x7B97;&#x65B9;&#x9635;&#x6216;&#x65B9;&#x9635;&#x7684;&#x6279;&#x6B21;&#x7684;&#x51B3;&#x5B9A;&#x56E0;&#x7D20;&#x3002;</p>
<p>Note</p>
<p>&#x5411;&#x540E;&#x901A;&#x8FC7; <code>DET&#xFF08;&#xFF09;</code>&#x5728;&#x5185;&#x90E8;&#x4F7F;&#x7528;&#x65F6;<code>&#x8F93;&#x5165;</code>&#x4E0D;&#x53EF;&#x9006;SVD&#x7684;&#x7ED3;&#x679C;&#x3002;&#x5728;&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x53CC;&#x5411;&#x540E;&#x901A;&#x8FC7; <code>DET&#xFF08;&#xFF09;</code>&#x5C06;&#x5728;&#x5F53;<code>&#x8F93;&#x5165;</code>&#x6CA1;&#x6709;&#x4E0D;&#x7A33;&#x5B9A;&#x4E0D;&#x540C;&#x5947;&#x5F02;&#x503C;&#x3002;&#x53C2;&#x89C1; <code>&#x5BF9;&#x4E8E;&#x7EC6;&#x8282;SVD&#xFF08;&#xFF09;</code>&#x3002;</p>
<p>Parameters</p>
<p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x5927;&#x5C0F;&#x7684;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#xFF08; <em>
&#x4E2D;&#xFF0C;n&#xFF0C;n&#xFF09;&#x5176;&#x4E2D; </em> &#x662F;&#x96F6;&#x7C73;&#x6216;&#x591A;&#x4E2A;&#x6279;&#x6B21;&#x7684;&#x5C3A;&#x5BF8;&#x3002;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; A = torch.randn(3, 3)
&gt;&gt;&gt; torch.det(A)
tensor(3.7641)

&gt;&gt;&gt; A = torch.randn(3, 2, 2)
&gt;&gt;&gt; A
tensor([[[ 0.9254, -0.6213],
         [-0.5787,  1.6843]],

        [[ 0.3242, -0.9665],
         [ 0.4539, -0.0887]],

        [[ 1.1336, -0.4025],
         [-0.7089,  0.9032]]])
&gt;&gt;&gt; A.det()
tensor([1.1990, 0.4099, 0.7386])
</code></pre><p><code>torch.``logdet</code>( <em>input</em> ) &#x2192; Tensor</p>
<p>&#x8BA1;&#x7B97;&#x65B9;&#x9635;&#x6216;&#x65B9;&#x9635;&#x7684;&#x6279;&#x6B21;&#x7684;&#x65E5;&#x5FD7;&#x51B3;&#x5B9A;&#x56E0;&#x7D20;&#x3002;</p>
<p>Note</p>
<p>&#x7ED3;&#x679C;&#x662F;<code>-INF</code>&#x5982;&#x679C;<code>&#x8F93;&#x5165;</code>&#x5177;&#x6709;&#x96F6;&#x65E5;&#x5FD7;&#x884C;&#x5217;&#x5F0F;&#xFF0C;&#x5E76;&#x4E14;&#x662F;<code>&#x6960;</code>&#x5982;&#x679C;<code>&#x8F93;&#x5165;</code>&#x5177;&#x6709;&#x8D1F;&#x7684;&#x51B3;&#x5B9A;&#x56E0;&#x7D20;&#x3002;</p>
<p>Note</p>
<p>&#x5411;&#x540E;&#x901A;&#x8FC7; <code>logdet&#xFF08;&#xFF09;</code>&#x5728;&#x5185;&#x90E8;&#x4F7F;&#x7528;&#x65F6;<code>&#x8F93;&#x5165;</code>&#x4E0D;&#x53EF;&#x9006;SVD&#x7684;&#x7ED3;&#x679C;&#x3002;&#x5728;&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x53CC;&#x5411;&#x540E;&#x901A;&#x8FC7; <code>logdet&#xFF08;&#xFF09;</code>&#x5C06;&#x5728;&#x5F53;<code>&#x8F93;&#x5165;</code>&#x6CA1;&#x6709;&#x4E0D;&#x7A33;&#x5B9A;&#x4E0D;&#x540C;&#x5947;&#x5F02;&#x503C;&#x3002;&#x53C2;&#x89C1; <code>&#x5BF9;&#x4E8E;&#x7EC6;&#x8282;SVD&#xFF08;&#xFF09;</code>&#x3002;</p>
<p>Parameters</p>
<p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x5927;&#x5C0F;&#x7684;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#xFF08; <em>
&#x4E2D;&#xFF0C;n&#xFF0C;n&#xFF09;&#x5176;&#x4E2D; </em> &#x662F;&#x96F6;&#x7C73;&#x6216;&#x591A;&#x4E2A;&#x6279;&#x6B21;&#x7684;&#x5C3A;&#x5BF8;&#x3002;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; A = torch.randn(3, 3)
&gt;&gt;&gt; torch.det(A)
tensor(0.2611)
&gt;&gt;&gt; torch.logdet(A)
tensor(-1.3430)
&gt;&gt;&gt; A
tensor([[[ 0.9254, -0.6213],
         [-0.5787,  1.6843]],

        [[ 0.3242, -0.9665],
         [ 0.4539, -0.0887]],

        [[ 1.1336, -0.4025],
         [-0.7089,  0.9032]]])
&gt;&gt;&gt; A.det()
tensor([1.1990, 0.4099, 0.7386])
&gt;&gt;&gt; A.det().log()
tensor([ 0.1815, -0.8917, -0.3031])
</code></pre><p><code>torch.``slogdet</code>( <em>input) - &gt; (Tensor</em>, <em>Tensor</em> )</p>
<p>&#x8BA1;&#x7B97;&#x7684;&#x7B26;&#x53F7;&#x548C;&#x8BB0;&#x5F55;&#x4E00;&#x4E2A;&#x65B9;&#x9635;&#x6216;&#x65B9;&#x9635;&#x7684;&#x6279;&#x6B21;&#x7684;&#x884C;&#x5217;&#x5F0F;&#xFF08;S&#xFF09;&#x7684;&#x7EDD;&#x5BF9;&#x503C;&#x3002;</p>
<p>Note</p>
<p>&#x5982;&#x679C;<code>&#x8F93;&#x5165;</code>&#x5177;&#x6709;&#x96F6;&#x884C;&#x5217;&#x5F0F;&#xFF0C;&#x8FD9;&#x5C06;&#x8FD4;&#x56DE;<code>&#xFF08;0&#xFF0C; -INF&#xFF09;</code>&#x3002;</p>
<p>Note</p>
<p>&#x5411;&#x540E;&#x901A;&#x8FC7; <code>slogdet&#xFF08;&#xFF09;</code>&#x5728;&#x5185;&#x90E8;&#x4F7F;&#x7528;&#x65F6;<code>&#x8F93;&#x5165;</code>&#x4E0D;&#x53EF;&#x9006;SVD&#x7684;&#x7ED3;&#x679C;&#x3002;&#x5728;&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x53CC;&#x5411;&#x540E;&#x901A;&#x8FC7; <code>slogdet&#xFF08;&#xFF09;</code>&#x5C06;&#x5728;&#x5F53;<code>&#x8F93;&#x5165;</code>&#x6CA1;&#x6709;&#x4E0D;&#x7A33;&#x5B9A;&#x4E0D;&#x540C;&#x5947;&#x5F02;&#x503C;&#x3002;&#x53C2;&#x89C1; <code>&#x5BF9;&#x4E8E;&#x7EC6;&#x8282;SVD&#xFF08;&#xFF09;</code>&#x3002;</p>
<p>Parameters</p>
<p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x5927;&#x5C0F;&#x7684;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#xFF08; <em>
&#x4E2D;&#xFF0C;n&#xFF0C;n&#xFF09;&#x5176;&#x4E2D; </em> &#x662F;&#x96F6;&#x7C73;&#x6216;&#x591A;&#x4E2A;&#x6279;&#x6B21;&#x7684;&#x5C3A;&#x5BF8;&#x3002;</p>
<p>Returns</p>
<p>&#x5C06;&#x542B;&#x6709;&#x884C;&#x5217;&#x5F0F;&#x7684;&#x7B26;&#x53F7;namedtuple&#xFF08;&#x6807;&#x5FD7;&#xFF0C;logabsdet&#xFF09;&#xFF0C;&#x5E76;&#x4E14;&#x7EDD;&#x5BF9;&#x884C;&#x5217;&#x5F0F;&#x7684;log&#x503C;&#x3002;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; A = torch.randn(3, 3)
&gt;&gt;&gt; A
tensor([[ 0.0032, -0.2239, -1.1219],
        [-0.6690,  0.1161,  0.4053],
        [-1.6218, -0.9273, -0.0082]])
&gt;&gt;&gt; torch.det(A)
tensor(-0.7576)
&gt;&gt;&gt; torch.logdet(A)
tensor(nan)
&gt;&gt;&gt; torch.slogdet(A)
torch.return_types.slogdet(sign=tensor(-1.), logabsdet=tensor(-0.2776))
</code></pre><p><code>torch.``lstsq</code>( <em>input</em> , <em>A</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>Computes the solution to the least squares and least norm problems for a full
rank matrix AAA of size (m&#xD7;n)(m \times n)(m&#xD7;n) and a matrix BBB of size
(m&#xD7;k)(m \times k)(m&#xD7;k) .</p>
<p>&#x5982;&#x679C; M  &#x2265; n&#x7684; &#x7C73;\ GEQ&#xD1;  M  &#x2265; n&#x7684; &#xFF0C; <code>lstsq&#xFF08;&#xFF09;</code>&#x89E3;&#x51B3;&#x4E86;&#x6700;&#x5C0F;&#x5E73;&#x65B9;&#x95EE;&#x9898;&#xFF1A;</p>
<p>min&#x2061;X&#x2225;AX&#x2212;B&#x2225;2.\begin{array}{ll} \min_X &amp; |AX-B|_2.
\end{array}minX&#x200B;&#x200B;&#x2225;AX&#x2212;B&#x2225;2&#x200B;.&#x200B;</p>
<p>&#x5982;&#x679C; M  &amp; LT ;  n&#x7684; M &amp; LT ; N  M  &amp; LT ;  n&#x7684; &#xFF0C; <code>lstsq&#xFF08;&#xFF09;</code>&#x89E3;&#x51B3;&#x4E86;&#x81F3;&#x5C11;&#x8303;&#x6570;&#x95EE;&#x9898;&#xFF1A;</p>
<p>min&#x2061;X&#x2225;X&#x2225;2subject toAX=B.\begin{array}{ll} \min_X &amp; |X|_2 &amp; \text{subject to}
&amp; AX = B. \end{array}minX&#x200B;&#x200B;&#x2225;X&#x2225;2&#x200B;&#x200B;subject to&#x200B;AX=B.&#x200B;</p>
<p>&#x8FD4;&#x56DE;&#x5F20;&#x91CF; X  X  X  &#x5177;&#x6709;&#x5F62;&#x72B6; &#xFF08; MAX  &#x2061; &#xFF08; M  &#xFF0C; n&#x7684; &#xFF09;  &#xD7; K  &#xFF09; &#xFF08;\ MAX&#xFF08;M&#xFF0C;N&#xFF09;\&#x500D;K&#xFF09; &#xFF08; MAX  &#xFF08; M  &#xFF0C;
n&#x7684; &#xFF09; &#xD7; K  &#xFF09; &#x3002;&#x7B2C;&#x4E00; n&#x7684; n&#x7684; n&#x7684; &#x7684;&#x884C; X  X  X  &#x5305;&#x542B;&#x6EB6;&#x6DB2;&#x3002;&#x5982;&#x679C; M  &#x2265; n&#x7684; &#x7C73;\ GEQ&#xD1;  M  &#x2265; n&#x7684;
&#xFF0C;&#x6B63;&#x65B9;&#x5F62;&#x7684;&#x7528;&#x4E8E;&#x6BCF;&#x4E00;&#x5217;&#x4E2D;&#x7684;&#x6EB6;&#x6DB2;&#x4E2D;&#x7684;&#x5269;&#x4F59;&#x4E4B;&#x548C;&#x7531;&#x5E73;&#x65B9;&#x548C;&#x7ED9;&#x5B9A;&#x5728;&#x5269;&#x4F59;&#x7684; M  &#x5143;&#x7D20; -  n&#x7684; &#x7C73; - N  M  -  n&#x7684; &#x8BE5;&#x5217;&#x7684;&#x884C;&#x3002;</p>
<p>Note</p>
<p>&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x5F53; M  &amp; LT ;  n&#x7684; M &amp; LT ; N  M  &amp; LT ;  n&#x7684; &#x4E0D;&#x652F;&#x6301;&#x5728;GPU&#x4E0A;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x77E9;&#x9635; B  B  B </p>
</li>
<li><p><strong>A</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x7684; M  M  M  &#x901A;&#x8FC7; n&#x7684; n&#x7684; n&#x7684; &#x77E9;&#x9635; A  A  A </p>
</li>
<li><p><strong>OUT</strong> &#xFF08;<a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>&#x5143;&#x7EC4;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x53EF;&#x9009;&#x7684;&#x76EE;&#x7684;&#x5730;&#x5F20;&#x91CF;</p>
</li>
</ul>
<p>Returns</p>
<p>&#x5C06;&#x542B;&#x6709;namedtuple&#xFF08;&#x6EB6;&#x6DB2;&#xFF0C;QR&#xFF09;&#xFF1A;</p>
<blockquote>
<ul>
<li><strong>&#x6EB6;&#x6DB2;</strong> &#xFF08; <em>&#x5F20;&#x91CF;</em> &#xFF09;&#xFF1A;&#x6700;&#x5C0F;&#x4E8C;&#x4E58;&#x89E3;</li>
</ul>
</blockquote>
<p>&gt;</p>
<blockquote>
<ul>
<li><strong>QR</strong> &#xFF08; <em>&#x5F20;&#x91CF;</em> &#xFF09;&#xFF1A;&#x5C06;QR&#x5206;&#x89E3;&#x7684;&#x7EC6;&#x8282;</li>
</ul>
</blockquote>
<p>&gt;</p>
<p>&gt;</p>
<p>Return type</p>
<p>(<a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>,
<a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>)</p>
<p>Note</p>
<p>&#x8FD4;&#x56DE;&#x7684;&#x77E9;&#x9635;&#x603B;&#x662F;&#x4F1A;&#x6362;&#x4F4D;&#x4E0D;&#x8BBA;&#x8F93;&#x5165;&#x77E9;&#x9635;&#x7684;&#x8FDB;&#x6B65;&#x3002;&#x4E5F;&#x5C31;&#x662F;&#x8BF4;&#xFF0C;&#x4ED6;&#x4EEC;&#x5C06;&#x6709;&#x6B65;&#x5E45;&#xFF08;1&#xFF0C;M&#xFF09;&#x800C;&#x975E;&#xFF08;M&#xFF0C;1&#xFF09;&#x3002;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; A = torch.tensor([[1., 1, 1],
                      [2, 3, 4],
                      [3, 5, 2],
                      [4, 2, 5],
                      [5, 4, 3]])
&gt;&gt;&gt; B = torch.tensor([[-10., -3],
                      [ 12, 14],
                      [ 14, 12],
                      [ 16, 16],
                      [ 18, 16]])
&gt;&gt;&gt; X, _ = torch.lstsq(B, A)
&gt;&gt;&gt; X
tensor([[  2.0000,   1.0000],
        [  1.0000,   1.0000],
        [  1.0000,   2.0000],
        [ 10.9635,   4.8501],
        [  8.9332,   5.2418]])
</code></pre><p><code>torch.``lu</code>( <em>A</em> , <em>pivot=True</em> , <em>get_infos=False</em> , <em>out=None</em>
)<a href="_modules/torch/functional.html#lu">[source]</a></p>
<p>&#x8BA1;&#x7B97;&#x65B9;&#x9635;&#x6216;&#x65B9;&#x9635;<code>A</code>&#x7684;&#x6279;&#x6B21;&#x7684;LU&#x5206;&#x89E3;&#x3002;&#x8FD4;&#x56DE;&#x5305;&#x542B;LU&#x5206;&#x89E3;&#x548C;<code>A</code>&#x67A2;&#x8F6C;&#x7684;&#x5143;&#x7EC4;&#x3002;&#x5982;&#x679C;<code>&#x67A2;</code>&#x8BBE;&#x7F6E;&#x4E3A;<code>&#x771F;</code>&#x65CB;&#x8F6C;&#x5B8C;&#x6210;&#x3002;</p>
<p>Note</p>
<p>&#x7531;&#x8BE5;&#x51FD;&#x6570;&#x8FD4;&#x56DE;&#x7684;&#x67A2;&#x8F74;&#x662F;1-&#x7D22;&#x5F15;&#x3002;&#x5982;&#x679C;<code>&#x67A2;&#x8F74;</code>&#x662F;<code>&#x5047;</code>&#xFF0C;&#x5219;&#x8FD4;&#x56DE;&#x7684;&#x67A2;&#x8F6C;&#x662F;&#x586B;&#x5145;&#x6709;&#x9002;&#x5F53;&#x5927;&#x5C0F;&#x7684;&#x96F6;&#x7684;&#x5F20;&#x91CF;&#x3002;</p>
<p>Note</p>
<p>LU&#x5206;&#x89E3;&#x4E0E;<code>&#x67A2;</code>= <code>&#x5047;</code>&#x4E0D;&#x9002;&#x7528;&#x4E8E;CPU&#xFF0C;&#x5E76;&#x8BD5;&#x56FE;&#x8FD9;&#x6837;&#x505A;&#x4F1A;&#x5F15;&#x53D1;&#x9519;&#x8BEF;&#x3002;&#x7136;&#x800C;&#xFF0C;LU&#x5206;&#x89E3;&#x4E0E;<code>&#x67A2;&#x8F74;</code>= <code>&#x5047;</code>&#x53EF;&#x7528;&#x4E8E;CUDA&#x3002;</p>
<p>Note</p>
<p>&#x6B64;&#x529F;&#x80FD;&#x4E0D;&#x68C0;&#x67E5;&#x5206;&#x89E3;&#x662F;&#x5426;&#x6210;&#x529F;&#xFF0C;&#x5982;&#x679C;<code>get_infos</code>&#x662F;<code>&#x771F;</code>&#x7531;&#x4E8E;&#x5206;&#x89E3;&#x7684;&#x72B6;&#x6001;&#x51FA;&#x73B0;&#x5728;&#x8FD4;&#x56DE;&#x7684;&#x5143;&#x7EC4;&#x7684;&#x7B2C;&#x4E09;&#x4E2A;&#x5143;&#x7D20;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>A</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x5F20;&#x91CF;&#x5BF9;&#x56E0;&#x5B50;&#x5927;&#x5C0F;&#x7684; &#xFF08; <em>  &#xFF0C; M  &#xFF0C; M  &#xFF09; &#xFF08;</em>&#xFF0C;M&#xFF0C;M&#xFF09; &#xFF08; *  M  &#xFF0C; M  &#xFF09;</p>
</li>
<li><p><strong>&#x67A2;&#x8F74;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x67A2;&#x8F6C;&#x63A7;&#x5236;&#x662F;&#x5426;&#x5DF2;&#x5B8C;&#x6210;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x771F;</code></p>
</li>
<li><p><strong>get_infos</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5982;&#x679C;&#x8BBE;&#x5B9A;&#x4E3A;<code>&#x771F;</code>&#xFF0C;&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x4FE1;&#x606F;IntTensor&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x5047;</code></p>
</li>
<li><p><strong>OUT</strong> &#xFF08;<a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>&#x5143;&#x7EC4;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x53EF;&#x9009;&#x7684;&#x8F93;&#x51FA;&#x5143;&#x7EC4;&#x3002;&#x5982;&#x679C;<code>get_infos</code>&#x662F;<code>&#x771F;</code>&#xFF0C;&#x7136;&#x540E;&#x5728;&#x6240;&#x8FF0;&#x5143;&#x7EC4;&#x7684;&#x5143;&#x7D20;&#x662F;&#x5F20;&#x91CF;&#xFF0C;IntTensor&#xFF0C;&#x548C;IntTensor&#x3002;&#x5982;&#x679C;<code>get_infos</code>&#x662F;<code>&#x5047;</code>&#xFF0C;&#x7136;&#x540E;&#x5728;&#x6240;&#x8FF0;&#x5143;&#x7EC4;&#x7684;&#x5143;&#x7D20;&#x662F;&#x5F20;&#x91CF;&#xFF0C;IntTensor&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x65E0;</code></p>
</li>
</ul>
<p>Returns</p>
<p>&#x5F20;&#x91CF;&#x7684;&#x542B;&#x6709;A&#x5143;&#x7EC4;</p>
<blockquote>
<ul>
<li><strong>&#x56E0;&#x5F0F;&#x5206;&#x89E3;</strong> &#xFF08; <em>&#x5F20;&#x91CF;</em> &#xFF09;&#xFF1A;&#x7684;&#x5927;&#x5C0F; &#x56E0;&#x5F0F;&#x5206;&#x89E3;&#xFF08; <em>  &#xFF0C; M  &#xFF0C; M  &#xFF09; &#xFF08;</em>&#xFF0C;M&#xFF0C;M &#xFF09; &#xFF08; *  &#xFF0C; M  &#xFF0C; M  &#xFF09;</li>
</ul>
</blockquote>
<p>&gt;</p>
<blockquote>
<ul>
<li><strong>&#x67A2;&#x8F74;</strong> &#xFF08; <em>IntTensor</em> &#xFF09;&#xFF1A;&#x5927;&#x5C0F; &#x7684;&#x67A2;&#x8F74;&#xFF08; <em>  &#xFF0C; M  &#xFF09; &#xFF08;</em>&#xFF0C;M&#xFF09; &#xFF08; *  &#xFF0C; M  &#xFF09;</li>
</ul>
</blockquote>
<p>&gt;</p>
<blockquote>
<ul>
<li><strong>&#x7684;&#x76F8;&#x5173;&#x4FE1;&#x606F;</strong> &#xFF08; <em>IntTensor</em> &#xFF0C; <em>&#x53EF;&#x9009;</em> &#xFF09;&#xFF1A;&#x5982;&#x679C;<code>get_infos</code>&#x662F;<code>&#x771F;</code>&#xFF0C;&#x8FD9;&#x662F;&#x5927;&#x5C0F; &#xFF08; <em>  &#xFF09;&#x7684;&#x5F20;&#x91CF;
&#xFF08;</em>&#xFF09; &#xFF08; *  &#xFF09;  &#x5176;&#x4E2D;&#x975E;&#x96F6;&#x503C;&#x6307;&#x793A;&#x56E0;&#x5F0F;&#x5206;&#x89E3;&#x5BF9;&#x77E9;&#x9635;&#x6216;&#x6BCF;&#x4E2A;minibatch&#x662F;&#x5426;&#x6210;&#x529F;&#x6216;&#x5931;&#x8D25;</li>
</ul>
</blockquote>
<p>&gt;</p>
<p>&gt;</p>
<p>Return type</p>
<p>&#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor">&#x5F20;&#x91CF;</a>&#xFF0C;IntTensor&#xFF0C;IntTensor&#xFF08;&#x53EF;&#x9009;&#xFF09;&#xFF09;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; A = torch.randn(2, 3, 3)
&gt;&gt;&gt; A_LU, pivots = torch.lu(A)
&gt;&gt;&gt; A_LU
tensor([[[ 1.3506,  2.5558, -0.0816],
         [ 0.1684,  1.1551,  0.1940],
         [ 0.1193,  0.6189, -0.5497]],

        [[ 0.4526,  1.2526, -0.3285],
         [-0.7988,  0.7175, -0.9701],
         [ 0.2634, -0.9255, -0.3459]]])
&gt;&gt;&gt; pivots
tensor([[ 3,  3,  3],
        [ 3,  3,  3]], dtype=torch.int32)
&gt;&gt;&gt; A_LU, pivots, info = torch.lu(A, get_infos=True)
&gt;&gt;&gt; if info.nonzero().size(0) == 0:
...   print(&apos;LU factorization succeeded for all samples!&apos;)
LU factorization succeeded for all samples!
</code></pre><p><code>torch.``lu_solve</code>( <em>input</em> , <em>LU_data</em> , <em>LU_pivots</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;LU&#x6C42;&#x89E3;&#x7EBF;&#x6027;&#x7CFB;&#x7EDF; &#x7684;A  &#xD7; =  b  Ax = b&#x7684; A  &#xD7; =  b  [HTG43&#x4F7F;&#x7528;&#x6765;&#x81EA; <code>torch.lu&#xFF08;&#xFF09;</code>
A&#x7684;&#x90E8;&#x5206;&#x67A2;&#x8F6C;LU&#x5206;&#x89E3;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x7684;&#x5927;&#x5C0F; &#x7684;RHS&#x5F20;&#x91CF;&#xFF08;  b  &#xFF0C; M  &#xFF0C; K  &#xFF09; &#xFF08;b&#xFF0C;M&#xFF0C;K&#xFF09; &#xFF08; b  M  &#xFF0C; K  &#xFF09;</p>
</li>
<li><p><strong>LU_data</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x4ECE; <code>A&#x7684;&#x67A2;&#x8F6C;LU&#x5206;&#x89E3;torch.lu&#xFF08;&#xFF09;</code>&#x5927;&#x5C0F;&#x7684; &#xFF08; b  &#xFF0C; M  &#xFF0C; M  &#xFF09; &#xFF08;b&#xFF0C;M&#xFF0C;M&#xFF09; &#xFF08; b  &#xFF0C; M  &#xFF0C; M  &#xFF09;</p>
</li>
<li><p><strong>LU_pivots</strong> &#xFF08; <em>IntTensor</em> &#xFF09; - &#x7684;LU&#x5206;&#x89E3;&#x7684;&#x4ECE; <code>&#x67A2;&#x8F74;torch.lu&#xFF08;&#xFF09;</code>&#x7684;&#x5927;&#x5C0F; &#xFF08; b  &#xFF0C; M  &#xFF09; &#xFF08;b&#xFF0C;M&#xFF09; &#xFF08; b  &#xFF0C; M  &#xFF09;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the optional output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; A = torch.randn(2, 3, 3)
&gt;&gt;&gt; b = torch.randn(2, 3, 1)
&gt;&gt;&gt; A_LU = torch.lu(A)
&gt;&gt;&gt; x = torch.lu_solve(b, *A_LU)
&gt;&gt;&gt; torch.norm(torch.bmm(A, x) - b)
tensor(1.00000e-07 *
       2.8312)
</code></pre><p><code>torch.``lu_unpack</code>( <em>LU_data</em> , <em>LU_pivots</em> , <em>unpack_data=True</em> ,
<em>unpack_pivots=True</em> )<a href="_modules/torch/functional.html#lu_unpack">[source]</a></p>
<p>&#x4ECE;&#x5F20;&#x91CF;&#x7684;LU&#x5206;&#x89E3;&#x89E3;&#x5305;&#x7684;&#x6570;&#x636E;&#xFF0C;&#x5E76;&#x67A2;&#x8F6C;&#x3002;</p>
<p>&#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x7684;&#x5143;&#x7EC4;&#x4E3A;<code>&#xFF08;&#x5728; &#x67A2;&#x8F6C;&#x65F6;&#xFF0C; &#x4E2D;&#x7684; L  &#x5F20;&#x91CF;&#xFF0C; &#x7684; U  &#x5F20;&#x91CF;&#xFF09;</code>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>LU_data</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x6253;&#x5305;LU&#x5206;&#x89E3;&#x6570;&#x636E;</p>
</li>
<li><p><strong>LU_pivots</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x586B;&#x5145;LU&#x5206;&#x89E3;&#x67A2;&#x8F6C;</p>
</li>
<li><p><strong>unpack_data</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a>&#xFF09; - &#x6807;&#x5FD7;&#xFF0C;&#x6307;&#x793A;&#x5982;&#x679C;&#x6570;&#x636E;&#x5E94;&#x88AB;&#x89E3;&#x538B;&#x7F29;</p>
</li>
<li><p><strong>unpack_pivots</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a>&#xFF09; - &#x6807;&#x5FD7;&#xFF0C;&#x6307;&#x793A;&#x5982;&#x679C;&#x67A2;&#x8F74;&#x5E94;&#x89E3;&#x538B;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; A = torch.randn(2, 3, 3)
&gt;&gt;&gt; A_LU, pivots = A.lu()
&gt;&gt;&gt; P, A_L, A_U = torch.lu_unpack(A_LU, pivots)
&gt;&gt;&gt;
&gt;&gt;&gt; # can recover A from factorization
&gt;&gt;&gt; A_ = torch.bmm(P, torch.bmm(A_L, A_U))
</code></pre><p><code>torch.``matmul</code>( <em>input</em> , <em>other</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>2&#x5F20;&#x91CF;&#x7684;&#x77E9;&#x9635;&#x4EA7;&#x54C1;&#x3002;</p>
<p>&#x8BE5;&#x884C;&#x4E3A;&#x53D6;&#x51B3;&#x4E8E;&#x5F20;&#x91CF;&#x7684;&#x7EF4;&#x6570;&#x5982;&#x4E0B;&#xFF1A;</p>
<ul>
<li><p>&#x5982;&#x679C;&#x4E24;&#x4E2A;&#x5F20;&#x91CF;&#x662F;1&#x7EF4;&#x7684;&#xFF0C;&#x70B9;&#x79EF;&#xFF08;&#x6807;&#x91CF;&#xFF09;&#x88AB;&#x8FD4;&#x56DE;&#x3002;</p>
</li>
<li><p>&#x5982;&#x679C;&#x4E24;&#x4E2A;&#x53C2;&#x6570;&#x662F;2&#x7EF4;&#x7684;&#xFF0C;&#x5219;&#x8FD4;&#x56DE;&#x77E9;&#x9635;&#xFF0C;&#x77E9;&#x9635;&#x4EA7;&#x54C1;&#x3002;</p>
</li>
<li><p>&#x5982;&#x679C;&#x7B2C;&#x4E00;&#x4E2A;&#x53C2;&#x6570;&#x662F;1&#x7EF4;&#x7684;&#xFF0C;&#x5E76;&#x4E14;&#x7B2C;&#x4E8C;&#x4E2A;&#x53C2;&#x6570;&#x662F;2&#x7EF4;&#x7684;&#xFF0C;&#x4E00;&#x4E2A;1&#x88AB;&#x9884;&#x7F6E;&#x5230;&#x5176;&#x5C3A;&#x5BF8;&#x4E3A;&#x77E9;&#x9635;&#x4E58;&#x6CD5;&#x7684;&#x76EE;&#x7684;&#x3002;&#x7684;&#x77E9;&#x9635;&#x4E58;&#x6CD5;&#x540E;&#xFF0C;&#x5C06;&#x9884;&#x8C0B;&#x5C3A;&#x5BF8;&#x88AB;&#x53BB;&#x9664;&#x3002;</p>
</li>
<li><p>&#x5982;&#x679C;&#x7B2C;&#x4E00;&#x4E2A;&#x53C2;&#x6570;&#x662F;2&#x7EF4;&#x7684;&#xFF0C;&#x5E76;&#x4E14;&#x7B2C;&#x4E8C;&#x4E2A;&#x53C2;&#x6570;&#x662F;1&#x7EF4;&#x7684;&#xFF0C;&#x5219;&#x8FD4;&#x56DE;&#x77E9;&#x9635;&#x77E2;&#x91CF;&#x4E58;&#x79EF;&#x3002;</p>
</li>
<li><p>&#x5982;&#x679C;&#x4E24;&#x4E2A;&#x53C2;&#x6570;&#x662F;&#x81F3;&#x5C11;&#x4E00;&#x7EF4;&#x548C;&#x81F3;&#x5C11;&#x4E00;&#x4E2A;&#x53C2;&#x6570;&#x662F;N&#x7EF4;&#xFF08;N &amp; GT&#x5176;&#x4E2D;; 2&#xFF09;&#xFF0C;&#x5219;&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x6279;&#x5904;&#x7406;&#x77E9;&#x9635;&#x4E58;&#x6CD5;&#x3002;&#x5982;&#x679C;&#x7B2C;&#x4E00;&#x4E2A;&#x53C2;&#x6570;&#x662F;1&#x7EF4;&#x7684;&#xFF0C;&#x4E00;&#x4E2A;1&#x88AB;&#x9884;&#x7F6E;&#x5230;&#x5176;&#x5C3A;&#x5BF8;&#x4E3A;&#x6210;&#x6279;&#x77E9;&#x9635;&#x4E58;&#x6CD5;&#x7684;&#x76EE;&#x7684;&#xFF0C;&#x5E76;&#x4E14;&#x4E4B;&#x540E;&#x88AB;&#x53BB;&#x9664;&#x3002;&#x5982;&#x679C;&#x7B2C;&#x4E8C;&#x4E2A;&#x53C2;&#x6570;&#x662F;1&#x7EF4;&#x7684;&#xFF0C;1&#x88AB;&#x9644;&#x52A0;&#x5230;&#x5176;&#x5C3A;&#x5BF8;&#x4E3A;&#x6210;&#x6279;&#x77E9;&#x9635;&#x7684;&#x591A;&#x4E2A;&#x76EE;&#x7684;&#x548C;&#x4E4B;&#x540E;&#x88AB;&#x53BB;&#x9664;&#x3002;&#x975E;&#x77E9;&#x9635;&#xFF08;&#x5373;&#x6279;&#xFF09;&#x5C3A;&#x5BF8;<a href="notes/broadcasting.html#broadcasting-semantics"> &#x5E7F;&#x64AD; </a>&#xFF08;&#x5E76;&#x56E0;&#x6B64;&#x5FC5;&#x987B;&#x662F;broadcastable&#xFF09;&#x3002;&#x4F8B;&#x5982;&#xFF0C;&#x5982;&#x679C;<code>&#x8F93;&#x5165;</code>&#x662F; &#xFF08; [HTG16&#xFF1A;J &#xD7; 1  &#xD7; n&#x7684; &#xD7; M  &#xFF09; &#xFF08;j \&#x500D;1 \ n&#x6B21;\&#x4E58;&#x4EE5;m&#xFF09; &#xFF08; [ HTG44&#xFF1A;J  &#xD7; 1  &#xD7; n&#x7684; &#xD7; M  &#xFF09; &#x5F20;&#x91CF;&#x548C;<code>&#x5176;&#x4ED6;</code>&#x662F; &#xFF08; K  &#xD7;[H TG103]  M  &#xD7; P  &#xFF09; &#xFF08;K \&#x500D;&#x7C73;\&#x500D;P&#xFF09; &#xFF08; K  &#xD7; M  &#xD7; p  &#xFF09; &#x5F20;&#x91CF;&#xFF0C;<code>OUT</code>&#x5C06;&#x662F; &#xFF08; [HTG168&#xFF1A;J &#xD7; K  &#xD7; n&#x7684; &#xD7; p  &#xFF09; &#xFF08;j \&#x4E58;K \ n&#x6B21;\&#x500D;p&#xFF09; &#xFF08; [HTG196&#xFF1A;J  &#xD7; K  &#xD7; n&#x7684; &#xD7; p  &#xFF09; &#x5F20;&#x91CF;&#x3002;</p>
</li>
</ul>
<p>Note</p>
<p>&#x8BE5;&#x51FD;&#x6570;&#x7684;1&#x7EF4;&#x7684;&#x70B9;&#x79EF;&#x7248;&#x672C;&#x4E0D;&#x652F;&#x6301;<code>OUT</code>&#x53C2;&#x6570;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x8981;&#x88AB;&#x76F8;&#x4E58;&#x7684;&#x7B2C;&#x4E00;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>&#x5176;&#x4ED6;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x8981;&#x88AB;&#x76F8;&#x4E58;&#x7684;&#x7B2C;&#x4E8C;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; # vector x vector
&gt;&gt;&gt; tensor1 = torch.randn(3)
&gt;&gt;&gt; tensor2 = torch.randn(3)
&gt;&gt;&gt; torch.matmul(tensor1, tensor2).size()
torch.Size([])
&gt;&gt;&gt; # matrix x vector
&gt;&gt;&gt; tensor1 = torch.randn(3, 4)
&gt;&gt;&gt; tensor2 = torch.randn(4)
&gt;&gt;&gt; torch.matmul(tensor1, tensor2).size()
torch.Size([3])
&gt;&gt;&gt; # batched matrix x broadcasted vector
&gt;&gt;&gt; tensor1 = torch.randn(10, 3, 4)
&gt;&gt;&gt; tensor2 = torch.randn(4)
&gt;&gt;&gt; torch.matmul(tensor1, tensor2).size()
torch.Size([10, 3])
&gt;&gt;&gt; # batched matrix x batched matrix
&gt;&gt;&gt; tensor1 = torch.randn(10, 3, 4)
&gt;&gt;&gt; tensor2 = torch.randn(10, 4, 5)
&gt;&gt;&gt; torch.matmul(tensor1, tensor2).size()
torch.Size([10, 3, 5])
&gt;&gt;&gt; # batched matrix x broadcasted matrix
&gt;&gt;&gt; tensor1 = torch.randn(10, 3, 4)
&gt;&gt;&gt; tensor2 = torch.randn(4, 5)
&gt;&gt;&gt; torch.matmul(tensor1, tensor2).size()
torch.Size([10, 3, 5])
</code></pre><p><code>torch.``matrix_power</code>( <em>input</em> , <em>n</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x5E42;<code>n&#x7684;</code>&#x4E3A;&#x65B9;&#x9635;&#x77E9;&#x9635;&#x3002;&#x5BF9;&#x4E8E;&#x6279;&#x91CF;&#x77E9;&#x9635;&#xFF0C;&#x6BCF;&#x4E2A;&#x5355;&#x72EC;&#x7684;&#x77E9;&#x9635;&#x88AB;&#x5347;&#x9AD8;&#x5230;&#x529F;&#x7387;<code>n&#x7684;</code>&#x3002;</p>
<p>&#x5982;&#x679C;<code>n&#x7684;</code>&#x662F;&#x5426;&#x5B9A;&#x7684;&#xFF0C;&#x5219;&#x77E9;&#x9635;&#xFF08;&#x5982;&#x679C;&#x53EF;&#x9006;&#xFF09;&#x7684;&#x9006;&#x88AB;&#x5347;&#x9AD8;&#x5230;&#x529F;&#x7387;<code>n&#x7684;</code>&#x3002;&#x5BF9;&#x4E8E;&#x95F4;&#x6B47;&#x77E9;&#x9635;&#xFF0C;&#x6210;&#x6279;&#x7684;&#x9006;&#xFF08;&#x5982;&#x679C;&#x53EF;&#x9006;&#xFF09;&#x63D0;&#x9AD8;&#x5230;&#x7535;&#x6E90;<code>n&#x7684;</code>&#x3002;&#x5982;&#x679C;<code>n&#x7684;</code>&#x4E3A;0&#xFF0C;&#x5219;&#x5355;&#x4F4D;&#x77E9;&#x9635;&#x88AB;&#x8FD4;&#x56DE;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>n&#x7684;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x529F;&#x7387;&#xFF0C;&#x4EE5;&#x63D0;&#x9AD8;&#x57FA;&#x8D28;&#x4E2D;&#x4EE5;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(2, 2, 2)
&gt;&gt;&gt; a
tensor([[[-1.9975, -1.9610],
         [ 0.9592, -2.3364]],

        [[-1.2534, -1.3429],
         [ 0.4153, -1.4664]]])
&gt;&gt;&gt; torch.matrix_power(a, 3)
tensor([[[  3.9392, -23.9916],
         [ 11.7357,  -0.2070]],

        [[  0.2468,  -6.7168],
         [  2.0774,  -0.8187]]])
</code></pre><p><code>torch.``matrix_rank</code>( <em>input</em> , <em>tol=None</em> , <em>bool symmetric=False</em> ) &#x2192;
Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;2-d&#x5F20;&#x91CF;&#x7684;&#x6570;&#x503C;&#x7B49;&#x7EA7;&#x3002;&#x8BA1;&#x7B97;&#x77E9;&#x9635;&#x7684;&#x79E9;&#x7684;&#x65B9;&#x6CD5;&#x662F;&#x4F7F;&#x7528;SVD&#x9ED8;&#x8BA4;&#x60C5;&#x51B5;&#x4E0B;&#x5B8C;&#x6210;&#x7684;&#x3002;&#x5982;&#x679C;<code>&#x5BF9;&#x79F0;</code>&#x662F;<code>&#x771F;</code>&#xFF0C;&#x7136;&#x540E;<code>&#x8F93;&#x5165;</code>&#x88AB;&#x5047;&#x8BBE;&#x4E3A;&#x662F;&#x5BF9;&#x79F0;&#x7684;&#xFF0C;&#x5E76;&#x4E14;&#x6392;&#x540D;&#x7684;&#x8BA1;&#x7B97;&#x662F;&#x901A;&#x8FC7;&#x83B7;&#x53D6;&#x7279;&#x5F81;&#x503C;&#x6765;&#x5B8C;&#x6210;&#x3002;</p>
<p><code>TOL</code>&#x662F;&#x4F4E;&#x4E8E;&#x8BE5;&#x5947;&#x5F02;&#x503C;&#xFF08;&#x6216;&#x672C;&#x5F81;&#x503C;&#x65F6;<code>&#x5BF9;&#x79F0;</code>&#x662F;<code>&#x771F;&#x9608;&#x503C;</code>&#xFF09;&#x90FD;&#x88AB;&#x8BA4;&#x4E3A;&#x662F;0&#x3002;&#x5982;&#x679C;<code>TOL</code>&#x6CA1;&#x6709;&#x6307;&#x5B9A;&#xFF0C;<code>TOL</code>&#x88AB;&#x8BBE;&#x5B9A;&#x4E3A;`S.max&#xFF08;&#xFF09;</p>
<ul>
<li>&#x6700;&#x5927;&#x503C;&#xFF08;S.size&#xFF08;&#xFF09;&#xFF09; *  EPS<code>&#x5176;&#x4E2D; S &#x4E3A;&#x5947;&#x5F02;&#x503C;&#xFF08;&#x6216;&#x672C;&#x5F81;&#x503C;&#x65F6;</code>&#x5BF9;&#x79F0; <code>&#x662F;</code>&#x771F; <code>&#xFF09;&#x548C;</code>EPS<code>&#x662F;&#x7528;&#x4E8E;</code>&#x8F93;&#x5165;
`&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x7684;&#x3B5;&#x503C;&#x3002;</li>
</ul>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x8F93;&#x5165;2-d&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>TOL</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x516C;&#x5DEE;&#x503C;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x65E0;</code></p>
</li>
<li><p><strong>&#x5BF9;&#x79F0;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x6307;&#x793A;&#x662F;&#x5426;<code>&#x8F93;&#x5165;</code>&#x662F;&#x5BF9;&#x79F0;&#x7684;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x5047;</code></p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.eye(10)
&gt;&gt;&gt; torch.matrix_rank(a)
tensor(10)
&gt;&gt;&gt; b = torch.eye(10)
&gt;&gt;&gt; b[0, 0] = 0
&gt;&gt;&gt; torch.matrix_rank(b)
tensor(9)
</code></pre><p><code>torch.``mm</code>( <em>input</em> , <em>mat2</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x6267;&#x884C;&#x77E9;&#x9635;<code>&#x8F93;&#x5165;</code>&#x548C;<code>MAT2</code>&#x7684;&#x77E9;&#x9635;&#x4E58;&#x6CD5;&#x3002;</p>
<p>&#x5982;&#x679C;<code>&#x8F93;&#x5165;</code>&#x662F; &#xFF08; n&#x7684; &#xD7; M  &#xFF09; &#xFF08;N \&#x4E58;&#x4EE5;m&#xFF09; &#xFF08; n&#x7684; &#xD7; M  &#xFF09; &#x5F20;&#x91CF;&#xFF0C;<code>MAT2</code>&#x662F; &#xFF08; M  &#xD7; p  &#xFF09; &#xFF08;M \&#x500D;p&#xFF09; &#xFF08;
M  &#xD7; p  &#xFF09; &#x5F20;&#x91CF;&#xFF0C;<code>OUT</code>&#x5C06;&#x662F; &#xFF08; N  &#xD7; p  &#xFF09; &#xFF08;N \&#x500D;p&#xFF09; &#xFF08; n&#x7684; &#xD7; p  &#xFF09; &#x5F20;&#x91CF;&#x3002;</p>
<p>Note</p>
<p>This function does not <a href="notes/broadcasting.html#broadcasting-%0Asemantics">broadcast</a>. For broadcasting matrix products, see <code>torch.matmul()</code>.</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x8981;&#x88AB;&#x76F8;&#x4E58;&#x7684;&#x7B2C;&#x4E00;&#x77E9;&#x9635;</p>
</li>
<li><p><strong>mat2</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the second matrix to be multiplied</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; mat1 = torch.randn(2, 3)
&gt;&gt;&gt; mat2 = torch.randn(3, 3)
&gt;&gt;&gt; torch.mm(mat1, mat2)
tensor([[ 0.4851,  0.5037, -0.3633],
        [-0.0760, -3.6705,  2.4784]])
</code></pre><p><code>torch.``mv</code>( <em>input</em> , <em>vec</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x6267;&#x884C;&#x77E9;&#x9635;<code>&#x8F93;&#x5165;</code>&#x548C;&#x7684;&#x77E9;&#x9635;&#x77E2;&#x91CF;&#x4E58;&#x79EF;&#x77E2;&#x91CF;<code>VEC</code>&#x3002;</p>
<p>&#x5982;&#x679C;<code>&#x8F93;&#x5165;</code>&#x662F; &#xFF08; n&#x7684; &#xD7; M  &#xFF09; &#xFF08;N \&#x4E58;&#x4EE5;m&#xFF09; &#xFF08; n&#x7684; &#xD7; M  &#xFF09; &#x5F20;&#x91CF;&#xFF0C;<code>VEC</code>&#x662F;&#x5927;&#x5C0F;[1-d&#x5F20;&#x91CF;HTG56]  M  M
[&#x5927;&#x5C0F;&#x7684;HTG72]  M  &#xFF0C;<code>OUT</code>&#x5C06;1-d  n&#x7684; n&#x7684; n&#x7684; &#x3002;</p>
<p>Note</p>
<p>This function does not <a href="notes/broadcasting.html#broadcasting-%0Asemantics">broadcast</a>.</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x8981;&#x88AB;&#x76F8;&#x4E58;&#x7684;&#x77E9;&#x9635;</p>
</li>
<li><p><strong>vec</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; vector to be multiplied</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; mat = torch.randn(2, 3)
&gt;&gt;&gt; vec = torch.randn(3)
&gt;&gt;&gt; torch.mv(mat, vec)
tensor([ 1.0404, -0.6361])
</code></pre><p><code>torch.``orgqr</code>( <em>input</em> , <em>input2</em> ) &#x2192; Tensor</p>
<p>&#x8BA1;&#x7B97;&#x6B63;&#x4EA4;&#x77E9;&#x9635; Q &#x4E00;&#x4E2A;QR&#x5206;&#x89E3;&#x7684;&#xFF0C;&#x4ECE;&#xFF08;&#x8F93;&#x5165;&#xFF0C;&#x8F93;&#x5165;2&#xFF09;&#x5143;&#x7EC4;&#x901A;&#x8FC7;&#x8FD4;&#x56DE; <code>torch.geqrf&#xFF08;&#xFF09;</code>&#x3002;</p>
<p>&#x8FD9;&#x76F4;&#x63A5;&#x8C03;&#x7528;&#x5E95;&#x5C42;&#x7684;LAPACK&#x51FD;&#x6570; orgqr [HTG1&#x3002;&#x53C2;&#x89C1;[HTG2&#x5BF9;&#x4E8E;orgqr &#x4E3A;&#x8FDB;&#x4E00;&#x6B65;&#x7684;&#x7EC6;&#x8282;LAPACK&#x6587;&#x6863;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x7684;&#x4E00;&#x4ECE; <code>torch.geqrf&#xFF08;&#xFF09;</code>&#x3002;</p>
</li>
<li><p><strong>&#x8F93;&#x5165;2</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x7684; tau&#x86CB;&#x767D;&#x4ECE; <code>torch.geqrf&#xFF08;&#xFF09;</code>&#x3002;</p>
</li>
</ul>
<p><code>torch.``ormqr</code>( <em>input</em> , <em>input2</em> , <em>input3</em> , <em>left=True</em> ,
<em>transpose=False</em> ) &#x2192; Tensor</p>
<p>&#x4E58;&#x6CD5;&#x57AB;&#x901A;&#x8FC7; <code>[&#x5F62;&#x6210;&#x7684;QR&#x5206;&#x89E3;&#x7684;&#x6B63;&#x4EA4; Q &#x77E9;&#x9635;HTG10&#xFF08;&#x7531;</code>&#x8F93;&#x5165;3<code>&#x7ED9;&#x51FA;&#xFF09; ] torch.geqrf&#xFF08;&#xFF09;</code>&#x7531;&#xFF08;A&#xFF0C;tau&#x86CB;&#x767D;&#xFF09;&#x8868;&#x793A;&#xFF08;&#x7531;&#xFF08;<code>&#x8F93;&#x5165;</code>&#xFF0C;[HTG20&#x7ED9;&#x51FA;] &#x8F93;&#x5165;2  &#xFF09;&#xFF09;&#x3002;</p>
<p>&#x8FD9;&#x76F4;&#x63A5;&#x8C03;&#x7528;&#x5E95;&#x5C42;&#x7684;LAPACK&#x51FD;&#x6570; ormqr [HTG1&#x3002;&#x53C2;&#x89C1;<a href="https://software.intel.com/en-us/mkl-developer-reference-c-ormqr" target="_blank"> LAPACK&#x6587;&#x6863;ormqr
</a>&#x4E3A;&#x8FDB;&#x4E00;&#x6B65;&#x7684;&#x7EC6;&#x8282;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the a from <code>torch.geqrf()</code>.</p>
</li>
<li><p><strong>input2</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; the tau from <code>torch.geqrf()</code>.</p>
</li>
<li><p><strong>&#x8F93;&#x5165;3</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x8981;&#x76F8;&#x4E58;&#x7684;&#x77E9;&#x9635;&#x3002;</p>
</li>
</ul>
<p><code>torch.``pinverse</code>( <em>input</em> , <em>rcond=1e-15</em> ) &#x2192; Tensor</p>
<p>&#x8BA1;&#x7B97;2D&#x5F20;&#x91CF;&#x7684;&#x4F2A;&#x9006;&#xFF08;&#x4E5F;&#x88AB;&#x79F0;&#x4E3A;Moore-Penrose&#x9006;&#xFF09;&#x3002;&#x8BF7;&#x770B;<a href="https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse" target="_blank"> Moore-
Penrose&#x9006;</a>&#x66F4;&#x591A;&#x7EC6;&#x8282;</p>
<p>Note</p>
<p>&#x8BE5;&#x65B9;&#x6CD5;&#x662F;&#x4F7F;&#x7528;&#x5947;&#x5F02;&#x503C;&#x5206;&#x89E3;&#x6765;&#x5B9E;&#x73B0;&#x3002;</p>
<p>Note</p>
<p>&#x4F2A;&#x9006;&#x4E0D;&#x4E00;&#x5B9A;&#x662F;&#x77E9;&#x9635;<a href="https://epubs.siam.org/doi/10.1137/0117004" target="_blank"> [1]
</a>&#x7684;&#x5143;&#x7D20;&#x7684;&#x8FDE;&#x7EED;&#x51FD;&#x6570;&#x3002;&#x56E0;&#x6B64;&#xFF0C;&#x884D;&#x751F;&#x7269;&#x5E76;&#x4E0D;&#x603B;&#x662F;&#x5B58;&#x5728;&#x7684;&#xFF0C;&#x5E76;&#x4E14;&#x5B58;&#x5728;&#x5BF9;&#x4E8E;&#x6052;&#x5B9A;&#x79E9;&#x53EA;<a href="https://www.jstor.org/stable/2156365" target="_blank">
[2]
</a>&#x3002;&#x4F46;&#x662F;&#xFF0C;&#x8FD9;&#x79CD;&#x65B9;&#x6CD5;&#x662F;backprop&#xFF0C;&#x7531;&#x4E8E;&#x80FD;&#x591F;&#x901A;&#x8FC7;&#x4F7F;&#x7528;SVD&#x7ED3;&#x679C;&#x7684;&#x6267;&#x884C;&#xFF0C;&#x53EF;&#x80FD;&#x662F;&#x4E0D;&#x7A33;&#x5B9A;&#x7684;&#x3002;&#x53CC;&#x843D;&#x540E;&#x4E5F;&#x5C06;&#x662F;&#x4E0D;&#x7A33;&#x5B9A;&#x7684;&#xFF0C;&#x7531;&#x4E8E;SVD&#x7684;&#x4F7F;&#x7528;&#x5185;&#x90E8;&#x3002;&#x53C2;&#x89C1;
<code>SVD&#xFF08;&#xFF09;</code>&#x7684;&#x66F4;&#x591A;&#x7EC6;&#x8282;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x7684;&#x8F93;&#x5165;&#x7684;2D&#x5F20;&#x91CF;&#x5C3A;&#x5BF8; M  &#xD7; n&#x7684; &#x7C73;\ n&#x6B21; M  &#xD7; n&#x7684;</p>
</li>
<li><p><strong>rcond</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a>&#xFF09; - &#x7532;&#x6D6E;&#x70B9;&#x503C;&#x6765;&#x786E;&#x5B9A;&#x7528;&#x4E8E;&#x5C0F;&#x5947;&#x5F02;&#x503C;&#x622A;&#x6B62;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;1E-15</p>
</li>
</ul>
<p>Returns</p>
<p>&#x7684;<code>&#x8F93;&#x5165;&#x5C3A;&#x5BF8;&#x7684;</code>n&#x7684; &#xD7;&#x4F2A;&#x9006; M  n&#x7684;\&#x4E58;&#x4EE5;m  n&#x7684; &#xD7; M</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; input = torch.randn(3, 5)
&gt;&gt;&gt; input
tensor([[ 0.5495,  0.0979, -1.4092, -0.1128,  0.4132],
        [-1.1143, -0.3662,  0.3042,  1.6374, -0.9294],
        [-0.3269, -0.5745, -0.0382, -0.5922, -0.6759]])
&gt;&gt;&gt; torch.pinverse(input)
tensor([[ 0.0600, -0.1933, -0.2090],
        [-0.0903, -0.0817, -0.4752],
        [-0.7124, -0.1631, -0.2272],
        [ 0.1356,  0.3933, -0.5023],
        [-0.0308, -0.1725, -0.5216]])
</code></pre><p><code>torch.``qr</code>( <em>input</em> , <em>some=True</em> , <em>out=None) - &gt; (Tensor</em>, <em>Tensor</em> )</p>
<p>&#x8BA1;&#x7B97;&#x4E00;&#x4E2A;&#x77E9;&#x9635;&#x7684;QR&#x5206;&#x89E3;&#x6216;&#x5206;&#x6279;&#x77E9;&#x9635;<code>&#x8F93;&#x5165;</code>&#x7684;&#xFF0C;&#x5E76;&#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x7684;namedtuple&#xFF08;Q&#xFF0C;R&#xFF09;&#xFF0C;&#x4F7F;&#x5F97; &#x8F93;&#x5165; =  Q  R  \&#x6587;&#x672C;{&#x8F93;&#x5165;} = QR  &#x8F93;&#x5165;
=  Q  R  &#x4E0E; Q  Q  Q  &#x4E3A;&#x6B63;&#x4EA4;&#x77E9;&#x9635;&#x7684;&#x6B63;&#x4EA4;&#x77E9;&#x9635;&#x6216;&#x5206;&#x6279;&#x548C; R  R  R  &#x4E3A;&#x4E0A;&#x4E09;&#x89D2;&#x77E9;&#x9635;&#x6216;&#x6279;&#x91CF;&#x4E0A;&#x4E09;&#x89D2;&#x77E9;&#x9635;&#x3002;</p>
<p>&#x5982;&#x679C;<code>&#x4E00;&#x4E9B;</code>&#x662F;<code>&#x771F;</code>&#xFF0C;&#x5219;&#x8BE5;&#x51FD;&#x6570;&#x5C06;&#x8FD4;&#x56DE;&#x8584;&#xFF08;&#x51CF;&#x5C0F;&#xFF09;QR&#x5206;&#x89E3;&#x3002;&#x5426;&#x5219;&#xFF0C;&#x5982;&#x679C;<code>&#x4E00;&#x4E9B;</code>&#x662F;<code>&#x5047;</code>&#xFF0C;&#x8BE5;&#x51FD;&#x6570;&#x8FD4;&#x56DE;&#x5B8C;&#x6574;&#x7684;QR&#x5206;&#x89E3;&#x3002;</p>
<p>Note</p>
<p>&#x5982;&#x679C;<code>&#x8F93;&#x5165;&#x7684;&#x5143;&#x7D20;&#x7684;&#x91CF;&#x503C;</code>&#x662F;&#x5927;&#x7CBE;&#x5EA6;&#x53EF;&#x80FD;&#x4F1A;&#x4E22;&#x5931;</p>
<p>Note</p>
<p>&#x867D;&#x7136;&#x5B83;&#x5E94;&#x8BE5;&#x603B;&#x662F;&#x7ED9;&#x4F60;&#x4E00;&#x4E2A;&#x6709;&#x6548;&#x7684;&#x5206;&#x89E3;&#xFF0C;&#x5B83;&#x53EF;&#x80FD;&#x4E0D;&#x4F1A;&#x7ED9;&#x4F60;&#x8DE8;&#x5E73;&#x53F0;&#x7684;&#x540C;&#x4E00;&#x4E2A; - &#x8FD9;&#x5C06;&#x53D6;&#x51B3;&#x4E8E;&#x4F60;&#x7684;LAPACK&#x5B9E;&#x73B0;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x5927;&#x5C0F; &#x7684;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#xFF08;  <em>  &#xFF0C; M  &#xFF0C; n&#x7684; &#xFF09; &#xFF08;</em>&#xFF0C;M&#xFF0C;N&#xFF09; &#xFF08; <em>  M  &#xFF0C; n&#x7684; &#xFF09; &#x5176;&#x4E2D; </em> &#x662F;&#x96F6;&#x4E2A;&#x6216;&#x591A;&#x4E2A;&#x7531;&#x5C3A;&#x5BF8; M &#x77E9;&#x9635;[&#x6279;&#x91CF;&#x5C3A;&#x5BF8;HTG68]&#xD7;  n&#x7684; &#x7C73;\ n&#x6B21; M  &#xD7; n&#x7684; &#x3002;</p>
</li>
<li><p><strong>&#x4E00;&#x4E9B;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x8BBE;&#x7F6E;&#x4E3A;<code>&#x771F; [ HTG13&#x7528;&#x4E8E;&#x51CF;&#x5C11;QR&#x5206;&#x89E3;&#x548C;</code>[HTG15&#x7528;&#x4E8E;&#x5B8C;&#x6574;QR&#x5206;&#x89E3;&#x5047; <code>&#x3002;</code></p>
</li>
<li><p><strong>OUT</strong> &#xFF08;<a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>&#x5143;&#x7EC4;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x7684; Q &#x548C;&#x5143;&#x7EC4; [R &#x5F20;&#x91CF;&#x6EE1;&#x8DB3;<code>&#x8F93;&#x5165; =  torch.matmul&#xFF08;Q&#xFF0C; R&#xFF09;</code>&#x3002;  Q &#x548C;&#x7684;&#x5C3A;&#x5BF8; R &#x662F; &#xFF08; <em>  &#xFF0C; M  &#xFF0C; K  &#xFF09; &#xFF08;</em>&#xFF0C;M&#xFF0C;K&#xFF09; &#xFF08; <em>  &#xFF0C; M  &#xFF0C; K  &#xFF09; &#x548C; &#xFF08; </em>  &#xFF0C; K  &#xFF0C; n&#x7684; &#xFF09; &#xFF08;<em>&#xFF0C;K&#xFF0C;N&#xFF09; &#xFF08; </em>  &#xFF0C; K  &#xFF0C; n&#x7684; &#xFF09; &#x5206;&#x522B;&#xFF0C;&#x5176;&#x4E2D; K  =  &#x5206;&#x949F;HTG143] &#x2061; &#xFF08; M  &#xFF0C; n&#x7684; &#xFF09; K = \&#x5206;&#x949F;&#xFF08;M&#xFF0C;N&#xFF09; K  =  &#x5206;&#x949F;HTG179] &#xFF08; M  &#xFF0C; n&#x7684; &#xFF09; &#x5982;&#x679C;<code>&#x4E00;&#x4E9B;&#xFF1A;</code>&#x662F;<code>&#x771F;</code>&#x548C; K  =  M  K = M  K  =  M  &#x5426;&#x5219;&#x3002;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.tensor([[12., -51, 4], [6, 167, -68], [-4, 24, -41]])
&gt;&gt;&gt; q, r = torch.qr(a)
&gt;&gt;&gt; q
tensor([[-0.8571,  0.3943,  0.3314],
        [-0.4286, -0.9029, -0.0343],
        [ 0.2857, -0.1714,  0.9429]])
&gt;&gt;&gt; r
tensor([[ -14.0000,  -21.0000,   14.0000],
        [   0.0000, -175.0000,   70.0000],
        [   0.0000,    0.0000,  -35.0000]])
&gt;&gt;&gt; torch.mm(q, r).round()
tensor([[  12.,  -51.,    4.],
        [   6.,  167.,  -68.],
        [  -4.,   24.,  -41.]])
&gt;&gt;&gt; torch.mm(q.t(), q).round()
tensor([[ 1.,  0.,  0.],
        [ 0.,  1., -0.],
        [ 0., -0.,  1.]])
&gt;&gt;&gt; a = torch.randn(3, 4, 5)
&gt;&gt;&gt; q, r = torch.qr(a, some=False)
&gt;&gt;&gt; torch.allclose(torch.matmul(q, r), a)
True
&gt;&gt;&gt; torch.allclose(torch.matmul(q.transpose(-2, -1), q), torch.eye(5))
True
</code></pre><p><code>torch.``solve</code>( <em>input</em> , <em>A</em> , <em>out=None) - &gt; (Tensor</em>, <em>Tensor</em> )</p>
<p>&#x8BE5;&#x51FD;&#x6570;&#x8FD4;&#x56DE;&#x6EB6;&#x6DB2;&#x5230;&#x7EBF;&#x6027;&#x65B9;&#x7A0B;&#x7EC4;&#x7531; A  X  = [&#x6240;&#x8868;&#x793A;&#x7684;&#x7CFB;&#x7EDF;HTG11]  B  AX = B  A  X  =  B
&#x548C;A&#x7684;LU&#x5206;&#x89E3;&#xFF0C;&#x4E3A;&#x4E86;&#x4F5C;&#x4E3A;namedtuple &#x6EB6;&#x6DB2;&#xFF0C;LU &#x3002;</p>
<p>LU &#x5305;&#x542B; L &#x548C; U &#x56E0;&#x7D20;&#x7684; A  LU&#x5206;&#x89E3;&#x3002;</p>
<p>torch.solve&#xFF08;B&#xFF0C;A&#xFF09;&#x53EF;&#x4EE5;&#x5728;&#x90A3;&#x4E9B;2D&#x77E9;&#x9635;&#x7684;&#x6279;&#x6B21;&#x7684;2D&#x8F93;&#x5165;&#x7AEF; B&#xFF0C;A &#x6216;&#x8F93;&#x5165;&#x3002;&#x5982;&#x679C;&#x8F93;&#x5165;&#x662F;&#x6279;&#x6B21;&#xFF0C;&#x7136;&#x540E;&#x8FD4;&#x56DE;&#x6210;&#x6279;&#x8F93;&#x51FA;&#x6EB6;&#x6DB2;&#xFF0C;LU &#x3002;</p>
<p>Note</p>
<p>&#x4E0D;&#x7BA1;&#x539F;&#x59CB;&#x8FDB;&#x5C55;&#x7684;&#xFF0C;&#x5219;&#x8FD4;&#x56DE;&#x7684;&#x77E9;&#x9635;&#x6EB6;&#x6DB2;&#x548C; LU &#x5C06;&#x88AB;&#x8F6C;&#x7F6E;&#xFF0C;&#x5373;&#x5177;&#x6709;&#x5982;&#x6B65;&#x5E45;B.contiguous&#xFF08;&#xFF09;&#x3002;&#x8F6C;&#x7F6E;&#xFF08;-1&#xFF0C;-2&#xFF09;&#x3002;&#x6B65;&#x5E45;&#xFF08;&#xFF09;&#x548C;
A.contiguous&#xFF08;&#xFF09;&#x3002;&#x8F6C;&#x7F6E;&#xFF08;-1&#xFF0C;-2&#xFF09;.stride&#xFF08;&#xFF09;&#x5206;&#x522B;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x8F93;&#x5165;&#x77E9;&#x9635; B  B  B  &#x5927;&#x5C0F;&#x7684; &#xFF08; <em>  &#xFF0C; M  &#xFF0C; K  &#xFF09; &#xFF08;</em>&#xFF0C;M&#xFF0C;K&#xFF09; &#xFF08; <em>  &#xFF0C; M  &#xFF0C; K  &#xFF09; &#xFF0C;&#x5176;&#x4E2D; </em>  <em>  </em>  &#x662F;&#x96F6;&#x7C73;&#x6216;&#x591A;&#x4E2A;&#x6279;&#x6B21;&#x7684;&#x5C3A;&#x5BF8;&#x3002;</p>
</li>
<li><p><strong>A</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x8F93;&#x5165;&#x5C3A;&#x5BF8; &#x7684;&#x65B9;&#x77E9;&#x9635;&#xFF08;  <em>  &#xFF0C; M  &#xFF0C; M  &#xFF09; &#xFF08;</em>&#xFF0C;M&#xFF0C;M&#xFF09; &#xFF08; <em>  M  &#xFF0C; M  &#xFF09; &#xFF0C;&#x5176;&#x4E2D; </em>  <em>  </em>  &#x662F;&#x96F6;&#x7C73;&#x6216;&#x591A;&#x4E2A;&#x6279;&#x6B21;&#x7684;&#x5C3A;&#x5BF8;&#x3002;</p>
</li>
<li><p><strong>OUT</strong> &#xFF08; <em>&#xFF08;</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a> <em>&#xFF0C;</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a> <em>&#xFF09;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x53EF;&#x9009;&#x7684;&#x8F93;&#x51FA;&#x5143;&#x7EC4;&#x3002;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; A = torch.tensor([[6.80, -2.11,  5.66,  5.97,  8.23],
                      [-6.05, -3.30,  5.36, -4.44,  1.08],
                      [-0.45,  2.58, -2.70,  0.27,  9.04],
                      [8.32,  2.71,  4.35,  -7.17,  2.14],
                      [-9.67, -5.14, -7.26,  6.08, -6.87]]).t()
&gt;&gt;&gt; B = torch.tensor([[4.02,  6.19, -8.22, -7.57, -3.03],
                      [-1.56,  4.00, -8.67,  1.75,  2.86],
                      [9.81, -4.09, -4.57, -8.61,  8.99]]).t()
&gt;&gt;&gt; X, LU = torch.solve(B, A)
&gt;&gt;&gt; torch.dist(B, torch.mm(A, X))
tensor(1.00000e-06 *
       7.0977)

&gt;&gt;&gt; # Batched solver example
&gt;&gt;&gt; A = torch.randn(2, 3, 1, 4, 4)
&gt;&gt;&gt; B = torch.randn(2, 3, 1, 4, 6)
&gt;&gt;&gt; X, LU = torch.solve(B, A)
&gt;&gt;&gt; torch.dist(B, A.matmul(X))
tensor(1.00000e-06 *
   3.6386)
</code></pre><p><code>torch.``svd</code>( <em>input</em> , <em>some=True</em> , <em>compute_uv=True</em> , <em>out=None) - &gt;
(Tensor</em>, <em>Tensor</em> , <em>Tensor</em> )</p>
<p>&#x8BE5;&#x51FD;&#x6570;&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;namedtuple <code>&#xFF08;U&#xFF0C; S&#xFF0C; V&#xFF09;</code>&#x8FD9;&#x662F;&#x4E00;&#x4E2A;&#x8F93;&#x5165;&#x5B9E;&#x77E9;&#x9635;&#x6216;&#x6279;&#x6B21;&#x7684;&#x5947;&#x5F02;&#x503C;&#x5206;&#x89E3;&#x5B9E;&#x6570;&#x77E9;&#x9635;<code>&#x8F93;&#x5165;</code>&#xFF0C;&#x4F7F;&#x5F97; i&#x7684; n&#x7684; p  U  T
=  U  &#xD7; d  i&#x7684; &#x4E00; &#x514B; &#xFF08; S  &#xFF09; &#xD7; [HTG51&#x3011;V  T  &#x8F93;&#x5165;= U \&#x500D;DIAG&#xFF08;S&#xFF09;\&#x500D;&#x4E8E;V ^ T  i&#x7684; n&#x7684; p  U  T
=  U  &#xD7; d  i&#x7684; &#x4E00; &#x514B; &#xFF08; S  &#xFF09; &#xD7; [HTG123&#x3011;V  T  &#x3002;</p>
<p>&#x5982;&#x679C;<code>&#x4E00;&#x4E9B;</code>&#x662F;<code>&#x771F;</code>&#xFF08;&#x9ED8;&#x8BA4;&#xFF09;&#xFF0C;&#x5219;&#x8BE5;&#x65B9;&#x6CD5;&#x8FD4;&#x56DE;&#x964D;&#x4F4E;&#x5947;&#x5F02;&#x503C;&#x5206;&#x89E3;&#xFF0C;&#x5373;&#xFF0C;&#x5982;&#x679C;[HTG8&#x6700;&#x540E;&#x4E24;&#x4E2A;&#x7EF4;&#x5EA6;] &#x8F93;&#x5165; &#x662F;<code>M</code>&#x548C;<code>n&#x7684;</code>&#xFF0C;&#x5219;&#x8FD4;&#x56DE;&#x7684; U
&#x548C;[HTG22&#x3011;V &#x77E9;&#x9635;&#x5C06;&#x4EC5;&#x5305;&#x542B; M  i&#x7684; n&#x7684; &#xFF08; n&#x7684; &#xFF0C; M  &#xFF09; &#x5206;&#x949F;&#xFF08;N&#xFF0C;M&#xFF09; M  i&#x7684; n&#x7684; &#xFF08; n&#x7684; &#xFF0C; M  &#xFF09; &#x6B63;&#x4EA4;&#x5217;&#x3002;</p>
<p>&#x5982;&#x679C;<code>compute_uv</code>&#x662F;<code>&#x5047;</code>&#xFF0C;&#x8FD4;&#x56DE; U &#x548C;[HTG10&#x3011;V &#x77E9;&#x9635;&#x5C06;&#x662F;&#x96F6;&#x7684;&#x5F62;&#x72B6; &#xFF08; &#x7C73;&#x77E9;&#x9635; &#xD7; M  &#xFF09; &#xFF08;M \&#x4E58;&#x4EE5;m&#xFF09; &#xFF08; M  &#xD7; M
&#xFF09; &#x548C; &#xFF08; n&#x7684; &#xD7;  n&#x7684; &#xFF09; &#xFF08;N \ n&#x6B21;&#xFF09; &#xFF08; n&#x7684; &#xD7; n&#x7684; &#xFF09; &#x5206;&#x522B;&#x3002; <code>&#x4E00;&#x4E9B;</code>&#x5C06;&#x5728;&#x8FD9;&#x91CC;&#x88AB;&#x5FFD;&#x7565;&#x4E86;&#x3002;</p>
<p>Note</p>
<p>SVD&#x7684;CPU&#x4E0A;&#x7684;&#x5B9E;&#x73B0;&#x4F7F;&#x7528;LAPACK&#x4F8B;&#x7A0B; gesdd &#xFF08;&#x5206;&#x800C;&#x6CBB;&#x4E4B;&#x7B97;&#x6CD5;&#xFF09;&#x4EE3;&#x66FF; gesvd [HTG3&#x7528;&#x4E8E;&#x901F;&#x5EA6;&#x3002;&#x7C7B;&#x4F3C;&#x5730;&#xFF0C;&#x5728;GPU&#x4E0A;&#x7684;SVD&#x4F7F;&#x7528;MAGMA&#x4F8B;&#x7A0B;
gesdd &#x4E3A;&#x597D;&#x3002;</p>
<p>Note</p>
<p>&#x4E0D;&#x7BA1;&#x539F;&#x59CB;&#x8FDB;&#x5C55;&#x7684;&#xFF0C;&#x5219;&#x8FD4;&#x56DE;&#x7684;&#x77E9;&#x9635; U &#x5C06;&#x88AB;&#x8F6C;&#x7F6E;&#xFF0C;&#x5373;&#x5177;&#x6709;&#x6B65;&#x5E45;<code>U.contiguous&#xFF08;&#xFF09;&#x3002;&#x8F6C;&#x7F6E;&#xFF08;-2&#xFF0C; -1&#xFF09;&#x3002;&#x6B65;&#x5E45;&#xFF08;&#xFF09;</code></p>
<p>Note</p>
<p>&#x683C;&#x5916;&#x5C0F;&#x5FC3;&#x9700;&#x8981;&#x901A;&#x8FC7; U &#x548C;[HTG2&#x3011;V &#x8F93;&#x51FA;&#x65F6;&#x5411;&#x540E;&#x670D;&#x7528;&#x3002;&#x8FD9;&#x6837;&#x7684;&#x64CD;&#x4F5C;&#x771F;&#x7684;&#x53EA;&#x6709;&#x7A33;&#x5B9A;&#x65F6;<code>&#x8F93;&#x5165;</code>&#x4E0E;&#x6240;&#x6709;&#x4E0D;&#x540C;&#x7684;&#x5947;&#x5F02;&#x503C;&#x6EE1;&#x79E9;&#x3002;&#x5426;&#x5219;&#xFF0C;<code>&#x7684;NaN</code>&#x53EF;&#x4EE5;&#x4F5C;&#x4E3A;&#x68AF;&#x5EA6;&#x672A;&#x6B63;&#x786E;&#x5B9A;&#x4E49;&#x51FA;&#x73B0;&#x3002;&#x6B64;&#x5916;&#xFF0C;&#x8BF7;&#x6CE8;&#x610F;&#x53CC;&#x5411;&#x540E;&#x901A;&#x5E38;&#x5C06;&#x901A;&#x8FC7; U &#x548C;&#x505A;&#x9644;&#x52A0;&#x7684;&#x5012;V&#x578B;&#x5373;&#x4F7F;&#x539F;&#x59CB;&#x5411;&#x540E;&#x53EA;&#x5728; S &#x3002;</p>
<p>Note</p>
<p>&#x5F53;<code>&#x4E00;&#x4E9B;</code>= <code>&#x5047;</code>&#xFF0C;&#x5728;<code>&#x68AF;&#x5EA6;U [...&#xFF0C; &#xFF1A; &#x5206;&#x949F;&#xFF08;&#x7C73;&#xFF0C; N&#xFF09;&#xFF1A;]</code>&#x548C;<code>[HTG19&#x3011;V [...&#xFF0C; &#xFF1A; &#x5206;&#x949F;&#xFF08;&#x7C73;&#xFF0C; N&#xFF09;&#xFF1A;]</code>&#x5C06;&#x5728;&#x5411;&#x540E;&#x5FFD;&#x7565;&#x90A3;&#x4E9B;&#x8F7D;&#x4F53;&#x53EF;&#x4EE5;&#x662F;&#x5B50;&#x7A7A;&#x95F4;&#x7684;&#x4EFB;&#x610F;&#x78B1;&#x3002;</p>
<p>Note</p>
<p>&#x5F53;<code>compute_uv</code>= <code>&#x5047;</code>&#xFF0C;&#x5411;&#x540E;&#x4E0D;&#x80FD;&#x56E0;&#x4E3A; U &#x8FDB;&#x884C;&#x5E76;[HTG10&#x3011;V &#x4ECE;&#x76F4;&#x4F20;&#x662F;&#x6240;&#x5FC5;&#x9700;&#x7684;&#x5411;&#x540E;&#x64CD;&#x4F5C;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x5927;&#x5C0F; &#x7684;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#xFF08;  <em>  &#xFF0C; M  &#xFF0C; n&#x7684; &#xFF09; &#xFF08;</em>&#xFF0C;M&#xFF0C;N&#xFF09; &#xFF08; <em>  M  &#xFF0C; n&#x7684; &#xFF09; &#x5176;&#x4E2D; </em> &#x662F;&#x96F6;&#x4E2A;&#x6216;&#x591A;&#x4E2A;&#x9009;&#x81EA;&#x7531;M &#xD7;&#x7684; &#x6279;&#x6B21;&#x5C3A;&#x5BF8; n&#x7684; &#x7C73;\ n&#x6B21; M  &#xD7; n&#x7684; &#x77E9;&#x9635;&#x3002;</p>
</li>
<li><p><strong>&#x4E00;&#x4E9B;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x63A7;&#x5236;&#x7684;&#x8FD4;&#x56DE;&#x5F62;&#x72B6;U &#x548C;[HTG12&#x3011;V </p>
</li>
<li><p><strong>compute_uv</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x9009;&#x9879;&#x662F;&#x5426;&#x8BA1;&#x7B97; U &#x548C;[ HTG12&#x3011;V &#x6216;&#x4E0D;</p>
</li>
<li><p><strong>OUT</strong> &#xFF08;<a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>&#x5143;&#x7EC4;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5F20;&#x91CF;&#x7684;&#x8F93;&#x51FA;&#x5143;&#x7EC4;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(5, 3)
&gt;&gt;&gt; a
tensor([[ 0.2364, -0.7752,  0.6372],
        [ 1.7201,  0.7394, -0.0504],
        [-0.3371, -1.0584,  0.5296],
        [ 0.3550, -0.4022,  1.5569],
        [ 0.2445, -0.0158,  1.1414]])
&gt;&gt;&gt; u, s, v = torch.svd(a)
&gt;&gt;&gt; u
tensor([[ 0.4027,  0.0287,  0.5434],
        [-0.1946,  0.8833,  0.3679],
        [ 0.4296, -0.2890,  0.5261],
        [ 0.6604,  0.2717, -0.2618],
        [ 0.4234,  0.2481, -0.4733]])
&gt;&gt;&gt; s
tensor([2.3289, 2.0315, 0.7806])
&gt;&gt;&gt; v
tensor([[-0.0199,  0.8766,  0.4809],
        [-0.5080,  0.4054, -0.7600],
        [ 0.8611,  0.2594, -0.4373]])
&gt;&gt;&gt; torch.dist(a, torch.mm(torch.mm(u, torch.diag(s)), v.t()))
tensor(8.6531e-07)
&gt;&gt;&gt; a_big = torch.randn(7, 5, 3)
&gt;&gt;&gt; u, s, v = torch.svd(a_big)
&gt;&gt;&gt; torch.dist(a_big, torch.matmul(torch.matmul(u, torch.diag_embed(s)), v.transpose(-2, -1)))
tensor(2.6503e-06)
</code></pre><p><code>torch.``symeig</code>( <em>input</em> , <em>eigenvectors=False</em> , <em>upper=True</em> , _out=None) -</p>
<blockquote>
<p>(Tensor<em>, _Tensor</em> )</p>
</blockquote>
<p>&#x8BE5;&#x51FD;&#x6570;&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x5B9E;&#x5BF9;&#x79F0;&#x77E9;&#x9635;<code>&#x8F93;&#x5165;</code>&#x6216;&#x95F4;&#x6B47;&#x5B9E;&#x5BF9;&#x79F0;&#x77E9;&#x9635;&#xFF0C;&#x7531;namedtuple&#xFF08;&#x672C;&#x5F81;&#x503C;&#xFF0C;&#x672C;&#x5F81;&#x77E2;&#x91CF;&#xFF09;&#x8868;&#x793A;&#x7684;&#x7279;&#x5F81;&#x5411;&#x91CF;&#x3002;</p>
<p>&#x6B64;&#x51FD;&#x6570;&#x8BA1;&#x7B97;&#x6240;&#x6709;&#x7279;&#x5F81;&#x503C;&#xFF08;&#x548C;&#x8F7D;&#x4F53;&#xFF09;&#x7684;<code>&#x8F93;&#x5165;</code>&#xFF0C;&#x4F7F;&#x5F97; &#x8F93;&#x5165; =  [HTG14&#x3011;V  DIAG  &#xFF08; E  &#xFF09; [HTG25&#x3011;V  T  \&#x6587;&#x672C;{&#x8F93;&#x5165;} =
V \&#x6587;&#x672C;{DIAG}&#xFF08;E&#xFF09;V ^ T  &#x8F93;&#x5165; =  [HTG54&#x3011;V  DIAG  &#xFF08; E  &#xFF09; [HTG67 &#x3011;V  T  &#x3002;</p>
<p>&#x5E03;&#x5C14;&#x53C2;&#x6570;<code>&#x672C;&#x5F81;&#x5411;&#x91CF;</code>&#x5B9A;&#x4E49;&#x4E86;&#x672C;&#x5F81;&#x5411;&#x91CF;&#x548C;&#x672C;&#x5F81;&#x503C;&#x4EC5;&#x6216;&#x7279;&#x5F81;&#x503C;&#x7684;&#x8BA1;&#x7B97;&#x3002;</p>
<p>&#x5982;&#x679C;&#x662F;<code>&#x5047;</code>&#xFF0C;&#x53EA;&#x6709;&#x7279;&#x5F81;&#x503C;&#x8BA1;&#x7B97;&#x3002;&#x5982;&#x679C;&#x662F;<code>&#x771F;</code>&#xFF0C;&#x4E8C;&#x8005;&#x7279;&#x5F81;&#x5411;&#x91CF;&#x88AB;&#x8BA1;&#x7B97;&#x3002;</p>
<p>&#x7531;&#x4E8E;&#x8F93;&#x5165;&#x77E9;&#x9635;<code>&#x8F93;&#x5165;</code>&#x5E94;&#x8BE5;&#x662F;&#x5BF9;&#x79F0;&#x7684;&#xFF0C;&#x4EC5;&#x4E0A;&#x4E09;&#x89D2;&#x5F62;&#x90E8;&#x5206;&#x9ED8;&#x8BA4;&#x4F7F;&#x7528;&#x3002;</p>
<p>&#x5982;&#x679C;<code>&#x4E0A;</code>&#x662F;<code>&#x5047;</code>&#xFF0C;&#x5219;&#x4F7F;&#x7528;&#x4E0B;&#x4E09;&#x89D2;&#x90E8;&#x5206;&#x3002;</p>
<p>Note</p>
<p>&#x4E0D;&#x7BA1;&#x539F;&#x59CB;&#x8FDB;&#x5C55;&#x7684;&#xFF0C;&#x5219;&#x8FD4;&#x56DE;&#x7684;&#x77E9;&#x9635;[HTG0&#x3011;V &#x5C06;&#x88AB;&#x8F6C;&#x7F6E;&#xFF0C;&#x5373;&#xFF0C;&#x6B65;&#x5E45; V.contiguous&#xFF08;&#xFF09;&#x3002;&#x8F6C;&#x7F6E;&#xFF08;-1&#xFF0C;-2&#xFF09;.stride&#xFF08;&#xFF09;&#x3002;</p>
<p>Note</p>
<p>&#x683C;&#x5916;&#x5C0F;&#x5FC3;&#xFF0C;&#x9700;&#x8981;&#x901A;&#x8FC7;&#x8F93;&#x51FA;&#x843D;&#x540E;&#x65F6;&#x91C7;&#x53D6;&#x3002;&#x8FD9;&#x6837;&#x7684;&#x64CD;&#x4F5C;&#x5B9E;&#x5728;&#x662F;&#x552F;&#x4E00;&#x7684;&#x7A33;&#x5B9A;&#xFF0C;&#x5F53;&#x6240;&#x6709;&#x7279;&#x5F81;&#x503C;&#x662F;&#x4E0D;&#x540C;&#x7684;&#x3002;&#x5426;&#x5219;&#xFF0C;<code>&#x7684;NaN</code>&#x53EF;&#x4EE5;&#x4F5C;&#x4E3A;&#x68AF;&#x5EA6;&#x672A;&#x6B63;&#x786E;&#x5B9A;&#x4E49;&#x51FA;&#x73B0;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x5927;&#x5C0F; &#x7684;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#xFF08;  <em>  &#xFF0C; n&#x7684; &#xFF0C; n&#x7684; &#xFF09; &#xFF08;</em>&#xFF0C;N&#xFF0C;N&#xFF09; &#xFF08; <em>  n&#x7684; &#xFF0C; n&#x7684; &#xFF09; &#x5176;&#x4E2D; </em> &#x662F;&#x96F6;&#x7C73;&#x6216;&#x591A;&#x4E2A;&#x7531;&#x5BF9;&#x79F0;&#x77E9;&#x9635;&#x7684;&#x6279;&#x6B21;&#x7684;&#x5C3A;&#x5BF8;&#x3002;</p>
</li>
<li><p><strong>&#x672C;&#x5F81;&#x5411;&#x91CF;</strong> &#xFF08; <em>&#x5E03;&#x5C14;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x63A7;&#x5236;&#x7279;&#x5F81;&#x5411;&#x91CF;&#x662F;&#x5426;&#x5FC5;&#x987B;&#x8BA1;&#x7B97;</p>
</li>
<li><p><strong>&#x4E0A;</strong> &#xFF08; <em>&#x5E03;&#x5C14;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x63A7;&#x5236;&#x662F;&#x5426;&#x8003;&#x8651;&#x4E0A;&#x4E09;&#x89D2;&#x6216;&#x4E0B;&#x4E09;&#x89D2;&#x533A;</p>
</li>
<li><p><strong>out</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>tuple</em></a> <em>,</em> <em>optional</em> ) &#x2013; the output tuple of (Tensor, Tensor)</p>
</li>
</ul>
<p>Returns</p>
<p>A namedtuple (eigenvalues, eigenvectors) containing</p>
<blockquote>
<ul>
<li><strong>&#x672C;&#x5F81;&#x503C;</strong> &#xFF08; <em>&#x5F20;&#x91CF;</em> &#xFF09;&#xFF1A;&#x5F62;&#x72B6; &#xFF08; <em>  &#xFF0C; M  &#xFF09; &#xFF08;</em>&#xFF0C;M&#xFF09; &#xFF08; *  &#xFF0C; M  &#xFF09; &#x3002;&#x6309;&#x5347;&#x5E8F;&#x6392;&#x5217;&#x7684;&#x7279;&#x5F81;&#x503C;&#x3002;</li>
</ul>
</blockquote>
<p>&gt;</p>
<blockquote>
<ul>
<li><strong>&#x672C;&#x5F81;&#x5411;&#x91CF;</strong> &#xFF08; <em>&#x5F20;&#x91CF;</em> &#xFF09;&#xFF1A;&#x5F62;&#x72B6; &#xFF08; <em>  &#xFF0C; M  &#xFF0C; M  &#xFF09; &#xFF08;</em>&#xFF0C;M&#xFF0C;M&#xFF09; &#xFF08; *  &#xFF0C; M  &#xFF0C; M  &#xFF09; &#x3002;&#x5982;&#x679C;<code>&#x7684;&#x7279;&#x5F81;&#x5411;&#x91CF;=&#x5047;</code>&#xFF0C;&#x5B83;&#x662F;&#x7528;&#x96F6;&#x586B;&#x5145;&#x7684;&#x5F20;&#x91CF;&#x3002;&#x5426;&#x5219;&#xFF0C;&#x8BE5;&#x5F20;&#x91CF;&#x5305;&#x542B;<code>&#x8F93;&#x5165;</code>&#x7684;&#x6B63;&#x4EA4;&#x7684;&#x7279;&#x5F81;&#x5411;&#x91CF;&#x3002;</li>
</ul>
</blockquote>
<p>&gt;</p>
<p>&gt;</p>
<p>Return type</p>
<p>(<a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>,
<a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>)</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(5, 5)
&gt;&gt;&gt; a = a + a.t()  # To make a symmetric
&gt;&gt;&gt; a
tensor([[-5.7827,  4.4559, -0.2344, -1.7123, -1.8330],
        [ 4.4559,  1.4250, -2.8636, -3.2100, -0.1798],
        [-0.2344, -2.8636,  1.7112, -5.5785,  7.1988],
        [-1.7123, -3.2100, -5.5785, -2.6227,  3.1036],
        [-1.8330, -0.1798,  7.1988,  3.1036, -5.1453]])
&gt;&gt;&gt; e, v = torch.symeig(a, eigenvectors=True)
&gt;&gt;&gt; e
tensor([-13.7012,  -7.7497,  -2.3163,   5.2477,   8.1050])
&gt;&gt;&gt; v
tensor([[ 0.1643,  0.9034, -0.0291,  0.3508,  0.1817],
        [-0.2417, -0.3071, -0.5081,  0.6534,  0.4026],
        [-0.5176,  0.1223, -0.0220,  0.3295, -0.7798],
        [-0.4850,  0.2695, -0.5773, -0.5840,  0.1337],
        [ 0.6415, -0.0447, -0.6381, -0.0193, -0.4230]])
&gt;&gt;&gt; a_big = torch.randn(5, 2, 2)
&gt;&gt;&gt; a_big = a_big + a_big.transpose(-2, -1)  # To make a_big symmetric
&gt;&gt;&gt; e, v = a_big.symeig(eigenvectors=True)
&gt;&gt;&gt; torch.allclose(torch.matmul(v, torch.matmul(e.diag_embed(), v.transpose(-2, -1))), a_big)
True
</code></pre><p><code>torch.``trapz</code>()</p>
<p><code>torch.``trapz</code>( <em>y</em> , <em>x</em> , <em>*</em> , <em>dim=-1</em> ) &#x2192; Tensor</p>
<p>&#x4F30;&#x8BA1; &#x222B; Y  d  &#xD7;  \ INT Y \&#xFF0C;DX  &#x222B; Y  d  &#xD7; &#x6CBF;&#x6697;&#x6DE1;&#x4F7F;&#x7528;&#x68AF;&#x5F62;&#x89C4;&#x5219;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>Y</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x8BE5;&#x51FD;&#x6570;&#x7684;&#x503C;&#xFF0C;&#x4EE5;&#x6574;&#x5408;</p>
</li>
<li><p><strong>&#xD7;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x5728;&#x8BE5;&#x51FD;&#x6570; Y &#x88AB;&#x91C7;&#x6837;&#x7684;&#x70B9;&#x3002;&#x5982;&#x679C;&#xD7;&#x4E0D;&#x662F;&#x5347;&#x5E8F;&#x6392;&#x5217;&#xFF0C;&#x95F4;&#x9694;&#x5728;&#x5176;&#x4E0A;&#x964D;&#x4F4E;&#x5230;&#x4F30;&#x8BA1;&#x7684;&#x79EF;&#x5206;&#xFF08;&#x5373;&#xFF0C;&#x516C;&#x7EA6; [&#x8D1F;&#x8D21;&#x732E;HTG16] &#x222B; &#x4E00; b  F  =  -  &#x222B; b  &#x4E00; F  \ int_a ^ BF = - \ int_b ^ AF  &#x222B; &#x4E00; b  F  =  [HT G98]  -  &#x222B; b  &#x4E00; F  &#x4E4B;&#x540E;&#xFF09;&#x3002;</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x6CBF;&#x5176;&#x5C3A;&#x5BF8;&#x96C6;&#x6210;&#x3002;&#x9ED8;&#x8BA4;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x4F7F;&#x7528;&#x6700;&#x540E;&#x4E00;&#x4E2A;&#x7EF4;&#x5EA6;&#x3002;</p>
</li>
</ul>
<p>Returns</p>
<p>&#x5177;&#x6709;&#x76F8;&#x540C;&#x5F62;&#x72B6;&#x7684;&#x8F93;&#x5165;&#xFF0C;&#x9664;&#x4E86;&#x4E0E;&#x4E00;&#x79CD;&#x5F20;&#x91CF;&#x6697;&#x6DE1;&#x9664;&#x53BB;&#x3002;&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x4EE3;&#x8868;&#x6240;&#x4F30;&#x8BA1;&#x7684;&#x79EF;&#x5206; &#x222B; Y  d  &#xD7; \ INT Y \&#xFF0C;DX  &#x222B; Y  d  &#xD7;
&#x6CBF;&#x7740;&#x6697;&#x6DE1;&#x3002;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; y = torch.randn((2, 3))
&gt;&gt;&gt; y
tensor([[-2.1156,  0.6857, -0.2700],
        [-1.2145,  0.5540,  2.0431]])
&gt;&gt;&gt; x = torch.tensor([[1, 3, 4], [1, 2, 3]])
&gt;&gt;&gt; torch.trapz(y, x)
tensor([-1.2220,  0.9683])
</code></pre><p><code>torch.``trapz</code>( <em>y</em> , <em>*</em> , <em>dx=1</em> , <em>dim=-1</em> ) &#x2192; Tensor</p>
<p>&#x5982;&#x4E0A;&#x8FF0;&#xFF0C;&#x4F46;&#x91C7;&#x6837;&#x70B9;&#x88AB;&#x5747;&#x5300;&#x5730;&#x4EE5; DX &#x7684;&#x8DDD;&#x79BB;&#x95F4;&#x9694;&#x5F00;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>y</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>Tensor</em></a>) &#x2013; The values of the function to integrate</p>
</li>
<li><p><strong>DX</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;</em> </a>&#xFF09; - &#x7684;&#x8DDD;&#x79BB;&#xFF0C;&#x5176;&#x4E2D; Y &#x88AB;&#x91C7;&#x6837;&#x70B9;&#x4E4B;&#x95F4;&#x3002;</p>
</li>
<li><p><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a>) &#x2013; The dimension along which to integrate. By default, use the last dimension.</p>
</li>
</ul>
<p>Returns</p>
<p>A Tensor with the same shape as the input, except with dim removed. Each
element of the returned tensor represents the estimated integral &#x222B;y dx\int
y\,dx&#x222B;ydx along dim.</p>
<p><code>torch.``triangular_solve</code>( <em>input</em> , <em>A</em> , <em>upper=True</em> , <em>transpose=False</em> ,
<em>unitriangular=False) - &gt; (Tensor</em>, <em>Tensor</em> )</p>
<p>&#x89E3;&#x51B3;&#x4E86;&#x65B9;&#x7A0B;&#x7CFB;&#x7EDF;&#x5177;&#x6709;&#x4E09;&#x89D2;&#x5F62;&#x7CFB;&#x6570;&#x77E9;&#x9635; A  A  A  &#x548C;&#x591A;&#x4E2A;&#x53F3;&#x624B;&#x4FA7; b  b  b  &#x3002;</p>
<p>&#x7279;&#x522B;&#x662F;&#xFF0C;&#x89E3;&#x51B3;&#x4E86; A  X  =  B  AX = b  A  X  =  b  &#x5E76;&#x5047;&#x5B9A; A  A  A  &#x662F;&#x4E0A;&#x4E09;&#x89D2;&#x4E0E;&#x7F3A;&#x7701;&#x5173;&#x952E;&#x5B57;&#x53C2;&#x6570;&#x3002;</p>
<p>torch.triangular_solve&#xFF08;B&#xFF0C;A&#xFF09;&#x53EF;&#x4EE5;&#x5728;2D&#x8F93;&#x5165; B&#xFF0C;A &#x6216;&#x662F;2D&#x77E9;&#x9635;&#x7684;&#x6279;&#x8F93;&#x5165;&#x3002;&#x5982;&#x679C;&#x8F93;&#x5165;&#x662F;&#x6279;&#x6B21;&#xFF0C;&#x7136;&#x540E;&#x8FD4;&#x56DE;&#x6210;&#x6279;&#x8F93;&#x51FA; X</p>
<p>Note</p>
<p>&#x7684;<code>OUT</code>&#x5173;&#x952E;&#x5B57;&#x4EC5;&#x652F;&#x6301;2D&#x77E9;&#x9635;&#x8F93;&#x5165;&#xFF0C;&#x5373;&#xFF0C; B&#xFF0C;A &#x5FC5;&#x987B;2D&#x77E9;&#x9635;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x5927;&#x5C0F; [HTG12&#x7684;&#x591A;&#x4E2A;&#x53F3;&#x624B;&#x4FA7;]&#xFF08;  <em>  &#xFF0C; M  &#xFF0C; K  &#xFF09; &#xFF08;</em>&#xFF0C;M&#xFF0C;K&#xFF09; &#xFF08; <em>  &#xFF0C; M  &#xFF0C; K  &#xFF09; &#x5176;&#x4E2D; </em>  <em>  </em>  &#x662F;&#x66F4;&#x6279;&#x91CF;&#x5C3A;&#x5BF8;&#x7684;&#x96F6;&#xFF08; b  b  B  &#xFF09;</p>
</li>
<li><p><strong>A</strong> &#xFF08;<a href="tensors.html#torch.Tensor" title="torch.Tensor"> <em>&#x5F20;&#x91CF;</em> </a>&#xFF09; - &#x5927;&#x5C0F; [&#x7684;&#x8F93;&#x5165;&#x4E09;&#x89D2;&#x5F62;&#x7CFB;&#x6570;&#x77E9;&#x9635;HTG12 ]&#xFF08;  <em>  &#xFF0C; M  &#xFF0C; M  &#xFF09; &#xFF08;</em>&#xFF0C;M&#xFF0C;M&#xFF09; &#xFF08; <em>  &#xFF0C; M  &#xFF0C; M  &#xFF09; &#x5176;&#x4E2D; </em>  <em>  </em>  &#x662F;&#x96F6;&#x70B9;&#x6216;&#x591A;&#x4E2A;&#x6279;&#x6B21;&#x7684;&#x5C3A;&#x5BF8;</p>
</li>
<li><p><strong>&#x4E0A;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x662F;&#x5426;&#x6C42;&#x89E3;&#x65B9;&#x7A0B;&#x7684;&#x4E0A;&#x4E09;&#x89D2;&#x7CFB;&#x7EDF;&#xFF08;&#x9ED8;&#x8BA4;&#xFF09;&#x6216;&#x65B9;&#x7A0B;&#x7684;&#x4E0B;&#x4E09;&#x89D2;&#x7CFB;&#x7EDF;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x771F; [HTG13&#x3002;</code></p>
</li>
<li><p><strong>&#x8F6C;&#x7F6E;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x662F;&#x5426; A  A  A  &#x5E94;&#x5F53;&#x88AB;&#x53D1;&#x9001;&#x5230;&#x6C42;&#x89E3;&#x5668;&#x4E4B;&#x524D;&#x88AB;&#x8C03;&#x6362;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x5047; [HTG37&#x3002;</code></p>
</li>
<li><p><strong>unitriangular</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x662F;&#x5426; A  A  A  &#x662F;&#x5355;&#x4F4D;&#x4E09;&#x89D2;&#x5F62;&#x3002;&#x5982;&#x679C;&#x4E3A;True&#xFF0C;&#x7684; A  &#x7684;&#x5BF9;&#x89D2;&#x5143;&#x7D20;A  A  &#x88AB;&#x5047;&#x8BBE;&#x4E3A;1&#xFF0C;&#x800C;&#x4E0D;&#x662F;&#x4ECE;&#x5F15;&#x7528; A  A  A  &#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x5047; [HTG85&#x3002;</code></p>
</li>
</ul>
<p>Returns</p>
<p>&#x7532;namedtuple &#xFF08;&#x6EB6;&#x6DB2;&#xFF0C;cloned_coefficient&#xFF09;&#x5176;&#x4E2D; cloned_coefficient &#x662F; &#x514B;&#x9686;A  A  A  &#x548C;&#x6EB6;&#x6DB2;&#x4E3A;&#x6EB6;&#x6DB2; X
X  X  &#x81F3; A  X  =  b  AX = b  A  X  =  b  &#xFF08;&#x6216;&#x4EFB;&#x4F55;&#x65B9;&#x7A0B;&#x7CFB;&#x7EDF;&#xFF0C;&#x6839;&#x636E;&#x6240;&#x8FF0;&#x5173;&#x952E;&#x5B57;&#x53C2;&#x6570;&#x7684;&#x53D8;&#x4F53;&#xFF09;&#x3002;</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; A = torch.randn(2, 2).triu()
&gt;&gt;&gt; A
tensor([[ 1.1527, -1.0753],
        [ 0.0000,  0.7986]])
&gt;&gt;&gt; b = torch.randn(2, 3)
&gt;&gt;&gt; b
tensor([[-0.0210,  2.3513, -1.5492],
        [ 1.5429,  0.7403, -1.0243]])
&gt;&gt;&gt; torch.triangular_solve(b, A)
torch.return_types.triangular_solve(
solution=tensor([[ 1.7841,  2.9046, -2.5405],
        [ 1.9320,  0.9270, -1.2826]]),
cloned_coefficient=tensor([[ 1.1527, -1.0753],
        [ 0.0000,  0.7986]]))
</code></pre><h2 id="&#x516C;&#x7528;&#x4E8B;&#x4E1A;">&#x516C;&#x7528;&#x4E8B;&#x4E1A;</h2>
<p><code>torch.``compiled_with_cxx11_abi</code>()<a href="_modules/torch.html#compiled_with_cxx11_abi">[source]</a></p>
<p>&#x8FD4;&#x56DE;PyTorch&#x662F;&#x5426;&#x4E0E;_GLIBCXX_USE_CXX11_ABI = 1&#x5EFA;&#x6210;</p>
<p><a href="tensors.html" title="torch.Tensor">Next <img src="_static/images/chevron-right-orange.svg" alt=""></a> <a href="community/persons_of_interest.html" title="PyTorch Governance | Persons of
Interest"><img src="_static/images/chevron-right-orange.svg" alt="">
Previous</a></p>
<hr>
<p>&#xA9;&#x7248;&#x6743;&#x6240;&#x6709;2019&#x5E74;&#xFF0C;Torch &#x8D21;&#x732E;&#x8005;&#x3002;</p>
<p><hr></p>
<div align="center">
    <p><a href="http://www.apachecn.org" target="_blank"><font face="KaiTi" size="6" color="red">&#x6211;&#x4EEC;&#x4E00;&#x76F4;&#x5728;&#x52AA;&#x529B;</font></a></p>
    <p><a href="https://github.com/apachecn/pytorch-doc-zh/" target="_blank">apachecn/pytorch-doc-zh</a></p>
    <p><iframe align="middle" src="https://ghbtns.com/github-btn.html?user=apachecn&amp;repo=pytorch-doc-zh&amp;type=watch&amp;count=true&amp;v=2" frameborder="0" scrolling="0" width="100px" height="25px"></iframe>
    <iframe align="middle" src="https://ghbtns.com/github-btn.html?user=apachecn&amp;repo=pytorch-doc-zh&amp;type=star&amp;count=true" frameborder="0" scrolling="0" width="100px" height="25px"></iframe>
    <iframe align="middle" src="https://ghbtns.com/github-btn.html?user=apachecn&amp;repo=pytorch-doc-zh&amp;type=fork&amp;count=true" frameborder="0" scrolling="0" width="100px" height="25px"></iframe>
    <a target="_blank" href="shang.qq.com/wpa/qunwpa"><img border="0" src="http://data.apachecn.org/img/logo/ApacheCN-group.png" alt="ML | ApacheCN" title="ML | ApacheCN"></a></p>
</div>
 <div style="text-align:center;margin:0 0 10.5px;">
     <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
     <ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-3565452474788507" data-ad-slot="2543897000">
     </ins>
     <script>(adsbygoogle = window.adsbygoogle || []).push({});</script>

    <script>
      var _hmt = _hmt || [];
      (function() {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91";
        var s = document.getElementsByTagName("script")[0]; 
        s.parentNode.insertBefore(hm, s);
      })();
    </script>

    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-102475051-10"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-102475051-10');
    </script>
</div>

<p><meta name="google-site-verification" content="pyo9N70ZWyh8JB43bIu633mhxesJ1IcwWCZlM3jUfFo"></p>
<p><iframe src="https://www.bilibili.com/read/cv2710377" style="display:none"></iframe>
<img src="http://t.cn/AiCoDHwb" hidden="hidden"></p>
<div>
    <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
    <script src="https://unpkg.com/gitalk@latest/dist/gitalk.min.js"></script>
    <script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>
    <div id="gitalk-container"></div>
    <script type="text/javascript">
        const gitalk = new Gitalk({
        clientID: '2e62dee5b9896e2eede6',
        clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53',
        repo: 'pytorch-doc-zh',
        owner: 'apachecn',
        admin: ['jiangzhonglian', 'wizardforcel'],
        id: md5(location.pathname),
        distractionFreeMode: false
        })
        gitalk.render('gitalk-container')
    </script>
</div>

<footer class="page-footer"><span class="copyright">Copyright &#xA9; ibooker.org.cn 2019 all right reserved&#xFF0C;&#x7531; ApacheCN &#x56E2;&#x961F;&#x63D0;&#x4F9B;&#x652F;&#x6301;</span><span class="footer-modification">&#x8BE5;&#x6587;&#x4EF6;&#x4FEE;&#x8BA2;&#x65F6;&#x95F4;&#xFF1A; 
2019-09-23 17:49:20
</span></footer>
<script>console.log("plugin-popup....");document.onclick = function(e){ e.target.tagName === "IMG" && window.open(e.target.src,e.target.src)}</script><style>img{cursor:pointer}</style>
                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                
                <a href="tensors.html" class="navigation navigation-next navigation-unique" aria-label="Next page: torch.Tensor">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"torch","level":"1.3.3.1","depth":3,"next":{"title":"torch.Tensor","level":"1.3.3.2","depth":3,"path":"tensors.md","ref":"tensors.md","articles":[]},"previous":{"title":"封装参考文献","level":"1.3.3","depth":2,"ref":"","articles":[{"title":"torch","level":"1.3.3.1","depth":3,"path":"torch.md","ref":"torch.md","articles":[]},{"title":"torch.Tensor","level":"1.3.3.2","depth":3,"path":"tensors.md","ref":"tensors.md","articles":[]},{"title":"Tensor Attributes","level":"1.3.3.3","depth":3,"path":"tensor_attributes.md","ref":"tensor_attributes.md","articles":[]},{"title":"Type Info","level":"1.3.3.4","depth":3,"path":"type_info.md","ref":"type_info.md","articles":[]},{"title":"torch.sparse","level":"1.3.3.5","depth":3,"path":"sparse.md","ref":"sparse.md","articles":[]},{"title":"torch.cuda","level":"1.3.3.6","depth":3,"path":"cuda.md","ref":"cuda.md","articles":[]},{"title":"torch.Storage","level":"1.3.3.7","depth":3,"path":"storage.md","ref":"storage.md","articles":[]},{"title":"torch.nn","level":"1.3.3.8","depth":3,"path":"nn.md","ref":"nn.md","articles":[]},{"title":"torch.nn.functional","level":"1.3.3.9","depth":3,"path":"nn.functional.md","ref":"nn.functional.md","articles":[]},{"title":"torch.nn.init","level":"1.3.3.10","depth":3,"path":"nn.init.md","ref":"nn.init.md","articles":[]},{"title":"torch.optim","level":"1.3.3.11","depth":3,"path":"optim.md","ref":"optim.md","articles":[]},{"title":"torch.autograd","level":"1.3.3.12","depth":3,"path":"autograd.md","ref":"autograd.md","articles":[]},{"title":"torch.distributed","level":"1.3.3.13","depth":3,"path":"distributed.md","ref":"distributed.md","articles":[]},{"title":"torch.distributions","level":"1.3.3.14","depth":3,"path":"distributions.md","ref":"distributions.md","articles":[]},{"title":"torch.hub","level":"1.3.3.15","depth":3,"path":"hub.md","ref":"hub.md","articles":[]},{"title":"torch.jit","level":"1.3.3.16","depth":3,"path":"jit.md","ref":"jit.md","articles":[]},{"title":"torch.multiprocessing","level":"1.3.3.17","depth":3,"path":"multiprocessing.md","ref":"multiprocessing.md","articles":[]},{"title":"torch.random","level":"1.3.3.18","depth":3,"path":"random.md","ref":"random.md","articles":[]},{"title":"torch.utils.bottleneck","level":"1.3.3.19","depth":3,"path":"bottleneck.md","ref":"bottleneck.md","articles":[]},{"title":"torch.utils.checkpoint","level":"1.3.3.20","depth":3,"path":"checkpoint.md","ref":"checkpoint.md","articles":[]},{"title":"torch.utils.cpp_extension","level":"1.3.3.21","depth":3,"path":"cpp_extension.md","ref":"cpp_extension.md","articles":[]},{"title":"torch.utils.data","level":"1.3.3.22","depth":3,"path":"data.md","ref":"data.md","articles":[]},{"title":"torch.utils.dlpack","level":"1.3.3.23","depth":3,"path":"dlpack.md","ref":"dlpack.md","articles":[]},{"title":"torch.utils.model_zoo","level":"1.3.3.24","depth":3,"path":"model_zoo.md","ref":"model_zoo.md","articles":[]},{"title":"torch.utils.tensorboard","level":"1.3.3.25","depth":3,"path":"tensorboard.md","ref":"tensorboard.md","articles":[]},{"title":"torch.onnx","level":"1.3.3.26","depth":3,"path":"onnx.md","ref":"onnx.md","articles":[]},{"title":"torch. config","level":"1.3.3.27","depth":3,"path":"__config__.md","ref":"__config__.md","articles":[]}]},"dir":"ltr"},"config":{"plugins":["github","github-buttons","-sharing","insert-logo","sharing-plus","back-to-top-button","code","copy-code-button","mathjax","pageview-count","edit-link","emphasize","alerts","auto-scroll-table","popup","hide-element","page-toc-button","tbfed-pagefooter","sitemap","advanced-emoji","expandable-chapters","splitter","search-pro"],"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"pluginsConfig":{"tbfed-pagefooter":{"copyright":"Copyright &copy ibooker.org.cn 2019","modify_label":"该文件修订时间： ","modify_format":"YYYY-MM-DD HH:mm:ss"},"emphasize":{},"github":{"url":"https://github.com/apachecn/pytorch-doc-zh"},"splitter":{},"search-pro":{},"search":{},"sharing-plus":{"qq":false,"all":["facebook","google","twitter","instapaper","linkedin","pocket","stumbleupon"],"douban":false,"facebook":true,"weibo":false,"instapaper":false,"whatsapp":false,"hatenaBookmark":false,"twitter":true,"messenger":false,"line":false,"vk":false,"pocket":true,"google":false,"viber":false,"stumbleupon":false,"qzone":false,"linkedin":false},"auto-scroll-table":{},"popup":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"code":{"copyButtons":true},"hide-element":{"elements":[".gitbook-link"]},"fontsettings":{"theme":"white","family":"sans","size":2},"highlight":{},"sitemap":{"hostname":"http://pytorch.apachecn.org"},"page-toc-button":{"maxTocDepth":4,"minTocSize":4},"back-to-top-button":{},"pageview-count":{},"alerts":{},"github-buttons":{"buttons":[{"user":"apachecn","repo":"pytorch-doc-zh","type":"star","count":true,"size":"small"}]},"mathjax":{"forceSVG":false,"version":"2.6-latest"},"copy-code-button":{},"advanced-emoji":{"embedEmojis":false},"sharing":{"qq":false,"all":["qq","douban","facebook","google","linkedin","twitter","weibo","whatsapp"],"douban":false,"facebook":false,"weibo":true,"whatsapp":false,"twitter":false,"line":false,"google":false,"qzone":true},"edit-link":{"label":"编辑本页","base":"https://github.com/apachecn/pytorch-doc-zh/blob/master/docs/1.2"},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false},"insert-logo":{"style":"background: none; max-height: 150px; min-height: 150px","url":"http://data.apachecn.org/img/logo.jpg"},"expandable-chapters":{}},"my_links":{"sidebar":{"Home":"https://www.baidu.com"}},"theme":"default","author":"ApacheCN","my_plugins":["donate","todo","-lunr","-search","expandable-chapters-small","chapter-fold","expandable-chapters","expandable-chapters-small","back-to-top-button","ga","baidu","sitemap","tbfed-pagefooter","advanced-emoji","sectionx","page-treeview","simple-page-toc","ancre-navigation","theme-apachecn@git+https://github.com/apachecn/theme-apachecn#HEAD","pagefooter-apachecn@git+https://github.com/apachecn/gitbook-plugin-pagefooter-apachecn#HEAD"],"my_pluginsConfig":{"page-treeview":{"copyright":"Copyright &#169; aleen42","minHeaderCount":"2","minHeaderDeep":"2"},"ignores":["node_modules"],"simple-page-toc":{"maxDepth":3,"skipFirstH1":true},"page-copyright":{"wisdom":"Designer, Frontend Developer & overall web enthusiast","noPowered":false,"copyright":"Copyright &#169; 你的名字","style":"normal","timeColor":"#666","utcOffset":"8","format":"YYYY-MM-dd hh:mm:ss","signature":"你的签名","copyrightColor":"#666","description":"modified at"},"donate":{"wechat":"微信收款的二维码URL","alipay":"支付宝收款的二维码URL","title":"","button":"赏","alipayText":"支付宝打赏","wechatText":"微信打赏"},"page-toc-button":{"maxTocDepth":2,"minTocSize":2},"github-buttons":{"buttons":[{"user":"apachecn","repo":"pytorch-doc-zh","type":"star","count":true,"size":"small"},{"user":"apachecn","width":"160","type":"follow","count":true,"size":"small"}]},"ga":{"token":"UA-102475051-10"},"baidu":{"token":"75439e2cbd22bdd813226000e9dcc12f"},"pagefooter-apachecn":{"copyright":"Copyright &copy ibooker.org.cn 2019","modify_label":"该文件修订时间： ","modify_format":"YYYY-MM-DD HH:mm:ss"}},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"variables":{},"title":"Pytorch 中文文档","language":"zh-hans","gitbook":"*","description":"Pytorch 中文文档: 教程和文档"},"file":{"path":"torch.md","mtime":"2019-09-23T17:49:20.003Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2019-09-23T17:53:13.712Z"},"basePath":".","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="gitbook/gitbook.js"></script>
    <script src="gitbook/theme.js"></script>
    
        
        <script src="gitbook/gitbook-plugin-github/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-github-buttons/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-insert-logo/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-sharing-plus/buttons.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-back-to-top-button/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-code/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-copy-code-button/toggle.js"></script>
        
    
        
        <script src="https://cdn.mathjax.org/mathjax/2.6-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-mathjax/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-pageview-count/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-edit-link/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-alerts/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-auto-scroll-table/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-hide-element/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-page-toc-button/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-expandable-chapters/expandable-chapters.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-splitter/splitter.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search-pro/jquery.mark.min.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search-pro/search.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

