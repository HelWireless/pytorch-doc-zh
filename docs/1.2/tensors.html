
<!DOCTYPE HTML>
<html lang="zh-hans" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>torch.Tensor · Pytorch 中文文档</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        <meta name="author" content="ApacheCN">
        
        
    
    <link rel="stylesheet" href="gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-insert-logo/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-back-to-top-button/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-code/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-pageview-count/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-emphasize/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-alerts/style.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-auto-scroll-table/website.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-page-toc-button/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-tbfed-pagefooter/footer.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-advanced-emoji/emoji-website.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-expandable-chapters/expandable-chapters.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-splitter/splitter.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-search-pro/search.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="tensor_attributes.html" />
    
    
    <link rel="prev" href="torch.html" />
    

    <style>
    @media only screen and (max-width: 640px) {
        .book-header .hidden-mobile {
            display: none;
        }
    }
    </style>
    <script>
        window["gitbook-plugin-github-buttons"] = {"buttons":[{"user":"apachecn","repo":"pytorch-doc-zh","type":"star","count":true,"size":"small"}]};
    </script>

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="输入并搜索" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="./">
            
                <a href="./">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" >
            
                <span>
            
                    
                    中文教程
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.1" >
            
                <span>
            
                    
                    入门
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.1.1" data-path="beginner/deep_learning_60min_blitz.html">
            
                <a href="beginner/deep_learning_60min_blitz.html">
            
                    
                    PyTorch 深度学习: 60 分钟极速入门
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.2" data-path="beginner/data_loading_tutorial.html">
            
                <a href="beginner/data_loading_tutorial.html">
            
                    
                    数据加载和处理教程
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.3" data-path="beginner/pytorch_with_examples.html">
            
                <a href="beginner/pytorch_with_examples.html">
            
                    
                    用例子学习 PyTorch
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.4" data-path="beginner/transfer_learning_tutorial.html">
            
                <a href="beginner/transfer_learning_tutorial.html">
            
                    
                    迁移学习教程
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.5" data-path="beginner/deploy_seq2seq_hybrid_frontend_tutorial.html">
            
                <a href="beginner/deploy_seq2seq_hybrid_frontend_tutorial.html">
            
                    
                    部署与TorchScript一个Seq2Seq模型
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.6" data-path="intermediate/tensorboard_tutorial.html">
            
                <a href="intermediate/tensorboard_tutorial.html">
            
                    
                    可视化模型，数据，和与训练TensorBoard
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.7" data-path="beginner/saving_loading_models.html">
            
                <a href="beginner/saving_loading_models.html">
            
                    
                    保存和加载模型
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.8" data-path="beginner/nn_tutorial.html">
            
                <a href="beginner/nn_tutorial.html">
            
                    
                    torch.nn 到底是什么？
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.2" >
            
                <span>
            
                    
                    图片
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.2.1" data-path="intermediate/torchvision_tutorial.html">
            
                <a href="intermediate/torchvision_tutorial.html">
            
                    
                    TorchVision对象检测教程细化和微调
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2.2" data-path="beginner/finetuning_torchvision_models_tutorial.html">
            
                <a href="beginner/finetuning_torchvision_models_tutorial.html">
            
                    
                    微调Torchvision模型
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2.3" data-path="intermediate/spatial_transformer_tutorial.html">
            
                <a href="intermediate/spatial_transformer_tutorial.html">
            
                    
                    空间变压器网络教程
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2.4" data-path="advanced/neural_style_tutorial.html">
            
                <a href="advanced/neural_style_tutorial.html">
            
                    
                    使用PyTorch进行神经网络传递
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2.5" data-path="beginner/fgsm_tutorial.html">
            
                <a href="beginner/fgsm_tutorial.html">
            
                    
                    对抗性示例生成
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2.6" data-path="beginner/dcgan_faces_tutorial.html">
            
                <a href="beginner/dcgan_faces_tutorial.html">
            
                    
                    DCGAN教程
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.3" >
            
                <span>
            
                    
                    音频
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.3.1" data-path="beginner/audio_preprocessing_tutorial.html">
            
                <a href="beginner/audio_preprocessing_tutorial.html">
            
                    
                    torchaudio教程
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.4" >
            
                <span>
            
                    
                    文本
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.4.1" data-path="intermediate/char_rnn_classification_tutorial.html">
            
                <a href="intermediate/char_rnn_classification_tutorial.html">
            
                    
                    NLP从头：判断名称与字符级RNN
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.4.2" data-path="intermediate/char_rnn_generation_tutorial.html">
            
                <a href="intermediate/char_rnn_generation_tutorial.html">
            
                    
                    NLP从头：生成名称与字符级RNN
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.4.3" data-path="intermediate/seq2seq_translation_tutorial.html">
            
                <a href="intermediate/seq2seq_translation_tutorial.html">
            
                    
                    NLP从无到有：用序列到序列网络和翻译注意
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.4.4" data-path="beginner/text_sentiment_ngrams_tutorial.html">
            
                <a href="beginner/text_sentiment_ngrams_tutorial.html">
            
                    
                    文本分类与TorchText 
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.4.5" data-path="beginner/torchtext_translation_tutorial.html">
            
                <a href="beginner/torchtext_translation_tutorial.html">
            
                    
                    语言翻译与TorchText 
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.4.6" data-path="beginner/transformer_tutorial.html">
            
                <a href="beginner/transformer_tutorial.html">
            
                    
                    序列到序列与nn.Transformer和TorchText建模
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.5" >
            
                <span>
            
                    
                    在生产部署PyTorch模型
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.5.1" data-path="intermediate/flask_rest_api_tutorial.html">
            
                <a href="intermediate/flask_rest_api_tutorial.html">
            
                    
                    1.部署PyTorch在Python经由REST API从Flask
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5.2" data-path="beginner/Intro_to_TorchScript_tutorial.html">
            
                <a href="beginner/Intro_to_TorchScript_tutorial.html">
            
                    
                    2.介绍TorchScript
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5.3" data-path="advanced/cpp_export.html">
            
                <a href="advanced/cpp_export.html">
            
                    
                    3.装载++一个TorchScript模型在C 
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5.4" data-path="advanced/super_resolution_with_onnxruntime.html">
            
                <a href="advanced/super_resolution_with_onnxruntime.html">
            
                    
                    4.（可选）从导出到PyTorch一个ONNX模型并使用ONNX运行时运行它
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.6" >
            
                <span>
            
                    
                    并行和分布式训练
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.6.1" data-path="intermediate/model_parallel_tutorial.html">
            
                <a href="intermediate/model_parallel_tutorial.html">
            
                    
                    1.型号并行最佳实践
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6.2" data-path="intermediate/ddp_tutorial.html">
            
                <a href="intermediate/ddp_tutorial.html">
            
                    
                    2.入门分布式数据并行
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6.3" data-path="intermediate/dist_tuto.html">
            
                <a href="intermediate/dist_tuto.html">
            
                    
                    3. PyTorch编写分布式应用
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6.4" data-path="beginner/aws_distributed_training_tutorial.html">
            
                <a href="beginner/aws_distributed_training_tutorial.html">
            
                    
                    4.（高级）PyTorch 1.0分布式训练与Amazon AWS
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.7" >
            
                <span>
            
                    
                    扩展PyTorch
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.7.1" data-path="advanced/torch_script_custom_ops.html">
            
                <a href="advanced/torch_script_custom_ops.html">
            
                    
                    使用自定义 C++ 扩展算TorchScript 
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.2" data-path="advanced/numpy_extensions_tutorial.html">
            
                <a href="advanced/numpy_extensions_tutorial.html">
            
                    
                    创建扩展使用numpy的和SciPy的
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.3" data-path="advanced/cpp_extension.html">
            
                <a href="advanced/cpp_extension.html">
            
                    
                    自定义 C++ 和CUDA扩展
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.8" >
            
                <span>
            
                    
                    PyTorch在其他语言
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.8.1" data-path="advanced/cpp_frontend.html">
            
                <a href="advanced/cpp_frontend.html">
            
                    
                    使用PyTorch C++ 前端
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3" >
            
                <span>
            
                    
                    中文文档
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.1" >
            
                <span>
            
                    
                    注解
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.1.1" data-path="notes/autograd.html">
            
                <a href="notes/autograd.html">
            
                    
                    自动求导机制
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.2" data-path="notes/broadcasting.html">
            
                <a href="notes/broadcasting.html">
            
                    
                    广播语义
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.3" data-path="notes/cpu_threading_torchscript_inference.html">
            
                <a href="notes/cpu_threading_torchscript_inference.html">
            
                    
                    CPU线程和TorchScript推理
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.4" data-path="notes/cuda.html">
            
                <a href="notes/cuda.html">
            
                    
                    CUDA语义
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.5" data-path="notes/extending.html">
            
                <a href="notes/extending.html">
            
                    
                    扩展PyTorch
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.6" data-path="notes/faq.html">
            
                <a href="notes/faq.html">
            
                    
                    常见问题
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.7" data-path="notes/large_scale_deployments.html">
            
                <a href="notes/large_scale_deployments.html">
            
                    
                    对于大规模部署的特点
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.8" data-path="notes/multiprocessing.html">
            
                <a href="notes/multiprocessing.html">
            
                    
                    多处理最佳实践
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.9" data-path="notes/randomness.html">
            
                <a href="notes/randomness.html">
            
                    
                    重复性
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.10" data-path="notes/serialization.html">
            
                <a href="notes/serialization.html">
            
                    
                    序列化语义
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.11" data-path="notes/windows.html">
            
                <a href="notes/windows.html">
            
                    
                    Windows 常见问题
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3.2" >
            
                <span>
            
                    
                    社区
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.2.1" data-path="community/contribution_guide.html">
            
                <a href="community/contribution_guide.html">
            
                    
                    PyTorch贡献说明书
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.2" data-path="community/governance.html">
            
                <a href="community/governance.html">
            
                    
                    PyTorch治理
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.3" data-path="community/persons_of_interest.html">
            
                <a href="community/persons_of_interest.html">
            
                    
                    PyTorch治|兴趣的人
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3.3" >
            
                <span>
            
                    
                    封装参考文献
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.3.1" data-path="torch.html">
            
                <a href="torch.html">
            
                    
                    torch
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="1.3.3.2" data-path="tensors.html">
            
                <a href="tensors.html">
            
                    
                    torch.Tensor
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.3" data-path="tensor_attributes.html">
            
                <a href="tensor_attributes.html">
            
                    
                    Tensor Attributes
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.4" data-path="type_info.html">
            
                <a href="type_info.html">
            
                    
                    Type Info
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.5" data-path="sparse.html">
            
                <a href="sparse.html">
            
                    
                    torch.sparse
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.6" data-path="cuda.html">
            
                <a href="cuda.html">
            
                    
                    torch.cuda
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.7" data-path="storage.html">
            
                <a href="storage.html">
            
                    
                    torch.Storage
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.8" data-path="nn.html">
            
                <a href="nn.html">
            
                    
                    torch.nn
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.9" data-path="nn.functional.html">
            
                <a href="nn.functional.html">
            
                    
                    torch.nn.functional
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.10" data-path="nn.init.html">
            
                <a href="nn.init.html">
            
                    
                    torch.nn.init
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.11" data-path="optim.html">
            
                <a href="optim.html">
            
                    
                    torch.optim
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.12" data-path="autograd.html">
            
                <a href="autograd.html">
            
                    
                    torch.autograd
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.13" data-path="distributed.html">
            
                <a href="distributed.html">
            
                    
                    torch.distributed
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.14" data-path="distributions.html">
            
                <a href="distributions.html">
            
                    
                    torch.distributions
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.15" data-path="hub.html">
            
                <a href="hub.html">
            
                    
                    torch.hub
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.16" data-path="jit.html">
            
                <a href="jit.html">
            
                    
                    torch.jit
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.17" data-path="multiprocessing.html">
            
                <a href="multiprocessing.html">
            
                    
                    torch.multiprocessing
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.18" data-path="random.html">
            
                <a href="random.html">
            
                    
                    torch.random
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.19" data-path="bottleneck.html">
            
                <a href="bottleneck.html">
            
                    
                    torch.utils.bottleneck
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.20" data-path="checkpoint.html">
            
                <a href="checkpoint.html">
            
                    
                    torch.utils.checkpoint
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.21" data-path="cpp_extension.html">
            
                <a href="cpp_extension.html">
            
                    
                    torch.utils.cpp_extension
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.22" data-path="data.html">
            
                <a href="data.html">
            
                    
                    torch.utils.data
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.23" data-path="dlpack.html">
            
                <a href="dlpack.html">
            
                    
                    torch.utils.dlpack
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.24" data-path="model_zoo.html">
            
                <a href="model_zoo.html">
            
                    
                    torch.utils.model_zoo
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.25" data-path="tensorboard.html">
            
                <a href="tensorboard.html">
            
                    
                    torch.utils.tensorboard
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.26" data-path="onnx.html">
            
                <a href="onnx.html">
            
                    
                    torch.onnx
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.27" data-path="__config__.md">
            
                <span>
            
                    
                    torch. config
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3.4" >
            
                <span>
            
                    
                    torchvision 参考文献
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.4.1" data-path="torchvision/">
            
                <a href="torchvision/">
            
                    
                    torchvision
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3.5" >
            
                <span>
            
                    
                    torchaudio Reference
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.5.1" >
            
                <a target="_blank" href="https://pytorch.org/audio">
            
                    
                    torchaudio
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3.6" >
            
                <span>
            
                    
                    torchtext Reference
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.6.1" >
            
                <a target="_blank" href="https://pytorch.org/text">
            
                    
                    torchtext
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

            </ul>
            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            本书使用 GitBook 发布
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href="." >torch.Tensor</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h1 id="torchtensor">torch.Tensor</h1>
<p>A<code>torch.Tensor</code>&#x662F;&#x5305;&#x542B;&#x5355;&#x4E00;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x7684;&#x5143;&#x7D20;&#x7684;&#x591A;&#x7EF4;&#x77E9;&#x9635;&#x3002;</p>
<p>Torch &#x5B9A;&#x4E49;&#x4E86;9&#x79CD;CPU&#x7C7B;&#x578B;&#x548C;&#x4E5D;&#x79CD;GPU&#x5F20;&#x91CF;&#x7C7B;&#x578B;&#xFF1A;</p>
<p>&#x6570;&#x636E;&#x7C7B;&#x578B;</p>
<p>|</p>
<p>D&#x578B;</p>
<p>|</p>
<p>CPU&#x5F20;&#x91CF;</p>
<p>|</p>
<p>GPU&#x5F20;&#x91CF;  </p>
<p>---|---|---|---  </p>
<p>32&#x4F4D;&#x6D6E;&#x70B9;</p>
<p>|</p>
<p><code>torch.float32</code>&#x6216;<code>torch.float</code></p>
<p>|</p>
<p><code>torch.FloatTensor</code></p>
<p>|</p>
<p><code>torch.cuda.FloatTensor</code> </p>
<p>64&#x4F4D;&#x6D6E;&#x70B9;</p>
<p>|</p>
<p><code>torch.float64</code>&#x6216;<code>torch.double</code></p>
<p>|</p>
<p><code>torch.DoubleTensor</code></p>
<p>|</p>
<p><code>torch.cuda.DoubleTensor</code> </p>
<p>16&#x4F4D;&#x6D6E;&#x70B9;</p>
<p>|</p>
<p><code>torch.float16</code>&#x6216;<code>torch.half</code></p>
<p>|</p>
<p><code>torch.HalfTensor</code></p>
<p>|</p>
<p><code>torch.cuda.HalfTensor</code> </p>
<p>8&#x4F4D;&#x6574;&#x6570;&#xFF08;&#x65E0;&#x7B26;&#x53F7;&#xFF09;</p>
<p>|</p>
<p><code>torch.uint8</code></p>
<p>|</p>
<p><code>torch.ByteTensor</code></p>
<p>|</p>
<p><code>torch.cuda.ByteTensor</code> </p>
<p>8&#x4F4D;&#x6574;&#x6570;&#xFF08;&#x7B7E;&#x540D;&#xFF09;</p>
<p>|</p>
<p><code>torch.int8</code></p>
<p>|</p>
<p><code>torch.CharTensor</code></p>
<p>|</p>
<p><code>torch.cuda.CharTensor</code> </p>
<p>16&#x4F4D;&#x6574;&#x6570;&#xFF08;&#x7B7E;&#x540D;&#xFF09;</p>
<p>|</p>
<p><code>torch.int16</code>&#x6216;<code>torch.short</code></p>
<p>|</p>
<p><code>torch.ShortTensor</code></p>
<p>|</p>
<p><code>torch.cuda.ShortTensor</code> </p>
<p>32&#x4F4D;&#x6574;&#x6570;&#xFF08;&#x7B7E;&#x540D;&#xFF09;</p>
<p>|</p>
<p><code>torch.int32</code>&#x6216;<code>torch.int</code></p>
<p>|</p>
<p><code>torch.IntTensor</code></p>
<p>|</p>
<p><code>torch.cuda.IntTensor</code> </p>
<p>64&#x4F4D;&#x6574;&#x6570;&#xFF08;&#x7B7E;&#x540D;&#xFF09;</p>
<p>|</p>
<p><code>torch.int64</code>&#x6216;<code>torch.long</code></p>
<p>|</p>
<p><code>torch.LongTensor</code></p>
<p>|</p>
<p><code>torch.cuda.LongTensor</code> </p>
<p>&#x5E03;&#x5C14;</p>
<p>|</p>
<p><code>torch.bool</code></p>
<p>|</p>
<p><code>torch.BoolTensor</code></p>
<p>|</p>
<p><code>torch.cuda.BoolTensor</code> </p>
<p><code>torch.Tensor</code>&#x662F;&#x9ED8;&#x8BA4;&#x5F20;&#x91CF;&#x7C7B;&#x578B;&#x7684;&#x522B;&#x540D;&#xFF08;<code>torch.FloatTensor</code>&#xFF09;&#x3002;</p>
<p>&#x5F20;&#x91CF;&#x53EF;&#x4EE5;&#x4ECE;&#x4E00;&#x4E2A;Python <a href="https://docs.python.org/3/library/stdtypes.html#list" title="\(in Python v3.7\)" target="_blank"> <code>&#x5217;&#x8868;</code></a>&#x6216;&#x5E8F;&#x5217;&#x4F7F;&#x7528;<a href="torch.html#torch.tensor" title="torch.tensor"> <code>torch.tensor&#xFF08;&#xFF09; [HTG10&#x88AB;&#x6784;&#x9020;]</code></a>&#x6784;&#x9020;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; torch.tensor([[1., -1.], [1., -1.]])
tensor([[ 1.0000, -1.0000],
        [ 1.0000, -1.0000]])
&gt;&gt;&gt; torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))
tensor([[ 1,  2,  3],
        [ 4,  5,  6]])
</code></pre><p>&#x8B66;&#x544A;</p>
<p><a href="torch.html#torch.tensor" title="torch.tensor"> <code>torch.tensor&#xFF08;&#xFF09;</code></a>&#x603B;&#x662F;&#x526F;&#x672C;<code>&#x6570;&#x636E;</code>&#x3002;&#x5982;&#x679C;&#x4F60;&#x6709;&#x4E00;&#x4E2A;&#x5F20;&#x91CF;<code>&#x6570;&#x636E;</code>&#xFF0C;&#x53EA;&#x662F;&#x60F3;&#x6539;&#x53D8;&#x5B83;&#x7684;<code>requires_grad</code>&#x6807;&#x5FD7;&#xFF0C;&#x4F7F;&#x7528; <code>requires_grad_ &#xFF08;&#xFF09;</code>&#x6216; <code>&#x5206;&#x79BB;&#xFF08;&#xFF09;</code>&#xFF0C;&#x4EE5;&#x907F;&#x514D;&#x526F;&#x672C;&#x3002;&#x5982;&#x679C;&#x4F60;&#x6709;&#x4E00;&#x4E2A;numpy&#x7684;&#x9635;&#x5217;&#xFF0C;&#x5E76;&#x5E0C;&#x671B;&#x907F;&#x514D;&#x62F7;&#x8D1D;&#xFF0C;&#x4F7F;&#x7528;[ <code>torch.as_tensor&#xFF08;&#xFF09;</code>
<a href="torch.html#torch.as_tensor" title="torch.as_tensor">HTG35&#x3002;</a></p>
<p>&#x7279;&#x5B9A;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x7684;&#x5F20;&#x91CF;&#x53EF;&#x4EE5;&#x901A;&#x8FC7;&#x4F7F;<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"> <code>torch.dtype</code></a>&#x548C;/&#x6216;<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"> <code>torch.device&#x6784;&#x5EFA;</code>
</a>&#x5BF9;&#x6784;&#x9020;&#x6216;&#x5F20;&#x91CF;&#x521B;&#x5EFA;OP&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; torch.zeros([2, 4], dtype=torch.int32)
tensor([[ 0,  0,  0,  0],
        [ 0,  0,  0,  0]], dtype=torch.int32)
&gt;&gt;&gt; cuda0 = torch.device(&apos;cuda:0&apos;)
&gt;&gt;&gt; torch.ones([2, 4], dtype=torch.float64, device=cuda0)
tensor([[ 1.0000,  1.0000,  1.0000,  1.0000],
        [ 1.0000,  1.0000,  1.0000,  1.0000]], dtype=torch.float64, device=&apos;cuda:0&apos;)
</code></pre><p>&#x5F20;&#x91CF;&#x7684;&#x5185;&#x5BB9;&#x53EF;&#x4EE5;&#x4F7F;&#x7528;Python&#x7684;&#x7D22;&#x5F15;&#x548C;&#x5207;&#x7247;&#x7B26;&#x53F7;&#x6765;&#x8BBF;&#x95EE;&#x548C;&#x4FEE;&#x6539;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; x = torch.tensor([[1, 2, 3], [4, 5, 6]])
&gt;&gt;&gt; print(x[1][2])
tensor(6)
&gt;&gt;&gt; x[0][1] = 8
&gt;&gt;&gt; print(x)
tensor([[ 1,  8,  3],
        [ 4,  5,  6]])
</code></pre><p>&#x4F7F;&#x7528; <code>torch.Tensor.item&#xFF08;&#xFF09;</code>&#x4EE5;&#x83B7;&#x5F97;&#x4ECE;&#x542B;&#x6709;&#x5355;&#x4E2A;&#x503C;&#x7684;&#x5F20;&#x91CF;&#x4E00;&#x4E2A;Python&#x6570;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; x = torch.tensor([[1]])
&gt;&gt;&gt; x
tensor([[ 1]])
&gt;&gt;&gt; x.item()
1
&gt;&gt;&gt; x = torch.tensor(2.5)
&gt;&gt;&gt; x
tensor(2.5000)
&gt;&gt;&gt; x.item()
2.5
</code></pre><p>&#x7532;&#x5F20;&#x91CF;&#x53EF;&#x4EE5;&#x7528;<code>requires_grad =&#x771F;</code>&#xFF0C;&#x4F7F;&#x5F97;<a href="autograd.html#module-%0Atorch.autograd" title="torch.autograd"> <code>torch.autograd</code></a>&#x5BF9;&#x5B83;&#x4EEC;&#x7684;&#x8BB0;&#x5F55;&#x64CD;&#x4F5C;&#x7684;&#x81EA;&#x52A8;&#x521B;&#x5EFA;&#x5206;&#x5316;&#x3002;</p>
<pre><code>&gt;&gt;&gt; x = torch.tensor([[1., -1.], [1., 1.]], requires_grad=True)
&gt;&gt;&gt; out = x.pow(2).sum()
&gt;&gt;&gt; out.backward()
&gt;&gt;&gt; x.grad
tensor([[ 2.0000, -2.0000],
        [ 2.0000,  2.0000]])
</code></pre><p>&#x5404;&#x5F20;&#x91CF;&#x5177;&#x6709;&#x76F8;&#x5173;&#x8054;&#x7684;<code>torch.Storage</code>&#xFF0C;&#x5176;&#x4FDD;&#x6301;&#x5B83;&#x7684;&#x6570;&#x636E;&#x3002;&#x5F20;&#x91CF;&#x7C7B;&#x63D0;&#x4F9B;&#x591A;&#x7EF4;&#x7684;&#xFF0C;<a href="https://en.wikipedia.org/wiki/Stride_of_an_array" target="_blank">&#x5B58;&#x50A8;&#x7684;&#x8DE8;&#x8DDD;</a>&#x89C6;&#x56FE;&#x5E76;&#x5728;&#x5176;&#x4E0A;&#x9650;&#x5B9A;&#x7684;&#x6570;&#x503C;&#x7684;&#x64CD;&#x4F5C;&#x3002;</p>
<p>&#x6CE8;&#x610F;</p>
<p>&#x6709;&#x5173;<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"> <code>&#x7684;&#x66F4;&#x591A;&#x4FE1;&#x606F;torch.dtype</code></a>&#xFF0C;<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"> <code>torch.device</code>
</a>&#xFF0C;&#x548C;<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"> <code>torch.layout</code></a>&#x7684;&#x5C5E;&#x6027;&#x7684; <code>torch.Tensor</code>&#x53C2;&#x89C1;<a href="tensor_attributes.html#tensor-attributes-doc"> &#x5F20;&#x91CF;&#x5C5E;&#x6027;
</a>&#x3002;</p>
<p>Note</p>
<p>&#x5176;&#x4E2D;&#x4E00;&#x4E2A;&#x7A81;&#x53D8;&#x7684;&#x65B9;&#x6CD5;&#x5F20;&#x6807;&#x6709;&#x4E0B;&#x5212;&#x7EBF;&#x7684;&#x540E;&#x7F00;&#x3002;&#x4F8B;&#x5982;&#xFF0C;<code>torch.FloatTensor.abs_&#xFF08;&#xFF09;</code>&#x8BA1;&#x7B97;&#x5C31;&#x5730;&#x7EDD;&#x5BF9;&#x503C;&#xFF0C;&#x5E76;&#x8FD4;&#x56DE;&#x6539;&#x6027;&#x5F20;&#x91CF;&#xFF0C;&#x800C;<code>torch.FloatTensor.abs&#xFF08;&#xFF09;</code>&#x8BA1;&#x7B97;&#x7ED3;&#x679C;&#x5728;&#x4E00;&#x4E2A;&#x65B0;&#x7684;&#x5F20;&#x91CF;&#x3002;</p>
<p>Note</p>
<p>&#x8981;&#x66F4;&#x6539;&#x73B0;&#x6709;&#x7684;&#x5F20;&#x91CF;&#x7684;<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"> <code>torch.device</code></a>&#x548C;/&#x6216;<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"> <code>torch.dtype</code>
</a> &#xFF0C;&#x53EF;&#x4EE5;&#x8003;&#x8651;&#x4F7F;&#x7528; <code>&#x81F3;&#xFF08;&#xFF09;</code>&#x5173;&#x4E8E;&#x5F20;&#x91CF;&#x7684;&#x65B9;&#x6CD5;&#x3002;</p>
<p>Warning</p>
<p>&#x7684; <code>torch.Tensor</code>
&#x5F53;&#x524D;&#x5B9E;&#x73B0;&#x5F15;&#x5165;&#x4E86;&#x5185;&#x5B58;&#x5F00;&#x9500;&#xFF0C;&#x4ECE;&#x800C;&#x53EF;&#x80FD;&#x5BFC;&#x81F4;&#x5728;&#x8BB8;&#x591A;&#x5FAE;&#x5C0F;&#x7684;&#x5F20;&#x91CF;&#x7684;&#x5E94;&#x7528;&#x51FA;&#x4E4E;&#x610F;&#x6599;&#x7684;&#x9AD8;&#x5185;&#x5B58;&#x4F7F;&#x7528;&#x60C5;&#x51B5;&#x3002;&#x5982;&#x679C;&#x60A8;&#x9047;&#x5230;&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#xFF0C;&#x53EF;&#x4EE5;&#x8003;&#x8651;&#x4F7F;&#x7528;&#x4E00;&#x4E2A;&#x5927;&#x7684;&#x7ED3;&#x6784;&#x3002;</p>
<p><em>class</em><code>torch.``Tensor</code></p>
<p>&#x6709;&#x4E24;&#x79CD;&#x521B;&#x5EFA;&#x4E00;&#x4E2A;&#x5F20;&#x91CF;&#xFF0C;&#x8FD9;&#x53D6;&#x51B3;&#x4E8E;&#x4F60;&#x7684;&#x4F7F;&#x7528;&#x60C5;&#x51B5;&#x7684;&#x51E0;&#x4E2A;&#x4E3B;&#x8981;&#x9014;&#x5F84;&#x3002;</p>
<ul>
<li><p>&#x8981;&#x521B;&#x5EFA;&#x5177;&#x6709;&#x9884;&#x5148;&#x5B58;&#x5728;&#x7684;&#x6570;&#x636E;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x7528;<a href="torch.html#torch.tensor" title="torch.tensor"> <code>torch.tensor&#xFF08;&#xFF09;</code></a>&#x3002;</p>
</li>
<li><p>&#x8981;&#x521B;&#x5EFA;&#x5177;&#x6709;&#x7279;&#x5B9A;&#x5927;&#x5C0F;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x4F7F;&#x7528;<code>Torch &#x3002;*</code>&#x5F20;&#x91CF;&#x521B;&#x5EFA;OPS&#xFF08;&#x89C1;<a href="torch.html#tensor-creation-ops"> &#x521B;&#x5EFA;&#x884C;&#x52A8; </a>&#xFF09;&#x3002;</p>
</li>
<li><p>&#x8981;&#x521B;&#x5EFA;&#x5177;&#x6709;&#x76F8;&#x540C;&#x7684;&#x5C3A;&#x5BF8;&#xFF08;&#x4EE5;&#x53CA;&#x7C7B;&#x4F3C;&#x7684;&#x7C7B;&#x578B;&#xFF09;&#x4F5C;&#x4E3A;&#x53E6;&#x4E00;&#x5F20;&#x5F20;&#x91CF;&#xFF0C;&#x4F7F;&#x7528;<code>Torch &#x3002;* _&#x50CF;</code>&#x5F20;&#x91CF;&#x521B;&#x5EFA;OPS&#xFF08;&#x89C1;<a href="torch.html#tensor-creation-ops"> &#x521B;&#x5EFA;&#x884C;&#x52A8; </a>&#xFF09;&#x3002;</p>
</li>
<li><p>&#x8981;&#x521B;&#x5EFA;&#x5177;&#x6709;&#x76F8;&#x4F3C;&#x7C7B;&#x578B;&#x4F46;&#x4E0D;&#x540C;&#x5927;&#x5C0F;&#x7684;&#x53E6;&#x4E00;&#x5F20;&#x5F20;&#x91CF;&#xFF0C;&#x4F7F;&#x7528;<code>tensor.new_ *</code>&#x521B;&#x5EFA;&#x6B22;&#x58F0;&#x7B11;&#x8BED;&#x3002;</p>
</li>
</ul>
<p><code>new_tensor</code>( <em>data</em> , <em>dtype=None</em> , <em>device=None</em> , <em>requires_grad=False</em> )
&#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E0E;<code>&#x6570;&#x636E;</code>&#x4F5C;&#x4E3A;&#x5F20;&#x91CF;&#x6570;&#x636E;&#x7684;&#x65B0;&#x7684;&#x5F20;&#x91CF;&#x3002;&#x9ED8;&#x8BA4;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x5177;&#x6709;&#x76F8;&#x540C;&#x7684;<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"> <code>torch.dtype</code>
</a>&#x548C;<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"> <code>torch.device</code></a>&#x4F5C;&#x4E3A;&#x672C;&#x5F20;&#x91CF;&#x3002;</p>
<p>Warning</p>
<p><code>new_tensor&#xFF08;&#xFF09;</code>&#x603B;&#x662F;&#x526F;&#x672C;<code>&#x6570;&#x636E;</code>&#x3002;&#x5982;&#x679C;&#x4F60;&#x6709;&#x4E00;&#x4E2A;&#x5F20;&#x91CF;<code>&#x6570;&#x636E;</code>&#xFF0C;&#x5E76;&#x5E0C;&#x671B;&#x907F;&#x514D;&#x62F7;&#x8D1D;&#xFF0C;&#x4F7F;&#x7528; <code>torch.Tensor.requires_grad_&#xFF08;&#xFF09;</code>&#x6216; <code>torch.Tensor.detach&#xFF08;&#xFF09;</code>
&#x3002;&#x5982;&#x679C;&#x4F60;&#x6709;&#x4E00;&#x4E2A;numpy&#x7684;&#x9635;&#x5217;&#xFF0C;&#x5E76;&#x5E0C;&#x671B;&#x907F;&#x514D;&#x62F7;&#x8D1D;&#xFF0C;&#x4F7F;&#x7528;[ <code>torch.from_numpy&#xFF08;&#xFF09;</code>
<a href="torch.html#torch.from_numpy" title="torch.from_numpy">HTG31&#x3002;</a></p>
<p>Warning</p>
<p>&#x5F53;&#x6570;&#x636E;&#x662F;&#x5F20;&#x91CF;&#xD7;&#xFF0C; <code>new_tensor&#xFF08;&#xFF09;</code>&#x8BFB;&#x51FA;&#x4ECE;&#x4E0D;&#x7BA1;&#x5B83;&#x662F;&#x901A;&#x8FC7; &apos;&#x6570;&#x636E;&apos;&#xFF0C;&#x5E76;&#x6784;&#x9020;&#x4E00;&#x4E2A;&#x53F6;&#x53D8;&#x91CF;&#x3002;&#x56E0;&#x6B64;<code>tensor.new_tensor&#xFF08;X&#xFF09;</code>&#x7B49;&#x4E8E;<code>x.clone&#xFF08;&#xFF09;&#x3002;&#x5206;&#x79BB;&#xFF08;&#xFF09;</code>&#x548C;<code>tensor.new_tensor&#xFF08;X&#xFF0C; requires_grad =&#x771F;&#xFF09;</code>&#x7B49;&#x4E8E;<code>x.clone&#xFF08;&#xFF09;&#x3002;&#x5206;&#x79BB;&#xFF08;&#xFF09;&#x3002;requires_grad_&#xFF08;&#x771F;&#xFF09;</code>&#x3002;&#x4F7F;&#x7528;<code>&#x514B;&#x9686;&#xFF08;&#xFF09;</code>&#x548C;<code>&#x5206;&#x79BB;&#xFF08;&#xFF09;&#x7684;&#x5F53;&#x91CF;&#x7684;&#x5EFA;&#x8BAE;</code>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x6570;&#x636E;</strong> &#xFF08; <em>array_like</em> &#xFF09; - &#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x4EFD;<code>&#x6570;&#x636E;</code>&#x3002;</p>
</li>
<li><p><strong>DTYPE</strong> &#xFF08;<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"> <code>torch.dtype</code></a>&#xFF0C;&#x53EF;&#x9009;&#xFF09; - &#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x7684;&#x6240;&#x671F;&#x671B;&#x7684;&#x7C7B;&#x578B;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;&#x5982;&#x679C;&#x65E0;&#xFF0C;&#x76F8;&#x540C;<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"> <code>torch.dtype</code></a>&#x5982;&#x8FD9;&#x4E2A;&#x5F20;&#x91CF;&#x3002;</p>
</li>
<li><p><strong>&#x88C5;&#x7F6E;</strong> &#xFF08;<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"> <code>torch.device</code></a>&#xFF0C;&#x53EF;&#x9009;&#xFF09; - &#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x7684;&#x6240;&#x9700;&#x7684;&#x8BBE;&#x5907;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;&#x5982;&#x679C;&#x65E0;&#xFF0C;&#x76F8;&#x540C;<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"> <code>torch.device</code></a>&#x5982;&#x8FD9;&#x4E2A;&#x5F20;&#x91CF;&#x3002;</p>
</li>
<li><p><strong>requires_grad</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5982;&#x679C;autograd&#x5E94;&#x8FD4;&#x56DE;&#x7684;&#x8BB0;&#x5F55;&#x5F20;&#x64CD;&#x4F5C;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x5047; [HTG13&#x3002;</code></p>
</li>
</ul>
<p>&#x4F8B;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; tensor = torch.ones((2,), dtype=torch.int8)
&gt;&gt;&gt; data = [[0, 1], [2, 3]]
&gt;&gt;&gt; tensor.new_tensor(data)
tensor([[ 0,  1],
        [ 2,  3]], dtype=torch.int8)
</code></pre><p><code>new_full</code>( <em>size</em> , <em>fill_value</em> , <em>dtype=None</em> , <em>device=None</em> ,
<em>requires_grad=False</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x5927;&#x5C0F; <code>&#x5927;&#x5C0F;&#x7684;&#x5F20;&#x91CF;&#x586B;&#x5145;</code>&#x4E0E;<code>fill_value</code>&#x3002;&#x9ED8;&#x8BA4;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x5177;&#x6709;&#x76F8;&#x540C;&#x7684;<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"> <code>torch.dtype</code>
</a>&#x548C;<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"> <code>torch.device</code></a>&#x4F5C;&#x4E3A;&#x672C;&#x5F20;&#x91CF;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>fill_value</strong> &#xFF08; <em>&#x6807;&#x91CF;</em> &#xFF09; - &#x7684;&#x6570;&#x91CF;&#x6765;&#x586B;&#x5145;&#x4E0E;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x3002;</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired type of returned tensor. Default: if None, same <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> as this tensor.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if None, same <a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a> as this tensor.</p>
</li>
<li><p><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; tensor = torch.ones((2,), dtype=torch.float64)
&gt;&gt;&gt; tensor.new_full((3, 4), 3.141592)
tensor([[ 3.1416,  3.1416,  3.1416,  3.1416],
        [ 3.1416,  3.1416,  3.1416,  3.1416],
        [ 3.1416,  3.1416,  3.1416,  3.1416]], dtype=torch.float64)
</code></pre><p><code>new_empty</code>( <em>size</em> , <em>dtype=None</em> , <em>device=None</em> , <em>requires_grad=False</em> ) &#x2192;
Tensor</p>
<p>&#x8FD4;&#x56DE;&#x5927;&#x5C0F; <code>&#x5927;&#x5C0F;&#x586B;&#x5145;</code>&#x4E0E;&#x672A;&#x521D;&#x59CB;&#x5316;&#x7684;&#x6570;&#x636E;&#x7684;&#x5F20;&#x91CF;&#x3002;&#x9ED8;&#x8BA4;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x5177;&#x6709;&#x76F8;&#x540C;&#x7684;<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"> <code>torch.dtype</code>
</a>&#x548C;<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"> <code>torch.device</code></a>&#x4F5C;&#x4E3A;&#x672C;&#x5F20;&#x91CF;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired type of returned tensor. Default: if None, same <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> as this tensor.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if None, same <a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a> as this tensor.</p>
</li>
<li><p><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; tensor = torch.ones(())
&gt;&gt;&gt; tensor.new_empty((2, 3))
tensor([[ 5.8182e-18,  4.5765e-41, -1.0545e+30],
        [ 3.0949e-41,  4.4842e-44,  0.0000e+00]])
</code></pre><p><code>new_ones</code>( <em>size</em> , <em>dtype=None</em> , <em>device=None</em> , <em>requires_grad=False</em> ) &#x2192;
Tensor</p>
<p>&#x8FD4;&#x56DE;&#x5927;&#x5C0F; &#x586B;&#x5145;&#x6709;<code>1``&#x5927;&#x5C0F;</code>&#x7684;&#x5F20;&#x91CF;&#x3002;&#x9ED8;&#x8BA4;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x5177;&#x6709;&#x76F8;&#x540C;&#x7684;<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"> <code>torch.dtype</code>
</a>&#x548C;<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"> <code>torch.device</code></a>&#x4F5C;&#x4E3A;&#x672C;&#x5F20;&#x91CF;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x5927;&#x5C0F;</strong> &#xFF08; <em>INT ...</em> &#xFF09; - &#x5217;&#x8868;&#xFF0C;&#x5143;&#x7EC4;&#xFF0C;&#x6216;<code>torch.Size</code>&#x5B9A;&#x4E49;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684;&#x5F62;&#x72B6;&#x7684;&#x6574;&#x6570;&#x3002;</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired type of returned tensor. Default: if None, same <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> as this tensor.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if None, same <a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a> as this tensor.</p>
</li>
<li><p><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; tensor = torch.tensor((), dtype=torch.int32)
&gt;&gt;&gt; tensor.new_ones((2, 3))
tensor([[ 1,  1,  1],
        [ 1,  1,  1]], dtype=torch.int32)
</code></pre><p><code>new_zeros</code>( <em>size</em> , <em>dtype=None</em> , <em>device=None</em> , <em>requires_grad=False</em> ) &#x2192;
Tensor</p>
<p>&#x8FD4;&#x56DE;&#x5927;&#x5C0F; &#x586B;&#x5145;&#x6709;<code>0``&#x5927;&#x5C0F;</code>&#x7684;&#x5F20;&#x91CF;&#x3002;&#x9ED8;&#x8BA4;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x5177;&#x6709;&#x76F8;&#x540C;&#x7684;<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"> <code>torch.dtype</code>
</a>&#x548C;<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"> <code>torch.device</code></a>&#x4F5C;&#x4E3A;&#x672C;&#x5F20;&#x91CF;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>size</strong> ( <em>int...</em> ) &#x2013; a list, tuple, or <code>torch.Size</code>of integers defining the shape of the output tensor.</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired type of returned tensor. Default: if None, same <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> as this tensor.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if None, same <a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a> as this tensor.</p>
</li>
<li><p><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a> <em>,</em> <em>optional</em> ) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; tensor = torch.tensor((), dtype=torch.float64)
&gt;&gt;&gt; tensor.new_zeros((2, 3))
tensor([[ 0.,  0.,  0.],
        [ 0.,  0.,  0.]], dtype=torch.float64)
</code></pre><p><code>is_cuda</code></p>
<p>&#x4E3A;<code>&#x771F; [HTG3&#x5982;&#x679C;&#x5F20;&#x91CF;&#x5B58;&#x50A8;&#x5728;GPU&#xFF0C;</code>&#x5047; [HTG7&#x5426;&#x5219;&#x3002;``</p>
<p><code>device</code></p>
<p>&#x662F;[ <code>torch.device</code><a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device">HTG5&#x5982;&#x679C;&#x672C;&#x5F20;&#x91CF;&#x3002;</a></p>
<p><code>grad</code></p>
<p>&#x8BE5;&#x5C5E;&#x6027;&#x662F;<code>&#x65E0;</code>&#x7F3A;&#x7701;&#x548C;&#x6210;&#x4E3A;&#x5F20;&#x91CF;&#x5728;&#x7B2C;&#x4E00;&#x65F6;&#x95F4; <code>&#x547C;&#x53EB;&#x5411;&#x540E;&#xFF08;&#xFF09;</code>&#x8BA1;&#x7B97;&#x68AF;&#x5EA6;&#x4E3A;<code>&#x81EA;</code>&#x3002;&#x7136;&#x540E;&#xFF0C;&#x5C5E;&#x6027;&#x5C06;&#x5305;&#x542B;&#x8BA1;&#x7B97;&#x51FA;&#x7684;&#x68AF;&#x5EA6;&#xFF0C;&#x5E76; <code>&#x5411;&#x540E;&#xFF08;&#xFF09;</code>
&#x5C06;&#x79EF;&#x7D2F;&#xFF08;&#x6DFB;&#x52A0;&#xFF09;&#x68AF;&#x5EA6;&#x5230;&#x5B83;&#x5C06;&#x6765;&#x7684;&#x547C;&#x53EB;&#x3002;</p>
<p><code>ndim</code></p>
<p>&#x522B;&#x540D; <code>&#x6697;&#x6DE1;&#xFF08;&#xFF09;</code></p>
<p><code>T</code></p>
<p>&#x8FD9;&#x662F;&#x5F20;&#x91CF;&#x4E0E;&#x5B83;&#x7684;&#x5C3A;&#x5BF8;&#x9006;&#x8F6C;&#x3002;</p>
<p>&#x5982;&#x679C;<code>n&#x7684;</code>&#x662F;&#x5C3A;&#x5BF8;&#x5728;<code>&#xD7;</code>&#x7684;&#x6570;&#x91CF;&#xFF0C;<code>XT</code>&#x7B49;&#x4EF7;&#x4E8E;<code>x.permute&#x7B2C;&#xFF08;n-1&#xFF0C; N-2&#xFF0C; ...&#xFF0C; 0&#xFF09;</code>&#x3002;</p>
<p><code>abs</code>() &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.abs" title="torch.abs"> <code>torch.abs&#xFF08;&#xFF09;</code></a></p>
<p><code>abs_</code>() &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>&#x7684;ABS&#xFF08;&#xFF09;</code></p>
<p><code>acos</code>() &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.acos" title="torch.acos"> <code>torch.acos&#xFF08;&#xFF09;</code></a></p>
<p><code>acos_</code>() &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>ACOS&#x7684;&#xFF08;&#xFF09;</code></p>
<p><code>add</code>( <em>value</em> ) &#x2192; Tensor</p>
<p>&#x6DFB;&#x52A0;&#xFF08;&#x503C;= 1&#xFF0C;&#x5176;&#x4ED6;&#xFF09; - &amp; GT ;&#x5F20;&#x91CF;</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.add" title="torch.add"> <code>torch.add&#xFF08;&#xFF09;</code></a></p>
<p><code>add_</code>( <em>value</em> ) &#x2192; Tensor</p>
<p>add_&#xFF08;&#x503C;= 1&#xFF0C;&#x5176;&#x4ED6;&#xFF09; - &amp; GT ;&#x5F20;&#x91CF;</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <code>&#x6DFB;&#x52A0;&#xFF08;&#xFF09;</code></p>
<p><code>addbmm</code>( <em>beta=1</em> , <em>alpha=1</em> , <em>batch1</em> , <em>batch2</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.addbmm" title="torch.addbmm"> <code>torch.addbmm&#xFF08;&#xFF09;</code></a></p>
<p><code>addbmm_</code>( <em>beta=1</em> , <em>alpha=1</em> , <em>batch1</em> , <em>batch2</em> ) &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>addbmm&#x7684;&#xFF08;&#xFF09;</code></p>
<p><code>addcdiv</code>( <em>value=1</em> , <em>tensor1</em> , <em>tensor2</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.addcdiv" title="torch.addcdiv"> <code>torch.addcdiv&#xFF08;&#x200B;&#x200B;&#xFF09;</code></a></p>
<p><code>addcdiv_</code>( <em>value=1</em> , <em>tensor1</em> , <em>tensor2</em> ) &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <code>addcdiv&#xFF08;&#x200B;&#x200B;&#xFF09;</code></p>
<p><code>addcmul</code>( <em>value=1</em> , <em>tensor1</em> , <em>tensor2</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.addcmul" title="torch.addcmul"> <code>torch.addcmul&#xFF08;&#xFF09;</code></a></p>
<p><code>addcmul_</code>( <em>value=1</em> , <em>tensor1</em> , <em>tensor2</em> ) &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>addcmul&#x7684;&#xFF08;&#xFF09;</code></p>
<p><code>addmm</code>( <em>beta=1</em> , <em>alpha=1</em> , <em>mat1</em> , <em>mat2</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.addmm" title="torch.addmm"> <code>torch.addmm&#xFF08;&#xFF09;</code></a></p>
<p><code>addmm_</code>( <em>beta=1</em> , <em>alpha=1</em> , <em>mat1</em> , <em>mat2</em> ) &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>addmm&#x7684;&#xFF08;&#xFF09;</code></p>
<p><code>addmv</code>( <em>beta=1</em> , <em>alpha=1</em> , <em>mat</em> , <em>vec</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.addmv" title="torch.addmv"> <code>torch.addmv&#xFF08;&#xFF09;</code></a></p>
<p><code>addmv_</code>( <em>beta=1</em> , <em>alpha=1</em> , <em>mat</em> , <em>vec</em> ) &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>addmv&#x7684;&#xFF08;&#xFF09;</code></p>
<p><code>addr</code>( <em>beta=1</em> , <em>alpha=1</em> , <em>vec1</em> , <em>vec2</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.addr" title="torch.addr"> <code>torch.addr&#xFF08;&#xFF09;</code></a></p>
<p><code>addr_</code>( <em>beta=1</em> , <em>alpha=1</em> , <em>vec1</em> , <em>vec2</em> ) &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>&#x7684;addr&#xFF08;&#xFF09;</code></p>
<p><code>allclose</code>( <em>other</em> , <em>rtol=1e-05</em> , <em>atol=1e-08</em> , <em>equal_nan=False</em> ) &#x2192;
Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.allclose" title="torch.allclose"> <code>torch.allclose&#xFF08;&#xFF09;</code></a></p>
<p><code>apply_</code>( <em>callable</em> ) &#x2192; Tensor</p>
<p>&#x9002;&#x7528;&#x7684;&#x51FD;&#x6570;<code>&#x53EF;&#x8C03;&#x7528;</code>&#xFF0C;&#x5728;&#x5F20;&#x91CF;&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x4EF6;&#xFF0C;&#x4E0E;&#x7531;<code>&#x53EF;&#x8C03;&#x7528;</code>&#x8FD4;&#x56DE;&#x7684;&#x503C;&#x66FF;&#x6362;&#x6BCF;&#x4E2A;&#x5143;&#x4EF6;&#x3002;</p>
<p>Note</p>
<p>&#x6B64;&#x529F;&#x80FD;&#x53EA;&#x9002;&#x7528;&#x4E8E;CPU&#x5F20;&#x91CF;&#xFF0C;&#x4E0D;&#x5E94;&#x8BE5;&#x5728;&#x8981;&#x6C42;&#x9AD8;&#x6027;&#x80FD;&#x7684;&#x4EE3;&#x7801;&#x6BB5;&#x4E2D;&#x4F7F;&#x7528;&#x3002;</p>
<p><code>argmax</code>( <em>dim=None</em> , <em>keepdim=False</em> ) &#x2192; LongTensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.argmax" title="torch.argmax"> <code>torch.argmax&#xFF08;&#xFF09;</code></a></p>
<p><code>argmin</code>( <em>dim=None</em> , <em>keepdim=False</em> ) &#x2192; LongTensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.argmin" title="torch.argmin"> <code>torch.argmin&#xFF08;&#xFF09;</code></a></p>
<p><code>argsort</code>( <em>dim=-1</em> , <em>descending=False</em> ) &#x2192; LongTensor</p>
<p>&#x53C2;&#x89C1;&#xFF1A;FUNC&#xFF1A; torch.argsort</p>
<p><code>asin</code>() &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.asin" title="torch.asin"> <code>torch.asin&#xFF08;&#xFF09;</code></a></p>
<p><code>asin_</code>() &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <code>ASIN&#xFF08;&#xFF09;</code></p>
<p><code>as_strided</code>( <em>size</em> , <em>stride</em> , <em>storage_offset=0</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.as_strided" title="torch.as_strided"> <code>torch.as_strided&#xFF08;&#xFF09;</code></a></p>
<p><code>atan</code>() &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.atan" title="torch.atan"> <code>torch.atan&#xFF08;&#xFF09;</code></a></p>
<p><code>atan2</code>( <em>other</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.atan2" title="torch.atan2"> <code>torch.atan2&#xFF08;&#xFF09;</code></a></p>
<p><code>atan2_</code>( <em>other</em> ) &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>ATAN2&#x7684;&#xFF08;&#xFF09;</code></p>
<p><code>atan_</code>() &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>ATAN&#x7684;&#xFF08;&#xFF09;</code></p>
<p><code>backward</code>( <em>gradient=None</em> , <em>retain_graph=None</em> , <em>create_graph=False</em>
)<a href="_modules/torch/tensor.html#Tensor.backward">[source]</a></p>
<p>&#x8BA1;&#x7B97;&#x5F53;&#x524D;&#x5F20;&#x91CF;w.r.t.&#x7684;&#x68AF;&#x5EA6;&#x56FE;&#x53F6;&#x3002;</p>
<p>&#x8BE5;&#x56FE;&#x662F;&#x4F7F;&#x7528;&#x94FE;&#x5F0F;&#x6CD5;&#x5219;&#x533A;&#x5206;&#x3002;&#x5982;&#x679C;&#x5F20;&#x91CF;&#x662F;&#x975E;&#x6807;&#x91CF;&#xFF08;&#x5373;&#xFF0C;&#x5176;&#x6570;&#x636E;&#x5177;&#x6709;&#x4E00;&#x4E2A;&#x4EE5;&#x4E0A;&#x7684;&#x5143;&#x7D20;&#xFF09;&#xFF0C;&#x5E76;&#x4E14;&#x9700;&#x8981;&#x7684;&#x68AF;&#x5EA6;&#xFF0C;&#x6240;&#x8FF0;&#x51FD;&#x6570;&#x53E6;&#x5916;&#x9700;&#x8981;&#x6307;&#x5B9A;<code>&#x68AF;&#x5EA6;</code>&#x3002;&#x5B83;&#x5E94;&#x8BE5;&#x662F;&#x5339;&#x914D;&#x7684;&#x7C7B;&#x578B;&#x548C;&#x4F4D;&#x7F6E;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x5305;&#x542B;&#x6709;&#x533A;&#x522B;&#x7684;&#x529F;&#x80FD;w.r.t.&#x7684;&#x68AF;&#x5EA6;<code>&#x81EA;</code>&#x3002;</p>
<p>&#x6B64;&#x529F;&#x80FD;&#x805A;&#x96C6;&#x5728;&#x53F6;&#x68AF;&#x5EA6; - &#x4F60;&#x53EF;&#x80FD;&#x9700;&#x8981;&#x8C03;&#x7528;&#x5B83;&#x4E4B;&#x524D;&#x4E3A;&#x96F6;&#x5B83;&#x4EEC;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x68AF;&#x5EA6;</strong> &#xFF08; <em>&#x5F20;&#x91CF;</em> <em>&#x6216;</em> <a href="https://docs.python.org/3/library/constants.html#None" title="\(in Python v3.7\)" target="_blank"> <em>&#x65E0;</em> </a>&#xFF09; - &#x68AF;&#x5EA6;w.r.t.&#x5F20;&#x91CF;&#x3002;&#x5982;&#x679C;&#x5B83;&#x662F;&#x4E00;&#x4E2A;&#x5F20;&#x91CF;&#xFF0C;&#x5B83;&#x5C06;&#x88AB;&#x81EA;&#x52A8;&#x8F6C;&#x6362;&#x4E3A;&#x4E0D;&#x9700;&#x8981;&#x7814;&#x7A76;&#x6240;&#x9664;&#x975E;<code>create_graph</code>&#x4E3A;True&#x5F20;&#x91CF;&#x3002;&#x65E0;&#x503C;&#x53EF;&#x4EE5;&#x7528;&#x4E8E;&#x6807;&#x91CF;&#x5F20;&#x91CF;&#x6216;&#x90A3;&#x4E9B;&#x4E0D;&#x8981;&#x6C42;&#x6BD5;&#x4E1A;&#x751F;&#x6307;&#x5B9A;&#x3002;&#x5982;&#x679C;&#x6CA1;&#x6709;&#x503C;&#x662F;&#x53EF;&#x4EE5;&#x63A5;&#x53D7;&#x7684;&#xFF0C;&#x7136;&#x540E;&#x8FD9;&#x79CD;&#x8BF4;&#x6CD5;&#x662F;&#x53EF;&#x9009;&#x7684;&#x3002;</p>
</li>
<li><p><strong>retain_graph</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5982;&#x679C;<code>&#x5047;</code>&#xFF0C;&#x7528;&#x4E8E;&#x8BA1;&#x7B97;&#x68AF;&#x5EA6;&#x7684;&#x56FE;&#x8868;&#x5C06;&#x88AB;&#x91CA;&#x653E;&#x3002;&#x8BF7;&#x6CE8;&#x610F;&#xFF0C;&#x5728;&#x51E0;&#x4E4E;&#x6240;&#x6709;&#x60C5;&#x51B5;&#x4E0B;&#x7684;&#x8BBE;&#x7F6E;&#x5219;&#x4E0D;&#x9700;&#x8981;&#x6B64;&#x9009;&#x9879;&#x8BBE;&#x7F6E;&#x4E3A;True&#xFF0C;&#x5F80;&#x5F80;&#x53EF;&#x4EE5;&#x4EE5;&#x66F4;&#x6709;&#x6548;&#x7684;&#x65B9;&#x5F0F;&#x56F4;&#x7ED5;&#x5DE5;&#x4F5C;&#x3002;&#x9ED8;&#x8BA4;&#x4E3A;<code>create_graph</code>&#x7684;&#x503C;&#x3002;</p>
</li>
<li><p><strong>create_graph</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5982;&#x679C;<code>&#x771F;</code>&#xFF0C;&#x6240;&#x8FF0;&#x884D;&#x751F;&#x7269;&#x7684;&#x56FE;&#x5F62;&#x5C06;&#x88AB;&#x6784;&#x5EFA;&#xFF0C;&#x4ECE;&#x800C;&#x5141;&#x8BB8;&#x8BA1;&#x7B97;&#x9AD8;&#x9636;&#x884D;&#x751F;&#x4EA7;&#x54C1;&#x3002;&#x9ED8;&#x8BA4;&#x4E3A;<code>&#x5047; [HTG17&#x3002;</code></p>
</li>
</ul>
<p><code>baddbmm</code>( <em>beta=1</em> , <em>alpha=1</em> , <em>batch1</em> , <em>batch2</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.baddbmm" title="torch.baddbmm"> <code>torch.baddbmm&#xFF08;&#xFF09;</code></a></p>
<p><code>baddbmm_</code>( <em>beta=1</em> , <em>alpha=1</em> , <em>batch1</em> , <em>batch2</em> ) &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>baddbmm&#x7684;&#xFF08;&#xFF09;</code></p>
<p><code>bernoulli</code>( <em>*</em> , <em>generator=None</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x7ED3;&#x679C;&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x4E2D;&#x6BCF;&#x4E2A; &#x5BFC;&#x81F4;[I]  \ texttt {&#x7ED3;&#x679C;[I]}  &#x5BFC;&#x81F4;[I]  &#x4ECE;&#x72EC;&#x7ACB;&#x5730;&#x53D6;&#x6837; &#x4F2F;&#x52AA;&#x5229; &#xFF08; &#x81EA;[I]  &#xFF09;  \&#x6587;&#x672C;{&#x4F2F;&#x52AA;&#x5229;}&#xFF08;\
texttt {&#x81EA;[I]}&#xFF09; &#x4F2F;&#x52AA;&#x5229; &#xFF08; &#x81EA;[I]  &#xFF09; &#x3002; <code>&#x81EA;</code>&#x5FC5;&#x987B;&#x6D6E;&#x70B9;<code>DTYPE</code>&#xFF0C;&#x7ED3;&#x679C;&#x5C06;&#x5177;&#x6709;&#x76F8;&#x540C;&#x7684;<code>DTYPE</code>&#x3002;</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.bernoulli" title="torch.bernoulli"> <code>torch.bernoulli&#xFF08;&#xFF09;</code></a></p>
<p><code>bernoulli_</code>()</p>
<p><code>bernoulli_</code>( <em>p=0.5</em> , <em>*</em> , <em>generator=None</em> ) &#x2192; Tensor</p>
<p>&#x586B;&#x5145;&#x7684;<code>&#x81EA;</code>&#x6BCF;&#x4E2A;&#x4F4D;&#x7F6E;&#x4E0E;&#x4E00;&#x4E2A;&#x72EC;&#x7ACB;&#x7684;&#x6837;&#x54C1;&#x4ECE; &#x4F2F;&#x52AA;&#x5229; &#xFF08; p  &#xFF09; \&#x6587;&#x672C;{&#x4F2F;&#x52AA;&#x5229;}&#xFF08;\ texttt {p}&#xFF09; &#x4F2F;&#x52AA;&#x5229; &#xFF08; p  &#xFF09; &#x3002; <code>&#x81EA;</code>&#x53EF;&#x4EE5;&#x5177;&#x6709;&#x79EF;&#x5206;<code>DTYPE</code>&#x3002;</p>
<p><code>bernoulli_</code>( <em>p_tensor</em> , <em>*</em> , <em>generator=None</em> ) &#x2192; Tensor</p>
<p><code>p_tensor</code>&#x5E94;&#x8BE5;&#x662F;&#x88AB;&#x7528;&#x4E8E;&#x7ED8;&#x5236;&#x4E8C;&#x8FDB;&#x5236;&#x968F;&#x673A;&#x6570;&#x5305;&#x542B;&#x6982;&#x7387;&#x7684;&#x5F20;&#x91CF;&#x3002;</p>
<p>&#x7684; i&#x7684; T  H  \&#x6587;&#x672C;{I} ^ {&#x7B2C;}  I  T  H  &#x7684;<code>&#x81EA;</code>&#x5F20;&#x91CF;&#x5143;&#x4EF6;&#x5C06;&#x88AB;&#x8BBE;&#x7F6E;&#x4E3A;&#x4ECE; &#x4F2F;&#x52AA;&#x5229; &#x53D6;&#x6837;&#x7684;&#x503C;&#xFF08; p_tensor [I]  &#xFF09;
\&#x6587;&#x672C;{&#x4F2F;&#x52AA;&#x5229;}&#xFF08;\ texttt {p \ _tensor [I]}&#xFF09; &#x4F2F;&#x52AA;&#x5229; &#xFF08; p_tensor [I]  &#xFF09; &#x3002;</p>
<p><code>&#x81EA;</code>&#x53EF;&#x4EE5;&#x6709;&#x79EF;&#x5206;<code>D&#x7C7B;</code>&#xFF0C;&#x800C;<code>p_tensor</code>&#x5FC5;&#x987B;&#x6D6E;&#x70B9;<code>DTYPE</code>&#x3002;</p>
<p>&#x53E6;&#x8BF7;&#x53C2;&#x89C1; <code>&#x4F2F;&#x52AA;&#x5229;&#xFF08;&#xFF09;</code>&#x548C;<a href="torch.html#torch.bernoulli" title="torch.bernoulli"> <code>torch.bernoulli&#xFF08;&#xFF09;</code></a></p>
<p><code>bfloat16</code>() &#x2192; Tensor</p>
<p><code>self.bfloat16&#xFF08;&#xFF09;</code>&#x7B49;&#x4E8E;<code>self.to&#xFF08;torch.bfloat16&#xFF09;</code>&#x3002;&#x53C2;&#x89C1; <code>&#x81F3;&#xFF08;&#xFF09;</code>&#x3002;</p>
<p><code>bincount</code>( <em>weights=None</em> , <em>minlength=0</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.bincount" title="torch.bincount"> <code>torch.bincount&#xFF08;&#xFF09;</code></a></p>
<p><code>bitwise_not</code>() &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.bitwise_not" title="torch.bitwise_not"> <code>torch.bitwise_not&#xFF08;&#xFF09;</code></a></p>
<p><code>bitwise_not_</code>() &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <code>bitwise_not&#xFF08;&#xFF09;</code></p>
<p><code>bmm</code>( <em>batch2</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.bmm" title="torch.bmm"> <code>torch.bmm&#xFF08;&#xFF09;</code></a></p>
<p><code>bool</code>() &#x2192; Tensor</p>
<p><code>self.bool&#xFF08;&#xFF09;</code>&#x7B49;&#x4E8E;<code>self.to&#xFF08;torch.bool&#xFF09;</code>&#x3002;&#x53C2;&#x89C1; <code>&#x81F3;&#xFF08;&#xFF09;</code>&#x3002;</p>
<p><code>byte</code>() &#x2192; Tensor</p>
<p><code>self.byte&#xFF08;&#xFF09;</code>&#x7B49;&#x4E8E;<code>self.to&#xFF08;torch.uint8&#xFF09;</code>&#x3002;&#x53C2;&#x89C1; <code>&#x81F3;&#xFF08;&#xFF09;</code>&#x3002;</p>
<p><code>cauchy_</code>( <em>median=0</em> , <em>sigma=1</em> , <em>*</em> , <em>generator=None</em> ) &#x2192; Tensor</p>
<p>&#x586B;&#x5145;&#x4E0E;&#x67EF;&#x897F;&#x5206;&#x5E03;&#x4E2D;&#x5956;&#x53F7;&#x7801;&#x7684;&#x5F20;&#x91CF;&#xFF1A;</p>
<p>f(x)=1&#x3C0;&#x3C3;(x&#x2212;median)2+&#x3C3;2f(x) = \dfrac{1}{\pi} \dfrac{\sigma}{(x -
\text{median})^2 + \sigma^2}f(x)=&#x3C0;1&#x200B;(x&#x2212;median)2+&#x3C3;2&#x3C3;&#x200B;</p>
<p><code>ceil</code>() &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.ceil" title="torch.ceil"> <code>torch.ceil&#xFF08;&#xFF09;</code></a></p>
<p><code>ceil_</code>() &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>&#x5C0F;&#x533A;&#x7684;&#xFF08;&#xFF09;</code></p>
<p><code>char</code>() &#x2192; Tensor</p>
<p><code>self.char&#xFF08;&#xFF09;</code>&#x7B49;&#x4E8E;<code>self.to&#xFF08;torch.int8&#xFF09;</code>&#x3002;&#x53C2;&#x89C1; <code>&#x81F3;&#xFF08;&#xFF09;</code>&#x3002;</p>
<p><code>cholesky</code>( <em>upper=False</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.cholesky" title="torch.cholesky"> <code>torch.cholesky&#xFF08;&#xFF09;</code></a></p>
<p><code>cholesky_inverse</code>( <em>upper=False</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.cholesky_inverse" title="torch.cholesky_inverse"> <code>torch.cholesky_inverse&#xFF08;&#xFF09;</code></a></p>
<p><code>cholesky_solve</code>( <em>input2</em> , <em>upper=False</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.cholesky_solve" title="torch.cholesky_solve"> <code>torch.cholesky_solve&#xFF08;&#xFF09;</code></a></p>
<p><code>chunk</code>( <em>chunks</em> , <em>dim=0</em> ) &#x2192; List of Tensors</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.chunk" title="torch.chunk"> <code>torch.chunk&#xFF08;&#xFF09;</code></a></p>
<p><code>clamp</code>( <em>min</em> , <em>max</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.clamp" title="torch.clamp"> <code>torch.clamp&#xFF08;&#xFF09;</code></a></p>
<p><code>clamp_</code>( <em>min</em> , <em>max</em> ) &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>&#x5939;&#x5177;&#xFF08;&#xFF09;</code></p>
<p><code>clone</code>() &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;<code>&#x81EA;</code>&#x5F20;&#x7684;&#x526F;&#x672C;&#x3002;&#x4F7F;&#x526F;&#x672C;&#x5177;&#x6709;&#x76F8;&#x540C;&#x7684;&#x5927;&#x5C0F;&#x548C;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x4E3A;<code>&#x81EA;</code>&#x3002;</p>
<p>Note</p>
<p>&#x4E0D;&#x50CF; copy_&#xFF08;&#xFF09;&#xFF0C;&#x8BE5;&#x51FD;&#x6570;&#x88AB;&#x8BB0;&#x5F55;&#x5728;&#x8BA1;&#x7B97;&#x56FE;&#x3002;&#x68AF;&#x5EA6;&#x4F20;&#x64AD;&#x5230;&#x514B;&#x9686;&#x7684;&#x5F20;&#x91CF;&#x5C06;&#x4F20;&#x64AD;&#x5230;&#x539F;&#x6765;&#x7684;&#x5F20;&#x91CF;&#x3002;</p>
<p><code>contiguous</code>() &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x5305;&#x542B;&#x76F8;&#x540C;&#x7684;&#x6570;&#x636E;<code>&#x81EA;</code>&#x5F20;&#x91CF;&#x7684;&#x8FDE;&#x7EED;&#x5F20;&#x91CF;&#x3002;&#x5982;&#x679C;<code>&#x81EA;</code>&#x5F20;&#x91CF;&#x662F;&#x8FDE;&#x7EED;&#x7684;&#xFF0C;&#x5219;&#x8BE5;&#x51FD;&#x6570;&#x8FD4;&#x56DE;<code>&#x81EA;</code>&#x5F20;&#x91CF;&#x3002;</p>
<p><code>copy_</code>( <em>src</em> , <em>non_blocking=False</em> ) &#x2192; Tensor</p>
<p>&#x62F7;&#x8D1D;&#x4ECE;<code>&#x7684;&#x5143;&#x7D20;&#x7684;src</code>&#x5230;<code>&#x81EA;</code>&#x5F20;&#x91CF;&#x5E76;&#x8FD4;&#x56DE;<code>&#x81EA;</code>&#x3002;</p>
<p>&#x7684;<code>SRC</code>&#x5F20;&#x91CF;&#x5FC5;&#x987B;&#x662F;<a href="notes/broadcasting.html#broadcasting-%0Asemantics"> broadcastable  </a>&#x4E0E;<code>&#x81EA;</code>&#x5F20;&#x91CF;&#x3002;&#x5B83;&#x53EF;&#x4EE5;&#x662F;&#x4E0D;&#x540C;&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x7684;&#x6216;&#x9A7B;&#x7559;&#x5728;&#x4E0D;&#x540C;&#x8BBE;&#x5907;&#x4E0A;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>SRC</strong> &#xFF08; <em>&#x5F20;&#x91CF;</em> &#xFF09; - &#x6E90;&#x5F20;&#x91CF;&#x4ECE;&#x590D;&#x5236;</p>
</li>
<li><p><strong>non_blocking</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a>&#xFF09; - &#x5982;&#x679C;<code>&#x771F;</code>&#xFF0C;&#x5E76;&#x5C06;&#x8BE5;&#x526F;&#x672C;&#x662F;CPU&#x548C;GPU&#x4E4B;&#x95F4;&#xFF0C;&#x53EF;&#x80FD;&#x4F1A;&#x51FA;&#x73B0;&#x590D;&#x5236;&#x5F02;&#x6B65;&#x76F8;&#x5BF9;&#x4E8E;&#x4E3B;&#x673A;&#x3002;&#x5BF9;&#x4E8E;&#x5176;&#x4ED6;&#x60C5;&#x51B5;&#xFF0C;&#x8FD9;&#x79CD;&#x8BF4;&#x6CD5;&#x6CA1;&#x6709;&#x4EFB;&#x4F55;&#x6548;&#x679C;&#x3002;</p>
</li>
</ul>
<p><code>cos</code>() &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.cos" title="torch.cos"> <code>torch.cos&#xFF08;&#xFF09;</code></a></p>
<p><code>cos_</code>() &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>COS&#x7684;&#xFF08;&#xFF09;</code></p>
<p><code>cosh</code>() &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.cosh" title="torch.cosh"> <code>torch.cosh&#xFF08;&#xFF09;</code></a></p>
<p><code>cosh_</code>() &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>COSH&#x7684;&#xFF08;&#xFF09;</code></p>
<p><code>cpu</code>() &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x6B64;&#x5BF9;&#x8C61;&#x7684;CPU&#x5185;&#x5B58;&#x62F7;&#x8D1D;&#x3002;</p>
<p>&#x5982;&#x679C;&#x8BE5;&#x5BF9;&#x8C61;&#x5DF2;&#x5728;CPU&#x5185;&#x5B58;&#x548C;&#x6B63;&#x786E;&#x7684;&#x8BBE;&#x5907;&#x4E0A;&#xFF0C;&#x5219;&#x6CA1;&#x6709;&#x6267;&#x884C;&#x590D;&#x5236;&#x64CD;&#x4F5C;&#xFF0C;&#x5E76;&#x8FD4;&#x56DE;&#x539F;&#x6765;&#x7684;&#x5BF9;&#x8C61;&#x3002;</p>
<p><code>cross</code>( <em>other</em> , <em>dim=-1</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.cross" title="torch.cross"> <code>torch.cross&#xFF08;&#xFF09;</code></a></p>
<p><code>cuda</code>( <em>device=None</em> , <em>non_blocking=False</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x6B64;&#x5BF9;&#x8C61;&#x7684;CUDA&#x5185;&#x5B58;&#x7684;&#x526F;&#x672C;&#x3002;</p>
<p>&#x5982;&#x679C;&#x8BE5;&#x5BF9;&#x8C61;&#x5DF2;&#x5728;CUDA&#x5185;&#x5B58;&#x548C;&#x6B63;&#x786E;&#x7684;&#x8BBE;&#x5907;&#x4E0A;&#xFF0C;&#x5219;&#x6CA1;&#x6709;&#x6267;&#x884C;&#x590D;&#x5236;&#x64CD;&#x4F5C;&#xFF0C;&#x5E76;&#x8FD4;&#x56DE;&#x539F;&#x6765;&#x7684;&#x5BF9;&#x8C61;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x88C5;&#x7F6E;</strong> &#xFF08;<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"> <code>torch.device</code></a>&#xFF09; - &#x76EE;&#x6807;GPU&#x8BBE;&#x5907;&#x3002;&#x9ED8;&#x8BA4;&#x4E3A;&#x5F53;&#x524D;CUDA&#x8BBE;&#x5907;&#x3002;</p>
</li>
<li><p><strong>non_blocking</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a>&#xFF09; - &#x5982;&#x679C;<code>&#x771F;</code>&#x548C;&#x6E90;&#x6781;&#x88AB;&#x56FA;&#x5B9A;&#x5B58;&#x50A8;&#x5668;&#xFF0C;&#x590D;&#x5236;&#x5C06;&#x662F;&#x5F02;&#x6B65;&#x76F8;&#x5BF9;&#x4E8E;&#x4E3B;&#x673A;&#x3002;&#x53E6;&#x5916;&#xFF0C;&#x53C2;&#x6570;&#x6CA1;&#x6709;&#x4EFB;&#x4F55;&#x5F71;&#x54CD;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x5047; [HTG13&#x3002;</code></p>
</li>
</ul>
<p><code>cumprod</code>( <em>dim</em> , <em>dtype=None</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.cumprod" title="torch.cumprod"> <code>torch.cumprod&#xFF08;&#xFF09;</code></a></p>
<p><code>cumsum</code>( <em>dim</em> , <em>dtype=None</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.cumsum" title="torch.cumsum"> <code>torch.cumsum&#xFF08;&#xFF09;</code></a></p>
<p><code>data_ptr</code>() &#x2192; int</p>
<p>&#x8FD4;&#x56DE;&#x7684;<code>&#x81EA;</code>&#x5F20;&#x91CF;&#x7684;&#x7B2C;&#x4E00;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x5730;&#x5740;&#x3002;</p>
<p><code>dequantize</code>() &#x2192; Tensor</p>
<p>&#x7ED9;&#x5B9A;&#x4E00;&#x4E2A;&#x91CF;&#x5316;&#x5F20;&#x91CF;&#xFF0C;&#x53BB;&#x91CF;&#x5316;&#x5B83;&#xFF0C;&#x5E76;&#x8FD4;&#x56DE;&#x53BB;&#x91CF;&#x5316;&#x7684;&#x6D6E;&#x52A8;&#x5F20;&#x91CF;&#x3002;</p>
<p><code>det</code>() &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.det" title="torch.det"> <code>torch.det&#xFF08;&#xFF09;</code></a></p>
<p><code>dense_dim</code>() &#x2192; int</p>
<p>&#x5982;&#x679C;<code>&#x81EA;</code>&#x662F;&#x4E00;&#x4E2A;&#x7A00;&#x758F;COO&#x5F20;&#x91CF;&#xFF08;&#x5373;&#xFF0C;&#x4E0E;<code>torch.sparse_coo</code>&#x5E03;&#x5C40;&#xFF09;&#xFF0C;&#x5B83;&#x8FD4;&#x56DE;&#x81F4;&#x5BC6;&#x7684;&#x7EF4;&#x6570;&#x3002;&#x5426;&#x5219;&#xFF0C;&#x8FD9;&#x5C06;&#x5F15;&#x53D1;&#x4E00;&#x4E2A;&#x9519;&#x8BEF;&#x3002;</p>
<p>&#x53E6;&#x8BF7;&#x53C2;&#x89C1; <code>Tensor.sparse_dim&#xFF08;&#xFF09;</code>&#x3002;</p>
<p><code>detach</code>()</p>
<p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x65B0;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x4ECE;&#x5F53;&#x524D;&#x56FE;&#x5F62;&#x5206;&#x79BB;&#x3002;</p>
<p>&#x5176;&#x7ED3;&#x679C;&#x5C06;&#x6C38;&#x8FDC;&#x4E0D;&#x9700;&#x8981;&#x68AF;&#x5EA6;&#x3002;</p>
<p>Note</p>
<p>&#x56DE;&#x5230;&#x5F20;&#x91CF;&#x80A1;&#x4E0E;&#x539F;&#x6765;&#x76F8;&#x540C;&#x7684;&#x5B58;&#x50A8;&#x3002;&#x5C31;&#x5730;&#x5BF9;&#x5B83;&#x4EEC;&#x4E2D;&#x7684;&#x4FEE;&#x6539;&#x53EF;&#x4EE5;&#x770B;&#x51FA;&#xFF0C;&#x5E76;&#x53EF;&#x80FD;&#x5F15;&#x53D1;&#x6B63;&#x786E;&#x6027;&#x68C0;&#x67E5;&#x9519;&#x8BEF;&#x3002;&#x91CD;&#x8981;&#x63D0;&#x793A;&#xFF1A;&#x4EE5;&#x524D;&#xFF0C;&#x5C31;&#x5730;&#x5C3A;&#x5BF8;/&#x6B65;&#x5E45;/&#x5B58;&#x50A8;&#x7684;&#x53D8;&#x5316;&#xFF08;&#x5982; resize<em>  /
resize_as</em>  /  SET<em>  /  transpose</em>
&#xFF09;&#x6765;&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x4E5F;&#x66F4;&#x65B0;&#x539F;&#x6709;&#x7684;&#x5F20;&#x91CF;&#x3002;&#x73B0;&#x5728;&#xFF0C;&#x8FD9;&#x4E9B;&#x5C31;&#x5730;&#x53D8;&#x5316;&#x5C06;&#x4E0D;&#x518D;&#x66F4;&#x65B0;&#x539F;&#x6709;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x800C;&#x4F1A;&#x89E6;&#x53D1;&#x4E00;&#x4E2A;&#x9519;&#x8BEF;&#x3002;&#x5BF9;&#x4E8E;&#x7A00;&#x758F;&#x5F20;&#x91CF;&#xFF1A;&#x5C31;&#x5730;&#x7D22;&#x5F15;/&#x503C;&#x7684;&#x53D8;&#x5316;&#xFF08;&#x5982; zero<em>  /
copy</em>  /  add_ &#xFF09;&#x53D1;&#x9001;&#x5230;&#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x5C06;&#x4E0D;&#x518D;&#x66F4;&#x65B0;&#x539F;&#x59CB;&#x5F20;&#x91CF;&#xFF0C;&#x800C;&#x4F1A;&#x89E6;&#x53D1;&#x4E00;&#x4E2A;&#x9519;&#x8BEF;&#x3002;</p>
<p><code>detach_</code>()</p>
<p>&#x5206;&#x79BB;&#x4ECE;&#x521B;&#x5EFA;&#x5B83;&#xFF0C;&#x4F7F;&#x5176;&#x6210;&#x4E3A;&#x4E00;&#x4E2A;&#x53F6;&#x5B50;&#x56FE;&#x8868;&#x4E2D;&#x7684;&#x5F20;&#x91CF;&#x3002;&#x610F;&#x89C1;&#x4E0D;&#x80FD;&#x5C31;&#x5730;&#x5206;&#x79BB;&#x3002;</p>
<p><code>diag</code>( <em>diagonal=0</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.diag" title="torch.diag"> <code>torch.diag&#xFF08;&#xFF09;</code></a></p>
<p><code>diag_embed</code>( <em>offset=0</em> , <em>dim1=-2</em> , <em>dim2=-1</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.diag_embed" title="torch.diag_embed"> <code>torch.diag_embed&#xFF08;&#xFF09;</code></a></p>
<p><code>diagflat</code>( <em>offset=0</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.diagflat" title="torch.diagflat"> <code>torch.diagflat&#xFF08;&#xFF09;</code></a></p>
<p><code>diagonal</code>( <em>offset=0</em> , <em>dim1=0</em> , <em>dim2=1</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.diagonal" title="torch.diagonal"> <code>torch.diagonal&#xFF08;&#xFF09;</code></a></p>
<p><code>fill_diagonal_</code>( <em>fill_value</em> , <em>wrap=False</em> ) &#x2192; Tensor</p>
<p>&#x586B;&#x5145;&#x6709;&#x81F3;&#x5C11;2&#x7EF4;&#x7684;&#x5F20;&#x91CF;&#x7684;&#x4E3B;&#x5BF9;&#x89D2;&#x7EBF;&#x3002;&#x5F53;&#x53D8;&#x6697;&amp; GT ; 2&#xFF0C;&#x8F93;&#x5165;&#x7684;&#x6240;&#x6709;&#x5C3A;&#x5BF8;&#x5FC5;&#x987B;&#x76F8;&#x7B49;&#x7684;&#x957F;&#x5EA6;&#x3002;&#x8FD9;&#x4E2A;&#x51FD;&#x6570;&#x4FEE;&#x6539;&#x5C31;&#x5730;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#xFF0C;&#x5E76;&#x8FD4;&#x56DE;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>fill_value</strong> &#xFF08;[HTG2&#x6807;&#x91CF;&#xFF09; - &#x586B;&#x5145;&#x503C;</p>
</li>
<li><p><strong>&#x5305;&#x88F9;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a>&#xFF09; - &#x5BF9;&#x89D2;N&#x5217;&#x9AD8;&#x5C42;&#x77E9;&#x9635;&#x540E;&#x201C;&#x5305;&#x88F9;&#x201D;&#x3002;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.zeros(3, 3)
&gt;&gt;&gt; a.fill_diagonal_(5)
tensor([[5., 0., 0.],
        [0., 5., 0.],
        [0., 0., 5.]])
&gt;&gt;&gt; b = torch.zeros(7, 3)
&gt;&gt;&gt; b.fill_diagonal_(5)
tensor([[5., 0., 0.],
        [0., 5., 0.],
        [0., 0., 5.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.]])
&gt;&gt;&gt; c = torch.zeros(7, 3)
&gt;&gt;&gt; c.fill_diagonal_(5, wrap=True)
tensor([[5., 0., 0.],
        [0., 5., 0.],
        [0., 0., 5.],
        [0., 0., 0.],
        [5., 0., 0.],
        [0., 5., 0.],
        [0., 0., 5.]])
</code></pre><p><code>digamma</code>() &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.digamma" title="torch.digamma"> <code>torch.digamma&#xFF08;&#xFF09;</code></a></p>
<p><code>digamma_</code>() &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>digamma&#x7684;&#xFF08;&#xFF09;</code></p>
<p><code>dim</code>() &#x2192; int</p>
<p>&#x8FD4;&#x56DE;&#x7684;<code>&#x81EA;</code>&#x5F20;&#x91CF;&#x7684;&#x7EF4;&#x6570;&#x3002;</p>
<p><code>dist</code>( <em>other</em> , <em>p=2</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.dist" title="torch.dist"> <code>torch.dist&#xFF08;&#xFF09;</code></a></p>
<p><code>div</code>( <em>value</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.div" title="torch.div"> <code>torch.div&#xFF08;&#xFF09;</code></a></p>
<p><code>div_</code>( <em>value</em> ) &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>&#x7684;div&#xFF08;&#xFF09;</code></p>
<p><code>dot</code>( <em>tensor2</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.dot" title="torch.dot"> <code>torch.dot&#xFF08;&#xFF09;</code></a></p>
<p><code>double</code>() &#x2192; Tensor</p>
<p><code>self.double&#xFF08;&#xFF09;</code>&#x7B49;&#x4E8E;<code>self.to&#xFF08;torch.float64&#xFF09;</code>&#x3002;&#x53C2;&#x89C1; <code>&#x81F3;&#xFF08;&#xFF09;</code>&#x3002;</p>
<p><code>eig</code>( <em>eigenvectors=False) - &gt; (Tensor</em>, <em>Tensor</em> )</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.eig" title="torch.eig"> <code>torch.eig&#xFF08;&#xFF09;</code></a></p>
<p><code>element_size</code>() &#x2192; int</p>
<p>&#x8FD4;&#x56DE;&#x5355;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x5B57;&#x8282;&#x5927;&#x5C0F;&#x3002;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.tensor([]).element_size()
4
&gt;&gt;&gt; torch.tensor([], dtype=torch.uint8).element_size()
1
</code></pre><p><code>eq</code>( <em>other</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.eq" title="torch.eq"> <code>torch.eq&#xFF08;&#xFF09;</code></a></p>
<p><code>eq_</code>( <em>other</em> ) &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>&#x5F53;&#x91CF;&#x7684;&#xFF08;&#xFF09;</code></p>
<p><code>equal</code>( <em>other</em> ) &#x2192; bool</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.equal" title="torch.equal"> <code>torch.equal&#xFF08;&#xFF09;</code></a></p>
<p><code>erf</code>() &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.erf" title="torch.erf"> <code>torch.erf&#xFF08;&#xFF09;</code></a></p>
<p><code>erf_</code>() &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>ERF&#x7684;&#xFF08;&#xFF09;</code></p>
<p><code>erfc</code>() &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.erfc" title="torch.erfc"> <code>torch.erfc&#xFF08;&#xFF09;</code></a></p>
<p><code>erfc_</code>() &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>ERFC&#x7684;&#xFF08;&#xFF09;</code></p>
<p><code>erfinv</code>() &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.erfinv" title="torch.erfinv"> <code>torch.erfinv&#xFF08;&#xFF09;</code></a></p>
<p><code>erfinv_</code>() &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>erfinv&#x7684;&#xFF08;&#xFF09;</code></p>
<p><code>exp</code>() &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.exp" title="torch.exp"> <code>torch.exp&#xFF08;&#xFF09;</code></a></p>
<p><code>exp_</code>() &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>EXP&#x7684;&#xFF08;&#xFF09;</code></p>
<p><code>expm1</code>() &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.expm1" title="torch.expm1"> <code>torch.expm1&#xFF08;&#xFF09;</code></a></p>
<p><code>expm1_</code>() &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>&#x7684;expm1&#x7684;&#xFF08;&#xFF09;</code></p>
<p><code>expand</code>( <em>*sizes</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;<code>&#x81EA;</code>&#x5F20;&#x91CF;&#x5177;&#x6709;&#x6269;&#x5C55;&#x5230;&#x66F4;&#x5927;&#x5C3A;&#x5BF8;&#x5355;&#x5C3A;&#x5BF8;&#x7684;&#x65B0;&#x89C6;&#x56FE;&#x3002;</p>
<p>&#x4F20;&#x9012;-1&#x4F5C;&#x4E3A;&#x5927;&#x5C0F;&#x4E3A;&#x4E00;&#x4E2A;&#x5C3A;&#x5BF8;&#x662F;&#x6307;&#x5728;&#x4E0D;&#x6539;&#x53D8;&#x5176;&#x5C3A;&#x5BF8;&#x7684;&#x5927;&#x5C0F;&#x3002;</p>
<p>&#x5F20;&#x91CF;&#xFF0C;&#x4E5F;&#x53EF;&#x4EE5;&#x6269;&#x5927;&#x5230;&#x5C3A;&#x5BF8;&#x7684;&#x6570;&#x91CF;&#x8F83;&#x591A;&#xFF0C;&#x800C;&#x65B0;&#x7684;&#x5C06;&#x5728;&#x524D;&#x9762;&#x8FFD;&#x52A0;&#x3002;&#x5BF9;&#x4E8E;&#x65B0;&#x7684;&#x5C3A;&#x5BF8;&#xFF0C;&#x5927;&#x5C0F;&#x4E0D;&#x80FD;&#x8BBE;&#x7F6E;&#x4E3A;-1&#x3002;</p>
<p>&#x6269;&#x5927;&#x7684;&#x5F20;&#x91CF;&#x4E0D;&#x5206;&#x914D;&#x65B0;&#x7684;&#x5185;&#x5B58;&#xFF0C;&#x4F46;&#x662F;&#x4EC5;&#x521B;&#x5EFA;&#x5176;&#x4E2D;&#x5927;&#x5C0F;&#x4E3A;&#x4E00;&#x7684;&#x5C3A;&#x5BF8;&#x662F;&#x901A;&#x8FC7;&#x8BBE;&#x7F6E;<code>&#x6B65;&#x5E45;
[HTG3&#x5230;0&#x6269;&#x5C55;&#x5230;&#x4E00;&#x4E2A;&#x66F4;&#x5927;&#x7684;&#x5C3A;&#x5BF8;&#x4E0A;&#x7684;&#x73B0;&#x6709;&#x5F20;&#x91CF;&#x7684;&#x65B0;&#x89C6;&#x56FE;&#x3002;&#x5C3A;&#x5BF8;1&#x7684;&#x4EFB;&#x4F55;&#x5C3A;&#x5BF8;&#x53EF;&#x6269;&#x5C55;&#x5230;&#x4EFB;&#x610F;&#x503C;&#x800C;&#x4E0D;&#x5206;&#x914D;&#x65B0;&#x7684;&#x5185;&#x5B58;&#x3002;</code></p>
<p>Parameters</p>
<p><strong>*&#x7684;&#x5927;&#x5C0F;</strong> &#xFF08; <em>torch.Size</em> <em>&#x6216;</em> <em>INT ...</em> &#xFF09; - &#x6240;&#x9700;&#x7684;&#x6269;&#x5C55;&#x5927;&#x5C0F;</p>
<p>Warning</p>
<p>&#x81A8;&#x80C0;&#x5F20;&#x91CF;&#x7684;&#x591A;&#x4E8E;&#x4E00;&#x4E2A;&#x7684;&#x5143;&#x4EF6;&#x53EF;&#x6307;&#x4EE3;&#x5355;&#x4E2A;&#x5B58;&#x50A8;&#x5668;&#x4F4D;&#x7F6E;&#x3002;&#x5176;&#x7ED3;&#x679C;&#x662F;&#xFF0C;&#x5C31;&#x5730;&#x64CD;&#x4F5C;&#xFF08;&#x7279;&#x522B;&#x662F;&#x90A3;&#x4E9B;&#x6709;&#x91CF;&#x5316;&#x7684;&#xFF09;&#x53EF;&#x80FD;&#x4F1A;&#x5BFC;&#x81F4;&#x4E0D;&#x6B63;&#x786E;&#x7684;&#x884C;&#x4E3A;&#x3002;&#x5982;&#x679C;&#x4F60;&#x9700;&#x8981;&#x5199;&#x5F20;&#x91CF;&#xFF0C;&#x8BF7;&#x5148;&#x514B;&#x9686;&#x5B83;&#x4EEC;&#x3002;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.tensor([[1], [2], [3]])
&gt;&gt;&gt; x.size()
torch.Size([3, 1])
&gt;&gt;&gt; x.expand(3, 4)
tensor([[ 1,  1,  1,  1],
        [ 2,  2,  2,  2],
        [ 3,  3,  3,  3]])
&gt;&gt;&gt; x.expand(-1, 4)   # -1 means not changing the size of that dimension
tensor([[ 1,  1,  1,  1],
        [ 2,  2,  2,  2],
        [ 3,  3,  3,  3]])
</code></pre><p><code>expand_as</code>( <em>other</em> ) &#x2192; Tensor</p>
<p>&#x5C55;&#x5F00;&#xFF0C;&#x8FD9;&#x4E2A;&#x5F20;&#x91CF;&#xFF0C;&#x4EE5;&#x76F8;&#x540C;&#x7684;&#x5C3A;&#x5BF8;&#x4E3A;<code>&#x5176;&#x4ED6;</code>&#x3002; <code>self.expand_as&#xFF08;&#x5176;&#x4ED6;&#xFF09;&#x200B;&#x200B;</code>&#x7B49;&#x4E8E;<code>self.expand&#xFF08;other.size&#xFF08;&#xFF09;&#xFF09;</code>&#x3002;</p>
<p>&#x8BF7;&#x53C2;&#x9605; <code>&#x6269;&#x5927;&#xFF08;&#xFF09;</code>&#x5173;&#x4E8E;<code>&#x66F4;&#x591A;&#x4FE1;&#x606F;&#x5C55;&#x5F00; [HTG9&#x3002;</code></p>
<p>Parameters</p>
<p><strong>&#x5176;&#x4ED6;</strong> &#xFF08; <code>torch.Tensor</code>&#xFF09; - &#x7ED3;&#x679C;&#x5F20;&#x91CF;&#x5177;&#x6709;&#x76F8;&#x540C;&#x7684;&#x5927;&#x5C0F;&#x4E3A;<code>&#x5176;&#x4ED6; [ HTG11&#x3002;</code></p>
<p><code>exponential_</code>( <em>lambd=1</em> , <em>*</em> , <em>generator=None</em> ) &#x2192; Tensor</p>
<p>&#x586B;&#x5145;<code>&#x81EA;</code>&#x5F20;&#x91CF;&#x4E0E;&#x4ECE;&#x6307;&#x6570;&#x5206;&#x5E03;&#x7ED8;&#x5236;&#x7684;&#x5143;&#x7D20;&#xFF1A;</p>
<p>f(x)=&#x3BB;e&#x2212;&#x3BB;xf(x) = \lambda e^{-\lambda x}f(x)=&#x3BB;e&#x2212;&#x3BB;x</p>
<p><code>fft</code>( <em>signal_ndim</em> , <em>normalized=False</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.fft" title="torch.fft"> <code>torch.fft&#xFF08;&#xFF09;</code></a></p>
<p><code>fill_</code>( <em>value</em> ) &#x2192; Tensor</p>
<p>&#x586B;&#x5145;&#x5177;&#x6709;&#x6307;&#x5B9A;&#x503C;<code>&#x81EA;</code>&#x5F20;&#x91CF;&#x3002;</p>
<p><code>flatten</code>( <em>input</em> , <em>start_dim=0</em> , <em>end_dim=-1</em> ) &#x2192; Tensor</p>
<p>&#x89C1;<a href="torch.html#torch.flatten" title="torch.flatten"> <code>torch.flatten&#xFF08;&#xFF09;</code></a></p>
<p><code>flip</code>( <em>dims</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.flip" title="torch.flip"> <code>torch.flip&#xFF08;&#xFF09;</code></a></p>
<p><code>float</code>() &#x2192; Tensor</p>
<p><code>self.float&#xFF08;&#xFF09;</code>&#x7B49;&#x4E8E;<code>self.to&#xFF08;torch.float32&#xFF09;</code>&#x3002;&#x53C2;&#x89C1; <code>&#x81F3;&#xFF08;&#xFF09;</code>&#x3002;</p>
<p><code>floor</code>() &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.floor" title="torch.floor"> <code>torch.floor&#xFF08;&#xFF09;</code></a></p>
<p><code>floor_</code>() &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>&#x5730;&#x677F;&#xFF08;&#xFF09;</code></p>
<p><code>fmod</code>( <em>divisor</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.fmod" title="torch.fmod"> <code>torch.fmod&#xFF08;&#xFF09;</code></a></p>
<p><code>fmod_</code>( <em>divisor</em> ) &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>FMOD&#x7684;&#xFF08;&#xFF09;</code></p>
<p><code>frac</code>() &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.frac" title="torch.frac"> <code>torch.frac&#xFF08;&#xFF09;</code></a></p>
<p><code>frac_</code>() &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>&#x538B;&#x88C2;&#x7684;&#xFF08;&#xFF09;</code></p>
<p><code>gather</code>( <em>dim</em> , <em>index</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.gather" title="torch.gather"> <code>torch.gather&#xFF08;&#xFF09;</code></a></p>
<p><code>ge</code>( <em>other</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.ge" title="torch.ge"> <code>torch.ge&#xFF08;&#xFF09;</code></a></p>
<p><code>ge_</code>( <em>other</em> ) &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684;Ge<code>&#xFF08;&#xFF09;</code></p>
<p><code>gels</code>( <em>A</em> )<a href="_modules/torch/tensor.html#Tensor.gels">[source]</a></p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.lstsq" title="torch.lstsq"> <code>torch.lstsq&#xFF08;&#xFF09;</code></a></p>
<p><code>geometric_</code>( <em>p</em> , <em>*</em> , <em>generator=None</em> ) &#x2192; Tensor</p>
<p>&#x586B;&#x5145;<code>&#x81EA;</code>&#x5F20;&#x91CF;&#x4E0E;&#x6765;&#x81EA;&#x51E0;&#x4F55;&#x5206;&#x5E03;&#x7ED8;&#x5236;&#x7684;&#x5143;&#x7D20;&#xFF1A;</p>
<p>f(X=k)=pk&#x2212;1(1&#x2212;p)f(X=k) = p^{k - 1} (1 - p)f(X=k)=pk&#x2212;1(1&#x2212;p)</p>
<p><code>geqrf</code>( <em>) - &gt; (Tensor</em>, <em>Tensor</em> )</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.geqrf" title="torch.geqrf"> <code>torch.geqrf&#xFF08;&#xFF09;</code></a></p>
<p><code>ger</code>( <em>vec2</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.ger" title="torch.ger"> <code>torch.ger&#xFF08;&#xFF09;</code></a></p>
<p><code>get_device</code>( <em>) - &gt; Device ordinal (Integer</em>)</p>
<p>&#x5BF9;&#x4E8E;CUDA&#x5F20;&#x91CF;&#xFF0C;&#x6B64;&#x51FD;&#x6570;&#x8FD4;&#x56DE;&#x5728;&#x5176;&#x4E0A;&#x9A7B;&#x7559;&#x5F20;&#x91CF;GPU&#x7684;&#x8BBE;&#x5907;&#x5E8F;&#x53F7;&#x3002;&#x5BF9;&#x4E8E;CPU&#x5F20;&#x91CF;&#xFF0C;&#x5219;&#x4F1A;&#x5F15;&#x53D1;&#x9519;&#x8BEF;&#x3002;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(3, 4, 5, device=&apos;cuda:0&apos;)
&gt;&gt;&gt; x.get_device()
0
&gt;&gt;&gt; x.cpu().get_device()  # RuntimeError: get_device is not implemented for type torch.FloatTensor
</code></pre><p><code>gt</code>( <em>other</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.gt" title="torch.gt"> <code>torch.gt&#xFF08;&#xFF09;</code></a></p>
<p><code>gt_</code>( <em>other</em> ) &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>GT&#x7684;&#xFF08;&#xFF09;</code></p>
<p><code>half</code>() &#x2192; Tensor</p>
<p><code>self.half&#xFF08;&#xFF09;</code>&#x7B49;&#x4E8E;<code>self.to&#xFF08;torch.float16&#xFF09;</code>&#x3002;&#x53C2;&#x89C1; <code>&#x81F3;&#xFF08;&#xFF09;</code>&#x3002;</p>
<p><code>hardshrink</code>( <em>lambd=0.5</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="nn.functional.html#torch.nn.functional.hardshrink" title="torch.nn.functional.hardshrink"> <code>torch.nn.functional.hardshrink&#xFF08;&#xFF09;</code>
</a></p>
<p><code>histc</code>( <em>bins=100</em> , <em>min=0</em> , <em>max=0</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.histc" title="torch.histc"> <code>torch.histc&#xFF08;&#xFF09;</code></a></p>
<p><code>ifft</code>( <em>signal_ndim</em> , <em>normalized=False</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.ifft" title="torch.ifft"> <code>torch.ifft&#xFF08;&#xFF09;</code></a></p>
<p><code>index_add_</code>( <em>dim</em> , <em>index</em> , <em>tensor</em> ) &#x2192; Tensor</p>
<p>&#x79EF;&#x7D2F;&#x7684;<a href="torch.html#torch.tensor" title="torch.tensor"> <code>&#x7684;&#x5143;&#x7D20;&#x5F20;&#x91CF;</code></a>&#x5230;<code>&#x81EA;</code>&#x5F20;&#x91CF;&#x901A;&#x8FC7;&#x589E;&#x52A0;&#x5728;&#x7ED9;&#x5B9A;&#x7684;&#x987A;&#x5E8F;&#x7684;&#x7D22;&#x5F15;<code>&#x7D22;&#x5F15;</code>&#x3002;&#x4F8B;&#x5982;&#xFF0C;&#x5982;&#x679C;<code>&#x6697;&#x6DE1; ==  0</code>&#x548C;<code>&#x7D22;&#x5F15;[I]  = =  [HTG27&#xFF1A;J</code>&#xFF0C;&#x5219;<code>i&#x7684;</code>&#x6B21;&#x7684;<a href="torch.html#torch.tensor" title="torch.tensor"> <code>&#x884C;&#x5F20;&#x91CF;</code>
</a>&#x88AB;&#x6DFB;&#x52A0;&#x5230;<code>[HTG41&#xFF1A;J</code>&#x6B21;&#x7684;<code>&#x81EA;</code>&#x884C;&#x3002;</p>
<p>&#x7684; <code>&#x6697;&#x6DE1;</code>&#x6B21;&#x7684;<a href="torch.html#torch.tensor" title="torch.tensor"> <code>&#x7EF4;&#x5F20;&#x91CF;</code></a>&#x5FC5;&#x987B;&#x5177;&#x6709;&#x5C3A;&#x5BF8;&#x4E3A;&#x957F;&#x5EA6;&#x76F8;&#x540C;&#x7684;<code>&#x7D22;&#x5F15;</code>&#xFF08;&#x5B83;&#x5FC5;&#x987B;&#x662F;&#x4E00;&#x4E2A;&#x77E2;&#x91CF;&#xFF09;&#xFF0C;&#x548C;&#x6240;&#x6709;&#x5176;&#x4ED6;&#x7684;&#x5C3A;&#x5BF8;&#x5FC5;&#x987B;&#x76F8;&#x7B26;<code>&#x81EA;</code>&#xFF0C;&#x6216;&#x9519;&#x8BEF;&#x5C06;&#x88AB;&#x63D0;&#x9AD8;&#x3002;</p>
<p>Note</p>
<p>&#x5F53;&#x4F7F;&#x7528;CUDA&#x540E;&#x7AEF;&#xFF0C;&#x8BE5;&#x64CD;&#x4F5C;&#x53EF;&#x4EE5;&#x8BF1;&#x5BFC;&#x975E;&#x786E;&#x5B9A;&#x6027;&#x7684;&#x884C;&#x4E3A;&#x662F;&#x4E0D;&#x5BB9;&#x6613;&#x65AD;&#x5F00;&#x3002;&#x8BF7;&#x53C2;&#x9605;<a href="notes/randomness.html"> &#x91CD;&#x590D;&#x6027; </a>&#x4E3A;&#x80CC;&#x666F;&#x7684;&#x97F3;&#x7B26;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x6697;&#x6DE1;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x7EF4;&#x6CBF;&#x7740;&#x8BE5;&#x7D22;&#x5F15;</p>
</li>
<li><p><strong>&#x7D22;&#x5F15;</strong> &#xFF08; <em>LongTensor</em> &#xFF09; - &#x7684;<a href="torch.html#torch.tensor" title="torch.tensor"> <code>&#x7D22;&#x5F15;&#x5F20;&#x91CF;</code></a>&#x4ECE;&#x9009;&#x62E9;</p>
</li>
<li><p><strong>&#x5F20;&#x91CF;</strong> &#xFF08; <em>&#x5F20;&#x91CF;</em> &#xFF09; - &#x5305;&#x542B;&#x503C;&#x5F20;&#x91CF;&#x6765;&#x6DFB;&#x52A0;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.ones(5, 3)
&gt;&gt;&gt; t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)
&gt;&gt;&gt; index = torch.tensor([0, 4, 2])
&gt;&gt;&gt; x.index_add_(0, index, t)
tensor([[  2.,   3.,   4.],
        [  1.,   1.,   1.],
        [  8.,   9.,  10.],
        [  1.,   1.,   1.],
        [  5.,   6.,   7.]])
</code></pre><p><code>index_add</code>( <em>dim</em> , <em>index</em> , <em>tensor</em> ) &#x2192; Tensor</p>
<p>&#x5916;&#x7684;&#x5730;&#x65B9;&#x7684; <code>&#x7248;&#x672C;torch.Tensor.index_add_&#xFF08;&#xFF09;</code></p>
<p><code>index_copy_</code>( <em>dim</em> , <em>index</em> , <em>tensor</em> ) &#x2192; Tensor</p>
<p>&#x7684;<a href="torch.html#torch.tensor" title="torch.tensor"> <code>&#x62F7;&#x8D1D;&#x7684;&#x5143;&#x7D20;&#x5F20;&#x91CF;</code></a>&#x6210;&#x901A;&#x8FC7;&#x5728;[HTG10&#x7ED9;&#x5B9A;&#x7684;&#x987A;&#x5E8F;&#x9009;&#x62E9;&#x6307;&#x6570;&#x7684;<code>&#x81EA;</code>&#x5F20;&#x91CF;] &#x7D22;&#x5F15; &#x3002;&#x4F8B;&#x5982;&#xFF0C;&#x5982;&#x679C;<code>&#x6697;&#x6DE1; ==  0</code>&#x548C;<code>&#x7D22;&#x5F15;[I]  = =  [HTG27&#xFF1A;J</code>&#xFF0C;&#x5219;<code>i&#x7684;</code>&#x6B21;&#x7684;<a href="torch.html#torch.tensor" title="torch.tensor"> <code>&#x884C;&#x5F20;&#x91CF;</code>
</a>&#x590D;&#x5236;&#x5230;<code>[HTG41&#xFF1A;J</code>&#x6B21;&#x7684;<code>&#x81EA;&#x884C;</code>&#x3002;</p>
<p>The <code>dim</code>th dimension of <a href="torch.html#torch.tensor" title="torch.tensor"><code>tensor</code></a>
must have the same size as the length of <code>index</code>(which must be a vector), and
all other dimensions must match <code>self</code>, or an error will be raised.</p>
<p>Parameters</p>
<ul>
<li><p><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a>) &#x2013; dimension along which to index</p>
</li>
<li><p><strong>index</strong> ( <em>LongTensor</em> ) &#x2013; indices of <a href="torch.html#torch.tensor" title="torch.tensor"><code>tensor</code></a> to select from</p>
</li>
<li><p><strong>&#x5F20;&#x91CF;</strong> &#xFF08; <em>&#x5F20;&#x91CF;</em> &#xFF09; - &#x5305;&#x542B;&#x503C;&#x5F20;&#x91CF;&#x6765;&#x590D;&#x5236;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.zeros(5, 3)
&gt;&gt;&gt; t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)
&gt;&gt;&gt; index = torch.tensor([0, 4, 2])
&gt;&gt;&gt; x.index_copy_(0, index, t)
tensor([[ 1.,  2.,  3.],
        [ 0.,  0.,  0.],
        [ 7.,  8.,  9.],
        [ 0.,  0.,  0.],
        [ 4.,  5.,  6.]])
</code></pre><p><code>index_copy</code>( <em>dim</em> , <em>index</em> , <em>tensor</em> ) &#x2192; Tensor</p>
<p>&#x5916;&#x7684;&#x5730;&#x65B9;&#x7684; <code>&#x7248;&#x672C;torch.Tensor.index_copy_&#xFF08;&#xFF09;</code></p>
<p><code>index_fill_</code>( <em>dim</em> , <em>index</em> , <em>val</em> ) &#x2192; Tensor</p>
<p>&#x586B;&#x5145;<code>&#x81EA;&#x7684;&#x5143;&#x7D20;</code>&#x5F20;&#x91CF;&#x4E0E;&#x503C;<code>VAL</code>&#x901A;&#x8FC7;&#x5728;<code>&#x7D22;&#x5F15;&#x7ED9;&#x5B9A;&#x7684;&#x987A;&#x5E8F;&#x9009;&#x62E9;&#x6240;&#x8FF0;&#x7D22;&#x5F15;</code>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a>) &#x2013; dimension along which to index</p>
</li>
<li><p><strong>&#x7D22;&#x5F15;</strong> &#xFF08; <em>LongTensor</em> &#xFF09; - &#x7684;<code>&#x6307;&#x6570;&#x81EA;</code>&#x5F20;&#x91CF;&#xFF0C;&#x4EE5;&#x586B;&#x8865;&#x5728;</p>
</li>
<li><p><strong>VAL</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a>&#xFF09; - &#x4EE5;&#x586B;&#x8865;&#x7684;&#x503C;</p>
</li>
</ul>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)
&gt;&gt;&gt; index = torch.tensor([0, 2])
&gt;&gt;&gt; x.index_fill_(1, index, -1)
tensor([[-1.,  2., -1.],
        [-1.,  5., -1.],
        [-1.,  8., -1.]])
</code></pre><p><code>index_fill</code>( <em>dim</em> , <em>index</em> , <em>value</em> ) &#x2192; Tensor</p>
<p>&#x5916;&#x7684;&#x5730;&#x65B9;&#x7684; <code>&#x7248;&#x672C;torch.Tensor.index_fill_&#xFF08;&#xFF09;</code></p>
<p><code>index_put_</code>( <em>indices</em> , <em>value</em> , <em>accumulate=False</em> ) &#x2192; Tensor</p>
<p>&#x4ECE;&#x5F20;&#x91CF;<code>&#x503C;</code>&#x628A;&#x503C;&#x4EE3;&#x5165;&#x5F20;&#x91CF;<code>&#x81EA;</code>&#x4F7F;&#x7528;&#x4E2D;&#x6307;&#x5B9A;&#x7684;&#x7D22;&#x5F15;<code>&#x6307;&#x6570;</code>&#xFF08;&#x8FD9;&#x662F;&#x5F20;&#x91CF;&#x7684;&#x5143;&#x7EC4;&#xFF09;&#x3002;&#x8868;&#x8FBE;&#x5F0F;<code>tensor.index_put_&#xFF08;&#x6307;&#x6570;&#xFF0C; &#x503C;&#xFF09;</code>&#x7B49;&#x4E8E;<code>&#x5F20;&#x91CF;[&#x6307;&#x6570;]  =  &#x503C;</code>&#x3002;&#x8FD4;&#x56DE;<code>&#x81EA; [HTG31&#x3002;</code></p>
<p>&#x5982;&#x679C;<code>&#x79EF;&#x7D2F;</code>&#x662F;<code>&#x771F;</code>&#xFF0C;&#x5143;&#x7D20;&#x5728;<a href="torch.html#torch.tensor" title="torch.tensor"> <code>&#x5F20;&#x91CF;</code></a>&#x6DFB;&#x52A0;&#x5230;<code>&#x81EA;</code>&#x3002;&#x5982;&#x679C;&#x7D2F;&#x79EF;&#x4E3A;<code>&#x5047;</code>&#xFF0C;&#x884C;&#x4E3A;&#x662F;&#x4E0D;&#x786E;&#x5B9A;&#x5982;&#x679C;&#x7D22;&#x5F15;&#x5305;&#x542B;&#x91CD;&#x590D;&#x7684;&#x5143;&#x7D20;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x6307;&#x6570;</strong> &#xFF08; <em>LongTensor</em> &#x7684;&#x5143;&#x7EC4;&#xFF09; - &#x7528;&#x4E8E;&#x7D22;&#x5F15;&#x5230;&#x81EA;&#x5F20;&#x91CF;&#x3002;</p>
</li>
<li><p><strong>&#x7684;&#x503C;</strong> &#xFF08; <em>&#x5F20;&#x91CF;</em> &#xFF09; - &#x76F8;&#x540C;&#x7C7B;&#x578B;&#x7684;&#x5F20;&#x91CF;&#x4E3A;&#x81EA;&#x3002;</p>
</li>
<li><p><strong>&#x79EF;&#x7D2F;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a>&#xFF09; - &#x662F;&#x5426;&#x79EF;&#x7D2F;&#x5230;&#x81EA;</p>
</li>
</ul>
<p><code>index_put</code>( <em>indices</em> , <em>value</em> , <em>accumulate=False</em> ) &#x2192; Tensor</p>
<p>&#x5916;&#x7684;&#x5730;&#x65B9;&#x7248;&#x672C;&#x7684; <code>index_put_&#xFF08;&#xFF09;</code></p>
<p><code>index_select</code>( <em>dim</em> , <em>index</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.index_select" title="torch.index_select"> <code>torch.index_select&#xFF08;&#xFF09;</code></a></p>
<p><code>indices</code>() &#x2192; Tensor</p>
<p>&#x5982;&#x679C;<code>&#x81EA;</code>&#x662F;&#x4E00;&#x4E2A;&#x7A00;&#x758F;COO&#x5F20;&#x91CF;&#xFF08;&#x5373;&#xFF0C;&#x4E0E;<code>torch.sparse_coo</code>&#x5E03;&#x5C40;&#xFF09;&#xFF0C;&#x5B83;&#x8FD4;&#x56DE;&#x6240;&#x5305;&#x542B;&#x7684;&#x7D22;&#x5F15;&#x5F20;&#x91CF;&#x7684;&#x56FE;&#x3002;&#x5426;&#x5219;&#xFF0C;&#x8FD9;&#x5C06;&#x5F15;&#x53D1;&#x4E00;&#x4E2A;&#x9519;&#x8BEF;&#x3002;</p>
<p>&#x53E6;&#x8BF7;&#x53C2;&#x89C1; <code>Tensor.values&#xFF08;&#xFF09;</code>&#x3002;</p>
<p>Note</p>
<p>&#x8FD9;&#x79CD;&#x65B9;&#x6CD5;&#x53EA;&#x80FD;&#x5728;&#x805A;&#x7ED3;&#x7684;&#x7A00;&#x758F;&#x5F20;&#x91CF;&#x88AB;&#x8C03;&#x7528;&#x3002;&#x53C2;&#x89C1;<code>&#x5BF9;&#x4E8E;&#x7EC6;&#x8282;Tensor.coalesce&#xFF08;&#xFF09;</code>&#x3002;</p>
<p><code>int</code>() &#x2192; Tensor</p>
<p><code>self.int&#xFF08;&#xFF09;</code>&#x7B49;&#x4E8E;<code>self.to&#xFF08;torch.int32&#xFF09;</code>&#x3002;&#x53C2;&#x89C1; <code>&#x81F3;&#xFF08;&#xFF09;</code>&#x3002;</p>
<p><code>int_repr</code>() &#x2192; Tensor</p>
<p>&#x7ED9;&#x5B9A;&#x4E00;&#x4E2A;&#x91CF;&#x5316;&#x7684;&#x5F20;&#x91CF;&#xFF0C;<code>self.int_repr&#xFF08;&#xFF09;</code>&#x8FD4;&#x56DE;&#x4E0E;uint8_t&#x4F5C;&#x4E3A;&#x5B58;&#x50A8;&#x7ED9;&#x5B9A;&#x7684;&#x5F20;&#x91CF;&#x7684;&#x5E95;&#x5C42;uint8_t&#x503C;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x7684;CPU&#x5F20;&#x91CF;&#x3002;</p>
<p><code>inverse</code>() &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.inverse" title="torch.inverse"> <code>torch.inverse&#xFF08;&#xFF09;</code></a></p>
<p><code>irfft</code>( <em>signal_ndim</em> , <em>normalized=False</em> , <em>onesided=True</em> ,
<em>signal_sizes=None</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.irfft" title="torch.irfft"> <code>torch.irfft&#xFF08;&#xFF09;</code></a></p>
<p><code>is_contiguous</code>() &#x2192; bool</p>
<p>&#x8FD4;&#x56DE;true&#x5982;&#x679C;<code>&#x81EA;</code>&#x5F20;&#x91CF;&#x662F;&#x5728;&#x7528;C&#x987A;&#x5E8F;&#x5B58;&#x50A8;&#x5668;&#x4E2D;&#x8FDE;&#x7EED;&#x3002;</p>
<p><code>is_floating_point</code>() &#x2192; bool</p>
<p>&#x8FD4;&#x56DE;true&#x7684;<code>&#x81EA;</code>&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x662F;&#x4E00;&#x4E2A;&#x6D6E;&#x70B9;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x3002;</p>
<p><code>is_leaf</code>()</p>
<p>&#x6709;&#x6240;&#x6709;&#x7684;&#x5F20;&#x91CF; <code>requires_grad</code>&#x662F;<code>&#x5047;</code>&#x5C06;&#x53F6;&#x6309;&#x7EA6;&#x5B9A;&#x5F20;&#x91CF;&#x3002;</p>
<p>&#x5BF9;&#x4E8E;&#x5F20;&#x91CF;&#x5177;&#x6709; <code>requires_grad</code>&#xFF0C;&#x5B83;&#x662F;<code>&#x771F;</code>&#xFF0C;&#x4ED6;&#x4EEC;&#x5C06;&#x53F6;&#x5F20;&#x91CF;&#x5982;&#x679C;&#x4ED6;&#x4EEC;&#x88AB;&#x521B;&#x5EFA;&#x7528;&#x6237;&#x3002;&#x8FD9;&#x610F;&#x5473;&#x7740;&#x5B83;&#x4EEC;&#x4E0D;&#x662F;&#x4E00;&#x4E2A;&#x64CD;&#x4F5C;&#x7684;&#x7ED3;&#x679C;&#x7B49;<code>grad_fn</code>&#x662F;&#x65E0;&#x3002;</p>
<p>&#x4EC5;&#x53F6;&#x5F20;&#x91CF;&#x4EBA;&#x5458;&#x5728; <code>GRAD</code>&#x81F3; <code>&#x5411;&#x540E;&#xFF08;&#x5728;&#x547C;&#x53EB;&#x671F;&#x95F4;&#x586B;&#x5145;&#xFF09;</code>&#x3002;&#x4E3A;&#x4E86;&#x5F97;&#x5230; <code>&#x6BD5;&#x4E1A;</code>&#x586B;&#x5145;&#x7684;&#x65E0;&#x53F6;&#x5F20;&#x91CF;&#xFF0C;&#x4F60;&#x53EF;&#x4EE5;&#x4F7F;&#x7528; <code>retain_grad&#xFF08;&#xFF09;</code>[ HTG23&#x3002;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.rand(10, requires_grad=True)
&gt;&gt;&gt; a.is_leaf
True
&gt;&gt;&gt; b = torch.rand(10, requires_grad=True).cuda()
&gt;&gt;&gt; b.is_leaf
False
# b was created by the operation that cast a cpu Tensor into a cuda Tensor
&gt;&gt;&gt; c = torch.rand(10, requires_grad=True) + 2
&gt;&gt;&gt; c.is_leaf
False
# c was created by the addition operation
&gt;&gt;&gt; d = torch.rand(10).cuda()
&gt;&gt;&gt; d.is_leaf
True
# d does not require gradients and so has no operation creating it (that is tracked by the autograd engine)
&gt;&gt;&gt; e = torch.rand(10).cuda().requires_grad_()
&gt;&gt;&gt; e.is_leaf
True
# e requires gradients and has no operations creating it
&gt;&gt;&gt; f = torch.rand(10, requires_grad=True, device=&quot;cuda&quot;)
&gt;&gt;&gt; f.is_leaf
True
# f requires grad, has no operation creating it
</code></pre><p><code>is_pinned</code>()<a href="_modules/torch/tensor.html#Tensor.is_pinned">[source]</a></p>
<p>&#x5982;&#x679C;&#x6B64;&#x5F20;&#x9A7B;&#x7559;&#x5728;&#x56FA;&#x5B9A;&#x7684;&#x5185;&#x5B58;&#xFF0C;&#x5219;&#x8FD4;&#x56DE;true</p>
<p><code>is_set_to</code>( <em>tensor</em> ) &#x2192; bool</p>
<p>&#x5982;&#x679C;&#x8BE5;&#x5BF9;&#x8C61;&#x662F;&#x6307;&#x4ECE;Torch C API&#x4F5C;&#x4E3A;&#x7ED9;&#x5B9A;&#x7684;&#x5F20;&#x91CF;&#x76F8;&#x540C;<code>THTensor</code>&#x5BF9;&#x8C61;&#x8FD4;&#x56DE;&#x771F;&#x3002;</p>
<p><code>is_shared</code>()<a href="_modules/torch/tensor.html#Tensor.is_shared">[source]</a></p>
<p>&#x68C0;&#x67E5;&#xFF0C;&#x5982;&#x679C;&#x5F20;&#x91CF;&#x662F;&#x5728;&#x5171;&#x4EAB;&#x5B58;&#x50A8;&#x5668;&#x4E2D;&#x3002;</p>
<p>&#x8FD9;&#x59CB;&#x7EC8;&#x662F;<code>&#x771F; [HTG3&#x5BF9;&#x4E8E;CUDA&#x5F20;&#x91CF;&#x3002;</code></p>
<p><code>is_signed</code>() &#x2192; bool</p>
<p>&#x5982;&#x679C;<code>&#x81EA;</code>&#x6570;&#x636E;&#x7C7B;&#x578B;&#x662F;&#x6709;&#x7B26;&#x53F7;&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;&#xFF0C;&#x5219;&#x8FD4;&#x56DE;True&#x3002;</p>
<p><code>is_sparse</code>()</p>
<p><code>item</code>() &#x2192; number</p>
<p>&#x8FD4;&#x56DE;&#x6B64;&#x5F20;&#x91CF;&#x4F5C;&#x4E3A;&#x6807;&#x51C6;Python&#x6570;&#x7684;&#x503C;&#x3002;&#x8FD9;&#x4EC5;&#x9002;&#x7528;&#x4E8E;&#x4E00;&#x4E2A;&#x5143;&#x7D20;&#x5F20;&#x91CF;&#x3002;&#x5BF9;&#x4E8E;&#x5176;&#x5B83;&#x60C5;&#x51B5;&#xFF0C;&#x53C2;&#x89C1; <code>tolist&#xFF08;&#xFF09;</code>&#x3002;</p>
<p>&#x6B64;&#x64CD;&#x4F5C;&#x4E0D;&#x53EF;&#x5FAE;&#x3002;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.tensor([1.0])
&gt;&gt;&gt; x.item()
1.0
</code></pre><p><code>kthvalue</code>( <em>k</em> , <em>dim=None</em> , <em>keepdim=False) - &gt; (Tensor</em>, <em>LongTensor</em> )</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.kthvalue" title="torch.kthvalue"> <code>torch.kthvalue&#xFF08;&#xFF09;</code></a></p>
<p><code>le</code>( <em>other</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.le" title="torch.le"> <code>torch.le&#xFF08;&#xFF09;</code></a></p>
<p><code>le_</code>( <em>other</em> ) &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <code>&#x4E50;&#xFF08;&#xFF09;</code></p>
<p><code>lerp</code>( <em>end</em> , <em>weight</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.lerp" title="torch.lerp"> <code>torch.lerp&#xFF08;&#xFF09;</code></a></p>
<p><code>lerp_</code>( <em>end</em> , <em>weight</em> ) &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>&#x7EBF;&#x6027;&#x63D2;&#x503C;&#x7684;&#xFF08;&#xFF09;</code></p>
<p><code>log</code>() &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.log" title="torch.log"> <code>torch.log&#xFF08;&#xFF09;</code></a></p>
<p><code>log_</code>() &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>&#x65E5;&#x5FD7;&#xFF08;&#xFF09;</code></p>
<p><code>logdet</code>() &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.logdet" title="torch.logdet"> <code>torch.logdet&#xFF08;&#xFF09;</code></a></p>
<p><code>log10</code>() &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.log10" title="torch.log10"> <code>torch.log10&#xFF08;&#xFF09;</code></a></p>
<p><code>log10_</code>() &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>LOG10&#x7684;&#xFF08;&#xFF09;</code></p>
<p><code>log1p</code>() &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.log1p" title="torch.log1p"> <code>torch.log1p&#xFF08;&#xFF09;</code></a></p>
<p><code>log1p_</code>() &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>log1p&#x7684;&#xFF08;&#xFF09;</code></p>
<p><code>log2</code>() &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.log2" title="torch.log2"> <code>torch.log2&#xFF08;&#xFF09;</code></a></p>
<p><code>log2_</code>() &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>&#x7684;log 2&#x7684;&#xFF08;&#xFF09;</code></p>
<p><code>log_normal_</code>( <em>mean=1</em> , <em>std=2</em> , <em>*</em> , <em>generator=None</em> )</p>
<p>&#x586B;&#x5145;<code>&#x81EA;</code>&#x5F20;&#x91CF;&#x4E0E;&#x4ECE;&#x6240;&#x8FF0;&#x7ED9;&#x5B9A;&#x53C2;&#x6570;&#x7684;&#x5BF9;&#x6570;&#x6B63;&#x6001;&#x5206;&#x5E03;&#x7684;&#x6570;&#x5B57;&#x6837;&#x672C;&#x610F;&#x5473;&#x7740; &#x3BC; \&#x4EA9; &#x3BC; &#x548C;&#x6807;&#x51C6;&#x504F;&#x5DEE; &#x3C3; \&#x897F;&#x683C;&#x739B; &#x3C3; &#x3002;&#x6CE8;&#x610F;&#xFF0C;<a href="torch.html#torch.mean" title="torch.mean"> <code>&#x610F;&#x5473;&#x7740;</code>
</a>&#x548C;<a href="torch.html#torch.std" title="torch.std"> <code>STD</code></a>&#x662F;&#x5E73;&#x5747;&#x503C;&#x548C;&#x5E95;&#x5C42;&#x7684;&#x6807;&#x51C6;&#x504F;&#x5DEE;&#x6B63;&#x6001;&#x5206;&#x5E03;&#xFF0C;&#x800C;&#x4E0D;&#x662F;&#x8FD4;&#x56DE;&#x7684;&#x5206;&#x5E03;&#xFF1A;</p>
<p>f(x)=1x&#x3C3;2&#x3C0; e&#x2212;(ln&#x2061;x&#x2212;&#x3BC;)22&#x3C3;2f(x) = \dfrac{1}{x \sigma \sqrt{2\pi}}\
e^{-\frac{(\ln x - \mu)^2}{2\sigma^2}}f(x)=x&#x3C3;2&#x3C0;&#x200B;1&#x200B; e&#x2212;2&#x3C3;2(lnx&#x2212;&#x3BC;)2&#x200B;</p>
<p><code>logsumexp</code>( <em>dim</em> , <em>keepdim=False</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.logsumexp" title="torch.logsumexp"> <code>torch.logsumexp&#xFF08;&#xFF09;</code></a></p>
<p><code>long</code>() &#x2192; Tensor</p>
<p><code>self.long&#xFF08;&#xFF09;</code>&#x7B49;&#x4E8E;<code>self.to&#xFF08;torch.int64&#xFF09;</code>&#x3002;&#x53C2;&#x89C1; <code>&#x81F3;&#xFF08;&#xFF09;</code>&#x3002;</p>
<p><code>lstsq</code>( <em>A) - &gt; (Tensor</em>, <em>Tensor</em> )</p>
<p>See <a href="torch.html#torch.lstsq" title="torch.lstsq"><code>torch.lstsq()</code></a></p>
<p><code>lt</code>( <em>other</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.lt" title="torch.lt"> <code>torch.lt&#xFF08;&#xFF09;</code></a></p>
<p><code>lt_</code>( <em>other</em> ) &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>LT&#x7684;&#xFF08;&#xFF09;</code></p>
<p><code>lu</code>( <em>pivot=True</em> , <em>get_infos=False</em>
)<a href="_modules/torch/tensor.html#Tensor.lu">[source]</a></p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.lu" title="torch.lu"> <code>torch.lu&#xFF08;&#xFF09;</code></a></p>
<p><code>lu_solve</code>( <em>LU_data</em> , <em>LU_pivots</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.lu_solve" title="torch.lu_solve"> <code>torch.lu_solve&#xFF08;&#xFF09;</code></a></p>
<p><code>map_</code>( <em>tensor</em> , <em>callable</em> )</p>
<p>&#x9002;&#x7528;<code>&#x53EF;&#x8C03;&#x7528;</code>&#x5728;<code>&#x81EA;</code>&#x5F20;&#x91CF;&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x4EF6;&#x548C;&#x7ED9;&#x5B9A;<a href="torch.html#torch.tensor" title="torch.tensor"> <code>&#x5F20;&#x91CF;</code></a>&#xFF0C;&#x5E76;&#x5C06;&#x7ED3;&#x679C;&#x5B58;&#x50A8;&#x5728;<code>&#x81EA;</code>&#x5F20;&#x91CF;&#x3002; <code>&#x81EA;</code>&#x5F20;&#x91CF;&#x548C;&#x7ED9;&#x5B9A;&#x7684;<a href="torch.html#torch.tensor" title="torch.tensor"> <code>&#x5F20;&#x91CF;</code></a>&#x5FC5;&#x987B;<a href="notes/broadcasting.html#broadcasting-%0Asemantics"> broadcastable  </a>&#x3002;</p>
<p>&#x5728;<code>&#x8C03;&#x7528;</code>&#x5E94;&#x5177;&#x5907;&#x7684;&#x7279;&#x5F81;&#xFF1A;</p>
<pre><code>def callable(a, b) -&gt; number
</code></pre><p><code>masked_scatter_</code>( <em>mask</em> , <em>source</em> )</p>
<p>&#x4ECE;<code>&#x6E90;</code>&#x590D;&#x5236;&#x5185;&#x5BB9;&#x7EB3;&#x5165;<code>&#x81EA;</code>&#x5728;&#x4F4D;&#x7F6E;&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x4E2D;<code>&#x63A9;&#x6A21;</code>&#x4E3A;&#x771F;&#x3002;&#x7684;<code>&#x5F62;&#x72B6;&#x63A9;&#x6A21;</code>&#x5FC5;&#x987B;<a href="notes/broadcasting.html#broadcasting-semantics"> broadcastable
</a>&#x4E0E;&#x4E0B;&#x9762;&#x7684;&#x5F20;&#x91CF;&#x7684;&#x5F62;&#x72B6;&#x3002;&#x7684;<code>&#x6E90;</code>&#x5E94;&#x5F53;&#x5177;&#x6709;&#x81F3;&#x5C11;&#x4E00;&#x6837;&#x591A;&#x7684;&#x5143;&#x7D20;&#x7684;&#x90A3;&#x4E9B;&#x4E2D;<code>&#x6570;&#x63A9;&#x6A21;</code></p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x63A9;&#x6A21;</strong> &#xFF08; <em>BoolTensor</em> &#xFF09; - &#x5E03;&#x5C14;&#x63A9;&#x7801;</p>
</li>
<li><p><strong>&#x6E90;</strong> &#xFF08; <em>&#x5F20;&#x91CF;</em> &#xFF09; - &#x5F20;&#x91CF;&#x4ECE;&#x590D;&#x5236;</p>
</li>
</ul>
<p>Note</p>
<p>&#x7684;<code>&#x63A9;&#x6A21;</code>&#x64CD;&#x4F5C;&#x4E0A;&#x7684;<code>&#x81EA;</code>&#x5F20;&#x91CF;&#xFF0C;&#x800C;&#x4E0D;&#x662F;&#x5728;&#x7ED9;&#x5B9A;&#x7684;<code>&#x6E90;</code>&#x5F20;&#x91CF;&#x3002;</p>
<p><code>masked_scatter</code>( <em>mask</em> , <em>tensor</em> ) &#x2192; Tensor</p>
<p>&#x5916;&#x7684;&#x5730;&#x65B9;&#x7684; <code>&#x7248;&#x672C;torch.Tensor.masked_scatter_&#xFF08;&#xFF09;</code></p>
<p><code>masked_fill_</code>( <em>mask</em> , <em>value</em> )</p>
<p>&#x586B;&#x5145;&#x7684;<code>&#x81EA;</code>&#x5F20;&#x91CF;&#x5143;&#x7D20;&#x4E0E;<code>&#x503C;</code>&#x5176;&#x4E2D;<code>&#x63A9;&#x6A21;</code>&#x4E3A;True&#x3002;&#x7684;<code>&#x5F62;&#x72B6;&#x63A9;&#x6A21;</code>&#x5FC5;&#x987B;<a href="notes/broadcasting.html#broadcasting-semantics"> broadcastable
</a>&#x4E0E;&#x4E0B;&#x9762;&#x7684;&#x5F20;&#x91CF;&#x7684;&#x5F62;&#x72B6;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>mask</strong> ( <em>BoolTensor</em>) &#x2013; the boolean mask</p>
</li>
<li><p><strong>&#x503C;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a>&#xFF09; - &#x7684;&#x503C;&#x4E0E;&#x586B;</p>
</li>
</ul>
<p><code>masked_fill</code>( <em>mask</em> , <em>value</em> ) &#x2192; Tensor</p>
<p>&#x5916;&#x7684;&#x5730;&#x65B9;&#x7684; <code>&#x7248;&#x672C;torch.Tensor.masked_fill_&#xFF08;&#xFF09;</code></p>
<p><code>masked_select</code>( <em>mask</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.masked_select" title="torch.masked_select"> <code>torch.masked_select&#xFF08;&#xFF09;</code></a></p>
<p><code>matmul</code>( <em>tensor2</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.matmul" title="torch.matmul"> <code>torch.matmul&#xFF08;&#xFF09;</code></a></p>
<p><code>matrix_power</code>( <em>n</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.matrix_power" title="torch.matrix_power"> <code>torch.matrix_power&#xFF08;&#xFF09;</code></a></p>
<p><code>max</code>( <em>dim=None</em> , <em>keepdim=False) - &gt; Tensor or (Tensor</em>, <em>Tensor</em> )</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.max" title="torch.max"> <code>torch.max&#xFF08;&#xFF09;</code></a></p>
<p><code>mean</code>( <em>dim=None</em> , <em>keepdim=False) - &gt; Tensor or (Tensor</em>, <em>Tensor</em> )</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.mean" title="torch.mean"> <code>torch.mean&#xFF08;&#xFF09;</code></a></p>
<p><code>median</code>( <em>dim=None</em> , <em>keepdim=False) - &gt; (Tensor</em>, <em>LongTensor</em> )</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.median" title="torch.median"> <code>torch.median&#xFF08;&#xFF09;</code></a></p>
<p><code>min</code>( <em>dim=None</em> , <em>keepdim=False) - &gt; Tensor or (Tensor</em>, <em>Tensor</em> )</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.min" title="torch.min"> <code>torch.min&#xFF08;&#xFF09;</code></a></p>
<p><code>mm</code>( <em>mat2</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.mm" title="torch.mm"> <code>torch.mm&#xFF08;&#xFF09;</code></a></p>
<p><code>mode</code>( <em>dim=None</em> , <em>keepdim=False) - &gt; (Tensor</em>, <em>LongTensor</em> )</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.mode" title="torch.mode"> <code>torch.mode&#xFF08;&#xFF09;</code></a></p>
<p><code>mul</code>( <em>value</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.mul" title="torch.mul"> <code>torch.mul&#xFF08;&#xFF09;</code></a></p>
<p><code>mul_</code>( <em>value</em> )</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>MUL&#x7684;&#xFF08;&#xFF09;</code></p>
<p><code>multinomial</code>( <em>num_samples</em> , <em>replacement=False</em> , <em>*</em> , <em>generator=None</em> )
&#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.multinomial" title="torch.multinomial"> <code>torch.multinomial&#xFF08;&#xFF09;</code></a></p>
<p><code>mv</code>( <em>vec</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.mv" title="torch.mv"> <code>torch.mv&#xFF08;&#xFF09;</code></a></p>
<p><code>mvlgamma</code>( <em>p</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.mvlgamma" title="torch.mvlgamma"> <code>torch.mvlgamma&#xFF08;&#xFF09;</code></a></p>
<p><code>mvlgamma_</code>( <em>p</em> ) &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>mvlgamma&#x7684;&#xFF08;&#xFF09;</code></p>
<p><code>narrow</code>( <em>dimension</em> , <em>start</em> , <em>length</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.narrow" title="torch.narrow"> <code>torch.narrow&#xFF08;&#xFF09;</code></a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
&gt;&gt;&gt; x.narrow(0, 0, 2)
tensor([[ 1,  2,  3],
        [ 4,  5,  6]])
&gt;&gt;&gt; x.narrow(1, 1, 2)
tensor([[ 2,  3],
        [ 5,  6],
        [ 8,  9]])
</code></pre><p><code>narrow_copy</code>( <em>dimension</em> , <em>start</em> , <em>length</em> ) &#x2192; Tensor</p>
<p>&#x540C; <code>Tensor.narrow&#xFF08;&#xFF09;</code>&#xFF0C;&#x9664;&#x4E86;&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x526F;&#x672C;&#xFF0C;&#x800C;&#x4E0D;&#x662F;&#x5171;&#x4EAB;&#x5B58;&#x50A8;&#x3002;&#x8FD9;&#x4E3B;&#x8981;&#x662F;&#x4E3A;&#x7A00;&#x758F;&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x4E0D;&#x5177;&#x6709;&#x5171;&#x4EAB;&#x5B58;&#x50A8;&#x7A84;&#x65B9;&#x6CD5;&#x3002;&#x4E3B;&#x53EB;<code>narrow_copy
`&#x4E0E;</code>dimemsion  &amp; GT ;  self.sparse_dim&#xFF08;&#xFF09; <code>&#x4F1A;&#x8FD4;&#x56DE;&#x7F29;&#x5C0F;&#x7684;&#x76F8;&#x5173;&#x5BC6;&#x96C6;&#x7EF4;&#x5EA6;&#x526F;&#x672C;&#xFF0C;</code>self.shape<code>
</code>&#x76F8;&#x5E94;&#x66F4;&#x65B0;&#x3002;</p>
<p><code>ndimension</code>() &#x2192; int</p>
<p>Alias for <code>dim()</code></p>
<p><code>ne</code>( <em>other</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.ne" title="torch.ne"> <code>torch.ne&#xFF08;&#xFF09;</code></a></p>
<p><code>ne_</code>( <em>other</em> ) &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>NE&#x7684;&#xFF08;&#xFF09;</code></p>
<p><code>neg</code>() &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.neg" title="torch.neg"> <code>torch.neg&#xFF08;&#xFF09;</code></a></p>
<p><code>neg_</code>() &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>NEG&#x7684;&#xFF08;&#xFF09;</code></p>
<p><code>nelement</code>() &#x2192; int</p>
<p>&#x522B;&#x540D; <code>numel&#xFF08;&#xFF09;</code></p>
<p><code>nonzero</code>() &#x2192; LongTensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.nonzero" title="torch.nonzero"> <code>torch.nonzero&#xFF08;&#xFF09;</code></a></p>
<p><code>norm</code>( <em>p=&apos;fro&apos;</em> , <em>dim=None</em> , <em>keepdim=False</em> , <em>dtype=None</em>
)<a href="_modules/torch/tensor.html#Tensor.norm">[source]</a></p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.norm" title="torch.norm"> <code>torch.norm&#xFF08;&#xFF09;</code></a></p>
<p><code>normal_</code>( <em>mean=0</em> , <em>std=1</em> , <em>*</em> , <em>generator=None</em> ) &#x2192; Tensor</p>
<p>&#x586B;&#x5145;<code>&#x81EA;</code>&#x4E0E;&#x5143;&#x7D20;&#x6837;&#x54C1;&#x5F20;&#x91CF;&#x4ECE;&#x6B63;&#x6001;&#x5206;&#x5E03;&#x7531;<a href="torch.html#torch.mean" title="torch.mean"> <code>&#x53C2;&#x6570;&#x5316;&#x7684;&#x610F;&#x601D;&#x662F;</code></a>&#x548C;<a href="torch.html#torch.std" title="torch.std"> <code>STD</code></a>&#x3002;</p>
<p><code>numel</code>() &#x2192; int</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.numel" title="torch.numel"> <code>torch.numel&#xFF08;&#xFF09;</code></a></p>
<p><code>numpy</code>() &#x2192; numpy.ndarray</p>
<p>&#x8FD4;&#x56DE;<code>&#x81EA;</code>&#x5F20;&#x91CF;&#x4F5C;&#x4E3A;NumPy&#x7684;<code>ndarray</code>&#x3002;&#x8FD9;&#x4E2A;&#x5F20;&#x91CF;&#x548C;&#x8FD4;&#x56DE;<code>ndarray</code>&#x5171;&#x4EAB;&#x540C;&#x4E00;&#x57FA;&#x7840;&#x5B58;&#x50A8;&#x3002;&#x4E3A;<code>&#x81EA;&#x53D8;&#x5316;</code>&#x5F20;&#x91CF;&#x5C06;&#x53CD;&#x6620;&#x5728;<code>ndarray</code>&#xFF0C;&#x53CD;&#x4E4B;&#x4EA6;&#x7136;&#x3002;</p>
<p><code>orgqr</code>( <em>input2</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.orgqr" title="torch.orgqr"> <code>torch.orgqr&#xFF08;&#xFF09;</code></a></p>
<p><code>ormqr</code>( <em>input2</em> , <em>input3</em> , <em>left=True</em> , <em>transpose=False</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.ormqr" title="torch.ormqr"> <code>torch.ormqr&#xFF08;&#xFF09;</code></a></p>
<p><code>permute</code>( <em>*dims</em> ) &#x2192; Tensor</p>
<p>&#x7F6E;&#x6362;&#xFF0C;&#x8FD9;&#x4E2A;&#x5F20;&#x91CF;&#x7684;&#x5C3A;&#x5BF8;&#x3002;</p>
<p>Parameters</p>
<p><strong>*&#x53D8;&#x6697;</strong> &#xFF08; <em>INT ...</em> &#xFF09; - &#x5C3A;&#x5BF8;&#x7684;&#x6240;&#x9700;&#x6392;&#x5E8F;</p>
<p>&#x4F8B;</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(2, 3, 5)
&gt;&gt;&gt; x.size()
torch.Size([2, 3, 5])
&gt;&gt;&gt; x.permute(2, 0, 1).size()
torch.Size([5, 2, 3])
</code></pre><p><code>pin_memory</code>() &#x2192; Tensor</p>
<p>&#x590D;&#x5236;&#x5F20;&#x91CF;&#x56FA;&#x5B9A;&#x7684;&#x5185;&#x5B58;&#xFF0C;&#x5982;&#x679C;&#x5B83;&#x4E0D;&#x662F;&#x5DF2;&#x7ECF;&#x56FA;&#x5B9A;&#x3002;</p>
<p><code>pinverse</code>() &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.pinverse" title="torch.pinverse"> <code>torch.pinverse&#xFF08;&#xFF09;</code></a></p>
<p><code>pow</code>( <em>exponent</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.pow" title="torch.pow"> <code>torch.pow&#xFF08;&#xFF09;</code></a></p>
<p><code>pow_</code>( <em>exponent</em> ) &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>POW&#x7684;&#xFF08;&#xFF09;</code></p>
<p><code>prod</code>( <em>dim=None</em> , <em>keepdim=False</em> , <em>dtype=None</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.prod" title="torch.prod"> <code>torch.prod&#xFF08;&#xFF09;</code></a></p>
<p><code>put_</code>( <em>indices</em> , <em>tensor</em> , <em>accumulate=False</em> ) &#x2192; Tensor</p>
<p>&#x62F7;&#x8D1D;&#x4ECE;<a href="torch.html#torch.tensor" title="torch.tensor"> <code>&#x7684;&#x5143;&#x7D20;&#x5F20;&#x91CF;</code></a>&#x5230;&#x7531;&#x7D22;&#x5F15;&#x6307;&#x5B9A;&#x7684;&#x4F4D;&#x7F6E;&#x3002;&#x7528;&#x4E8E;&#x7D22;&#x5F15;&#x7684;&#x76EE;&#x7684;&#xFF0C;<code>&#x81EA;</code>&#x5F20;&#x91CF;&#x5C31;&#x597D;&#x50CF;&#x5B83;&#x662F;&#x4E00;&#x4E2A;1-d&#x5F20;&#x91CF;&#x5904;&#x7406;&#x3002;</p>
<p>If <code>accumulate</code>is <code>True</code>, the elements in <a href="torch.html#torch.tensor" title="torch.tensor"><code>tensor</code></a> are added to <code>self</code>. If accumulate is <code>False</code>, the behavior is
undefined if indices contain duplicate elements.</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x6307;&#x6570;</strong> &#xFF08; <em>LongTensor</em> &#xFF09; - &#x7D22;&#x5F15;&#x4E3A;&#x81EA;</p>
</li>
<li><p><strong>&#x5F20;&#x91CF;</strong> &#xFF08; <em>&#x5F20;&#x91CF;</em> &#xFF09; - &#x5305;&#x542B;&#x503C;&#x4ECE;&#x590D;&#x5236;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>accumulate</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a>) &#x2013; whether to accumulate into self</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; src = torch.tensor([[4, 3, 5],
                        [6, 7, 8]])
&gt;&gt;&gt; src.put_(torch.tensor([1, 3]), torch.tensor([9, 10]))
tensor([[  4,   9,   5],
        [ 10,   7,   8]])
</code></pre><p><code>qr</code>( <em>some=True) - &gt; (Tensor</em>, <em>Tensor</em> )</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.qr" title="torch.qr"> <code>torch.qr&#xFF08;&#xFF09;</code></a></p>
<p><code>qscheme</code>() &#x2192; torch.qscheme</p>
<p>&#x8FD4;&#x56DE;&#x7ED9;&#x5B9A;QTensor&#x7684;&#x91CF;&#x5316;&#x65B9;&#x6848;&#x3002;</p>
<p><code>q_scale</code>() &#x2192; float</p>
<p>&#x9274;&#x4E8E;&#x7EBF;&#x6027;&#xFF08;&#x4EFF;&#x5C04;&#xFF09;&#x91CF;&#x5316;&#x91CF;&#x5316;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x8FD4;&#x56DE;&#x5E95;&#x5C42;&#x91CF;&#x5316;&#x5668;&#x7684;&#x89C4;&#x6A21;&#xFF08;&#xFF09;&#x3002;</p>
<p><code>q_zero_point</code>() &#x2192; int</p>
<p>&#x9274;&#x4E8E;&#x7EBF;&#x6027;&#xFF08;&#x4EFF;&#x5C04;&#xFF09;&#x91CF;&#x5316;&#x91CF;&#x5316;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x8FD4;&#x56DE;&#x5E95;&#x5C42;&#x91CF;&#x5316;&#x5668;&#x7684;zero_point&#xFF08;&#xFF09;&#x3002;</p>
<p><code>random_</code>( <em>from=0</em> , <em>to=None</em> , <em>*</em> , <em>generator=None</em> ) &#x2192; Tensor</p>
<p>&#x586B;&#x5145;<code>&#x81EA;</code>&#x8D85;&#x8FC7;<code>[&#x4ECE;&#x79BB;&#x6563;&#x5747;&#x5300;&#x5206;&#x5E03;&#x91C7;&#x6837;&#x6570;&#x5F20;&#x91CF;&#x4ECE; &#x81F3; -  1]</code>&#x3002;&#x5982;&#x679C;&#x672A;&#x6307;&#x5B9A;&#xFF0C;&#x5219;&#x8FD9;&#x4E9B;&#x503C;&#x901A;&#x5E38;&#x4EC5;&#x7531;<code>&#x81EA;</code>&#x5F20;&#x91CF;&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x7684;&#x9650;&#x5236;&#x3002;&#x7136;&#x800C;&#xFF0C;&#x5BF9;&#x4E8E;&#x6D6E;&#x70B9;&#x7C7B;&#x578B;&#xFF0C;&#x5982;&#x679C;&#x672A;&#x6307;&#x5B9A;&#xFF0C;&#x8303;&#x56F4;&#x5C06;&#x662F;<code>[0&#xFF0C; 2 ^&#x5C3E;&#x6570;]</code>&#xFF0C;&#x4EE5;&#x786E;&#x4FDD;&#x6BCF;&#x4E00;&#x4E2A;&#x503C;&#x53EF;&#x8868;&#x793A;&#x3002;&#x4F8B;&#x5982;&#xFF0C;
torch.tensor&#xFF08;1&#xFF0C;D&#x578B;&#x7EC6;&#x80DE;= torch.double&#xFF09;.random_&#xFF08;&#xFF09;&#x5C06;&#x5747;&#x5300;&#x7684;<code>[0&#xFF0C; 2 ^ 53]</code>&#x3002;</p>
<p><code>reciprocal</code>() &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.reciprocal" title="torch.reciprocal"> <code>torch.reciprocal&#xFF08;&#xFF09;</code></a></p>
<p><code>reciprocal_</code>() &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <code>&#x5012;&#x6570;&#xFF08;&#xFF09;</code></p>
<p><code>register_hook</code>( <em>hook</em>
)<a href="_modules/torch/tensor.html#Tensor.register_hook">[source]</a></p>
<p>&#x5BC4;&#x5B58;&#x5668;&#x5411;&#x540E;&#x94A9;&#x3002;</p>
<p>&#x94A9;&#x5C06;&#x88AB;&#x79F0;&#x4E3A;&#x6BCF;&#x76F8;&#x5BF9;&#x4E8E;&#x8BE5;&#x5F20;&#x91CF;&#x7684;&#x68AF;&#x5EA6;&#x88AB;&#x8BA1;&#x7B97;&#x7684;&#x65F6;&#x95F4;&#x3002;&#x94A9;&#x5B50;&#x5E94;&#x8BE5;&#x5177;&#x6709;&#x4EE5;&#x4E0B;&#x7279;&#x5F81;&#xFF1A;</p>
<pre><code>hook(grad) -&gt; Tensor or None
</code></pre><p>&#x94A9;&#x4E0D;&#x5E94;&#x8BE5;&#x4FEE;&#x6539;&#x5B83;&#x7684;&#x53C2;&#x6570;&#xFF0C;&#x4F46;&#x5B83;&#x53EF;&#x4EE5;&#x4EFB;&#x9009;&#x5730;&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x65B0;&#x7684;&#x68AF;&#x5EA6;&#xFF0C;&#x8FD9;&#x5C06;&#x4EE3;&#x66FF; <code>GRAD</code>&#x4E00;&#x8D77;&#x4F7F;&#x7528;&#x3002;</p>
<p>&#x8BE5;&#x51FD;&#x6570;&#x8FD4;&#x56DE;&#x4E0E;&#x65B9;&#x6CD5;<code>handle.remove&#x624B;&#x67C4;&#xFF08;&#xFF09;</code>&#xFF0C;&#x5176;&#x53BB;&#x9664;&#x4ECE;&#x6240;&#x8FF0;&#x6A21;&#x5757;&#x7684;&#x94A9;&#x3002;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; v = torch.tensor([0., 0., 0.], requires_grad=True)
&gt;&gt;&gt; h = v.register_hook(lambda grad: grad * 2)  # double the gradient
&gt;&gt;&gt; v.backward(torch.tensor([1., 2., 3.]))
&gt;&gt;&gt; v.grad

 2
 4
 6
[torch.FloatTensor of size (3,)]

&gt;&gt;&gt; h.remove()  # removes the hook
</code></pre><p><code>remainder</code>( <em>divisor</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.remainder" title="torch.remainder"> <code>torch.remainder&#xFF08;&#xFF09;</code></a></p>
<p><code>remainder_</code>( <em>divisor</em> ) &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>&#x5269;&#x4F59;&#x7684;&#xFF08;&#xFF09;</code></p>
<p><code>renorm</code>( <em>p</em> , <em>dim</em> , <em>maxnorm</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.renorm" title="torch.renorm"> <code>torch.renorm&#xFF08;&#xFF09;</code></a></p>
<p><code>renorm_</code>( <em>p</em> , <em>dim</em> , <em>maxnorm</em> ) &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>&#x91CD;&#x5F52;&#x4E00;&#x5316;&#x7684;&#xFF08;&#xFF09;</code></p>
<p><code>repeat</code>( <em>*sizes</em> ) &#x2192; Tensor</p>
<p>&#x91CD;&#x590D;&#x6CBF;&#x7740;&#x6307;&#x5B9A;&#x7684;&#x5C3A;&#x5BF8;&#xFF0C;&#x8FD9;&#x4E2A;&#x5F20;&#x91CF;&#x3002;</p>
<p>&#x4E0D;&#x540C;&#x4E8E; <code>&#x6269;&#x5927;&#xFF08;&#xFF09;</code>&#xFF0C;&#x8BE5;&#x51FD;&#x6570;&#x5C06;&#x5F20;&#x91CF;&#x7684;&#x6570;&#x636E;&#x3002;</p>
<p>Warning</p>
<p><code>torch.repeat&#xFF08;&#xFF09;</code>&#x4ECE;<a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.repeat.html" target="_blank"> numpy.repeat
</a>&#x884C;&#x4E3A;&#x4E0D;&#x540C;&#xFF0C;&#x4F46;&#x662F;&#x66F4;&#x7C7B;&#x4F3C;&#x4E8E;<a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.tile.html" target="_blank">
numpy.tile
</a>&#x3002;&#x5BF9;&#x4E8E;&#x7C7B;&#x4F3C;&#x4E8E;&#x64CD;&#x4F5C;&#x8005;numpy.repeat
&#x53C2;&#x89C1;<a href="torch.html#torch.repeat_interleave" title="torch.repeat_interleave"> <code>torch.repeat_interleave&#xFF08;&#xFF09;</code></a>&#x3002;</p>
<p>Parameters</p>
<p><strong>&#x5C3A;&#x5BF8;</strong> &#xFF08; <em>torch.Size</em> <em>&#x6216;</em> <em>INT ...</em> &#xFF09; - &#x7684;&#x6B21;&#x6570;&#x91CD;&#x590D;&#x6B64;&#x5F20;&#x91CF;&#x6CBF;&#x7740;&#x6BCF;&#x4E2A;&#x7EF4;&#x5EA6;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.tensor([1, 2, 3])
&gt;&gt;&gt; x.repeat(4, 2)
tensor([[ 1,  2,  3,  1,  2,  3],
        [ 1,  2,  3,  1,  2,  3],
        [ 1,  2,  3,  1,  2,  3],
        [ 1,  2,  3,  1,  2,  3]])
&gt;&gt;&gt; x.repeat(4, 2, 1).size()
torch.Size([4, 2, 3])
</code></pre><p><code>repeat_interleave</code>( <em>repeats</em> , <em>dim=None</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.repeat_interleave" title="torch.repeat_interleave"> <code>torch.repeat_interleave&#xFF08;&#xFF09;</code></a>&#x3002;</p>
<p><code>requires_grad</code>()</p>
<p>&#x662F;<code>&#x771F; [HTG3&#x5982;&#x679C;&#x68AF;&#x5EA6;&#x9700;&#x8981;&#x8BA1;&#x7B97;&#x8BE5;&#x5F20;&#x91CF;&#xFF0C;</code>&#x5047; [HTG7&#x5426;&#x5219;&#x3002;``</p>
<p>Note</p>
<p>&#x8BE5;&#x68AF;&#x5EA6;&#x9700;&#x8981;&#x8BA1;&#x7B97;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x8FD9;&#x4E00;&#x4E8B;&#x5B9E;&#x5E76;&#x4E0D;&#x610F;&#x5473;&#x7740; <code>&#x6BD5;&#x4E1A;</code>&#x5C5E;&#x6027;&#x5C06;&#x88AB;&#x586B;&#x5145;&#xFF0C;&#x8BF7;&#x53C2;&#x9605; <code>is_leaf</code>&#x7684;&#x66F4;&#x591A;&#x7EC6;&#x8282;&#x3002;</p>
<p><code>requires_grad_</code>( <em>requires_grad=True</em> ) &#x2192; Tensor</p>
<p>&#x6539;&#x53D8;&#xFF0C;&#x5982;&#x679C;autograd&#x5E94;&#x5728;&#x6B64;&#x5F20;&#x8BB0;&#x5F55;&#x64CD;&#x4F5C;&#xFF1A;&#x8BBE;&#x7F6E;&#x8FD9;&#x4E2A;&#x5F20;&#x91CF;&#x7684; <code>requires_grad</code>&#x5C5E;&#x6027;&#x539F;&#x5730;&#x3002;&#x8FD4;&#x56DE;&#x6B64;&#x5F20;&#x91CF;&#x3002;</p>
<p><code>require_grad_&#xFF08;&#xFF09;&#x7684;</code>&#x4E3B;&#x8981;&#x4F7F;&#x7528;&#x60C5;&#x5F62;&#x662F;&#x544A;&#x8BC9;autograd&#x5230;&#x5F00;&#x59CB;&#x8BB0;&#x5F55;&#x7684;&#x64CD;&#x4F5C;&#x7684;&#x5F20;&#x91CF;<code>&#x5F20;&#x91CF;</code>&#x3002;&#x5982;&#x679C;<code>&#x5F20;&#x91CF;</code>&#x5177;&#x6709;<code>requires_grad =&#x5047;</code>&#xFF08;&#x56E0;&#x4E3A;&#x5B83;&#x88AB;&#x901A;&#x8FC7;&#x7684;DataLoader&#x83B7;&#x5F97;&#xFF0C;&#x6216;&#x8005;&#x9700;&#x8981;&#x9884;&#x5904;&#x7406;&#x6216;&#x521D;&#x59CB;&#x5316;&#xFF09;&#xFF0C;<code>tensor.requires_grad_&#xFF08;&#xFF09;</code>&#x4F7F;&#x5F97;&#x5B83;&#x4F7F;autograd&#x5C06;&#x5F00;&#x59CB;&#x8BB0;&#x5F55;&#x64CD;&#x4F5C;&#x4E0A;<code>&#x5F20; [HTG23&#x3002;</code></p>
<p>Parameters</p>
<p><strong>requires_grad</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em>
</a>&#xFF09;</p>
<ul>
<li>&#x5982;&#x679C;autograd&#x5E94;&#x5728;&#x6B64;&#x5F20;&#x8BB0;&#x5F55;&#x64CD;&#x4F5C;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&#x771F; [HTG9&#x3002;</code></li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; # Let&apos;s say we want to preprocess some saved weights and use
&gt;&gt;&gt; # the result as new weights.
&gt;&gt;&gt; saved_weights = [0.1, 0.2, 0.3, 0.25]
&gt;&gt;&gt; loaded_weights = torch.tensor(saved_weights)
&gt;&gt;&gt; weights = preprocess(loaded_weights)  # some function
&gt;&gt;&gt; weights
tensor([-0.5503,  0.4926, -2.1158, -0.8303])

&gt;&gt;&gt; # Now, start to record operations done to weights
&gt;&gt;&gt; weights.requires_grad_()
&gt;&gt;&gt; out = weights.pow(2).sum()
&gt;&gt;&gt; out.backward()
&gt;&gt;&gt; weights.grad
tensor([-1.1007,  0.9853, -4.2316, -1.6606])
</code></pre><p><code>reshape</code>( <em>*shape</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E0E;&#x5143;&#x7D20;<code>&#x81EA;</code>&#x4F46;&#x5177;&#x6709;&#x6307;&#x5B9A;&#x5F62;&#x72B6;&#x7684;&#x76F8;&#x540C;&#x7684;&#x6570;&#x636E;&#x548C;&#x6570;&#x5B57;&#x7684;&#x5F20;&#x91CF;&#x3002;&#x5982;&#x679C;<code>&#x5B9A;&#x578B;</code>&#x662F;&#x4E0E;&#x5F53;&#x524D;&#x7684;&#x5F62;&#x72B6;&#x76F8;&#x5BB9;&#x7684;&#x6B64;&#x65B9;&#x6CD5;&#x8FD4;&#x56DE;&#x7684;&#x56FE;&#x3002;&#x53C2;&#x89C1; <code>torch.Tensor.view&#xFF08;&#xFF09;</code>&#x4E0A;&#x65F6;&#xFF0C;&#x5B83;&#x53EF;&#x4EE5;&#x8FD4;&#x56DE;&#x7684;&#x56FE;&#x3002;</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.reshape" title="torch.reshape"> <code>torch.reshape&#xFF08;&#xFF09;</code></a></p>
<p>Parameters</p>
<p><strong>&#x5B9A;&#x578B;</strong> &#xFF08; <em>&#x87D2;&#x7684;&#x5143;&#x7EC4;&#xFF1A;&#x6574;&#x6570;</em> <em>&#x6216;</em> <em>INT ...</em> &#xFF09; - &#x6240;&#x9700;&#x7684;&#x5F62;&#x72B6;</p>
<p><code>reshape_as</code>( <em>other</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x8BE5;&#x5F20;&#x91CF;&#x4E3A;&#x76F8;&#x540C;&#x7684;&#x5F62;&#x72B6;&#xFF0C;<code>&#x5176;&#x4ED6;</code>&#x3002; <code>self.reshape_as&#xFF08;&#x5176;&#x4ED6;&#xFF09;&#x200B;&#x200B;</code>&#x7B49;&#x4E8E;<code>self.reshape&#xFF08;other.sizes&#xFF08;&#xFF09;&#xFF09;</code>&#x3002;&#x5982;&#x679C;<code>other.sizes&#xFF08;&#xFF09;</code>&#x662F;&#x4E0E;&#x5F53;&#x524D;&#x7684;&#x5F62;&#x72B6;&#x76F8;&#x5BB9;&#x7684;&#x6B64;&#x65B9;&#x6CD5;&#x8FD4;&#x56DE;&#x7684;&#x56FE;&#x3002;&#x53C2;&#x89C1; <code>torch.Tensor.view&#xFF08;&#xFF09;</code>&#x4E0A;&#x65F6;&#xFF0C;&#x5B83;&#x53EF;&#x4EE5;&#x8FD4;&#x56DE;&#x7684;&#x56FE;&#x3002;</p>
<p>&#x8BF7;&#x53C2;&#x9605;<a href="torch.html#torch.reshape" title="torch.reshape"> <code>&#x91CD;&#x5851;&#xFF08;&#xFF09;</code></a>&#x5173;&#x4E8E;<code>&#x66F4;&#x591A;&#x4FE1;&#x606F;&#x91CD;&#x5851; [HTG9&#x3002;</code></p>
<p>Parameters</p>
<p><strong>&#x5176;&#x4ED6;</strong> &#xFF08; <code>torch.Tensor</code>&#xFF09; - &#x7ED3;&#x679C;&#x5F20;&#x91CF;&#x5177;&#x6709;&#x76F8;&#x540C;&#x7684;&#x5F62;&#x72B6;<code>&#x5176;&#x4ED6; [ HTG11&#x3002;</code></p>
<p><code>resize_</code>( <em>*sizes</em> ) &#x2192; Tensor</p>
<p>&#x8C03;&#x6574;&#x5927;&#x5C0F;<code>&#x81EA;</code>&#x5F20;&#x91CF;&#xFF0C;&#x4EE5;&#x6307;&#x5B9A;&#x7684;&#x5927;&#x5C0F;&#x3002;&#x5982;&#x679C;&#x5143;&#x4EF6;&#x7684;&#x6570;&#x91CF;&#x5927;&#x4E8E;&#x5F53;&#x524D;&#x5B58;&#x50A8;&#x5927;&#x5C0F;&#xFF0C;&#x5219;&#x5E95;&#x5C42;&#x5B58;&#x50A8;&#x88AB;&#x8C03;&#x6574;&#x5927;&#x5C0F;&#xFF0C;&#x4EE5;&#x9002;&#x5E94;&#x5143;&#x4EF6;&#x7684;&#x65B0;&#x53F7;&#x7801;&#x3002;&#x5982;&#x679C;&#x5143;&#x4EF6;&#x7684;&#x6570;&#x76EE;&#x8F83;&#x5C0F;&#x65F6;&#xFF0C;&#x5E95;&#x5C42;&#x7684;&#x5B58;&#x50A8;&#x4E0D;&#x88AB;&#x6539;&#x53D8;&#x3002;&#x5B58;&#x5728;&#x7684;&#x5143;&#x7D20;&#x5C06;&#x4F1A;&#x4FDD;&#x7559;&#xFF0C;&#x4F46;&#x4EFB;&#x4F55;&#x65B0;&#x7684;&#x5185;&#x5B58;&#x672A;&#x521D;&#x59CB;&#x5316;&#x3002;</p>
<p>Warning</p>
<p>&#x8FD9;&#x662F;&#x4E00;&#x4E2A;&#x4F4E;&#x7EA7;&#x522B;&#x7684;&#x65B9;&#x6CD5;&#x3002;&#x5B58;&#x50A8;&#x88AB;&#x91CD;&#x65B0;&#x89E3;&#x91CA;&#x4E3A;C-&#x8FDE;&#x7EED;&#x7684;&#xFF0C;&#x5FFD;&#x7565;&#x5F53;&#x524D;&#x8FDB;&#x5C55;&#xFF08;&#x9664;&#x975E;&#x76EE;&#x6807;&#x5927;&#x5C0F;&#x7B49;&#x4E8E;&#x7535;&#x6D41;&#x7684;&#x5927;&#x5C0F;&#xFF0C;&#x5728;&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x5F20;&#x91CF;&#x4FDD;&#x6301;&#x4E0D;&#x53D8;&#xFF09;&#x3002;&#x5728;&#x5927;&#x591A;&#x6570;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x4F60;&#x53CD;&#x800C;&#x8981;&#x4F7F;&#x7528; <code>&#x89C6;&#x56FE;&#xFF08;&#xFF09;</code>&#xFF0C;&#x5176;&#x68C0;&#x67E5;&#x8FDE;&#x7EED;&#x6027;&#xFF0C;&#x6216; <code>&#x91CD;&#x5851;&#xFF08;&#xFF09;</code>&#xFF0C;&#x5176;&#x590D;&#x5236;&#x6570;&#x636E;&#x5982;&#x679C;&#x9700;&#x8981;&#x7684;&#x8BDD;&#x3002;&#x82E5;&#x8981;&#x66F4;&#x6539;&#x5C31;&#x5730;&#x81EA;&#x5B9A;&#x4E49;&#x6B65;&#x5E45;&#x5927;&#x5C0F;&#xFF0C;&#x8BF7;&#x53C2;&#x9605; <code>SET_&#xFF08;&#xFF09;</code>&#x3002;</p>
<p>Parameters</p>
<p><strong>&#x5C3A;&#x5BF8;</strong> &#xFF08; <em>torch.Size</em> <em>&#x6216;</em> <em>INT ...</em> &#xFF09; - &#x6240;&#x9700;&#x7684;&#x5927;&#x5C0F;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.tensor([[1, 2], [3, 4], [5, 6]])
&gt;&gt;&gt; x.resize_(2, 2)
tensor([[ 1,  2],
        [ 3,  4]])
</code></pre><p><code>resize_as_</code>( <em>tensor</em> ) &#x2192; Tensor</p>
<p>&#x8C03;&#x6574;&#x5927;&#x5C0F;&#x7684;<code>&#x81EA;</code>&#x5F20;&#x91CF;&#x662F;&#x5927;&#x5C0F;&#x76F8;&#x540C;&#x7684;&#x6307;&#x5B9A;<a href="torch.html#torch.tensor" title="torch.tensor"> <code>&#x5F20;&#x91CF;</code></a>&#x3002;&#x8FD9;&#x7B49;&#x540C;&#x4E8E;<code>self.resize_&#xFF08;tensor.size&#xFF08;&#xFF09;&#xFF09;</code>&#x3002;</p>
<p><code>retain_grad</code>()<a href="_modules/torch/tensor.html#Tensor.retain_grad">[source]</a></p>
<p>&#x542F;&#x7528;&#x4E86;&#x975E;&#x53F6;&#x5F20;&#x91CF;.grad&#x5C5E;&#x6027;&#x3002;</p>
<p><code>rfft</code>( <em>signal_ndim</em> , <em>normalized=False</em> , <em>onesided=True</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.rfft" title="torch.rfft"> <code>torch.rfft&#xFF08;&#xFF09;</code></a></p>
<p><code>roll</code>( <em>shifts</em> , <em>dims</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.roll" title="torch.roll"> <code>torch.roll&#xFF08;&#xFF09;</code></a></p>
<p><code>rot90</code>( <em>k</em> , <em>dims</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.rot90" title="torch.rot90"> <code>torch.rot90&#xFF08;&#xFF09;</code></a></p>
<p><code>round</code>() &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.round" title="torch.round"> <code>torch.round&#xFF08;&#xFF09;</code></a></p>
<p><code>round_</code>() &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <code>&#x8F6E;&#xFF08;&#xFF09;</code></p>
<p><code>rsqrt</code>() &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.rsqrt" title="torch.rsqrt"> <code>torch.rsqrt&#xFF08;&#xFF09;</code></a></p>
<p><code>rsqrt_</code>() &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>rsqrt&#x7684;&#xFF08;&#xFF09;</code></p>
<p><code>scatter</code>( <em>dim</em> , <em>index</em> , <em>source</em> ) &#x2192; Tensor</p>
<p>&#x5916;&#x7684;&#x5730;&#x65B9;&#x7684; <code>&#x7248;&#x672C;torch.Tensor.scatter_&#xFF08;&#xFF09;</code></p>
<p><code>scatter_</code>( <em>dim</em> , <em>index</em> , <em>src</em> ) &#x2192; Tensor</p>
<p>&#x5199;&#x5165;&#x6240;&#x6709;&#x503C;&#x4ECE;&#x5F20;&#x91CF;<code>SRC</code>&#x5230;<code>&#x81EA;</code>&#x5728;<code>&#x7D22;&#x5F15; [&#x6307;&#x5B9A;&#x7684;&#x7D22;&#x5F15;HTG11]&#x5F20;&#x91CF;&#x3002;&#x5BF9;&#x4E8E;SRC</code>&#xFF0C;&#x5B83;&#x7684;&#x8F93;&#x51FA;&#x7D22;&#x5F15;&#x662F;&#x7531;&#x5B83;&#x7684;&#x6307;&#x6570;<code>SRC</code>&#x4E3A;<code>&#x7EF4;[HTG22&#x6307;&#x5B9A;&#x5728;</code>&#x6BCF;&#x4E2A;&#x503C;] &#xFF01;=  &#x6697;&#x6DE1; <code>&#x548C;&#x5728;</code>&#x7D22;&#x5F15; <code>&#x4E3A;</code>&#x7EF4; [&#x76F8;&#x5E94;&#x7684;&#x503C;HTG35] = <code>&#x6697;&#x6DE1;</code>&#x3002;</p>
<p>&#x5BF9;&#x4E8E;3-d&#x5F20;&#x91CF;&#xFF0C;<code>&#x81EA;</code>&#x88AB;&#x66F4;&#x65B0;&#x4E3A;&#xFF1A;</p>
<pre><code>self[index[i][j][k]][j][k] = src[i][j][k]  # if dim == 0
self[i][index[i][j][k]][k] = src[i][j][k]  # if dim == 1
self[i][j][index[i][j][k]] = src[i][j][k]  # if dim == 2
</code></pre><p>&#x8FD9;&#x662F;&#x5728; <code>&#x805A;&#x96C6;&#xFF08;&#xFF09;</code>&#x6240;&#x63CF;&#x8FF0;&#x7684;&#x65B9;&#x5F0F;&#x76F8;&#x53CD;&#x7684;&#x64CD;&#x4F5C;&#x3002;</p>
<p><code>&#x81EA;</code>&#xFF0C;<code>&#x7D22;&#x5F15;</code>&#x548C;<code>SRC</code>&#xFF08;&#x5982;&#x679C;&#x5B83;&#x662F;&#x4E00;&#x4E2A;&#x5F20;&#x91CF;&#xFF09;&#x5E94;&#x5177;&#x6709;&#x76F8;&#x540C;&#x7684;&#x7EF4;&#x6570;&#x3002;&#x5B83;&#x4E5F;&#x8981;&#x6C42;<code>index.size&#xFF08;d&#xFF09; &amp; LT ; =
src.size&#xFF08;d&#xFF09; [HTG19&#x7528;&#x4E8E;&#x6240;&#x6709;&#x7EF4;&#x5EA6;</code>d<code>&#xFF0C;&#x4EE5;&#x53CA;</code>index.size&#xFF08;d&#xFF09; &amp; LT ; =  self.size&#xFF08;d&#xFF09;
<code>&#x9488;&#x5BF9;&#x6240;&#x6709;&#x7EF4;&#x5EA6;</code>d  &#xFF01;=  &#x6697;&#x6DE1; <code>&#x3002;</code></p>
<p>&#x6B64;&#x5916;&#xFF0C;&#x5BF9;&#x4E8E; <code>&#x805A;&#x96C6;&#xFF08;&#xFF09;</code>&#xFF0C;<code>&#x7D22;&#x5F15;&#x7684;&#x503C;</code>&#x5FC5;&#x987B;&#x4ECB;&#x4E8E;<code>0</code>&#x548C;<code>self.size&#xFF08;DIM&#xFF09; -  1</code>&#x4EE5;&#x4E0B;&#xFF0C;&#x5E76;&#x4E14;&#x6240;&#x6709;&#x5728;&#x4E00;&#x6392;&#x503C;&#x4E00;&#x8D77;&#x6307;&#x5B9A;&#x7684;&#x5C3A;&#x5BF8;
<code>&#x6697;&#x6DE1;</code>&#x5FC5;&#x987B;&#x662F;&#x552F;&#x4E00;&#x7684;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x6697;&#x6DE1;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x7684;&#x8F74;&#xFF0C;&#x6CBF;&#x7740;&#x8BE5;&#x7D22;&#x5F15;</p>
</li>
<li><p><strong>&#x7D22;&#x5F15;</strong> &#xFF08; <em>LongTensor</em> &#xFF09; - &#x5143;&#x4EF6;&#x4EE5;&#x5206;&#x6563;&#x7684;&#x6307;&#x6570;&#xFF0C;&#x53EF;&#x4EE5;&#x662F;&#x7A7A;&#x7684;&#x6216;SRC&#x7684;&#x76F8;&#x540C;&#x7684;&#x5C3A;&#x5BF8;&#x3002;&#x5F53;&#x7A7A;&#xFF0C;&#x64CD;&#x4F5C;&#x8FD4;&#x56DE;&#x8EAB;&#x4EFD;</p>
</li>
<li><p><strong>SRC</strong> &#xFF08; <em>&#x5F20;&#x91CF;</em> &#xFF09; - &#x6E90;&#x5143;&#x7D20;&#xFF08;&#x4E00;&#x4E2A;&#x6216;&#x591A;&#x4E2A;&#xFF09;&#x4EE5;&#x6563;&#x5C04;&#xFF0C;&#x67DC;&#x9762;&#x503C;&#x672A;&#x6307;&#x5B9A;&#x88AB;</p>
</li>
<li><p><strong>&#x503C;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#float" title="\(in Python v3.7\)" target="_blank"> <em>&#x6D6E;&#x52A8;</em> </a>&#xFF09; - &#x6E90;&#x5143;&#x7D20;&#xFF08;&#x4E00;&#x4E2A;&#x6216;&#x591A;&#x4E2A;&#xFF09;&#x4EE5;&#x6563;&#x5C04;&#xFF0C;&#x67DC;&#x9762; SRC &#x672A;&#x6307;&#x5B9A;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.rand(2, 5)
&gt;&gt;&gt; x
tensor([[ 0.3992,  0.2908,  0.9044,  0.4850,  0.6004],
        [ 0.5735,  0.9006,  0.6797,  0.4152,  0.1732]])
&gt;&gt;&gt; torch.zeros(3, 5).scatter_(0, torch.tensor([[0, 1, 2, 0, 0], [2, 0, 0, 1, 2]]), x)
tensor([[ 0.3992,  0.9006,  0.6797,  0.4850,  0.6004],
        [ 0.0000,  0.2908,  0.0000,  0.4152,  0.0000],
        [ 0.5735,  0.0000,  0.9044,  0.0000,  0.1732]])

&gt;&gt;&gt; z = torch.zeros(2, 4).scatter_(1, torch.tensor([[2], [3]]), 1.23)
&gt;&gt;&gt; z
tensor([[ 0.0000,  0.0000,  1.2300,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  1.2300]])
</code></pre><p><code>scatter_add_</code>( <em>dim</em> , <em>index</em> , <em>other</em> ) &#x2192; Tensor</p>
<p>&#x5C06;&#x4ECE;&#x5F20;&#x91CF;<code>&#x7684;&#x6240;&#x6709;&#x503C;&#x5176;&#x4ED6;</code>&#x5230;<code>&#x81EA;</code>&#x5728;<code>&#x7D22;&#x5F15; [&#x6307;&#x5B9A;&#x7684;&#x7D22;&#x5F15;HTG11]&#x5F20;&#x91CF;&#x4EE5;&#x7C7B;&#x4F3C;&#x7684;&#x65B9;&#x5F0F;&#x4E3A;</code>scatter_&#xFF08;&#xFF09; <code>&#x3002;&#x5BF9;&#x4E8E;&#x5728;</code>&#x5176;&#x4ED6;
<code>&#xFF0C;&#x5B83;&#x88AB;&#x6DFB;&#x52A0;&#x5230;&#x7D22;&#x5F15;&#x4E2D;</code>&#x81EA; <code>&#x8FD9;&#x662F;&#x7531;&#x5176;&#x7D22;&#x5F15;&#x6307;&#x5B9A;&#x5728;</code>[HTG27&#x6BCF;&#x4E2A;&#x503C;]&#x7B49; <code>&#x4E3A;</code>&#x7EF4; &#xFF01;=  &#x6697;&#x6DE1; <code>&#x548C;&#x5728;</code>&#x7D22;&#x5F15;&#x5BF9;&#x5E94;&#x7684;&#x503C; <code>&#x4E3A;</code>&#x7EF4; =  &#x6697;&#x6DE1; <code>&#x3002;</code></p>
<p>For a 3-D tensor, <code>self</code>is updated as:</p>
<pre><code>self[index[i][j][k]][j][k] += other[i][j][k]  # if dim == 0
self[i][index[i][j][k]][k] += other[i][j][k]  # if dim == 1
self[i][j][index[i][j][k]] += other[i][j][k]  # if dim == 2
</code></pre><p><code>&#x81EA;</code>&#xFF0C;<code>&#x7D22;&#x5F15;</code>&#x548C;<code>&#x5176;&#x4ED6;</code>&#x5E94;&#x5F53;&#x5177;&#x6709;&#x76F8;&#x540C;&#x7684;&#x7EF4;&#x6570;&#x3002;&#x5B83;&#x4E5F;&#x8981;&#x6C42;<code>index.size&#xFF08;d&#xFF09; &amp; LT ; =  other.size&#xFF08;d&#xFF09;
[HTG19&#x7528;&#x4E8E;&#x6240;&#x6709;&#x7EF4;&#x5EA6;</code>d<code>&#xFF0C;&#x4EE5;&#x53CA;</code>index.size&#xFF08;d&#xFF09; &amp; LT ; =  self.size&#xFF08;d&#xFF09; <code>&#x9488;&#x5BF9;&#x6240;&#x6709;&#x7EF4;&#x5EA6;</code>d  &#xFF01;=  &#x6697;&#x6DE1;
<code>&#x3002;</code></p>
<p>&#x6B64;&#x5916;&#xFF0C;&#x5BF9;&#x4E8E; <code>&#x805A;&#x96C6;&#xFF08;&#xFF09;</code>&#xFF0C;<code>&#x7D22;&#x5F15;&#x7684;&#x503C;</code>&#x5FC5;&#x987B;&#x4ECB;&#x4E8E;<code>0</code>&#x548C;<code>self.size&#xFF08;DIM&#xFF09; -  1</code>&#x4EE5;&#x4E0B;&#xFF0C;&#x5E76;&#x4E14;&#x6240;&#x6709;&#x5728;&#x4E00;&#x6392;&#x503C;&#x4E00;&#x8D77;&#x6307;&#x5B9A;&#x7684;&#x5C3A;&#x5BF8;
<code>&#x6697;&#x6DE1;</code>&#x5FC5;&#x987B;&#x662F;&#x552F;&#x4E00;&#x7684;&#x3002;</p>
<p>Note</p>
<p>When using the CUDA backend, this operation may induce nondeterministic
behaviour that is not easily switched off. Please see the notes on
<a href="notes/randomness.html">Reproducibility</a> for background.</p>
<p>Parameters</p>
<ul>
<li><p><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a>) &#x2013; the axis along which to index</p>
</li>
<li><p><strong>&#x7D22;&#x5F15;</strong> &#xFF08; <em>LongTensor</em> &#xFF09; - &#x5143;&#x4EF6;&#x7684;&#x6563;&#x5C04;&#xFF0C;&#x5E76;&#x6DFB;&#x52A0;&#xFF0C;&#x6307;&#x6570;&#x53EF;&#x4EE5;&#x662F;&#x7A7A;&#x7684;&#x6216;SRC&#x7684;&#x76F8;&#x540C;&#x7684;&#x5C3A;&#x5BF8;&#x3002;&#x5F53;&#x7A7A;&#xFF0C;&#x64CD;&#x4F5C;&#x8FD4;&#x56DE;&#x8EAB;&#x4EFD;&#x3002;</p>
</li>
<li><p><strong>&#x5176;&#x4ED6;</strong> &#xFF08; <em>&#x5F20;&#x91CF;</em> &#xFF09; - &#x6E90;&#x5143;&#x4EF6;&#x6563;&#x5C04;&#x548C;&#x6DFB;&#x52A0;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.rand(2, 5)
&gt;&gt;&gt; x
tensor([[0.7404, 0.0427, 0.6480, 0.3806, 0.8328],
        [0.7953, 0.2009, 0.9154, 0.6782, 0.9620]])
&gt;&gt;&gt; torch.ones(3, 5).scatter_add_(0, torch.tensor([[0, 1, 2, 0, 0], [2, 0, 0, 1, 2]]), x)
tensor([[1.7404, 1.2009, 1.9154, 1.3806, 1.8328],
        [1.0000, 1.0427, 1.0000, 1.6782, 1.0000],
        [1.7953, 1.0000, 1.6480, 1.0000, 1.9620]])
</code></pre><p><code>scatter_add</code>( <em>dim</em> , <em>index</em> , <em>source</em> ) &#x2192; Tensor</p>
<p>&#x5916;&#x7684;&#x5730;&#x65B9;&#x7684; <code>&#x7248;&#x672C;torch.Tensor.scatter_add_&#xFF08;&#xFF09;</code></p>
<p><code>select</code>( <em>dim</em> , <em>index</em> ) &#x2192; Tensor</p>
<p>&#x5207;&#x7247;&#x7684;<code>&#x81EA;</code>&#x6CBF;&#x7740;&#x7ED9;&#x5B9A;&#x7684;&#x7D22;&#x5F15;&#x5728;&#x6240;&#x9009;&#x62E9;&#x7684;&#x5C3A;&#x5BF8;&#x5F20;&#x91CF;&#x3002;&#x8FD9;&#x4E2A;&#x51FD;&#x6570;&#x8FD4;&#x56DE;&#x5220;&#x9664;&#x4E86;&#x7ED9;&#x5B9A;&#x5C3A;&#x5BF8;&#x7684;&#x5F20;&#x91CF;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x6697;&#x6DE1;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x7684;&#x5C3A;&#x5BF8;&#x5207;</p>
</li>
<li><p><strong>&#x7D22;&#x5F15;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x7D22;&#x5F15;&#x4E0E;&#x9009;&#x62E9;</p>
</li>
</ul>
<p>Note</p>
<p><code>&#x9009;&#x62E9;&#xFF08;&#xFF09;</code>&#x7B49;&#x4EF7;&#x4E8E;&#x5207;&#x7247;&#x3002;&#x4F8B;&#x5982;&#xFF0C;<code>tensor.select&#xFF08;0&#xFF0C; &#x7D22;&#x5F15;&#xFF09;</code>&#x7B49;&#x4E8E;<code>&#x5F20;&#x91CF;[&#x6307;&#x6570;]</code>&#x548C;<code>tensor.select&#xFF08;2&#xFF0C; &#x7D22;&#x5F15;&#xFF09;</code>&#x7B49;&#x4E8E;<code>&#x5F20;&#x91CF;[&#xFF1A;&#xFF0C;&#xFF1A;&#xFF0C;&#x6307;&#x6570;]</code>&#x3002;</p>
<p><code>set_</code>( <em>source=None</em> , <em>storage_offset=0</em> , <em>size=None</em> , <em>stride=None</em> ) &#x2192;
Tensor</p>
<p>&#x8BBE;&#x7F6E;&#x5E95;&#x5C42;&#x5B58;&#x50A8;&#xFF0C;&#x5927;&#x5C0F;&#x548C;&#x8FDB;&#x6B65;&#x3002;&#x5982;&#x679C;<code>&#x6E90;</code>&#x662F;&#x4E00;&#x4E2A;&#x5F20;&#x91CF;&#xFF0C;<code>&#x81EA;</code>&#x5F20;&#x91CF;&#x5C06;&#x5171;&#x4EAB;&#x76F8;&#x540C;&#x7684;&#x5B58;&#x50A8;&#xFF0C;&#x5E76;&#x4E14;&#x5177;&#x6709;&#x76F8;&#x540C;&#x7684;&#x5C3A;&#x5BF8;&#x548C;&#x6B65;&#x5E45;&#x4E3A;<code>&#x6E90;</code>&#x3002;&#x5728;&#x4E00;&#x4E2A;&#x5F20;&#x91CF;&#x5143;&#x7D20;&#x7684;&#x53D8;&#x5316;&#x5C06;&#x53CD;&#x6620;&#x5728;&#x5176;&#x4ED6;&#x3002;</p>
<p>&#x5982;&#x679C;<code>&#x6E90;</code>&#x662F;<code>&#x5B58;&#x653E;</code>&#xFF0C;&#x8BE5;&#x65B9;&#x6CD5;&#x5C06;&#x5E95;&#x5C42;&#x5B58;&#x50A8;&#xFF0C;&#x504F;&#x79FB;&#x91CF;&#xFF0C;&#x5927;&#x5C0F;&#xFF0C;&#x548C;&#x6B65;&#x5E45;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x6E90;</strong> &#xFF08; <em>&#x5F20;&#x91CF;</em> <em>&#x6216;</em> <em>&#x5B58;&#x653E;</em> &#xFF09; - &#x5F20;&#x91CF;&#x6216;&#x5B58;&#x50A8;&#x4F7F;&#x7528;</p>
</li>
<li><p><strong>storage_offset</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5728;&#x5B58;&#x50A8;&#x5668;&#x4E2D;&#x7684;&#x504F;&#x79FB;</p>
</li>
<li><p><strong>&#x5927;&#x5C0F;</strong> &#xFF08; <em>torch.Size</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x6240;&#x9700;&#x7684;&#x5927;&#x5C0F;&#x3002;&#x9ED8;&#x8BA4;&#x4E3A;&#x6E90;&#x7684;&#x5927;&#x5C0F;&#x3002;</p>
</li>
<li><p><strong>&#x6B65;&#x5E45;</strong> &#xFF08;<a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python v3.7\)" target="_blank"> <em>&#x5143;&#x7EC4;</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x6240;&#x9700;&#x7684;&#x6B65;&#x5E45;&#x3002;&#x9ED8;&#x8BA4;&#x4E3A;C-&#x8FDE;&#x7EED;&#x8FDB;&#x5C55;&#x3002;</p>
</li>
</ul>
<p><code>share_memory_</code>()<a href="_modules/torch/tensor.html#Tensor.share_memory_">[source]</a></p>
<p>&#x79FB;&#x52A8;&#x5E95;&#x5C42;&#x5B58;&#x50A8;&#x5230;&#x5171;&#x4EAB;&#x5B58;&#x50A8;&#x5668;&#x4E2D;&#x3002;</p>
<p>&#x8FD9;&#x662F;&#x5982;&#x679C;&#x5E95;&#x5C42;&#x5B58;&#x50A8;&#x5DF2;&#x7ECF;&#x5728;&#x5171;&#x4EAB;&#x5B58;&#x50A8;&#x5668;&#x548C;&#x7528;&#x4E8E;CUDA&#x5F20;&#x91CF;&#x65E0;&#x64CD;&#x4F5C;&#x3002;&#x5728;&#x5171;&#x4EAB;&#x5B58;&#x50A8;&#x5668;&#x4E2D;&#x5F20;&#x91CF;&#x4E0D;&#x80FD;&#x88AB;&#x8C03;&#x6574;&#x5927;&#x5C0F;&#x3002;</p>
<p><code>short</code>() &#x2192; Tensor</p>
<p><code>self.short&#xFF08;&#xFF09;</code>&#x7B49;&#x4E8E;<code>self.to&#xFF08;torch.int16&#xFF09;</code>&#x3002;&#x53C2;&#x89C1; <code>&#x81F3;&#xFF08;&#xFF09;</code>&#x3002;</p>
<p><code>sigmoid</code>() &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.sigmoid" title="torch.sigmoid"> <code>torch.sigmoid&#xFF08;&#xFF09;</code></a></p>
<p><code>sigmoid_</code>() &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>&#x4E59;&#x72B6;&#x7ED3;&#x80A0;&#x7684;&#xFF08;&#xFF09;</code></p>
<p><code>sign</code>() &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.sign" title="torch.sign"> <code>torch.sign&#xFF08;&#xFF09;</code></a></p>
<p><code>sign_</code>() &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>&#x7B26;&#x53F7;&#x7684;&#xFF08;&#xFF09;</code></p>
<p><code>sin</code>() &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.sin" title="torch.sin"> <code>torch.sin&#xFF08;&#xFF09;</code></a></p>
<p><code>sin_</code>() &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>&#x7F6A;&#x7684;&#xFF08;&#xFF09;</code></p>
<p><code>sinh</code>() &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.sinh" title="torch.sinh"> <code>torch.sinh&#xFF08;&#xFF09;</code></a></p>
<p><code>sinh_</code>() &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <code>&#x7684;sinh&#xFF08;&#xFF09;</code></p>
<p><code>size</code>() &#x2192; torch.Size</p>
<p>&#x8FD4;&#x56DE;<code>&#x81EA;</code>&#x5F20;&#x91CF;&#x7684;&#x5927;&#x5C0F;&#x3002;&#x8FD4;&#x56DE;&#x7684;&#x503C;&#x662F;<a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="\(in Python
v3.7\)" target="_blank"> <code>&#x5143;&#x7EC4;</code>
</a>&#x7684;&#x5B50;&#x7C7B;&#x3002;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.empty(3, 4, 5).size()
torch.Size([3, 4, 5])
</code></pre><p><code>slogdet</code>( <em>) - &gt; (Tensor</em>, <em>Tensor</em> )</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.slogdet" title="torch.slogdet"> <code>torch.slogdet&#xFF08;&#xFF09;</code></a></p>
<p><code>solve</code>( <em>A</em> ) &#x2192; Tensor, Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.solve" title="torch.solve"> <code>torch.solve&#xFF08;&#xFF09;</code></a></p>
<p><code>sort</code>( <em>dim=-1</em> , <em>descending=False) - &gt; (Tensor</em>, <em>LongTensor</em> )</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.sort" title="torch.sort"> <code>torch.sort&#xFF08;&#xFF09;</code></a></p>
<p><code>split</code>( <em>split_size</em> , <em>dim=0</em>
)<a href="_modules/torch/tensor.html#Tensor.split">[source]</a></p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.split" title="torch.split"> <code>torch.split&#xFF08;&#xFF09;</code></a></p>
<p><code>sparse_mask</code>( <em>input</em> , <em>mask</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE; &#x8FC7;&#x6EE4;&#x901A;&#x8FC7;<code>&#x6307;&#x6570;&#x4E0E;&#x503C;&#x7684;&#x65B0;SparseTensor&#x4ECE;&#x5F20;&#x91CF;</code>&#x8F93;&#x5165;&#x63A9;&#x7801; <code>&#x548C;&#x503C;&#x5C06;&#x88AB;&#x5FFD;&#x7565;&#x3002;</code>&#x8F93;&#x5165; <code>&#x548C;</code>&#x63A9;&#x6A21; <code>&#x5FC5;&#x987B;&#x5177;&#x6709;&#x76F8;&#x540C;&#x7684;&#x5F62;&#x72B6;&#x3002;</code></p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#xFF08; <em>&#x5F20;&#x91CF;</em> &#xFF09; - &#x7684;&#x8F93;&#x5165;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>&#x63A9;&#x6A21;</strong> &#xFF08; <em>SparseTensor</em> &#xFF09; - &#x4E00;&#x4E2A;SparseTensor&#x6211;&#x4EEC;&#x7B5B;&#x9009;<code>&#x6839;&#x636E;&#x5B83;&#x7684;&#x7D22;&#x5F15;&#x8F93;&#x5165;</code></p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; nnz = 5
&gt;&gt;&gt; dims = [5, 5, 2, 2]
&gt;&gt;&gt; I = torch.cat([torch.randint(0, dims[0], size=(nnz,)),
                   torch.randint(0, dims[1], size=(nnz,))], 0).reshape(2, nnz)
&gt;&gt;&gt; V = torch.randn(nnz, dims[2], dims[3])
&gt;&gt;&gt; size = torch.Size(dims)
&gt;&gt;&gt; S = torch.sparse_coo_tensor(I, V, size).coalesce()
&gt;&gt;&gt; D = torch.randn(dims)
&gt;&gt;&gt; D.sparse_mask(S)
tensor(indices=tensor([[0, 0, 0, 2],
                       [0, 1, 4, 3]]),
       values=tensor([[[ 1.6550,  0.2397],
                       [-0.1611, -0.0779]],

                      [[ 0.2326, -1.0558],
                       [ 1.4711,  1.9678]],

                      [[-0.5138, -0.0411],
                       [ 1.9417,  0.5158]],

                      [[ 0.0793,  0.0036],
                       [-0.2569, -0.1055]]]),
       size=(5, 5, 2, 2), nnz=4, layout=torch.sparse_coo)
</code></pre><p><code>sparse_dim</code>() &#x2192; int</p>
<p>&#x5982;&#x679C;<code>&#x81EA;</code>&#x662F;&#x4E00;&#x4E2A;&#x7A00;&#x758F;COO&#x5F20;&#x91CF;&#xFF08;&#x5373;&#xFF0C;&#x4E0E;<code>torch.sparse_coo</code>&#x5E03;&#x5C40;&#xFF09;&#xFF0C;&#x5B83;&#x8FD4;&#x56DE;&#x7A00;&#x758F;&#x7684;&#x7EF4;&#x5EA6;&#x6570;&#x76EE;&#x3002;&#x5426;&#x5219;&#xFF0C;&#x8FD9;&#x5C06;&#x5F15;&#x53D1;&#x4E00;&#x4E2A;&#x9519;&#x8BEF;&#x3002;</p>
<p>&#x53E6;&#x8BF7;&#x53C2;&#x89C1; <code>Tensor.dense_dim&#xFF08;&#xFF09;</code>&#x3002;</p>
<p><code>sqrt</code>() &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.sqrt" title="torch.sqrt"> <code>torch.sqrt&#xFF08;&#xFF09;</code></a></p>
<p><code>sqrt_</code>() &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <code>SQRT&#xFF08;&#xFF09;</code></p>
<p><code>squeeze</code>( <em>dim=None</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.squeeze" title="torch.squeeze"> <code>torch.squeeze&#xFF08;&#xFF09;</code></a></p>
<p><code>squeeze_</code>( <em>dim=None</em> ) &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>&#x6324;&#x538B;&#x7684;&#xFF08;&#xFF09;</code></p>
<p><code>std</code>( <em>dim=None</em> , <em>unbiased=True</em> , <em>keepdim=False</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.std" title="torch.std"> <code>torch.std&#xFF08;&#xFF09;</code></a></p>
<p><code>stft</code>( <em>n_fft</em> , <em>hop_length=None</em> , <em>win_length=None</em> , <em>window=None</em> ,
<em>center=True</em> , <em>pad_mode=&apos;reflect&apos;</em> , <em>normalized=False</em> , <em>onesided=True</em>
)<a href="_modules/torch/tensor.html#Tensor.stft">[source]</a></p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.stft" title="torch.stft"> <code>torch.stft&#xFF08;&#xFF09;</code></a></p>
<p>Warning</p>
<p>&#x6B64;&#x529F;&#x80FD;&#x5728;0.4.1&#x7248;&#x672C;&#x4E2D;&#x66F4;&#x6539;&#x7B7E;&#x540D;&#x3002;&#x4E0E;&#x5148;&#x524D;&#x7684;&#x7B7E;&#x540D;&#x8C03;&#x7528;&#x53EF;&#x80FD;&#x4F1A;&#x5BFC;&#x81F4;&#x9519;&#x8BEF;&#x6216;&#x8FD4;&#x56DE;&#x4E0D;&#x6B63;&#x786E;&#x7684;&#x7ED3;&#x679C;&#x3002;</p>
<p><code>storage</code>() &#x2192; torch.Storage</p>
<p>&#x8FD4;&#x56DE;&#x5E95;&#x5C42;&#x5B58;&#x50A8;&#x3002;</p>
<p><code>storage_offset</code>() &#x2192; int</p>
<p>&#x8FD4;&#x56DE;<code>&#x81EA;</code>&#x5F20;&#x91CF;&#x5728;&#x5E95;&#x5C42;&#x5B58;&#x50A8;&#x7684;&#x504F;&#x79FB;&#x5728;&#x5B58;&#x50A8;&#x5143;&#x4EF6;&#xFF08;&#x672A;&#x5B57;&#x8282;&#xFF09;&#x7684;&#x6570;&#x76EE;&#x65B9;&#x9762;&#x3002;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.tensor([1, 2, 3, 4, 5])
&gt;&gt;&gt; x.storage_offset()
0
&gt;&gt;&gt; x[3:].storage_offset()
3
</code></pre><p><code>storage_type</code>() &#x2192; type</p>
<p>&#x8FD4;&#x56DE;&#x5E95;&#x5C42;&#x5B58;&#x50A8;&#x7684;&#x7C7B;&#x578B;&#x3002;</p>
<p><code>stride</code>( <em>dim</em> ) &#x2192; tuple or int</p>
<p>&#x8FD4;&#x56DE;<code>&#x81EA;</code>&#x5F20;&#x91CF;&#x6B65;&#x5E45;&#x3002;</p>
<p>&#x6B65;&#x5E45;&#x662F;&#x5FC5;&#x8981;&#x8DF3;&#x8F6C;&#x5230;&#x4ECE;&#x4E00;&#x4E2A;&#x5143;&#x4EF6;&#x5230;&#x4E0B;&#x4E00;&#x4E2A;&#x6307;&#x5B9A;&#x7EF4;&#x5EA6; <code>&#x6697;&#x6DE1;</code>&#x3002;&#x5F53;&#x6CA1;&#x6709;&#x53C2;&#x6570;&#x5728;&#x88AB;&#x4F20;&#x9012;&#x8FD4;&#x56DE;&#x6240;&#x6709;&#x6B65;&#x5E45;&#x7684;&#x5143;&#x7EC4;&#xFF0C;&#x5426;&#x5219;&#xFF0C;&#x4E00;&#x4E2A;&#x6574;&#x6570;&#x503C;&#x8FD4;&#x56DE;&#x4E3A;&#x5728;&#x7279;&#x5B9A;&#x7EF4;&#x5EA6; <code>&#x6B65;&#x5E45;&#x6697;&#x6DE1;</code>&#x3002;</p>
<p>Parameters</p>
<p><strong>&#x6697;&#x6DE1;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in
Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x5176;&#x4E2D;&#x9700;&#x8981;&#x6B65;&#x5E45;&#x6240;&#x5E0C;&#x671B;&#x7684;&#x5C3A;&#x5BF8;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])
&gt;&gt;&gt; x.stride()
(5, 1)
&gt;&gt;&gt;x.stride(0)
5
&gt;&gt;&gt; x.stride(-1)
1
</code></pre><p><code>sub</code>( <em>value</em> , <em>other</em> ) &#x2192; Tensor</p>
<p>&#x51CF;&#x53BB;&#x4ECE;<code>&#x81EA;</code>&#x5F20;&#x91CF;&#x6807;&#x91CF;&#x6216;&#x5F20;&#x91CF;&#x3002;&#x5982;&#x679C;&#x4E24;&#x4E2A;<code>&#x503C;</code>&#x548C;<code>&#x5176;&#x4ED6;&#x88AB;&#x6307;&#x5B9A;</code>&#x7684;<code>&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x7B49;</code>&#x88AB;&#x7F29;&#x653E;&#x901A;&#x8FC7;<code>&#x503C;</code>&#x4E4B;&#x524D;&#x88AB;&#x4F7F;&#x7528;&#x3002;</p>
<p>&#x5F53;<code>&#x5176;&#x4ED6;</code>&#x662F;&#x4E00;&#x4E2A;&#x5F20;&#x91CF;&#xFF0C;&#x7684;<code>&#x5F62;&#x72B6;&#x5176;&#x4ED6;</code>&#x5FC5;&#x987B;<a href="notes/broadcasting.html#broadcasting-semantics"> broadcastable
</a>&#x4E0E;&#x5E95;&#x5C42;&#x5F20;&#x91CF;&#x7684;&#x5F62;&#x72B6;&#x3002;</p>
<p><code>sub_</code>( <em>x</em> ) &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>&#x5B50;&#x7684;&#xFF08;&#xFF09;</code></p>
<p><code>sum</code>( <em>dim=None</em> , <em>keepdim=False</em> , <em>dtype=None</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.sum" title="torch.sum"> <code>torch.sum&#xFF08;&#xFF09;</code></a></p>
<p><code>sum_to_size</code>( <em>*size</em> ) &#x2192; Tensor</p>
<p>&#x603B;&#x548C;<code>&#x8FD9;&#x91CC;</code>&#x5F20;&#x91CF;&#x4E3A; <code>&#x5927;&#x5C0F;</code>&#x3002;<code>&#x5927;&#x5C0F;</code>&#x5FC5;&#x987B;broadcastable&#x4E3A;<code>&#x8FD9;&#x91CC;</code>&#x5F20;&#x91CF;&#x7684;&#x5927;&#x5C0F;&#x3002;</p>
<p>Parameters</p>
<p><strong>&#x5927;&#x5C0F;</strong> &#xFF08; <em>INT ...</em> &#xFF09; - &#x5B9A;&#x4E49;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684;&#x5F62;&#x72B6;&#x7684;&#x6574;&#x6570;&#x5E8F;&#x5217;&#x3002;</p>
<p><code>svd</code>( <em>some=True</em> , <em>compute_uv=True) - &gt; (Tensor</em>, <em>Tensor</em> , <em>Tensor</em> )</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.svd" title="torch.svd"> <code>torch.svd&#xFF08;&#xFF09;</code></a></p>
<p><code>symeig</code>( <em>eigenvectors=False</em> , <em>upper=True) - &gt; (Tensor</em>, <em>Tensor</em> )</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.symeig" title="torch.symeig"> <code>torch.symeig&#xFF08;&#xFF09;</code></a></p>
<p><code>t</code>() &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.t" title="torch.t"> <code>torch.t&#xFF08;&#xFF09;</code></a></p>
<p><code>t_</code>() &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <code>T&#xFF08;&#xFF09;</code></p>
<p><code>to</code>( <em>*args</em> , <em>**kwargs</em> ) &#x2192; Tensor</p>
<p>&#x6267;&#x884C;&#x5F20;&#x91CF;D&#x578B;&#x7EC6;&#x80DE;&#x548C;/&#x6216;&#x8BBE;&#x5907;&#x7684;&#x8F6C;&#x6362;&#x3002; A <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"> <code>torch.dtype</code>
</a>&#x548C;<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"> <code>torch.device</code></a>&#x4ECE;&#x7684;&#x53C2;&#x6570;&#x63A8;&#x65AD;&#x51FA;<code>self.to&#xFF08;*&#x6307;&#x5B9A;&#x53C2;&#x6570;&#x65F6;&#xFF0C; ** kwargs&#xFF09;</code>&#x3002;</p>
<p>Note</p>
<p>&#x5982;&#x679C;<code>&#x81EA;</code>&#x5F20;&#x91CF;&#x5DF2;&#x7ECF;&#x6709;&#x4E86;&#x6B63;&#x786E;&#x7684;<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"> <code>torch.dtype</code></a>&#x548C;<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"> <code>torch.device</code>
</a>&#xFF0C;&#x7136;&#x540E;<code>&#x81EA;&#x88AB;&#x8FD4;&#x56DE;</code>&#x3002;&#x5426;&#x5219;&#xFF0C;&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x662F;<code>&#x81EA;</code>&#x4E0E;&#x590D;&#x5236;&#x6240;&#x9700;&#x7684;<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"> <code>torch.dtype</code>
</a>&#x548C;<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"> <code>torch.device</code></a>&#x3002;</p>
<p>&#x4E0B;&#x9762;&#x662F;&#x8C03;&#x7528;<code>&#x81F3;</code>&#x65B9;&#x5F0F;&#xFF1A;</p>
<p><code>to</code>( <em>dtype</em> , <em>non_blocking=False</em> , <em>copy=False</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E0E;&#x6307;&#x5B9A;<code>DTYPE</code>&#x5F20;&#x91CF;</p>
<p><code>to</code>( <em>device=None</em> , <em>dtype=None</em> , <em>non_blocking=False</em> , <em>copy=False</em> ) &#x2192;
Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E0E;&#x6307;&#x5B9A; <code>&#x88C5;&#x7F6E;</code>&#x548C;&#x5F20;&#x91CF;&#xFF08;&#x53EF;&#x9009;&#xFF09;<code>DTYPE</code>&#x3002;&#x5982;&#x679C;<code>DTYPE</code>&#x662F;<code>&#x65E0;</code>&#x5B83;&#x88AB;&#x63A8;&#x65AD;&#x4E3A;<code>self.dtype</code>&#x3002;&#x5F53;<code>non_blocking</code>&#xFF0C;&#x5C1D;&#x8BD5;&#x5982;&#x679C;&#x53EF;&#x80FD;&#x7684;&#x8BDD;&#xFF0C;&#x4EE5;&#x76F8;&#x5BF9;&#x4E8E;&#x5F02;&#x6B65;&#x8F6C;&#x6362;&#x5230;&#x4E3B;&#x673A;&#xFF0C;&#x4F8B;&#x5982;&#xFF0C;&#x8F6C;&#x6362;CPU&#x5F20;&#x91CF;&#x4E0E;&#x56FA;&#x5B9A;&#x5185;&#x5B58;&#x5230;CUDA&#x5F20;&#x91CF;&#x3002;&#x5F53;<code>&#x590D;&#x5236;</code>&#x8BBE;&#x7F6E;&#xFF0C;&#x5373;&#x4F7F;&#x5F53;&#x5DF2;&#x7ECF;&#x5F20;&#x91CF;&#x76F8;&#x5339;&#x914D;&#x7684;&#x6240;&#x9700;&#x7684;&#x8F6C;&#x5316;&#x521B;&#x5EFA;&#x4E00;&#x4E2A;&#x65B0;&#x7684;&#x5F20;&#x91CF;&#x3002;</p>
<p><code>to</code>( <em>other</em> , <em>non_blocking=False</em> , <em>copy=False</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E0E;&#x76F8;&#x540C;<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"> <code>torch.dtype</code></a>&#x548C;<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"> <code>torch.device</code>
</a>&#x4F5C;&#x4E3A;&#x4E00;&#x4E2A;&#x5F20;&#x91CF;&#x5F20;&#x91CF;<code>&#x5176;&#x4ED6;</code>&#x3002;&#x5F53;<code>non_blocking</code>&#xFF0C;&#x5C1D;&#x8BD5;&#x5982;&#x679C;&#x53EF;&#x80FD;&#x7684;&#x8BDD;&#xFF0C;&#x4EE5;&#x76F8;&#x5BF9;&#x4E8E;&#x5F02;&#x6B65;&#x8F6C;&#x6362;&#x5230;&#x4E3B;&#x673A;&#xFF0C;&#x4F8B;&#x5982;&#xFF0C;&#x8F6C;&#x6362;CPU&#x5F20;&#x91CF;&#x4E0E;&#x56FA;&#x5B9A;&#x5185;&#x5B58;&#x5230;CUDA&#x5F20;&#x91CF;&#x3002;&#x5F53;<code>&#x590D;&#x5236;</code>&#x8BBE;&#x7F6E;&#xFF0C;&#x5373;&#x4F7F;&#x5F53;&#x5DF2;&#x7ECF;&#x5F20;&#x91CF;&#x76F8;&#x5339;&#x914D;&#x7684;&#x6240;&#x9700;&#x7684;&#x8F6C;&#x5316;&#x521B;&#x5EFA;&#x4E00;&#x4E2A;&#x65B0;&#x7684;&#x5F20;&#x91CF;&#x3002;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; tensor = torch.randn(2, 2)  # Initially dtype=float32, device=cpu
&gt;&gt;&gt; tensor.to(torch.float64)
tensor([[-0.5044,  0.0005],
        [ 0.3310, -0.0584]], dtype=torch.float64)

&gt;&gt;&gt; cuda0 = torch.device(&apos;cuda:0&apos;)
&gt;&gt;&gt; tensor.to(cuda0)
tensor([[-0.5044,  0.0005],
        [ 0.3310, -0.0584]], device=&apos;cuda:0&apos;)

&gt;&gt;&gt; tensor.to(cuda0, dtype=torch.float64)
tensor([[-0.5044,  0.0005],
        [ 0.3310, -0.0584]], dtype=torch.float64, device=&apos;cuda:0&apos;)

&gt;&gt;&gt; other = torch.randn((), dtype=torch.float64, device=cuda0)
&gt;&gt;&gt; tensor.to(other, non_blocking=True)
tensor([[-0.5044,  0.0005],
        [ 0.3310, -0.0584]], dtype=torch.float64, device=&apos;cuda:0&apos;)
</code></pre><p><code>to_mkldnn</code>() &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;<code>torch.mkldnn</code>&#x5E03;&#x5C40;&#x5F20;&#x7684;&#x526F;&#x672C;&#x3002;</p>
<p><code>take</code>( <em>indices</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.take" title="torch.take"> <code>torch.take&#xFF08;&#xFF09;</code></a></p>
<p><code>tan</code>() &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.tan" title="torch.tan"> <code>torch.tan&#xFF08;&#xFF09;</code></a></p>
<p><code>tan_</code>() &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <code>&#x9EC4;&#x8910;&#x8272;&#xFF08;&#xFF09;</code></p>
<p><code>tanh</code>() &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.tanh" title="torch.tanh"> <code>torch.tanh&#xFF08;&#xFF09;</code></a></p>
<p><code>tanh_</code>() &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>&#x7684;tanh&#xFF08;&#xFF09;</code></p>
<p><code>tolist</code>()</p>
<p>&#x201D; tolist&#xFF08;&#xFF09; - &amp; GT ;&#x5217;&#x8868;&#x6216;&#x6570;</p>
<p>&#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x4E3A;&#xFF08;&#x5D4C;&#x5957;&#xFF09;&#x540D;&#x5355;&#x3002;&#x6807;&#x91CF;&#xFF0C;&#x5219;&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x6807;&#x51C6;&#x7684;Python&#x6570;&#x76EE;&#xFF0C;&#x53EA;&#x662F;&#xFF08;&#xFF09; &#x50CF; <code>&#x9879;&#x3002;&#x5F20;&#x91CF;&#x81EA;&#x52A8;&#x79FB;&#x52A8;&#x5230;CPU&#x9996;&#x5148;&#xFF0C;&#x5982;&#x679C;&#x5FC5;&#x8981;&#x7684;&#x3002;</code></p>
<p>This operation is not differentiable.</p>
<p>&#x4F8B;&#x5B50;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(2, 2)
&gt;&gt;&gt; a.tolist()
[[0.012766935862600803, 0.5415473580360413],
 [-0.08909505605697632, 0.7729271650314331]]
&gt;&gt;&gt; a[0,0].tolist()
0.012766935862600803
</code></pre><p><code>topk</code>( <em>k</em> , <em>dim=None</em> , <em>largest=True</em> , <em>sorted=True) - &gt; (Tensor</em>,
<em>LongTensor</em> )</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.topk" title="torch.topk"> <code>torch.topk&#xFF08;&#xFF09;</code></a></p>
<p><code>to_sparse</code>( <em>sparseDims</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x7684;&#x7A00;&#x758F;&#x526F;&#x672C;&#x3002; PyTorch&#x652F;&#x6301;<a href="sparse.html#sparse-docs"> &#x5750;&#x6807;&#x683C;&#x5F0F; </a>&#x7A00;&#x758F;&#x5F20;&#x91CF;&#x3002;</p>
<p>Parameters</p>
<p><strong>sparseDims</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x7A00;&#x758F;&#x7EF4;&#x6570;&#xFF0C;&#x4EE5;&#x5728;&#x65B0;&#x7684;&#x7A00;&#x758F;&#x5F20;&#x91CF;&#x5305;&#x62EC;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; d = torch.tensor([[0, 0, 0], [9, 0, 10], [0, 0, 0]])
&gt;&gt;&gt; d
tensor([[ 0,  0,  0],
        [ 9,  0, 10],
        [ 0,  0,  0]])
&gt;&gt;&gt; d.to_sparse()
tensor(indices=tensor([[1, 1],
                       [0, 2]]),
       values=tensor([ 9, 10]),
       size=(3, 3), nnz=2, layout=torch.sparse_coo)
&gt;&gt;&gt; d.to_sparse(1)
tensor(indices=tensor([[1]]),
       values=tensor([[ 9,  0, 10]]),
       size=(3, 3), nnz=1, layout=torch.sparse_coo)
</code></pre><p><code>trace</code>() &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.trace" title="torch.trace"> <code>torch.trace&#xFF08;&#xFF09;</code></a></p>
<p><code>transpose</code>( <em>dim0</em> , <em>dim1</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.transpose" title="torch.transpose"> <code>torch.transpose&#xFF08;&#xFF09;</code></a></p>
<p><code>transpose_</code>( <em>dim0</em> , <em>dim1</em> ) &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <code>&#x8F6C;&#x7F6E;&#xFF08;&#xFF09;</code></p>
<p><code>triangular_solve</code>( <em>A</em> , <em>upper=True</em> , <em>transpose=False</em> ,
<em>unitriangular=False) - &gt; (Tensor</em>, <em>Tensor</em> )</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.triangular_solve" title="torch.triangular_solve"> <code>torch.triangular_solve&#xFF08;&#xFF09;</code></a></p>
<p><code>tril</code>( <em>k=0</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.tril" title="torch.tril"> <code>torch.tril&#xFF08;&#xFF09;</code></a></p>
<p><code>tril_</code>( <em>k=0</em> ) &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <code>TRIL&#xFF08;&#xFF09;</code></p>
<p><code>triu</code>( <em>k=0</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.triu" title="torch.triu"> <code>torch.triu&#xFF08;&#xFF09;</code></a></p>
<p><code>triu_</code>( <em>k=0</em> ) &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>triu&#x7684;&#xFF08;&#xFF09;</code></p>
<p><code>trunc</code>() &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.trunc" title="torch.trunc"> <code>torch.trunc&#xFF08;&#xFF09;</code></a></p>
<p><code>trunc_</code>() &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C; <code>TRUNC&#x7684;&#xFF08;&#xFF09;</code></p>
<p><code>type</code>( <em>dtype=None</em> , <em>non_blocking=False</em> , <em>**kwargs</em> ) &#x2192; str or Tensor</p>
<p>&#x8FD4;&#x56DE;&#x7C7B;&#x578B;&#xFF0C;&#x5982;&#x679C; DTYPE &#x4E0D;&#x8BBE;&#x7F6E;&#xFF0C;&#x5426;&#x5219;&#x94F8;&#x4EF6;&#x6B64;&#x5BF9;&#x8C61;&#x4E3A;&#x6307;&#x5B9A;&#x7684;&#x7C7B;&#x578B;&#x3002;</p>
<p>&#x5982;&#x679C;&#x8FD9;&#x662F;&#x6B63;&#x786E;&#x7684;&#x7C7B;&#x578B;&#x5DF2;&#x7ECF;&#x6CA1;&#x6709;&#x6267;&#x884C;&#x590D;&#x5236;&#x64CD;&#x4F5C;&#xFF0C;&#x5E76;&#x8FD4;&#x56DE;&#x539F;&#x6765;&#x7684;&#x5BF9;&#x8C61;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>DTYPE</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#type" title="\(in Python v3.7\)" target="_blank"> <em>&#x8F93;&#x5165;</em> </a> <em>&#x6216;</em> <em>&#x4E32;</em> &#xFF09; - &#x6240;&#x9700;&#x7684;&#x7C7B;&#x578B;</p>
</li>
<li><p><strong>non_blocking</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a>&#xFF09; - &#x5982;&#x679C;<code>&#x771F;</code>&#xFF0C;&#x5E76;&#x4E14;&#x6E90;&#x662F;&#x5728;&#x56FA;&#x5B9A;&#x5B58;&#x50A8;&#x5668;&#x548C;&#x76EE;&#x7684;&#x5730;&#x662F;&#x5728;GPU&#x6216;&#x53CD;&#x4E4B;&#x4EA6;&#x7136;&#xFF0C;&#x526F;&#x672C;&#x88AB;&#x76F8;&#x5BF9;&#x4E8E;&#x6240;&#x8FF0;&#x4E3B;&#x673A;&#x5F02;&#x6B65;&#x5730;&#x6267;&#x884C;&#x3002;&#x53E6;&#x5916;&#xFF0C;&#x53C2;&#x6570;&#x6CA1;&#x6709;&#x4EFB;&#x4F55;&#x5F71;&#x54CD;&#x3002;</p>
</li>
<li><p><strong>** kwargs</strong> - &#x5BF9;&#x4E8E;&#x76F8;&#x5BB9;&#x6027;&#xFF0C;&#x53EF;&#x4EE5;&#x542B;&#x6709;&#x952E;<code>&#x5F02;&#x6B65;</code>&#x4EE3;&#x66FF;<code>non_blocking</code>&#x53C2;&#x6570;&#x7684;&#x3002;&#x7684;<code>&#x5F02;&#x6B65;</code>ARG&#x88AB;&#x5F03;&#x7528;&#x3002;</p>
</li>
</ul>
<p><code>type_as</code>( <em>tensor</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x6B64;&#x5F20;&#x6295;&#x7ED9;&#x5B9A;&#x5F20;&#x7684;&#x7C7B;&#x578B;&#x3002;</p>
<p>&#x8FD9;&#x662F;&#x4E00;&#x4E2A;&#x65E0;&#x64CD;&#x4F5C;&#xFF0C;&#x5982;&#x679C;&#x5F20;&#x91CF;&#x5DF2;&#x7ECF;&#x662F;&#x6B63;&#x786E;&#x7684;&#x7C7B;&#x578B;&#x3002;&#x8FD9;&#x7B49;&#x540C;&#x4E8E;<code>self.type&#xFF08;tensor.type&#xFF08;&#xFF09;&#xFF09;</code></p>
<p>Parameters</p>
<p><strong>&#x5F20;&#x91CF;</strong> &#xFF08; <em>&#x5F20;&#x91CF;</em> &#xFF09; - &#x5177;&#x6709;&#x6240;&#x9700;&#x7C7B;&#x578B;&#x7684;&#x5F20;&#x91CF;</p>
<p><code>unbind</code>( <em>dim=0</em> ) &#x2192; seq</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.unbind" title="torch.unbind"> <code>torch.unbind&#xFF08;&#xFF09;</code></a></p>
<p><code>unfold</code>( <em>dimension</em> , <em>size</em> , <em>step</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x5305;&#x542B;&#x5927;&#x5C0F; <code>&#x5927;&#x5C0F;</code>&#x7684;&#x6240;&#x6709;&#x7247;&#x7684;&#x5F20;&#x91CF;&#x4ECE;&#x5728;&#x5C3A;&#x5BF8;<code>[</code>&#x81EA; <code>&#x5F20;&#x91CF;HTG11]&#x7EF4;</code>&#x3002;</p>
<p>&#x4E24;&#x7247;&#x4E4B;&#x95F4;&#x7684;&#x6B65;&#x9AA4;&#x662F;&#x901A;&#x8FC7;<code>&#x6B65;&#x9AA4;&#x7ED9;&#x51FA;</code>&#x3002;</p>
<p>&#x5982;&#x679C; sizedim &#x662F;&#x7EF4;&#x5EA6;&#x7684;&#x5927;&#x5C0F;<code>&#x7EF4;</code>&#x4E3A;<code>&#x81EA;</code>&#xFF0C;&#x5C3A;&#x5BF8;<code>[&#x5927;&#x5C0F;HTG11 ]&#x7EF4;</code>&#x5728;&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x5C06;&#x662F;&#xFF08;sizedim - &#x5927;&#x5C0F;&#xFF09;/&#x6B65;+ 1 &#x3002;</p>
<p>&#x5927;&#x5C0F; <code>&#x5927;&#x5C0F;&#x7684;&#x989D;&#x5916;&#x7EF4;&#x5EA6;</code>&#x5728;&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x8FFD;&#x52A0;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x7EF4;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x7EF4;&#xFF0C;&#x5176;&#x4E2D;&#x5C55;&#x5F00;&#x53D1;&#x751F;</p>
</li>
<li><p><strong>&#x5927;&#x5C0F;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x5176;&#x88AB;&#x5C55;&#x5F00;&#x6BCF;&#x4E2A;&#x5207;&#x7247;&#x7684;&#x5927;&#x5C0F;</p>
</li>
<li><p><strong>&#x6B65;&#x9AA4;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x6BCF;&#x4E2A;&#x5207;&#x7247;&#x4E4B;&#x95F4;&#x7684;&#x53F0;&#x9636;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.arange(1., 8)
&gt;&gt;&gt; x
tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.])
&gt;&gt;&gt; x.unfold(0, 2, 1)
tensor([[ 1.,  2.],
        [ 2.,  3.],
        [ 3.,  4.],
        [ 4.,  5.],
        [ 5.,  6.],
        [ 6.,  7.]])
&gt;&gt;&gt; x.unfold(0, 2, 2)
tensor([[ 1.,  2.],
        [ 3.,  4.],
        [ 5.,  6.]])
</code></pre><p><code>uniform_</code>( <em>from=0</em> , <em>to=1</em> ) &#x2192; Tensor</p>
<p>&#x586B;&#x5145;<code>&#x81EA;</code>&#x5F20;&#x91CF;&#x4E0E;&#x4ECE;&#x8FDE;&#x7EED;&#x5747;&#x5300;&#x5206;&#x5E03;&#x91C7;&#x6837;&#x7684;&#x6570;&#x5B57;&#xFF1A;</p>
<p>P(x)=1to&#x2212;fromP(x) = \dfrac{1}{\text{to} - \text{from}} P(x)=to&#x2212;from1&#x200B;</p>
<p><code>unique</code>( <em>sorted=True</em> , <em>return_inverse=False</em> , <em>return_counts=False</em> ,
<em>dim=None</em> )<a href="_modules/torch/tensor.html#Tensor.unique">[source]</a></p>
<p>&#x8FD4;&#x56DE;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x7684;&#x72EC;&#x7279;&#x5143;&#x7D20;&#x3002;</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.unique" title="torch.unique"> <code>torch.unique&#xFF08;&#xFF09;</code></a></p>
<p><code>unique_consecutive</code>( <em>return_inverse=False</em> , <em>return_counts=False</em> ,
<em>dim=None</em> )<a href="_modules/torch/tensor.html#Tensor.unique_consecutive">[source]</a></p>
<p>&#x6D88;&#x9664;&#x4E86;&#x6240;&#x6709;&#x7684;&#x4F46;&#x7B49;&#x6548;&#x4ECE;&#x5143;&#x4EF6;&#x7684;&#x6BCF;&#x4E2A;&#x8FDE;&#x7EED;&#x7EC4;&#x7684;&#x7B2C;&#x4E00;&#x4E2A;&#x5143;&#x7D20;&#x3002;</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.unique_consecutive" title="torch.unique_consecutive"> <code>torch.unique_consecutive&#xFF08;&#xFF09;</code></a></p>
<p><code>unsqueeze</code>( <em>dim</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.unsqueeze" title="torch.unsqueeze"> <code>torch.unsqueeze&#xFF08;&#xFF09;</code></a></p>
<p><code>unsqueeze_</code>( <em>dim</em> ) &#x2192; Tensor</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <code>unsqueeze&#xFF08;&#xFF09;</code></p>
<p><code>values</code>() &#x2192; Tensor</p>
<p>&#x5982;&#x679C;<code>&#x81EA;</code>&#x662F;&#x4E00;&#x4E2A;&#x7A00;&#x758F;COO&#x5F20;&#x91CF;&#xFF08;&#x5373;&#xFF0C;&#x4E0E;<code>torch.sparse_coo</code>&#x5E03;&#x5C40;&#xFF09;&#xFF0C;&#x8FD9;&#x5C06;&#x8FD4;&#x56DE;&#x5305;&#x542B;&#x7684;&#x503C;&#x5F20;&#x91CF;&#x7684;&#x56FE;&#x3002;&#x5426;&#x5219;&#xFF0C;&#x8FD9;&#x5C06;&#x5F15;&#x53D1;&#x4E00;&#x4E2A;&#x9519;&#x8BEF;&#x3002;</p>
<p>&#x53E6;&#x8BF7;&#x53C2;&#x89C1; <code>Tensor.indices&#xFF08;&#xFF09;</code>&#x3002;</p>
<p>Note</p>
<p>This method can only be called on a coalesced sparse tensor. See
<code>Tensor.coalesce()</code>for details.</p>
<p><code>var</code>( <em>dim=None</em> , <em>unbiased=True</em> , <em>keepdim=False</em> ) &#x2192; Tensor</p>
<p>&#x53C2;&#x89C1;<a href="torch.html#torch.var" title="torch.var"> <code>torch.var&#xFF08;&#xFF09;</code></a></p>
<p><code>view</code>( <em>*shape</em> ) &#x2192; Tensor</p>
<p>&#x8FD4;&#x56DE;&#x4E0E;&#x76F8;&#x540C;&#x7684;&#x6570;&#x636E;<code>&#x81EA;</code>&#x5F20;&#x91CF;&#x4F46;&#x5177;&#x6709;&#x4E0D;&#x540C;&#x7684;<code>&#x5F62;&#x72B6;</code>&#x7684;&#x65B0;&#x7684;&#x5F20;&#x91CF;&#x3002;</p>
<p>&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x5171;&#x4EAB;&#x76F8;&#x540C;&#x7684;&#x6570;&#x636E;&#xFF0C;&#x5E76;&#x4E14;&#x5FC5;&#x987B;&#x6709;&#x76F8;&#x540C;&#x6570;&#x91CF;&#x7684;&#x5143;&#x7D20;&#xFF0C;&#x4F46;&#x53EF;&#x4EE5;&#x5177;&#x6709;&#x4E0D;&#x540C;&#x7684;&#x5927;&#x5C0F;&#x3002;&#x5BF9;&#x4E8E;&#x5F85;&#x89C2;&#x5BDF;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x65B0;&#x89C6;&#x56FE;&#x5927;&#x5C0F;&#x5FC5;&#x987B;&#x4E0E;&#x5B83;&#x7684;&#x539F;&#x59CB;&#x5C3A;&#x5BF8;&#x548C;&#x6B65;&#x5E45;&#xFF0C;&#x5373;&#x517C;&#x5BB9;&#xFF0C;&#x6BCF;&#x4E00;&#x4E2A;&#x65B0;&#x7684;&#x89C6;&#x56FE;&#x5C3A;&#x5BF8;&#x5FC5;&#x987B;&#x662F;&#x4E00;&#x4E2A;&#x539F;&#x59CB;&#x5C3A;&#x5BF8;&#x7684;&#x5B50;&#x7A7A;&#x95F4;&#xFF0C;&#x6216;&#x4EC5;&#x8DE8;&#x8DE8;&#x5EA6;&#x539F;&#x59CB;&#x5C3A;&#x5BF8;
d  &#xFF0C; d  +  1  ...  &#xFF0C; d  +  K  d&#xFF0C;d + 1&#xFF0C; \&#x70B9;&#xFF0C;d + K  d  &#xFF0C; d  +  1  &#xFF0C; ...  &#xFF0C; d
+  K  &#x6EE1;&#x8DB3;&#x4EE5;&#x4E0B;&#x7684;&#x90BB;&#x63A5;&#x6837;&#x6761;&#x4EF6; &#x2200; i&#x7684; =  0  &#xFF0C; ...  &#xFF0C; K  -  1  \ forall&#x7684;I = 0&#xFF0C;\&#x70B9;&#xFF0C;K-1  &#x2200; i&#x7684; =
0  &#xFF0C; ...  &#xFF0C; &#x137;  -  1</p>
<p>stride[i]=stride[i+1]&#xD7;size[i+1]\text{stride}[i] = \text{stride}[i+1] \times
\text{size}[i+1]stride[i]=stride[i+1]&#xD7;size[i+1]</p>
<p>&#x5426;&#x5219;&#xFF0C; <code>&#x8FDE;&#x7EED;&#xFF08;&#xFF09;&#x9700;&#x8981;</code>&#x88AB;&#x79F0;&#x4E3A;&#x53EF;&#x89C2;&#x5BDF;&#x7684;&#x5F20;&#x91CF;&#x4E4B;&#x524D;&#x3002;&#x53C2;&#x89C1;&#xFF1A;<a href="torch.html#torch.reshape" title="torch.reshape"> <code>&#x91CD;&#x5851;&#xFF08;&#xFF09;</code></a>&#xFF0C;&#x5982;&#x679C;&#x5F62;&#x72B6;&#x662F;&#x517C;&#x5BB9;&#x5B83;&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x89C6;&#x56FE;&#xFF0C;&#x5E76;&#x590D;&#x5236;&#xFF08;&#x7B49;&#x6548;&#x4E8E;&#x8C03;&#x7528; <code>&#x8FDE;&#x7EED;&#x7684;&#xFF08;&#xFF09;</code>&#xFF09;&#x5426;&#x5219;&#x3002;</p>
<p>Parameters</p>
<p><strong>&#x5B9A;&#x578B;</strong> &#xFF08; <em>torch.Size</em> <em>&#x6216;</em> <em>INT ...</em> &#xFF09; - &#x6240;&#x9700;&#x7684;&#x5927;&#x5C0F;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(4, 4)
&gt;&gt;&gt; x.size()
torch.Size([4, 4])
&gt;&gt;&gt; y = x.view(16)
&gt;&gt;&gt; y.size()
torch.Size([16])
&gt;&gt;&gt; z = x.view(-1, 8)  # the size -1 is inferred from other dimensions
&gt;&gt;&gt; z.size()
torch.Size([2, 8])

&gt;&gt;&gt; a = torch.randn(1, 2, 3, 4)
&gt;&gt;&gt; a.size()
torch.Size([1, 2, 3, 4])
&gt;&gt;&gt; b = a.transpose(1, 2)  # Swaps 2nd and 3rd dimension
&gt;&gt;&gt; b.size()
torch.Size([1, 3, 2, 4])
&gt;&gt;&gt; c = a.view(1, 3, 2, 4)  # Does not change tensor layout in memory
&gt;&gt;&gt; c.size()
torch.Size([1, 3, 2, 4])
&gt;&gt;&gt; torch.equal(b, c)
False
</code></pre><p><code>view_as</code>( <em>other</em> ) &#x2192; Tensor</p>
<p>&#x67E5;&#x770B;&#x8FD9;&#x4E2A;&#x5F20;&#x91CF;&#x4E3A;&#x76F8;&#x540C;&#x7684;&#x5927;&#x5C0F;&#x4E3A;<code>&#x5176;&#x4ED6;</code>&#x3002; <code>self.view_as&#xFF08;&#x5176;&#x4ED6;&#xFF09;&#x200B;&#x200B;</code>&#x7B49;&#x4E8E;<code>self.view&#xFF08;other.size&#xFF08;&#xFF09;&#xFF09;</code>&#x3002;</p>
<p>&#x8BF7;&#x53C2;&#x89C1; <code>&#x89C6;&#x56FE;&#xFF08;&#xFF09;</code>&#x7EA6;<code>&#x89C6;&#x56FE;</code>&#x7684;&#x66F4;&#x591A;&#x4FE1;&#x606F;&#x3002;</p>
<p>Parameters</p>
<p><strong>other</strong> (<code>torch.Tensor</code>) &#x2013; The result tensor has the same size as <code>other</code>.</p>
<p><code>where</code>( <em>condition</em> , <em>y</em> ) &#x2192; Tensor</p>
<p><code>self.where&#xFF08;&#x6761;&#x4EF6;&#xFF0C; y&#xFF09;&#x7684;</code>&#x7B49;&#x4E8E;<code>torch.where&#xFF08;&#x6761;&#x4EF6;&#xFF0C; &#x81EA;&#xFF0C; y&#xFF09;&#x7684;</code>&#x3002;&#x53C2;&#x89C1;<a href="torch.html#torch.where" title="torch.where"> <code>torch.where&#xFF08;&#xFF09;</code>
</a></p>
<p><code>zero_</code>() &#x2192; Tensor</p>
<p>&#x586B;&#x5145;<code>&#x81EA;</code>&#x7528;&#x96F6;&#x5F20;&#x91CF;&#x3002;</p>
<p><em>class</em><code>torch.``BoolTensor</code></p>
<p>&#x4E0B;&#x9762;&#x7684;&#x65B9;&#x6CD5;&#x662F;&#x72EC;&#x7279;&#x7684; <code>torch.BoolTensor</code>&#x3002;</p>
<p><code>all</code>()</p>
<p><code>all</code>() &#x2192; bool</p>
<p>&#x5982;&#x679C;&#x5F20;&#x91CF;&#x7684;&#x6240;&#x6709;&#x5143;&#x7D20;&#x90FD;&#x662F;&#x771F;&#xFF0C;&#x5047;&#xFF0C;&#x5426;&#x5219;&#x8FD4;&#x56DE;True&#x3002;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.rand(1, 2).bool()
&gt;&gt;&gt; a
tensor([[False, True]], dtype=torch.bool)
&gt;&gt;&gt; a.all()
tensor(False, dtype=torch.bool)
</code></pre><p><code>all</code>( <em>dim</em> , <em>keepdim=False</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x5982;&#x679C;&#x5F20;&#x91CF;&#x7684;&#x6BCF;&#x4E00;&#x884C;&#x4E2D;&#x7684;&#x6240;&#x6709;&#x5143;&#x7D20;&#x5728;&#x6307;&#x5B9A;&#x7EF4;&#x5EA6;<code>&#x6697;&#x6DE1;</code>&#x662F;&#x771F;&#xFF0C;&#x5047;&#xFF0C;&#x5426;&#x5219;&#x8FD4;&#x56DE;True&#x3002;</p>
<p>&#x5982;&#x679C;<code>keepdim</code>&#x662F;<code>&#x771F;</code>&#xFF0C;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x662F;&#x76F8;&#x540C;&#x7684;&#x5927;&#x5C0F;&#x4E3A;<code>&#x8F93;&#x5165;</code>&#x9664;&#x4E86;&#x5728;&#x5C3A;&#x5BF8;<code>&#x6697;&#x6DE1;</code>&#x5176;&#x4E2D;&#x5B83;&#x662F;&#x5C3A;&#x5BF8;1&#x7684;&#x5426;&#x5219;&#xFF0C;<code>&#x6697;&#x6DE1;</code>&#x88AB;&#x6324;&#x51FA;&#xFF08;&#x89C1;<a href="torch.html#torch.squeeze" title="torch.squeeze"> <code>torch.squeeze&#xFF08;&#xFF09;</code></a>&#xFF09;&#xFF0C;&#x5BFC;&#x81F4;&#x5177;&#x6709;&#x6BD4;1&#x79CD;<code>&#x8F93;&#x5165;</code>&#x66F4;&#x5C11;&#x5C3A;&#x5BF8;&#x7684;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x6697;&#x6DE1;</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>INT</em> </a>&#xFF09; - &#x7684;&#x5C3A;&#x5BF8;&#xFF0C;&#x4EE5;&#x51CF;&#x5C11;</p>
</li>
<li><p><strong>keepdim</strong> &#xFF08;<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>&#x5E03;&#x5C14;</em> </a>&#xFF09; - &#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x662F;&#x5426;&#x6709;<code>&#x6697;&#x6DE1;</code>&#x4FDD;&#x7559;&#x6216;&#x4E0D;</p>
</li>
<li><p><strong>OUT</strong> &#xFF08; <em>&#x5F20;&#x91CF;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em> &#xFF09; - &#x8F93;&#x51FA;&#x5F20;&#x91CF;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.rand(4, 2).bool()
&gt;&gt;&gt; a
tensor([[True, True],
        [True, False],
        [True, True],
        [True, True]], dtype=torch.bool)
&gt;&gt;&gt; a.all(dim=1)
tensor([ True, False,  True,  True], dtype=torch.bool)
&gt;&gt;&gt; a.all(dim=0)
tensor([ True, False], dtype=torch.bool)
</code></pre><p><code>any</code>()</p>
<p><code>any</code>() &#x2192; bool</p>
<p>&#x5982;&#x679C;&#x5728;&#x4EFB;&#x4F55;&#x5F20;&#x5143;&#x7D20;&#x662F;&#x771F;&#xFF0C;&#x5047;&#xFF0C;&#x5426;&#x5219;&#x8FD4;&#x56DE;True&#x3002;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.rand(1, 2).bool()
&gt;&gt;&gt; a
tensor([[False, True]], dtype=torch.bool)
&gt;&gt;&gt; a.any()
tensor(True, dtype=torch.bool)
</code></pre><p><code>any</code>( <em>dim</em> , <em>keepdim=False</em> , <em>out=None</em> ) &#x2192; Tensor</p>
<p>&#x5982;&#x679C;&#x5F20;&#x91CF;&#x7684;&#x6BCF;&#x884C;&#x4E2D;&#x7684;&#x4EFB;&#x4F55;&#x5143;&#x4EF6;&#x5728;&#x7ED9;&#x5B9A;&#x7684;&#x5C3A;&#x5BF8;<code>&#x6697;&#x6DE1;</code>&#x662F;&#x771F;&#xFF0C;&#x5047;&#x5426;&#x5219;&#x8FD4;&#x56DE;&#x771F;&#x3002;</p>
<p>If <code>keepdim</code>is <code>True</code>, the output tensor is of the same size as <code>input</code>
except in the dimension <code>dim</code>where it is of size 1. Otherwise, <code>dim</code>is
squeezed (see <a href="torch.html#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a>),
resulting in the output tensor having 1 fewer dimension than <code>input</code>.</p>
<p>Parameters</p>
<ul>
<li><p><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="\(in Python v3.7\)" target="_blank"> <em>int</em></a>) &#x2013; the dimension to reduce</p>
</li>
<li><p><strong>keepdim</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="\(in Python v3.7\)" target="_blank"> <em>bool</em></a>) &#x2013; whether the output tensor has <code>dim</code>retained or not</p>
</li>
<li><p><strong>out</strong> ( <em>Tensor</em> <em>,</em> <em>optional</em> ) &#x2013; the output tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 2) &lt; 0
&gt;&gt;&gt; a
tensor([[ True,  True],
        [False,  True],
        [ True,  True],
        [False, False]])
&gt;&gt;&gt; a.any(1)
tensor([ True,  True,  True, False])
&gt;&gt;&gt; a.any(0)
tensor([True, True])
</code></pre><p><a href="tensor_attributes.html" title="Tensor Attributes">Next <img src="_static/images/chevron-right-orange.svg" alt=""></a> <a href="torch.html" title="torch"><img src="_static/images/chevron-right-orange.svg" alt="">
Previous</a></p>
<hr>
<p>&#xA9;&#x7248;&#x6743;&#x6240;&#x6709;2019&#x5E74;&#xFF0C;Torch &#x8D21;&#x732E;&#x8005;&#x3002;</p>
<p><hr></p>
<div align="center">
    <p><a href="http://www.apachecn.org" target="_blank"><font face="KaiTi" size="6" color="red">&#x6211;&#x4EEC;&#x4E00;&#x76F4;&#x5728;&#x52AA;&#x529B;</font></a></p>
    <p><a href="https://github.com/apachecn/pytorch-doc-zh/" target="_blank">apachecn/pytorch-doc-zh</a></p>
    <p><iframe align="middle" src="https://ghbtns.com/github-btn.html?user=apachecn&amp;repo=pytorch-doc-zh&amp;type=watch&amp;count=true&amp;v=2" frameborder="0" scrolling="0" width="100px" height="25px"></iframe>
    <iframe align="middle" src="https://ghbtns.com/github-btn.html?user=apachecn&amp;repo=pytorch-doc-zh&amp;type=star&amp;count=true" frameborder="0" scrolling="0" width="100px" height="25px"></iframe>
    <iframe align="middle" src="https://ghbtns.com/github-btn.html?user=apachecn&amp;repo=pytorch-doc-zh&amp;type=fork&amp;count=true" frameborder="0" scrolling="0" width="100px" height="25px"></iframe>
    <a target="_blank" href="shang.qq.com/wpa/qunwpa"><img border="0" src="http://data.apachecn.org/img/logo/ApacheCN-group.png" alt="ML | ApacheCN" title="ML | ApacheCN"></a></p>
</div>
 <div style="text-align:center;margin:0 0 10.5px;">
     <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
     <ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-3565452474788507" data-ad-slot="2543897000">
     </ins>
     <script>(adsbygoogle = window.adsbygoogle || []).push({});</script>

    <script>
      var _hmt = _hmt || [];
      (function() {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91";
        var s = document.getElementsByTagName("script")[0]; 
        s.parentNode.insertBefore(hm, s);
      })();
    </script>

    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-102475051-10"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-102475051-10');
    </script>
</div>

<p><meta name="google-site-verification" content="pyo9N70ZWyh8JB43bIu633mhxesJ1IcwWCZlM3jUfFo"></p>
<p><iframe src="https://www.bilibili.com/read/cv2710377" style="display:none"></iframe>
<img src="http://t.cn/AiCoDHwb" hidden="hidden"></p>
<div>
    <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
    <script src="https://unpkg.com/gitalk@latest/dist/gitalk.min.js"></script>
    <script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>
    <div id="gitalk-container"></div>
    <script type="text/javascript">
        const gitalk = new Gitalk({
        clientID: '2e62dee5b9896e2eede6',
        clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53',
        repo: 'pytorch-doc-zh',
        owner: 'apachecn',
        admin: ['jiangzhonglian', 'wizardforcel'],
        id: md5(location.pathname),
        distractionFreeMode: false
        })
        gitalk.render('gitalk-container')
    </script>
</div>

<footer class="page-footer"><span class="copyright">Copyright &#xA9; ibooker.org.cn 2019 all right reserved&#xFF0C;&#x7531; ApacheCN &#x56E2;&#x961F;&#x63D0;&#x4F9B;&#x652F;&#x6301;</span><span class="footer-modification">&#x8BE5;&#x6587;&#x4EF6;&#x4FEE;&#x8BA2;&#x65F6;&#x95F4;&#xFF1A; 
2019-09-23 17:49:19
</span></footer>
<script>console.log("plugin-popup....");document.onclick = function(e){ e.target.tagName === "IMG" && window.open(e.target.src,e.target.src)}</script><style>img{cursor:pointer}</style>
                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="torch.html" class="navigation navigation-prev " aria-label="Previous page: torch">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="tensor_attributes.html" class="navigation navigation-next " aria-label="Next page: Tensor Attributes">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"torch.Tensor","level":"1.3.3.2","depth":3,"next":{"title":"Tensor Attributes","level":"1.3.3.3","depth":3,"path":"tensor_attributes.md","ref":"tensor_attributes.md","articles":[]},"previous":{"title":"torch","level":"1.3.3.1","depth":3,"path":"torch.md","ref":"torch.md","articles":[]},"dir":"ltr"},"config":{"plugins":["github","github-buttons","-sharing","insert-logo","sharing-plus","back-to-top-button","code","copy-code-button","mathjax","pageview-count","edit-link","emphasize","alerts","auto-scroll-table","popup","hide-element","page-toc-button","tbfed-pagefooter","sitemap","advanced-emoji","expandable-chapters","splitter","search-pro"],"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"pluginsConfig":{"tbfed-pagefooter":{"copyright":"Copyright &copy ibooker.org.cn 2019","modify_label":"该文件修订时间： ","modify_format":"YYYY-MM-DD HH:mm:ss"},"emphasize":{},"github":{"url":"https://github.com/apachecn/pytorch-doc-zh"},"splitter":{},"search-pro":{},"search":{},"sharing-plus":{"qq":false,"all":["facebook","google","twitter","instapaper","linkedin","pocket","stumbleupon"],"douban":false,"facebook":true,"weibo":false,"instapaper":false,"whatsapp":false,"hatenaBookmark":false,"twitter":true,"messenger":false,"line":false,"vk":false,"pocket":true,"google":false,"viber":false,"stumbleupon":false,"qzone":false,"linkedin":false},"auto-scroll-table":{},"popup":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"code":{"copyButtons":true},"hide-element":{"elements":[".gitbook-link"]},"fontsettings":{"theme":"white","family":"sans","size":2},"highlight":{},"sitemap":{"hostname":"http://pytorch.apachecn.org"},"page-toc-button":{"maxTocDepth":4,"minTocSize":4},"back-to-top-button":{},"pageview-count":{},"alerts":{},"github-buttons":{"buttons":[{"user":"apachecn","repo":"pytorch-doc-zh","type":"star","count":true,"size":"small"}]},"mathjax":{"forceSVG":false,"version":"2.6-latest"},"copy-code-button":{},"advanced-emoji":{"embedEmojis":false},"sharing":{"qq":false,"all":["qq","douban","facebook","google","linkedin","twitter","weibo","whatsapp"],"douban":false,"facebook":false,"weibo":true,"whatsapp":false,"twitter":false,"line":false,"google":false,"qzone":true},"edit-link":{"label":"编辑本页","base":"https://github.com/apachecn/pytorch-doc-zh/blob/master/docs/1.2"},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false},"insert-logo":{"style":"background: none; max-height: 150px; min-height: 150px","url":"http://data.apachecn.org/img/logo.jpg"},"expandable-chapters":{}},"my_links":{"sidebar":{"Home":"https://www.baidu.com"}},"theme":"default","author":"ApacheCN","my_plugins":["donate","todo","-lunr","-search","expandable-chapters-small","chapter-fold","expandable-chapters","expandable-chapters-small","back-to-top-button","ga","baidu","sitemap","tbfed-pagefooter","advanced-emoji","sectionx","page-treeview","simple-page-toc","ancre-navigation","theme-apachecn@git+https://github.com/apachecn/theme-apachecn#HEAD","pagefooter-apachecn@git+https://github.com/apachecn/gitbook-plugin-pagefooter-apachecn#HEAD"],"my_pluginsConfig":{"page-treeview":{"copyright":"Copyright &#169; aleen42","minHeaderCount":"2","minHeaderDeep":"2"},"ignores":["node_modules"],"simple-page-toc":{"maxDepth":3,"skipFirstH1":true},"page-copyright":{"wisdom":"Designer, Frontend Developer & overall web enthusiast","noPowered":false,"copyright":"Copyright &#169; 你的名字","style":"normal","timeColor":"#666","utcOffset":"8","format":"YYYY-MM-dd hh:mm:ss","signature":"你的签名","copyrightColor":"#666","description":"modified at"},"donate":{"wechat":"微信收款的二维码URL","alipay":"支付宝收款的二维码URL","title":"","button":"赏","alipayText":"支付宝打赏","wechatText":"微信打赏"},"page-toc-button":{"maxTocDepth":2,"minTocSize":2},"github-buttons":{"buttons":[{"user":"apachecn","repo":"pytorch-doc-zh","type":"star","count":true,"size":"small"},{"user":"apachecn","width":"160","type":"follow","count":true,"size":"small"}]},"ga":{"token":"UA-102475051-10"},"baidu":{"token":"75439e2cbd22bdd813226000e9dcc12f"},"pagefooter-apachecn":{"copyright":"Copyright &copy ibooker.org.cn 2019","modify_label":"该文件修订时间： ","modify_format":"YYYY-MM-DD HH:mm:ss"}},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"variables":{},"title":"Pytorch 中文文档","language":"zh-hans","gitbook":"*","description":"Pytorch 中文文档: 教程和文档"},"file":{"path":"tensors.md","mtime":"2019-09-23T17:49:19.999Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2019-09-23T17:53:13.712Z"},"basePath":".","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="gitbook/gitbook.js"></script>
    <script src="gitbook/theme.js"></script>
    
        
        <script src="gitbook/gitbook-plugin-github/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-github-buttons/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-insert-logo/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-sharing-plus/buttons.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-back-to-top-button/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-code/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-copy-code-button/toggle.js"></script>
        
    
        
        <script src="https://cdn.mathjax.org/mathjax/2.6-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-mathjax/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-pageview-count/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-edit-link/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-alerts/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-auto-scroll-table/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-hide-element/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-page-toc-button/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-expandable-chapters/expandable-chapters.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-splitter/splitter.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search-pro/jquery.mark.min.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search-pro/search.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

